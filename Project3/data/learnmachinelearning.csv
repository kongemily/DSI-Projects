approved_at_utc,subreddit,selftext,author_fullname,saved,mod_reason_title,gilded,clicked,title,link_flair_richtext,subreddit_name_prefixed,hidden,pwls,link_flair_css_class,downs,thumbnail_height,hide_score,name,quarantine,link_flair_text_color,upvote_ratio,author_flair_background_color,subreddit_type,ups,total_awards_received,media_embed,thumbnail_width,author_flair_template_id,is_original_content,user_reports,secure_media,is_reddit_media_domain,is_meta,category,secure_media_embed,link_flair_text,can_mod_post,score,approved_by,author_premium,thumbnail,edited,author_flair_css_class,author_flair_richtext,gildings,post_hint,content_categories,is_self,mod_note,created,link_flair_type,wls,removed_by_category,banned_by,author_flair_type,domain,allow_live_comments,selftext_html,likes,suggested_sort,banned_at_utc,view_count,archived,no_follow,is_crosspostable,pinned,over_18,preview,all_awardings,awarders,media_only,link_flair_template_id,can_gild,spoiler,locked,author_flair_text,treatment_tags,visited,removed_by,num_reports,distinguished,subreddit_id,mod_reason_by,removal_reason,link_flair_background_color,id,is_robot_indexable,report_reasons,author,discussion_type,num_comments,send_replies,whitelist_status,contest_mode,mod_reports,author_patreon_flair,author_flair_text_color,permalink,parent_whitelist_status,stickied,url,subreddit_subscribers,created_utc,num_crossposts,media,is_video,crosspost_parent_list,crosspost_parent,media_metadata,author_cakeday
,learnmachinelearning,,t2_4r7zftln,False,,0,False,Control the car by using your index finger swinging in the air. Made using TensorFlow's Handpose model. Check out the live simulation (Link in the comment section),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,68.0,False,t3_ggz8pz,False,light,0.92,,public,44,0,{},140.0,,False,[],,True,False,,{},Project,False,44,,False,https://b.thumbs.redditmedia.com/PE15Vs7lAno-O86IPJT0YbaS6l-_ZQCpeziScN9oMtA.jpg,False,,[],{},image,,False,,1589136022.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?format=png8&amp;s=95427a2f62a7fe10defa4c0b21362192c30642a9', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=b5cd184a1460a3a84490125d433d6f30326b6e11', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=432b90ffc0c513427a9adf6ebe0d4bc1cfe7f5a8', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=9d503f3c1df0b684ae10f34b56a48d23fea0eec7', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=01ce3c699382ace866516b329784e672a55dc15c', 'width': 640, 'height': 314}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?s=7b6d30868ca6fc9fa332156b34c65ea9bc5007e8', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;crop=smart&amp;s=252a2bc32709b6f9f9747981443d41d78af64f36', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;crop=smart&amp;s=380654721b85ca25278878682b9b3e7b7840346d', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;crop=smart&amp;s=41ac613a6d800acdc96928edfd4c25b86e395bea', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;crop=smart&amp;s=7f1f7aee2fad8d9b0c0012da361bf47499e107d2', 'width': 640, 'height': 314}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?format=mp4&amp;s=81c2b69d52618118bb2bc3dbeac400970c60e1f5', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;format=mp4&amp;s=b301ff122d1e0ca967222f008330495e6b7f2201', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;format=mp4&amp;s=f1628766de4a4eb4d2e7b55de7ad255498b9762b', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;format=mp4&amp;s=7bc82dcb20f8ca2d2a0cd20f66ac9aec66c18f08', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;format=mp4&amp;s=d17075ed2b10c5e50bfafcac1190597c67f8e4f8', 'width': 640, 'height': 314}]}}, 'id': 'EMThTd8mK0bzLpxVTWROT-Pip3UIlJKNtK5PD0GbXpA'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggz8pz,True,,h4wk_3y3,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggz8pz/control_the_car_by_using_your_index_finger/,all_ads,False,https://i.redd.it/abniciln1xx41.gif,155203,1589107222.0,0,,False,,,,
,learnmachinelearning,"Hi all,

Iâ€™ve read a lot of posts asking for recommendations for textbooks to learn the math behind machine learning so I figured Iâ€™d make a self-study guide that walks you through it all including the recommended subjects and corresponding textbooks. You should have more than enough mathematical maturity to work through ESL and the Deep Learning book by the time youâ€™re done. Iâ€™ll update it periodically and if anyone has any questions or comments those are always welcome!

https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0",t2_4qzspo30,False,,0,False,A comprehensive self-study guide for the math behind machine learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggpzk2,False,dark,0.98,,public,327,1,{},,,False,[],,False,False,,{},,False,327,,False,self,False,,[],{},self,,True,,1589094933.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Iâ€™ve read a lot of posts asking for recommendations for textbooks to learn the math behind machine learning so I figured Iâ€™d make a self-study guide that walks you through it all including the recommended subjects and corresponding textbooks. You should have more than enough mathematical maturity to work through ESL and the Deep Learning book by the time youâ€™re done. Iâ€™ll update it periodically and if anyone has any questions or comments those are always welcome!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0""&gt;https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?auto=webp&amp;s=8844a2f59ff2ffe318ad0e2b7846e1e37aed5128', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=20970d5f7f7c343396aff132ae5ffcee0ce03cb8', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'KMbn3CCPvk6_ukzEK1zzIufQuyyC0aay4raAkv3Slng'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_a2506925-fc82-4d6c-ae3b-b7217e09d7f0', 'penny_donate': None, 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png', 'days_of_premium': 0, 'icon_height': 2048, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=16&amp;height=16&amp;auto=webp&amp;s=4e475e8c3265ec7148d7f4204f07d33949482f21', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=32&amp;height=32&amp;auto=webp&amp;s=42e32a4b9f1e70791716c3be283e89951e212a69', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=48&amp;height=48&amp;auto=webp&amp;s=5adb621fede4e8e66b952a379ad038fcc1b8ad13', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=64&amp;height=64&amp;auto=webp&amp;s=6161edea19569bbee73ef322a2e5470535ec1787', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=128&amp;height=128&amp;auto=webp&amp;s=5d2c75f44f176f430e936204f9a53b8a2957f2fc', 'width': 128, 'height': 128}], 'icon_width': 2048, 'start_date': None, 'is_enabled': True, 'description': 'A golden splash of respect', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'name': 'Narwhal Salute', 'icon_format': None, 'award_sub_type': 'PREMIUM', 'penny_price': None, 'award_type': 'global'}]",[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggpzk2,True,,PersonalPsychology2,,37,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggpzk2/a_comprehensive_selfstudy_guide_for_the_math/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggpzk2/a_comprehensive_selfstudy_guide_for_the_math/,155203,1589066133.0,0,,False,,,,
,learnmachinelearning,"I have written a multi-part series on the concepts and implementation of Counterfactual Regret Minimization (CFR), with Python code and toy example. CFR is at the heart of most advanced Poker AIs including the famed [Pluribus](https://en.wikipedia.org/wiki/Pluribus_(poker_bot))

I hope you find it interesting: [Steps to building a Poker AI (Part 1)](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-1-outline-and-history-58fbedaf6ded)

If there's enough interest I plan to continue the series in some form to talk more about the aspects specific to Texas Hold'em and some more advanced stuff that is necessary to make a strong AI.

Direct links to the other parts:

[Part 2: Modelling Imperfect Information Games](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-2-modelling-imperfect-information-games-c384b7c71edf)  
[Part 3: Regrets and Minimizing Regrets in One-Shot Games](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-3-regrets-and-minimizing-regrets-in-one-shot-games-ca7cdc8f66d0)  
[Part 4: Regret matching for Rock-Paper-Scissors in Python](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-4-regret-matching-for-rock-paper-scissors-in-python-168411edbb13)  
[Part 5: Sequential Games, Kuhn Poker and Counterfactual Regrets](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-5-sequential-games-kuhn-poker-and-counterfactual-regrets-311f533f786e)  
[Part 6: Beating Kuhn Poker with CFR using Python](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python-1b4172a6ab2d)  
[Part 7: Exploitability, Multiplayer CFR and 3-player Kuhn Poker](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-7-exploitability-multiplayer-cfr-and-3-player-kuhn-poker-25f313bf83cf)

(X-post from r/poker)",t2_9t1bg,False,,0,False,Gentle introduction to the basics of Poker AI,[],r/learnmachinelearning,False,6,,0,,False,t3_gh01ca,False,dark,0.9,,public,14,0,{},,,False,[],,False,False,,{},,False,14,,False,self,False,,[],{},,,True,,1589139811.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have written a multi-part series on the concepts and implementation of Counterfactual Regret Minimization (CFR), with Python code and toy example. CFR is at the heart of most advanced Poker AIs including the famed &lt;a href=""https://en.wikipedia.org/wiki/Pluribus_(poker_bot""&gt;Pluribus&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I hope you find it interesting: &lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-1-outline-and-history-58fbedaf6ded""&gt;Steps to building a Poker AI (Part 1)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If there&amp;#39;s enough interest I plan to continue the series in some form to talk more about the aspects specific to Texas Hold&amp;#39;em and some more advanced stuff that is necessary to make a strong AI.&lt;/p&gt;

&lt;p&gt;Direct links to the other parts:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-2-modelling-imperfect-information-games-c384b7c71edf""&gt;Part 2: Modelling Imperfect Information Games&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-3-regrets-and-minimizing-regrets-in-one-shot-games-ca7cdc8f66d0""&gt;Part 3: Regrets and Minimizing Regrets in One-Shot Games&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-4-regret-matching-for-rock-paper-scissors-in-python-168411edbb13""&gt;Part 4: Regret matching for Rock-Paper-Scissors in Python&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-5-sequential-games-kuhn-poker-and-counterfactual-regrets-311f533f786e""&gt;Part 5: Sequential Games, Kuhn Poker and Counterfactual Regrets&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python-1b4172a6ab2d""&gt;Part 6: Beating Kuhn Poker with CFR using Python&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-7-exploitability-multiplayer-cfr-and-3-player-kuhn-poker-25f313bf83cf""&gt;Part 7: Exploitability, Multiplayer CFR and 3-player Kuhn Poker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(X-post from &lt;a href=""/r/poker""&gt;r/poker&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh01ca,True,,tt293,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh01ca/gentle_introduction_to_the_basics_of_poker_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh01ca/gentle_introduction_to_the_basics_of_poker_ai/,155203,1589111011.0,0,,False,,,,
,learnmachinelearning,,t2_10lnxu,False,,0,False,A Hackerâ€™s Guide to Efficiently Train Deep Learning Models ðŸš€,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_ggyyj0,False,dark,1.0,,public,6,0,{},140.0,,False,[],,False,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/m9EnopsuRsbSI5bQc-Vhth-12231vsIhEjlcpi0mNio.jpg,False,,[],{},link,,False,,1589134575.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?auto=webp&amp;s=352f5f8dc8a416c2beed67b2de1f66119d994e25', 'width': 1200, 'height': 899}, 'resolutions': [{'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2de1e06f8a9cabf293ba5d8e58d61bdd69a5a445', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=26e139597dee2c395b38a530c9fe2ede540b55dc', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc52dd1aa9c63a64f8d293a64a9ef011296b7295', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc511a35831290685235b4747a75d0eb310a63a', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d10673ec1555bd3198f20b0b80deae7a2af4b487', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f747225a0b4467e62c0300566668629520643a9', 'width': 1080, 'height': 809}], 'variants': {}, 'id': '0qpsjUZBTl7oyb3qI6C8Ab094Znns3q0ATMDI8M1Etc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggyyj0,True,,ahmedbesbes,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyyj0/a_hackers_guide_to_efficiently_train_deep/,all_ads,False,https://medium.com/@ahmedbesbes/a-hackers-guide-to-efficiently-train-deep-learning-models-b2cccbd1bc0a,155203,1589105775.0,0,,False,,,,
,learnmachinelearning,"I'm really fascinated by apps like Spotify and Shazam, and would like to pursue a career related to music information retrieval (MIR) and recommender systems. It seems like an esoteric field, especially since deep learning these days seems to primarily be focused on image and text data.

Any of you interested in (or working in) similar areas? Maybe we can connect and form a study group! Maybe we can share interesting papers (e.g. from ISMIR and ACM RecSys conferences), work on projects together, and share study materials.",t2_3loxj2cz,False,,0,False,[D] Anyone interested in music recommendation?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggwqus,False,light,0.9,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,self,False,,[],{},,,True,,1589122994.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m really fascinated by apps like Spotify and Shazam, and would like to pursue a career related to music information retrieval (MIR) and recommender systems. It seems like an esoteric field, especially since deep learning these days seems to primarily be focused on image and text data.&lt;/p&gt;

&lt;p&gt;Any of you interested in (or working in) similar areas? Maybe we can connect and form a study group! Maybe we can share interesting papers (e.g. from ISMIR and ACM RecSys conferences), work on projects together, and share study materials.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggwqus,True,,hedgehogist,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwqus/d_anyone_interested_in_music_recommendation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwqus/d_anyone_interested_in_music_recommendation/,155203,1589094194.0,0,,False,,,,
,learnmachinelearning,"This video explains how to claim the offer, consider watching:

 [https://www.youtube.com/watch?v=i8MQIlLXLIM](https://www.youtube.com/watch?v=i8MQIlLXLIM)",t2_5ggm5svc,False,,0,False,"Hey all, consider looking at IBM Data Science and AI programs free for 30 days","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggxhj8,False,light,0.77,,public,7,0,{},,,False,[],,False,False,,{},Discussion,False,7,,False,self,False,,[],{},self,,True,,1589126857.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This video explains how to claim the offer, consider watching:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=i8MQIlLXLIM""&gt;https://www.youtube.com/watch?v=i8MQIlLXLIM&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?auto=webp&amp;s=3f39d9840b0aef690cedb12db73aac501db37cf2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=39c3ba0bbbdb04d0a9baa011585f7d29788d8095', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=421deb33acebaa5c3ad15395c22f13df00c2c62e', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72d6737b82ee38552392be0bac5e39fe3998d074', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'jBGO9k_PntalzUMXgd5n09Ji5dIE8PWC6C-HbXdTtq8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggxhj8,True,,awsconsultant,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxhj8/hey_all_consider_looking_at_ibm_data_science_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggxhj8/hey_all_consider_looking_at_ibm_data_science_and/,155203,1589098057.0,0,,False,,,,
,learnmachinelearning,"I am thinking of doing Stanford's CS229 Machine Learning course. It's the heavier version of Coursera's ML course. Stanford released 2018 version of this course on [YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) recently. Also, this version uses Python, which is a plus.

Andrew mentions in first few minutes of [first lecture](https://youtu.be/jGwO_UgTS7I?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) that having a study group would help a lot in getting through the course and since I am doing this as self-study, it becomes even more important.

So let me know if you are interested. Please remember that the course is of intermediate or intermediate+ level - this will only make all the efforts even more worthwhile. Check out [Problem Set 1](https://drive.google.com/file/d/1K_h2dCHfAQCnboCVxF49ay9B9UjB-a_l/view?usp=sharing) and [Syllabus](http://cs229.stanford.edu/syllabus-autumn2018.html) to get an idea.

P.S. I will admit that Problem Set 1 scared the shit out of me on first view, and that's what motivated me to do the course.

&amp;#x200B;

Update2:Since group grew far too quickly than expected - it wasn't possible to manage things on Telegram.

[Lentor3579](https://www.reddit.com/user/Lentor3579/) has created a discord server ([https://discord.gg/RcVEVuX](https://discord.gg/RcVEVuX)). So please join there - all further information will be on discord only. Telegram link won't work anymore.

&amp;#x200B;

Update1:

~~Ok, so around 10 or so people seem interested. I have created a Telegram group (~~[~~https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ~~](https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ)~~) where we ask normal doubts and then have a weekly session on Zoom or Teams (or whatever people agree on) to discuss the progress made. I will make a Google sheet to track what we are doing for a given week once people reply to the poll regarding pacing.~~

~~If you guys have a better idea, please suggest.~~

~~On telegram, your number is only visible to people who already have your contact saved or you have their contact saved. Others can only see your username/name.Source:~~ [~~https://telegram.org/faq#q-who-can-see-my-phone-number~~](https://telegram.org/faq#q-who-can-see-my-phone-number)",t2_37d2oexf,False,,0,False,Stanford's CS229 ML Course Study Partner,[],r/learnmachinelearning,False,6,,0,,False,t3_ggg8e5,False,dark,0.97,,public,134,0,{},,,False,[],,False,False,,{},,False,134,,False,self,1589106475.0,,[],{},self,,True,,1589062945.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am thinking of doing Stanford&amp;#39;s CS229 Machine Learning course. It&amp;#39;s the heavier version of Coursera&amp;#39;s ML course. Stanford released 2018 version of this course on &lt;a href=""https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;YouTube&lt;/a&gt; recently. Also, this version uses Python, which is a plus.&lt;/p&gt;

&lt;p&gt;Andrew mentions in first few minutes of &lt;a href=""https://youtu.be/jGwO_UgTS7I?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;first lecture&lt;/a&gt; that having a study group would help a lot in getting through the course and since I am doing this as self-study, it becomes even more important.&lt;/p&gt;

&lt;p&gt;So let me know if you are interested. Please remember that the course is of intermediate or intermediate+ level - this will only make all the efforts even more worthwhile. Check out &lt;a href=""https://drive.google.com/file/d/1K_h2dCHfAQCnboCVxF49ay9B9UjB-a_l/view?usp=sharing""&gt;Problem Set 1&lt;/a&gt; and &lt;a href=""http://cs229.stanford.edu/syllabus-autumn2018.html""&gt;Syllabus&lt;/a&gt; to get an idea.&lt;/p&gt;

&lt;p&gt;P.S. I will admit that Problem Set 1 scared the shit out of me on first view, and that&amp;#39;s what motivated me to do the course.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Update2:Since group grew far too quickly than expected - it wasn&amp;#39;t possible to manage things on Telegram.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/user/Lentor3579/""&gt;Lentor3579&lt;/a&gt; has created a discord server (&lt;a href=""https://discord.gg/RcVEVuX""&gt;https://discord.gg/RcVEVuX&lt;/a&gt;). So please join there - all further information will be on discord only. Telegram link won&amp;#39;t work anymore.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Update1:&lt;/p&gt;

&lt;p&gt;&lt;del&gt;Ok, so around 10 or so people seem interested. I have created a Telegram group (&lt;/del&gt;&lt;a href=""https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ""&gt;&lt;del&gt;https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ&lt;/del&gt;&lt;/a&gt;&lt;del&gt;) where we ask normal doubts and then have a weekly session on Zoom or Teams (or whatever people agree on) to discuss the progress made. I will make a Google sheet to track what we are doing for a given week once people reply to the poll regarding pacing.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;If you guys have a better idea, please suggest.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;On telegram, your number is only visible to people who already have your contact saved or you have their contact saved. Others can only see your username/name.Source:&lt;/del&gt; &lt;a href=""https://telegram.org/faq#q-who-can-see-my-phone-number""&gt;&lt;del&gt;https://telegram.org/faq#q-who-can-see-my-phone-number&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?auto=webp&amp;s=7dad7e69084fe17dc70d1948b6f58fc13d288f0c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54a29aa3b0894b9f5f031e6ec93d6896ab4450', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'WdNtCcCE5kkWeUtaFzPEmvl-T9SFUg0GJCbpJncoGqQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggg8e5,True,,kaiNbleu,,52,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggg8e5/stanfords_cs229_ml_course_study_partner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggg8e5/stanfords_cs229_ml_course_study_partner/,155203,1589034145.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggnghh,False,dark,0.89,,public,38,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZnpZsiy_p2M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggnghh', 'height': 338}",,False,38,,False,https://b.thumbs.redditmedia.com/XIiBENV4Jh8C1XtbgKh57N-_DRf1hSh6wF6AEKDkY2Q.jpg,False,,[],{},rich:video,,False,,1589086393.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?auto=webp&amp;s=238eba7e302b2bad4e95eb10c7c2a81ec22fbc64', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fd76389c456600962df909dda5856ab3eaeb983', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7beefb5236f0e5e75b2c75ab26ef7bdfccc15a89', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a61edef3c746f461398d18b95c73661903d6935', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fnLVMezrmrrxLFHs_XstZpLKuC8tnGlQPsQW-zWY_Es'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggnghh,True,,OnlyProggingForFun,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggnghh/what_are_gans_introduction_to_generative/,all_ads,False,https://www.youtube.com/watch?v=ZnpZsiy_p2M,155203,1589057593.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZnpZsiy_p2M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}}",False,,,,
,learnmachinelearning,"Hi everyone,

I'm interested in image generation and image-to-image translation, and through my reads I have noticed that people favour the use of upsampling and convolution layers over transposed convolution layers.
In ([StarGAN v2](https://arxiv.org/abs/1912.01865)), regarding their generator architecture (Section E. Table 5.), the encoding part uses pooling layers, instead of convolutions with ```stride = 2```, while their decoder uses upsampling layers, instead of transposed convolutions with ```stride = 2```.

If I got it right, both pooling and upsampling operations can be learnt by convolution and transposed convolution layers. If this is true, then why not learning transposed convolutions instead of upsampling? Same goes for convolutions and pooling?

Also, can convolutions and transposed convolutions be equivalent?
Here's a small example:
```py
import torch
from torch.nn import Conv2d, ConvTranspose2d

n = 1
h, w = 128, 128
i, o = 16, 32
k, p = 3, 1

a = Conv2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)
b = ConvTranspose2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)

x = torch.ones(n, i, h, w)
print(x.shape)

y = a(x)
z = b(x)

print(y.shape)
print(z.shape)

```
Given the same input, can both layers produce the same output? If yes, is there a relation between their weights (and bias)?

Thanks in advance,

Piollinas",t2_5zbb2ujo,False,,0,False,Why are Upsampling+Convolutions better than Transposed Convolutions?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggyz05,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,1589106321.0,,[],{},,,True,,1589134650.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in image generation and image-to-image translation, and through my reads I have noticed that people favour the use of upsampling and convolution layers over transposed convolution layers.
In (&lt;a href=""https://arxiv.org/abs/1912.01865""&gt;StarGAN v2&lt;/a&gt;), regarding their generator architecture (Section E. Table 5.), the encoding part uses pooling layers, instead of convolutions with &lt;code&gt;stride = 2&lt;/code&gt;, while their decoder uses upsampling layers, instead of transposed convolutions with &lt;code&gt;stride = 2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If I got it right, both pooling and upsampling operations can be learnt by convolution and transposed convolution layers. If this is true, then why not learning transposed convolutions instead of upsampling? Same goes for convolutions and pooling?&lt;/p&gt;

&lt;p&gt;Also, can convolutions and transposed convolutions be equivalent?
Here&amp;#39;s a small example:
```py
import torch
from torch.nn import Conv2d, ConvTranspose2d&lt;/p&gt;

&lt;p&gt;n = 1
h, w = 128, 128
i, o = 16, 32
k, p = 3, 1&lt;/p&gt;

&lt;p&gt;a = Conv2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)
b = ConvTranspose2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)&lt;/p&gt;

&lt;p&gt;x = torch.ones(n, i, h, w)
print(x.shape)&lt;/p&gt;

&lt;p&gt;y = a(x)
z = b(x)&lt;/p&gt;

&lt;p&gt;print(y.shape)
print(z.shape)&lt;/p&gt;

&lt;p&gt;```
Given the same input, can both layers produce the same output? If yes, is there a relation between their weights (and bias)?&lt;/p&gt;

&lt;p&gt;Thanks in advance,&lt;/p&gt;

&lt;p&gt;Piollinas&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggyz05,True,,Piollinas,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyz05/why_are_upsamplingconvolutions_better_than/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggyz05/why_are_upsamplingconvolutions_better_than/,155203,1589105850.0,0,,False,,,,
,learnmachinelearning,,t2_10lnxu,False,,0,False,"Complete end to end machine learning tutorial: from data collection to deployment: scrape and collect data, train a model, design an app, and deploy it to AWS + Full code in Python","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,93.0,True,t3_gh1uyp,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Project,False,1,,False,https://b.thumbs.redditmedia.com/OgKSJCBSEZ6mSgu1KsbH7XyEfbG4qhezDPDIVYJeQ5Q.jpg,False,,[],{},link,,False,,1589147565.0,richtext,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?auto=webp&amp;s=9e166dab7a82900f94cc841798746f7b587863c5', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5282f0a54dd7969b78ed7b59d8a32b9ad7f39cd7', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f41a4c90c2dae002dbb0df8aa52485db14effff3', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=254234782ad9497172f41d0dfa08de7a3802f420', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09eecc8b7e2cc52a1056320b990e96723184fd78', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c35680018f4f76c3e26e72ca06179a30f615628d', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e509dac49de5552616980151b13603ee7be56a84', 'width': 1080, 'height': 720}], 'variants': {}, 'id': '7aFHhITed9tDeUdnl57ECf3ekWwSCkVJbtmvq7bmvSk'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gh1uyp,True,,ahmedbesbes,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh1uyp/complete_end_to_end_machine_learning_tutorial/,all_ads,False,https://medium.com/datadriveninvestor/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203,155203,1589118765.0,0,,False,,,,
,learnmachinelearning,"Hi, 
I have few texts that have been graded ( out of 10). Now, I want to  predict the score on my test data (that hasn't been scored ).

Regression, BOW, NLP, etc can be used but i cannot figure out how to predict based on similarity w. R. T my trained dataset.

Idk how should I go about it.",t2_30p1s3ws,False,,0,False,Predict marks from the text,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,True,t3_gh1cf3,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589145471.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, 
I have few texts that have been graded ( out of 10). Now, I want to  predict the score on my test data (that hasn&amp;#39;t been scored ).&lt;/p&gt;

&lt;p&gt;Regression, BOW, NLP, etc can be used but i cannot figure out how to predict based on similarity w. R. T my trained dataset.&lt;/p&gt;

&lt;p&gt;Idk how should I go about it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gh1cf3,True,,Mayank008,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh1cf3/predict_marks_from_the_text/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh1cf3/predict_marks_from_the_text/,155203,1589116671.0,0,,False,,,,
,learnmachinelearning,"so currently im a IT diploma student, and im pretty new in programming. I'll be continuing my bachelor of computer science in the following month and im going to take data science as my major. Im interested in data science and im planning to take short online course before going to my bachelor's degree. can someone guide to the right direction?",t2_692wpcym,False,,0,False,Data science advice needed,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0w4y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589143629.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;so currently im a IT diploma student, and im pretty new in programming. I&amp;#39;ll be continuing my bachelor of computer science in the following month and im going to take data science as my major. Im interested in data science and im planning to take short online course before going to my bachelor&amp;#39;s degree. can someone guide to the right direction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0w4y,True,,Scorlibpl,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0w4y/data_science_advice_needed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0w4y/data_science_advice_needed/,155203,1589114829.0,0,,False,,,,
,learnmachinelearning,"Greetings,

&amp;#x200B;

I'm doing a research project on an experimental process that contains 2 main control loops (One SISO for force via airpressure and one MIMO for temperature with 6 pyrometers and 3 IR emitter). As a part of this project I'm supposed to implement ML to speed up the fine adjustments for the control process. The conditions are as following: I have a data set of vectors containing the target position,velocity, pressure and temperature for each point of a trajectory. I have a bunch of sensors to observe the controlled variable and a bunch of actuators to influence the reference value. The values aren't bound that hard, but they have a small influence on each other. The AI is supposed to learn the control process parameters and to tell the actuators when and how to start actuating. Now, I have read into some basics, but I'm missing experience with ML so I don't know where to start. Do you guys have some thoughts, advice or ideas on what would be the best suited ML approach to such a problem and where I can find some good (non general) guides or tutorials?",t2_57fvtgr,False,,0,False,Simple ML in MIMO controll process ?,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0lhl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589142338.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m doing a research project on an experimental process that contains 2 main control loops (One SISO for force via airpressure and one MIMO for temperature with 6 pyrometers and 3 IR emitter). As a part of this project I&amp;#39;m supposed to implement ML to speed up the fine adjustments for the control process. The conditions are as following: I have a data set of vectors containing the target position,velocity, pressure and temperature for each point of a trajectory. I have a bunch of sensors to observe the controlled variable and a bunch of actuators to influence the reference value. The values aren&amp;#39;t bound that hard, but they have a small influence on each other. The AI is supposed to learn the control process parameters and to tell the actuators when and how to start actuating. Now, I have read into some basics, but I&amp;#39;m missing experience with ML so I don&amp;#39;t know where to start. Do you guys have some thoughts, advice or ideas on what would be the best suited ML approach to such a problem and where I can find some good (non general) guides or tutorials?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0lhl,True,,Nemonekto,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0lhl/simple_ml_in_mimo_controll_process/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0lhl/simple_ml_in_mimo_controll_process/,155203,1589113538.0,0,,False,,,,
,learnmachinelearning,,t2_x8ze3l8,False,,0,False,Understand LSTMS with an Illustrated Guide with a step by step explanation,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gg9agg,False,dark,0.99,,public,308,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Illustrated Guide to LSTM's and GRU's: A step by step explanation"", 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Michael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8HyCNIVRbSU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gg9agg', 'height': 338}",,False,308,,False,https://b.thumbs.redditmedia.com/frgFRjdVN2jVxvaz-xuMQlyl7ObAkhaoMr5O3jLK2Gg.jpg,False,,[],{},rich:video,,False,,1589029602.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?auto=webp&amp;s=e31ba2f9138801dcf0280de960072e93c615ab2a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab52e4909b466a3e617fe52357bafa662f80247', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f00c0f43fe9fbca1525d5fcbe9c429e4293cd26', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecea310beba44167a1963bf1b14dffbdad062481', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'udBbbGTWJYKhiFOWBI2jaHP8TR7u6wcU0Cjls77_HOI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg9agg,True,,LearnedVector,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9agg/understand_lstms_with_an_illustrated_guide_with_a/,all_ads,False,https://www.youtube.com/watch?v=8HyCNIVRbSU,155203,1589000802.0,1,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Illustrated Guide to LSTM's and GRU's: A step by step explanation"", 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Michael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8HyCNIVRbSU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}}",False,,,,
,learnmachinelearning,,t2_6d8czqvi,False,,0,False,What parts of AI work/development are generally seen as the most interesting?,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0c43,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589141156.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0c43,True,,begintomorrow,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0c43/what_parts_of_ai_workdevelopment_are_generally/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0c43/what_parts_of_ai_workdevelopment_are_generally/,155203,1589112356.0,0,,False,,,,
,learnmachinelearning,,t2_6fkdqkbf,False,,0,False,Tool to quickly add single or multiclass labels to images without using your mouse,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,104.0,False,t3_ggsr0y,False,light,1.0,,public,6,0,{},140.0,,False,[],,True,False,,{},Project,False,6,,False,https://b.thumbs.redditmedia.com/iW95-MeMMSy64glEP17cuthvxoHZuv_7oQmsUvEtW5w.jpg,False,,[],{},image,,False,,1589105092.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?format=png8&amp;s=35d252f811ffaaf44b003403a20f3d71e3213617', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=ca2e8a3d6a2de36a7366b73f0ffc587abcada928', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=43b4d0972942b9ea9521b322ab3d53bf3a59fd5c', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=ad87ce2c15ec25fa3f797a9fa1d8df94a1216c2d', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=686e5b6da191b6345a103bfb6b3aeb5b1a1808e2', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=a305faf010774f5e2acbb35fd04a5b122c9aedc9', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=8fb92ad6569e85f16039e1ce387fdfe63146c735', 'width': 1080, 'height': 808}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?s=bfbbb6aa4b64b9836ebf590f46e1bc3fef280238', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;crop=smart&amp;s=74f3d073695943e2dbdde3e6e442d69d264932cf', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;crop=smart&amp;s=a14b6fdeafe6e8144faec22a749ba8a485cd0d0b', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;crop=smart&amp;s=aa09aceeab44827b17f74bbf71bf9b634f3c2a93', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;crop=smart&amp;s=ec7e7d51d4610aa3a7162c82dc7173f81664cac7', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;crop=smart&amp;s=b900016aa781f5fde6eb2350b9fa2e6f3532353b', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;crop=smart&amp;s=e57f007755c7ce1c254ad975133e14ca83aa0521', 'width': 1080, 'height': 808}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?format=mp4&amp;s=6891ac6fd19ad534a334960b1d73e57795dd6d18', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;format=mp4&amp;s=2cd0e6b1b26b8a70661631753210ae85d78fbb04', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;format=mp4&amp;s=332623600d17adfd362095813a7a913d1c89da25', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;format=mp4&amp;s=33101b4610a37eca01dc119e70d4e3937393179d', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;format=mp4&amp;s=850270644dcaf9ab08dcd42df130e5b226faef40', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;format=mp4&amp;s=38e01e4b22b004ae12d999bd424cc5b8f273ff90', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;format=mp4&amp;s=cb51b88f8fd88412b519f7ef7e16bb427303d4d5', 'width': 1080, 'height': 808}]}}, 'id': 'rE_kFMT_CVGUvLfwT6S1ksJiB8bOKDPcI7u4f7UVBrc'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggsr0y,True,,diffu5e,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggsr0y/tool_to_quickly_add_single_or_multiclass_labels/,all_ads,False,https://i.redd.it/b2j0wm5ehux41.gif,155203,1589076292.0,0,,False,,,,
,learnmachinelearning,"How do articles which explain CNN generate images( like over-imposong kernel grids on images) 

Not the best example [google search result](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQyQzgHCZvW-xYtZCPq-UbI-Xz5rYc5O2ZzH9Ll1WiLEk1IjpEQ&amp;usqp=CAU)",t2_5cuwjp5q,False,,0,False,Cool animations or still images explaining cnn,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gh01rj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1589139867.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do articles which explain CNN generate images( like over-imposong kernel grids on images) &lt;/p&gt;

&lt;p&gt;Not the best example &lt;a href=""https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQyQzgHCZvW-xYtZCPq-UbI-Xz5rYc5O2ZzH9Ll1WiLEk1IjpEQ&amp;amp;usqp=CAU""&gt;google search result&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?auto=webp&amp;s=75ce352f178579adec3ef61b3528f21df15c7e0d', 'width': 638, 'height': 479}, 'resolutions': [{'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aea76a051418f62e54cd29dd03aab1169b4b896', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b3099e307da1c2386ec9974d2b07b48a5e29067', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9f4e35eb9a0c1d3e33b15ab7dc2829c979f728a', 'width': 320, 'height': 240}], 'variants': {}, 'id': '22vBmEUCfXCxxqlGrwamhcVQA6BhRkZx03_LM6CHuYM'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gh01rj,True,,r2d2FortNite,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh01rj/cool_animations_or_still_images_explaining_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh01rj/cool_animations_or_still_images_explaining_cnn/,155203,1589111067.0,0,,False,,,,
,learnmachinelearning,,t2_6awn0nqk,False,,0,False,Humble Book Bundle: Definitive Guides to All Things Programming by O'Reilly (pay what you want and help charity),"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_ggzs42,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,False,https://a.thumbs.redditmedia.com/9sEEZTYgufRV_l9JANb2QXL_XLIDlCVzqgvCLYgYTf8.jpg,False,,[],{},link,,False,,1589138632.0,richtext,6,,,text,humblebundle.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?auto=webp&amp;s=cc7ee2b99a7c43423dd1de45202ecbe68a66f75e', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1471c31024e209de5e30ef75cb9709355e6b3d06', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e83e37a65359332cf95abf71d2dc331bbce502a5', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b21cd8c75dd690a5561f07fc65efdecfa54c1d8', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10c26d04996b393f69d8ecda3a5edc2f786caf62', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6847adb66fb19cb5856719aaf94f9a7271b372d', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6f67268c24241f85cef951e8add494d0ec45282', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'W1dtqkKZOrsWPWsN7_TNU7cM-pajRzK1ke6VAHOGqWo'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggzs42,True,,big_clips,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggzs42/humble_book_bundle_definitive_guides_to_all/,all_ads,False,https://www.humblebundle.com/books/definitive-guides-to-all-things-programming-oreilly-books?partner=repsup,155203,1589109832.0,0,,False,,,,
,learnmachinelearning,,t2_5bi9w5n3,False,,0,False,"Hello everyone, i finished learning python and i wanted to learn machine learning as i'm a beginner and need to know where to start, can you help me in this?",[],r/learnmachinelearning,False,6,,0,,False,t3_ggzpwv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589138340.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggzpwv,True,,imashadowguys,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggzpwv/hello_everyone_i_finished_learning_python_and_i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggzpwv/hello_everyone_i_finished_learning_python_and_i/,155203,1589109540.0,0,,False,,,,
,learnmachinelearning,"Fast gradient sign method/attack is popularized with the panda-to-gibbon picture on GoogLeNet

My question is simple: does it also work for other types of model with well-defined loss functions, such as SVM, logistic regression, etc? (for inputs that are perhaps not images)

I'm trying to think up a toy example to illustrate FGSM without having to write 50 lines of Pytorch code.",t2_2kpphupx,False,,0,False,Does fast gradient sign attack also work for non-neural network models?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwe6r,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589121200.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Fast gradient sign method/attack is popularized with the panda-to-gibbon picture on GoogLeNet&lt;/p&gt;

&lt;p&gt;My question is simple: does it also work for other types of model with well-defined loss functions, such as SVM, logistic regression, etc? (for inputs that are perhaps not images)&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to think up a toy example to illustrate FGSM without having to write 50 lines of Pytorch code.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwe6r,True,,fromnighttilldawn,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwe6r/does_fast_gradient_sign_attack_also_work_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwe6r/does_fast_gradient_sign_attack_also_work_for/,155203,1589092400.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

**MACHINE LEARNING : Ensemble Learning Methods**

Combine all â€œweakâ€ learner to form ensemble.

Averaging : Equal weights are assigned to different model.

&amp;#x200B;

Bagging (reduces variance) :

Bagging or bootstrap aggregation â€˜reduces varianceâ€™ of an estimate by taking mean of multiple estimates.

Steps :

1. Create randomly sampled datasets of the original training data.
2. 2. Build and fit several classifiers to each of these diverse copies.
3. 3. Take the average of all the predictions to make final overall prediction.

Boosting (reduces bias) :

Boosting â€˜reduces biasâ€™ by training weak learner sequentially,each trying to correct its predecessor.

Steps:

1. Train a Classfier H1 that best classifies the data with respect to accuracy.
2. 2. Identify the region where H1 produces errors,add weights to it and produce a H2 classifier.
3. 3. Exaggerate those samples for which H1 gives a different result from H2 and produces H3 classifier. Repeat step 02 for a new classifier.

Adaboost : Consider a scenario, where there are â€˜+â€™ and â€˜-â€˜

Adaboost Working : Step 1

1. Assign equal weights to each data point.
2. 2. Apply a decision stump to classify them as +(plus) and -(minus)
3. 3. Decision stump (D1) has generated vertical plane at left side to classify.
4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.

Adaboost Working : Step 2

1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.
2. 2. The second decision stump (D2) will try to predict them correctly.
3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.
4. 4. D2 has also caused mis-classification errors to three -(minus)

Adaboost Working : Step 3

1. D3 adds higher weights to three â€“ (minus)
2. 2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.

Adaboost Working : Step 4

1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.

Adaboost Algorithm :

Step 1 : Initially each data point is weighted equally with weight .

. Wi = 1/n

. Where n is the number of samples.

Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.

Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.

. @\^t =1/2 ln 1-e/ e

. @ â€“ alpha , ln â€“ log ,

. e â€“ error , t â€“ time ,

. \^ â€“ to power

Step 4 : Weight after time â€˜tâ€™ is given as :

. Wi\^t+1/z e\^-at.h1(x).y(x)

.

. Z â€“ Normalizing factor

. h1(x).y(x) â€“ sign of the current output

Gradient Boosting (GBM) : Gradient boosting involves three elements.

1. A loss function to be optimised.
2. 2. A weak learner to make predictions.
3. 3. An additive model to add weak learners to minimize the loss function.

GBM Mechanism :

1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.
2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.
3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.
4. 4. Modeling is stopped when residuals do not have any pattern that can be modeled.

Adaboost Working : Step 1

1. Assign equal weights to each data point.
2. 2. Apply a decision stump to classify them as +(plus) and -(minus)
3. 3. Decision stump (D1) has generated vertical plane at left side to classify.
4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.

Adaboost Working : Step 2

1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.
2. 2. The second decision stump (D2) will try to predict them correctly.
3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.
4. 4. D2 has also caused mis-classification errors to three -(minus)

Adaboost Working : Step 3

1. D3 adds higher weights to three â€“ (minus)
2. 2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.

Adaboost Working : Step 4

1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.

Adaboost Algorithm :

Step 1 : Initially each data point is weighted equally with weight .

. Wi = 1/n

. Where n is the number of samples.

Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.

Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.

. @\^t =1/2 ln 1-e/ e

. @ â€“ alpha , ln â€“ log ,

. e â€“ error , t â€“ time ,

. \^ â€“ to power

Step 4 : Weight after time â€˜tâ€™ is given as :

. Wi\^t+1/z e\^-at.h1(x).y(x)

.

. Z â€“ Normalizing factor

. h1(x).y(x) â€“ sign of the current output

Gradient Boosting (GBM) : Gradient boosting involves three elements.

1. A loss function to be optimised.
2. 2. A weak learner to make predictions.
3. 3. An additive model to add weak learners to minimize the loss function.

GBM Mechanism :

1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.
2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.
3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.
4. 4. Modelling is stopped when residuals do not have any pattern that can be modelled.

&amp;#x200B;

GBM Algorithm Steps :

1. Fit a simple regression or classification model.
2. 2. Calculate error residuals (actual value â€“ predicted value)
3. 3. Fit a new model on error residuals as targets variable with same input variables.
4. 4. Add the predicted residuals to the previous predictions.
5. 5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.

XGBoost: eXtreme Gradient Boosting is a library for developing fast and high -performance gradient boosting tree models.XGBoost is extensively used in ML competitions as it is almost 10 times faster than other gradient boosting techniques.

XGBoost Parameters :

1. General Parameters: Number of threads.
2. 2. Booster Parameters: a. Step size. b. Regularisation
3. 3. Task Parameters: a. Objective. b. Evaluation metric.

General Parameters :

nthread :

1. Number of parallel threads.
2. 2. If no value is entered,algorithm automatically detects the number of cores and runs on all the cores.

booster :

1. gbtree : tree-based model
2.  gblinear : linear function

Silent \[default =0\] :

1. If set to 1, no running messages will be printed.Hence,keep it â€˜0â€™ as the messages might help in understanding the model.

Booster Parameters : Booster parameters guide individual booster (Tree/Regression) at each.

Parameters for tree booster :

eta : Step size shrinkage is used in update to prevent overfitting. Range in \[0,1\], default 0.3

gamma : Minimum loss reduction required to make a split. Range \[0,infinite\],default 0

max\_depth : Maximum depth of the tree. Range \[1,infinite\],default 6

min\_child\_weight : minimum sum of instance weight needed in a child. Range \[0,Range\] , default 1

**For Videos and More:** [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)

 **Blog Source:** 

[https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41](https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41)",t2_205ygpnb,False,,0,False,What is 'Ensemble Learning Methods' and How to do it?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggum9d,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1589112721.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MACHINE LEARNING : Ensemble Learning Methods&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Combine all â€œweakâ€ learner to form ensemble.&lt;/p&gt;

&lt;p&gt;Averaging : Equal weights are assigned to different model.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Bagging (reduces variance) :&lt;/p&gt;

&lt;p&gt;Bagging or bootstrap aggregation â€˜reduces varianceâ€™ of an estimate by taking mean of multiple estimates.&lt;/p&gt;

&lt;p&gt;Steps :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create randomly sampled datasets of the original training data.&lt;/li&gt;
&lt;li&gt;2. Build and fit several classifiers to each of these diverse copies.&lt;/li&gt;
&lt;li&gt;3. Take the average of all the predictions to make final overall prediction.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Boosting (reduces bias) :&lt;/p&gt;

&lt;p&gt;Boosting â€˜reduces biasâ€™ by training weak learner sequentially,each trying to correct its predecessor.&lt;/p&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train a Classfier H1 that best classifies the data with respect to accuracy.&lt;/li&gt;
&lt;li&gt;2. Identify the region where H1 produces errors,add weights to it and produce a H2 classifier.&lt;/li&gt;
&lt;li&gt;3. Exaggerate those samples for which H1 gives a different result from H2 and produces H3 classifier. Repeat step 02 for a new classifier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost : Consider a scenario, where there are â€˜+â€™ and â€˜-â€˜&lt;/p&gt;

&lt;p&gt;Adaboost Working : Step 1&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assign equal weights to each data point.&lt;/li&gt;
&lt;li&gt;2. Apply a decision stump to classify them as +(plus) and -(minus)&lt;/li&gt;
&lt;li&gt;3. Decision stump (D1) has generated vertical plane at left side to classify.&lt;/li&gt;
&lt;li&gt;4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 2&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.&lt;/li&gt;
&lt;li&gt;2. The second decision stump (D2) will try to predict them correctly.&lt;/li&gt;
&lt;li&gt;3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.&lt;/li&gt;
&lt;li&gt;4. D2 has also caused mis-classification errors to three -(minus)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 3&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D3 adds higher weights to three â€“ (minus)&lt;/li&gt;
&lt;li&gt;2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 4&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Algorithm :&lt;/p&gt;

&lt;p&gt;Step 1 : Initially each data point is weighted equally with weight .&lt;/p&gt;

&lt;p&gt;. Wi = 1/n&lt;/p&gt;

&lt;p&gt;. Where n is the number of samples.&lt;/p&gt;

&lt;p&gt;Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.&lt;/p&gt;

&lt;p&gt;Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.&lt;/p&gt;

&lt;p&gt;. @^t =1/2 ln 1-e/ e&lt;/p&gt;

&lt;p&gt;. @ â€“ alpha , ln â€“ log ,&lt;/p&gt;

&lt;p&gt;. e â€“ error , t â€“ time ,&lt;/p&gt;

&lt;p&gt;. ^ â€“ to power&lt;/p&gt;

&lt;p&gt;Step 4 : Weight after time â€˜tâ€™ is given as :&lt;/p&gt;

&lt;p&gt;. Wi^t+1/z e^-at.h1(x).y(x)&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;. Z â€“ Normalizing factor&lt;/p&gt;

&lt;p&gt;. h1(x).y(x) â€“ sign of the current output&lt;/p&gt;

&lt;p&gt;Gradient Boosting (GBM) : Gradient boosting involves three elements.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A loss function to be optimised.&lt;/li&gt;
&lt;li&gt;2. A weak learner to make predictions.&lt;/li&gt;
&lt;li&gt;3. An additive model to add weak learners to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GBM Mechanism :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GBM predicts the residuals or error of prior models and then sums them to make the final prediction.&lt;/li&gt;
&lt;li&gt;2. One weak learner is added at a time and existing weak learners in the model are left unchanged.&lt;/li&gt;
&lt;li&gt;3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.&lt;/li&gt;
&lt;li&gt;4. Modeling is stopped when residuals do not have any pattern that can be modeled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 1&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assign equal weights to each data point.&lt;/li&gt;
&lt;li&gt;2. Apply a decision stump to classify them as +(plus) and -(minus)&lt;/li&gt;
&lt;li&gt;3. Decision stump (D1) has generated vertical plane at left side to classify.&lt;/li&gt;
&lt;li&gt;4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 2&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.&lt;/li&gt;
&lt;li&gt;2. The second decision stump (D2) will try to predict them correctly.&lt;/li&gt;
&lt;li&gt;3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.&lt;/li&gt;
&lt;li&gt;4. D2 has also caused mis-classification errors to three -(minus)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 3&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D3 adds higher weights to three â€“ (minus)&lt;/li&gt;
&lt;li&gt;2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 4&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Algorithm :&lt;/p&gt;

&lt;p&gt;Step 1 : Initially each data point is weighted equally with weight .&lt;/p&gt;

&lt;p&gt;. Wi = 1/n&lt;/p&gt;

&lt;p&gt;. Where n is the number of samples.&lt;/p&gt;

&lt;p&gt;Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.&lt;/p&gt;

&lt;p&gt;Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.&lt;/p&gt;

&lt;p&gt;. @^t =1/2 ln 1-e/ e&lt;/p&gt;

&lt;p&gt;. @ â€“ alpha , ln â€“ log ,&lt;/p&gt;

&lt;p&gt;. e â€“ error , t â€“ time ,&lt;/p&gt;

&lt;p&gt;. ^ â€“ to power&lt;/p&gt;

&lt;p&gt;Step 4 : Weight after time â€˜tâ€™ is given as :&lt;/p&gt;

&lt;p&gt;. Wi^t+1/z e^-at.h1(x).y(x)&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;. Z â€“ Normalizing factor&lt;/p&gt;

&lt;p&gt;. h1(x).y(x) â€“ sign of the current output&lt;/p&gt;

&lt;p&gt;Gradient Boosting (GBM) : Gradient boosting involves three elements.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A loss function to be optimised.&lt;/li&gt;
&lt;li&gt;2. A weak learner to make predictions.&lt;/li&gt;
&lt;li&gt;3. An additive model to add weak learners to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GBM Mechanism :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GBM predicts the residuals or error of prior models and then sums them to make the final prediction.&lt;/li&gt;
&lt;li&gt;2. One weak learner is added at a time and existing weak learners in the model are left unchanged.&lt;/li&gt;
&lt;li&gt;3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.&lt;/li&gt;
&lt;li&gt;4. Modelling is stopped when residuals do not have any pattern that can be modelled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;GBM Algorithm Steps :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fit a simple regression or classification model.&lt;/li&gt;
&lt;li&gt;2. Calculate error residuals (actual value â€“ predicted value)&lt;/li&gt;
&lt;li&gt;3. Fit a new model on error residuals as targets variable with same input variables.&lt;/li&gt;
&lt;li&gt;4. Add the predicted residuals to the previous predictions.&lt;/li&gt;
&lt;li&gt;5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;XGBoost: eXtreme Gradient Boosting is a library for developing fast and high -performance gradient boosting tree models.XGBoost is extensively used in ML competitions as it is almost 10 times faster than other gradient boosting techniques.&lt;/p&gt;

&lt;p&gt;XGBoost Parameters :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;General Parameters: Number of threads.&lt;/li&gt;
&lt;li&gt;2. Booster Parameters: a. Step size. b. Regularisation&lt;/li&gt;
&lt;li&gt;3. Task Parameters: a. Objective. b. Evaluation metric.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;General Parameters :&lt;/p&gt;

&lt;p&gt;nthread :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of parallel threads.&lt;/li&gt;
&lt;li&gt;2. If no value is entered,algorithm automatically detects the number of cores and runs on all the cores.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;booster :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;gbtree : tree-based model&lt;/li&gt;
&lt;li&gt; gblinear : linear function&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Silent [default =0] :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If set to 1, no running messages will be printed.Hence,keep it â€˜0â€™ as the messages might help in understanding the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Booster Parameters : Booster parameters guide individual booster (Tree/Regression) at each.&lt;/p&gt;

&lt;p&gt;Parameters for tree booster :&lt;/p&gt;

&lt;p&gt;eta : Step size shrinkage is used in update to prevent overfitting. Range in [0,1], default 0.3&lt;/p&gt;

&lt;p&gt;gamma : Minimum loss reduction required to make a split. Range [0,infinite],default 0&lt;/p&gt;

&lt;p&gt;max_depth : Maximum depth of the tree. Range [1,infinite],default 6&lt;/p&gt;

&lt;p&gt;min_child_weight : minimum sum of instance weight needed in a child. Range [0,Range] , default 1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For Videos and More:&lt;/strong&gt; &lt;a href=""https://www.facebook.com/seevecoding""&gt;www.facebook.com/seevecoding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blog Source:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41""&gt;https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vKw6ateLXak-cgwwbDwy3FEyQp7ntHJChB8-5DhkkkQ.jpg?auto=webp&amp;s=ec1415d6df378dd491dad04991f28c1ee5a52d18', 'width': 200, 'height': 200}, 'resolutions': [{'url': 'https://external-preview.redd.it/vKw6ateLXak-cgwwbDwy3FEyQp7ntHJChB8-5DhkkkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a51180563bf098a2101bb79f4b2caad7ca3e7bcf', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'wukwke95vIPcLfRmc-wWi8RZ2RVJUsRM30JL36CjMWE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggum9d,True,,iamrealadvait,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/,155203,1589083921.0,0,,False,,,,
,learnmachinelearning,,t2_6dcghpa8,False,,0,False,Step wise guide to build handwritten Digit Recognition using cnn and opencv,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggyumn,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Handwritten Digit Recognition using machine learning | keras and opencv | [github] |full explanation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'ProgrammingHut', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qmY36LzdPHo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCx1_WfGX9D9rmsJNBM5qsMA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggyumn', 'height': 338}",Project,False,1,,False,https://b.thumbs.redditmedia.com/XtBkLCQnPdyLpZFdzuA2uxZvMddPt3tNR78f72awlkk.jpg,False,,[],{},rich:video,,False,,1589134009.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?auto=webp&amp;s=31f862a3e09805072774d557184a0068e1d45f77', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b740404c0b0d2ef7d74a84c95ca2a692f65be505', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=926868336c7a747215182e7531f1e6d354577238', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6c169579a3953b0a0aacf4af73335f673bfbf2f', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'wg8I7HoSBM_S_yx7864fojmVuGyghz-jMbMki6tD5_s'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggyumn,True,,Pawan315,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyumn/step_wise_guide_to_build_handwritten_digit/,all_ads,False,https://youtu.be/qmY36LzdPHo,155203,1589105209.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Handwritten Digit Recognition using machine learning | keras and opencv | [github] |full explanation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'ProgrammingHut', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qmY36LzdPHo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCx1_WfGX9D9rmsJNBM5qsMA'}}",False,,,,
,learnmachinelearning,,t2_4w7oamkl,False,,0,False,"In Variational Autoencoders, does the generative model generates samples from latent variables which are sampled from a multivariate distribution? If yes, then is this similar in case of GANs?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggyh0m,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589132040.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggyh0m,True,,HTKasd,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyh0m/in_variational_autoencoders_does_the_generative/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggyh0m/in_variational_autoencoders_does_the_generative/,155203,1589103240.0,0,,False,,,,
,learnmachinelearning,,t2_6fnjwm42,False,,0,False,Guys check out our FREE AI and machine learning course,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggydpl,False,light,1.0,,public,1,0,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'm.youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'description': ""An intro on Artificial intelligence and Machine Learning Ep : 1 PS that's a crow in the beginning"", 'title': 'Learn AI and Machine learning basics', 'url': 'https://www.youtube.com/watch?v=dqWUIJjFrmM', 'type': 'video', 'author_name': 'AI washingmachine', 'height': 338, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,False,,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggydpl', 'height': 338}",Project,False,1,,False,https://a.thumbs.redditmedia.com/dY5JfMGrsw9r9TGoMZxlwrkr1sCcyBTIMHTOBSXxVY0.jpg,False,,[],{},rich:video,,False,,1589131563.0,richtext,6,,,text,m.youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?auto=webp&amp;s=1733febf5a0bd6fae80eb674d48df8d52d71e9fc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9be822395a8b233483be8ba694b9810839ebb07', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c54724b4e15a9584f83a5404e64a9f6472582e6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8833c15b014479bdbf037635f34c11ea2cd2662', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KtY5la4OC1bioPef9YIyQumQxjs6NhTgWt9gt1HjmLU'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggydpl,True,,AI_washingmachine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggydpl/guys_check_out_our_free_ai_and_machine_learning/,all_ads,False,https://m.youtube.com/watch?feature=youtu.be&amp;v=dqWUIJjFrmM,155203,1589102763.0,0,"{'type': 'm.youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'description': ""An intro on Artificial intelligence and Machine Learning Ep : 1 PS that's a crow in the beginning"", 'title': 'Learn AI and Machine learning basics', 'url': 'https://www.youtube.com/watch?v=dqWUIJjFrmM', 'type': 'video', 'author_name': 'AI washingmachine', 'height': 338, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,"Awesome talk on Tensorflow.js, Machine Learning in javascript by Jason Mayes Senior Advocate at Google",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggxz3x,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tensorflow.js : Machine Learning in JavaScript', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/x-608hSAhCA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggxz3x', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/FAoo3VgO6L-UrZGx6hbJjX9T_wEZs-Fqg6uH4uBonTs.jpg,False,,[],{},rich:video,,False,,1589129434.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?auto=webp&amp;s=5b594ea3d7cb5c146c1eb9a8c3f89d08b39e6bea', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3af1cebb089cf1f826c5e0102b8b71d8b8ca70a0', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=246df40b605e32386c6fa4fd1c3d1bd8f061e606', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b77451d214bbdf1ed7475220c108372a4a205e5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'oHrzZyYUjrYCiVRz9gXci67sj67Xe5e1wCTo94LX_Ho'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggxz3x,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxz3x/awesome_talk_on_tensorflowjs_machine_learning_in/,all_ads,False,https://www.youtube.com/watch?v=x-608hSAhCA&amp;feature=emb_logo,155203,1589100634.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tensorflow.js : Machine Learning in JavaScript', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/x-608hSAhCA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,,,,
,learnmachinelearning,"I've an matrix X with size (20000000, 10), and set KFold split into 5 folds.

When I split the data iteratively, as:

`for train_id, test_id in kfold.split(x):`

  `x_train = data[train_id]`

It shows the error:

 `'Passing list-likes to .loc or [] with any missing labels is no longer supported` 

So I check for the train\_id if there is any problem, and it shows the first fold ID:

 `[ 4022530  4022531  4022532 ... 20112646 20112647 20112648]` 

&amp;#x200B;

This is strange result, for I should get 4000000 each fold (total size / n\_fold).

I believe this is the problem, but I can't figure out why. My setting of KFold doesn't seem to have any problem:

`kfold = sklearn.model_selection.KFold(n_splits = 5)`

Could anyone help me?",t2_11cquw,False,,0,False,KFold in sklearn give strange results of train_id,[],r/learnmachinelearning,False,6,,0,,False,t3_ggxo9c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589127847.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve an matrix X with size (20000000, 10), and set KFold split into 5 folds.&lt;/p&gt;

&lt;p&gt;When I split the data iteratively, as:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for train_id, test_id in kfold.split(x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x_train = data[train_id]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It shows the error:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;#39;Passing list-likes to .loc or [] with any missing labels is no longer supported&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;So I check for the train_id if there is any problem, and it shows the first fold ID:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;[ 4022530  4022531  4022532 ... 20112646 20112647 20112648]&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is strange result, for I should get 4000000 each fold (total size / n_fold).&lt;/p&gt;

&lt;p&gt;I believe this is the problem, but I can&amp;#39;t figure out why. My setting of KFold doesn&amp;#39;t seem to have any problem:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kfold = sklearn.model_selection.KFold(n_splits = 5)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Could anyone help me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggxo9c,True,,Laurence-Lin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxo9c/kfold_in_sklearn_give_strange_results_of_train_id/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggxo9c/kfold_in_sklearn_give_strange_results_of_train_id/,155203,1589099047.0,0,,False,,,,
,learnmachinelearning,"Iâ€™ve been struggling to find a thesis topic and after a lot of research I finally found something Iâ€™m interested it and want to get some opinions on it. 

Basically I was wondering if ML learning could be used to look at information about a case and connect it to existing legal precedents? 

Iâ€™m in a combined bs/ms program and just starting out so I could use some direction!",t2_i7bmsnh,False,,0,False,Machine learning to Identify Legal Precedent. MS DS thesis topic?,[],r/learnmachinelearning,False,6,,0,,False,t3_gguebw,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589111779.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™ve been struggling to find a thesis topic and after a lot of research I finally found something Iâ€™m interested it and want to get some opinions on it. &lt;/p&gt;

&lt;p&gt;Basically I was wondering if ML learning could be used to look at information about a case and connect it to existing legal precedents? &lt;/p&gt;

&lt;p&gt;Iâ€™m in a combined bs/ms program and just starting out so I could use some direction!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gguebw,True,,whyshali,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gguebw/machine_learning_to_identify_legal_precedent_ms/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gguebw/machine_learning_to_identify_legal_precedent_ms/,155203,1589082979.0,0,,False,,,,
,learnmachinelearning,I completed the first assignment in deeplearning.ai and got 100/100.But i still dont understand how numbers equate image values and it all seems like magic to me.Is it normal or there's a better explanation or am i doing something wrong?,t2_1af95696,False,,0,False,Need help with deeper understanding of what's going on in neural networks.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggx4fu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1589124966.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I completed the first assignment in deeplearning.ai and got 100/100.But i still dont understand how numbers equate image values and it all seems like magic to me.Is it normal or there&amp;#39;s a better explanation or am i doing something wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggx4fu,True,,_notdivyanshuuuu,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggx4fu/need_help_with_deeper_understanding_of_whats/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggx4fu/need_help_with_deeper_understanding_of_whats/,155203,1589096166.0,0,,False,,,,
,learnmachinelearning,"In google colab, I'd turn on the GPU in the setting. However, recently it kept showing up ""You are connected to GPU, but not utlilizing it.""

And I found that my lightgbm training is really slow. I tried to set the parameters in lightgbm 'device\_type' to 'gpu', but it's still slow and the message still indicate that GPU is not utilized.

Now I'm wondering if I should reinstall lightgbm to configure GPU setting. 

Should I set up any GPU settings while my device contains one, in order to achieve speed up? I'd never done that before, this is the first time problem occur.

&amp;#x200B;

Thanks for any help!",t2_11cquw,False,,0,False,"If my envorinment(ex: Google Colab) enables GPU, should I set up GPU environment for speed up training and other code running?",[],r/learnmachinelearning,False,6,,0,,False,t3_ggwrsv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589123130.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In google colab, I&amp;#39;d turn on the GPU in the setting. However, recently it kept showing up &amp;quot;You are connected to GPU, but not utlilizing it.&amp;quot;&lt;/p&gt;

&lt;p&gt;And I found that my lightgbm training is really slow. I tried to set the parameters in lightgbm &amp;#39;device_type&amp;#39; to &amp;#39;gpu&amp;#39;, but it&amp;#39;s still slow and the message still indicate that GPU is not utilized.&lt;/p&gt;

&lt;p&gt;Now I&amp;#39;m wondering if I should reinstall lightgbm to configure GPU setting. &lt;/p&gt;

&lt;p&gt;Should I set up any GPU settings while my device contains one, in order to achieve speed up? I&amp;#39;d never done that before, this is the first time problem occur.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwrsv,True,,Laurence-Lin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwrsv/if_my_envorinmentex_google_colab_enables_gpu/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwrsv/if_my_envorinmentex_google_colab_enables_gpu/,155203,1589094330.0,0,,False,,,,
,learnmachinelearning,"Hi Iâ€™m high school student 

Iâ€™m now studying intro to machine learning with TF nanodegree and I finished Ai programming with python nano degree 

but I feel like Udacity doesnâ€™t provide the full information so I sometimes use google or hands on machine learning with sklear..

But I forgot some of what 
I learnt Before two weeks

Is it because Iâ€™m learning 
fast or thatâ€™s regular ?

Or I have to  practicing everything ?
Like practice in kaggle notebooks 

How much time I have to put in algorithm ?

Please help me 
I already learn a lot of linear algebra 
and some of probability and calculus
And I have some knowledge in 
python ,NumPy ,pandas ,sklearn",t2_3xc4pyo7,False,,0,False,Iâ€™m LOST,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggwqco,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589122921.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Iâ€™m high school student &lt;/p&gt;

&lt;p&gt;Iâ€™m now studying intro to machine learning with TF nanodegree and I finished Ai programming with python nano degree &lt;/p&gt;

&lt;p&gt;but I feel like Udacity doesnâ€™t provide the full information so I sometimes use google or hands on machine learning with sklear..&lt;/p&gt;

&lt;p&gt;But I forgot some of what 
I learnt Before two weeks&lt;/p&gt;

&lt;p&gt;Is it because Iâ€™m learning 
fast or thatâ€™s regular ?&lt;/p&gt;

&lt;p&gt;Or I have to  practicing everything ?
Like practice in kaggle notebooks &lt;/p&gt;

&lt;p&gt;How much time I have to put in algorithm ?&lt;/p&gt;

&lt;p&gt;Please help me 
I already learn a lot of linear algebra 
and some of probability and calculus
And I have some knowledge in 
python ,NumPy ,pandas ,sklearn&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggwqco,True,,i3zM,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwqco/im_lost/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwqco/im_lost/,155203,1589094121.0,0,,False,,,,
,learnmachinelearning,"Hey guys, has anyone used [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) to organize their projects in the past? I'm fairly new to it and am trying to figure out how to integrate my existing data pipeline code into a cookiecutter format.   


If you have used cookiecutter for data science before, please message me. Any help appreciated. Thanks!",t2_3loxj2cz,False,,0,False,Help with using Cookiecutter Data Science,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwlof,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589122248.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, has anyone used &lt;a href=""https://drivendata.github.io/cookiecutter-data-science/""&gt;Cookiecutter Data Science&lt;/a&gt; to organize their projects in the past? I&amp;#39;m fairly new to it and am trying to figure out how to integrate my existing data pipeline code into a cookiecutter format.   &lt;/p&gt;

&lt;p&gt;If you have used cookiecutter for data science before, please message me. Any help appreciated. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwlof,True,,hedgehogist,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwlof/help_with_using_cookiecutter_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwlof/help_with_using_cookiecutter_data_science/,155203,1589093448.0,0,,False,,,,
,learnmachinelearning,,t2_21n30k0y,False,,0,False,Unit Neurons v1.0 (C++ Neural Network Library) Release Trailer,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggtgr0,False,light,1.0,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggtgr0', 'height': 338}",Project,False,2,,False,https://b.thumbs.redditmedia.com/8Ic4laTF05YVxRsRJ0NuRknbRTJbJKAtEnN5KkFkzzE.jpg,False,,[],{},rich:video,,False,,1589107900.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?auto=webp&amp;s=7558654250d1f6bba3e596f2aa61e5168d718a79', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60b7736699cec361507c993cb3acd3d1e4e58683', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eb26974ca87809c0d5b6bb511eee0e1b0790e67', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e86e8b32bd9bbf7b6d4f3da9b87cd37ef85fc39', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cLsNNIo94xZ7ympHy6nSQb3vA1nSiSo5r5OnkWpcHKA'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggtgr0,True,,johnlime3301,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggtgr0/unit_neurons_v10_c_neural_network_library_release/,all_ads,False,https://youtu.be/3yW3_18dBIg,155203,1589079100.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}",False,"[{'approved_at_utc': None, 'subreddit': 'DecisionTheory', 'selftext': '', 'author_fullname': 't2_21n30k0y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Unit Neurons v1.0 (C++ Neural Network Library) Release Trailer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/DecisionTheory', 'hidden': False, 'pwls': None, 'link_flair_css_class': 'reinforcementLearning', 'downs': 0, 'thumbnail_height': 105, 'hide_score': False, 'name': 't3_ggi4gu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggi4gu', 'height': 338}, 'link_flair_text': 'RL', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'author_premium': False, 'thumbnail': 'image', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1589069467.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?auto=webp&amp;s=7558654250d1f6bba3e596f2aa61e5168d718a79', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60b7736699cec361507c993cb3acd3d1e4e58683', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eb26974ca87809c0d5b6bb511eee0e1b0790e67', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e86e8b32bd9bbf7b6d4f3da9b87cd37ef85fc39', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cLsNNIo94xZ7ympHy6nSQb3vA1nSiSo5r5OnkWpcHKA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '5ada5386-b7b1-11e5-b99a-0eed2e8dfa3b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_38gi4', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ggi4gu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'johnlime3301', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/DecisionTheory/comments/ggi4gu/unit_neurons_v10_c_neural_network_library_release/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://youtu.be/3yW3_18dBIg', 'subreddit_subscribers': 1649, 'created_utc': 1589040667.0, 'num_crossposts': 4, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}, 'is_video': False}]",t3_ggi4gu,,
,learnmachinelearning,"I'm currently learning about CNN (Convolutional Neural Network) and apparently  MLP (Multi Layer Perceptrion) is commonly used as fully connected in CNN.

Recently i heard about GAP (Global Average Pooling) as alternative of MLP as fully connected in CNN and it works well on few simple cases i tested.

So i wonder if there are any alternative for fully connected in CNN besides MLP and GAP?",t2_4weapg0g,False,,0,False,Are there alternative for fully connected in CNN besides MLP and GAP?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwf44,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589121325.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently learning about CNN (Convolutional Neural Network) and apparently  MLP (Multi Layer Perceptrion) is commonly used as fully connected in CNN.&lt;/p&gt;

&lt;p&gt;Recently i heard about GAP (Global Average Pooling) as alternative of MLP as fully connected in CNN and it works well on few simple cases i tested.&lt;/p&gt;

&lt;p&gt;So i wonder if there are any alternative for fully connected in CNN besides MLP and GAP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwf44,True,,05e981ae,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwf44/are_there_alternative_for_fully_connected_in_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwf44/are_there_alternative_for_fully_connected_in_cnn/,155203,1589092525.0,0,,False,,,,
,learnmachinelearning,,t2_6fnjwm42,False,,0,False,Guysss check out our FREE AI and machine learning course,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggwexd,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Learn AI and Machine learning basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI washingmachine', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggwexd', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/exEYZe8wBZihfyhoIkuvtdBfxF9VoU1uLu7YdgRfGNc.jpg,False,,[],{},rich:video,,False,,1589121301.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?auto=webp&amp;s=1733febf5a0bd6fae80eb674d48df8d52d71e9fc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9be822395a8b233483be8ba694b9810839ebb07', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c54724b4e15a9584f83a5404e64a9f6472582e6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8833c15b014479bdbf037635f34c11ea2cd2662', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KtY5la4OC1bioPef9YIyQumQxjs6NhTgWt9gt1HjmLU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwexd,True,,AI_washingmachine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwexd/guysss_check_out_our_free_ai_and_machine_learning/,all_ads,False,https://youtu.be/dqWUIJjFrmM,155203,1589092501.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Learn AI and Machine learning basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI washingmachine', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,,,,
,learnmachinelearning,"Hey! I just created a blog linking to some of the best resources out there to get started working with Machine Learning, from a Deep Learning approach. Feel free to let me know what you think.  [https://sciteens.org/blog/zero-to-hero-ai](https://sciteens.org/blog/zero-to-hero-ai)",t2_9bn8ubx,False,,0,False,Zero to Hero: The (Mostly) Free Guide to Getting Started in A.I.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggipa5,False,dark,0.87,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1589071240.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey! I just created a blog linking to some of the best resources out there to get started working with Machine Learning, from a Deep Learning approach. Feel free to let me know what you think.  &lt;a href=""https://sciteens.org/blog/zero-to-hero-ai""&gt;https://sciteens.org/blog/zero-to-hero-ai&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggipa5,True,,JSutie,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggipa5/zero_to_hero_the_mostly_free_guide_to_getting/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggipa5/zero_to_hero_the_mostly_free_guide_to_getting/,155203,1589042440.0,0,,False,,,,
,learnmachinelearning,"Hi, so I recently started studying about data science, machine learning, neural networks, deep learning etc and my PC is so old that my cpu goes 100% even in the simplest kaggle tasks. I think that investing in threadripper 2920x , a x399 mobo , 32gb ram and one graphics card is a good idea for now. Do you have any suggestions? Thank you.",t2_2jobwlgs,False,,0,False,Pc build for machine learning and data science.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggqtlb,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589097885.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, so I recently started studying about data science, machine learning, neural networks, deep learning etc and my PC is so old that my cpu goes 100% even in the simplest kaggle tasks. I think that investing in threadripper 2920x , a x399 mobo , 32gb ram and one graphics card is a good idea for now. Do you have any suggestions? Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggqtlb,True,,CarelessWrangler3,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggqtlb/pc_build_for_machine_learning_and_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggqtlb/pc_build_for_machine_learning_and_data_science/,155203,1589069085.0,0,,False,,,,
,learnmachinelearning,,t2_1084g6,False,,0,False,Multilabel Image Classifier for predicting Fashion clothing Type,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggw0rx,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clothing Feature Detection Demo', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YX9uwoPgG5E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggw0rx', 'height': 338}",Project,False,1,,False,https://a.thumbs.redditmedia.com/dhHDSMw5oT8uUxC9KBjKy3VOYDNkboJPu3a-nvhuzw0.jpg,False,,[],{},rich:video,,False,,1589119326.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?auto=webp&amp;s=d2259c26a8fd38457e0521a2db9cc0f93dd75e13', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f84e46a98a210e84480d5c5bf683ea1e449188b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a58310ec673b47208a596dcf7fc7f2ae4132300', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c8122068f8df8a0acec4d295eb33a6b241beb57', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'MCzyABrx413d1EjvUD377rlmIlkuVL7gvPVxqLBy4Ys'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggw0rx,True,,rednivrug,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggw0rx/multilabel_image_classifier_for_predicting/,all_ads,False,https://www.youtube.com/watch?v=YX9uwoPgG5E,155203,1589090526.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clothing Feature Detection Demo', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YX9uwoPgG5E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,,,,
,learnmachinelearning,"What is the difference between additive attention and concat alignment method in multiplicative attention paper ?

I know that they use decoder hidden state from different locations.

But the thing I don't understand is , in additive attention do we do a element-wise addition , as the name ?

And in concat do we concatenate each encoder hidden state with with decoder hidden state ?

Thank you",t2_4fsxelmr,False,,0,False,Additive Attention and Multiplicative Attention Question,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggvx0e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589118820.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is the difference between additive attention and concat alignment method in multiplicative attention paper ?&lt;/p&gt;

&lt;p&gt;I know that they use decoder hidden state from different locations.&lt;/p&gt;

&lt;p&gt;But the thing I don&amp;#39;t understand is , in additive attention do we do a element-wise addition , as the name ?&lt;/p&gt;

&lt;p&gt;And in concat do we concatenate each encoder hidden state with with decoder hidden state ?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggvx0e,True,,chirathpansilu,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggvx0e/additive_attention_and_multiplicative_attention/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggvx0e/additive_attention_and_multiplicative_attention/,155203,1589090020.0,0,,False,,,,
,learnmachinelearning,,t2_4an3hpeg,False,,0,False,"This chair model was trained in RunwayML. The projection was made in GoogleColab, using StyleGAN2, to explore the latent space between the ChairGAN with the three well-known chairs.","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,87.0,False,t3_ggib63,False,light,1.0,,public,10,0,{},140.0,,False,[],,True,False,,{},Discussion,False,10,,False,https://a.thumbs.redditmedia.com/ca4tlrLrbasM0tlTVcA5FIKSe4VuuriK9kD-2AlPLI4.jpg,False,,[],{},link,,False,,1589070038.0,richtext,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?auto=webp&amp;s=31cf1745f48c7939b423e01c1c0dcdd2bc28463c', 'width': 1152, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7aa46bcf0e9b9439ed3d9fe45a8918f8ded2717c', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba3aa360495a2dcf97f250539be33c0e37a8b083', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=69ea0ff433860d13a4926baf1c95b38f5541d195', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b3a102a75d93faea80bf612f66b9b81afe7b978', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2c704b653622e499ec44b61ad65d9adfbbac2e9', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52a04959135cf357898927c1f77f52a84bbeda76', 'width': 1080, 'height': 675}], 'variants': {}, 'id': '3yQRNas98pOcf_girT9Tuy94g63nNxxUzDdon56PecU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggib63,True,,PopescuG,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggib63/this_chair_model_was_trained_in_runwayml_the/,all_ads,False,https://v.redd.it/l5sbj44ipox41,155203,1589041238.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'VisualMath', 'selftext': '', 'author_fullname': 't2_65o7vhtm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'This chair model was trained in RunwayML. The projection was made in GoogleColab, using StyleGAN2, to explore the latent space between the ChairGAN with the three well-known chairs.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/VisualMath', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 87, 'hide_score': False, 'name': 't3_ggaf5x', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'reddit_video': {'fallback_url': 'https://v.redd.it/l5sbj44ipox41/DASH_720?source=fallback', 'height': 720, 'width': 1152, 'scrubber_media_url': 'https://v.redd.it/l5sbj44ipox41/DASH_96', 'dash_url': 'https://v.redd.it/l5sbj44ipox41/DASHPlaylist.mpd', 'duration': 30, 'hls_url': 'https://v.redd.it/l5sbj44ipox41/HLSPlaylist.m3u8', 'is_gif': True, 'transcoding_status': 'completed'}}, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 55, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/ca4tlrLrbasM0tlTVcA5FIKSe4VuuriK9kD-2AlPLI4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'hosted:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1589034942.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'v.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?format=pjpg&amp;auto=webp&amp;s=13efca55ef411d589478829a1b9c279d5287f08a', 'width': 1152, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2fc3441a0fce7fe9505a7a354dcc7992205d6c94', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5a2197bcf4d2c30db9211e505617e81bfba3b349', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5547f809f62ed3b7b0537e604b51d5989cdac8ee', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5f07504187a32eadf6b0c71e39facffd55938068', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e4889ff75679a2a5cfdbacfc3b2d137186587b2a', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=827670d76145c925a56b92e780da95067a479b69', 'width': 1080, 'height': 675}], 'variants': {}, 'id': '3yQRNas98pOcf_girT9Tuy94g63nNxxUzDdon56PecU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2jaju8', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ggaf5x', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'FunVisualMath', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/VisualMath/comments/ggaf5x/this_chair_model_was_trained_in_runwayml_the/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://v.redd.it/l5sbj44ipox41', 'subreddit_subscribers': 2862, 'created_utc': 1589006142.0, 'num_crossposts': 3, 'media': {'reddit_video': {'fallback_url': 'https://v.redd.it/l5sbj44ipox41/DASH_720?source=fallback', 'height': 720, 'width': 1152, 'scrubber_media_url': 'https://v.redd.it/l5sbj44ipox41/DASH_96', 'dash_url': 'https://v.redd.it/l5sbj44ipox41/DASHPlaylist.mpd', 'duration': 30, 'hls_url': 'https://v.redd.it/l5sbj44ipox41/HLSPlaylist.m3u8', 'is_gif': True, 'transcoding_status': 'completed'}}, 'is_video': True}]",t3_ggaf5x,,
,learnmachinelearning,"Hi, I'm learning the reinforcement learning algorithms.  I came across the advantage function and state value function.  Most places say that the advantage function A(***s***,***a***) = Q function Q(***s***,***a***) minus the state value function V(***s***).  Some places say that  Q**(*****s, a***) is the value of action ***a*** taken at state ***s***, and V(***s***) which is the value of the state, or the average of all rewards (caused by all actions taken) at state ***s*** .  But when I look at the actual equations for Q(s,a) and V(s), they are actually the same, e.g., both equal to the expected value of the sum of discounted rewards.  I'm confused.  Could somebody explain to me what's exactly the advantage function (or maybe the difference between Q and V)?  Thank you.",t2_643sg3ws,False,,0,False,Could somebody clarify the difference b/w advantage function and state value function?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggusc6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589113481.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m learning the reinforcement learning algorithms.  I came across the advantage function and state value function.  Most places say that the advantage function A(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;,&lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt;) = Q function Q(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;,&lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt;) minus the state value function V(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;).  Some places say that  Q&lt;strong&gt;(&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;s, a&lt;/em&gt;&lt;/strong&gt;) is the value of action &lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt; taken at state &lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;, and V(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;) which is the value of the state, or the average of all rewards (caused by all actions taken) at state &lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt; .  But when I look at the actual equations for Q(s,a) and V(s), they are actually the same, e.g., both equal to the expected value of the sum of discounted rewards.  I&amp;#39;m confused.  Could somebody explain to me what&amp;#39;s exactly the advantage function (or maybe the difference between Q and V)?  Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggusc6,True,,TobinC1,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggusc6/could_somebody_clarify_the_difference_bw/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggusc6/could_somebody_clarify_the_difference_bw/,155203,1589084681.0,0,,False,,,,
,learnmachinelearning," As the title says, me and probably many others have alot of free time now, I'd like to learn more about Cloud and probably have more hands-on experience rather than just theortical knowledge, I personally have digged abit into AWS technologies such as EC2, but I feel this is very minimal and can be futher improved, which cloud services do you find the most beneficial for a data scientist to learn?

Thanks!",t2_kfdra,False,,0,False,"I have alot of free time, I want to learn some cloud which technologies/cloud services should I mostly be learning about",[],r/learnmachinelearning,False,6,,0,,False,t3_ggorpx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589090712.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title says, me and probably many others have alot of free time now, I&amp;#39;d like to learn more about Cloud and probably have more hands-on experience rather than just theortical knowledge, I personally have digged abit into AWS technologies such as EC2, but I feel this is very minimal and can be futher improved, which cloud services do you find the most beneficial for a data scientist to learn?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggorpx,True,,Unchart3disOP,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggorpx/i_have_alot_of_free_time_i_want_to_learn_some/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggorpx/i_have_alot_of_free_time_i_want_to_learn_some/,155203,1589061912.0,0,,False,,,,
,learnmachinelearning,"So right now my CNN is getting about 40% accuracy on 12 classes, which isn't horrible for my first neural network, but whenever I change the network architecture(like num of neurons and layers) it just stops learning and I need to go back to 16,32,64. Is there any sort of patter to how to optimize the architecture?",t2_n3wq5,False,,0,False,Advice on optimizing CNN Architecture?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggq8s6,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589095816.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So right now my CNN is getting about 40% accuracy on 12 classes, which isn&amp;#39;t horrible for my first neural network, but whenever I change the network architecture(like num of neurons and layers) it just stops learning and I need to go back to 16,32,64. Is there any sort of patter to how to optimize the architecture?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggq8s6,True,,10macattack,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggq8s6/advice_on_optimizing_cnn_architecture/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggq8s6/advice_on_optimizing_cnn_architecture/,155203,1589067016.0,0,,False,,,,
,learnmachinelearning,"Hi guys,

I am currently at a loss for ideas. I need to create a sudoku solver in C++ that uses ML. I know how to solve sudoku, but the ML part simply beats me. 

I've read on different sites, articles and in different publications that reinforcement learning might be the way to go, since I already have a dataset and is mandatory to use it (this is a college project). And that RNN are a good idea too. And that wouldn't be a problem if coding in python, since it has all it's wonderful libraries. But what would you choose if you had to do this using C++ and some libraries for it?",t2_5uttszv3,False,,0,False,What would be the best approach for a ML algorithm that has to be written in C++ and solves sudoku?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggt84m,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1589106963.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I am currently at a loss for ideas. I need to create a sudoku solver in C++ that uses ML. I know how to solve sudoku, but the ML part simply beats me. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read on different sites, articles and in different publications that reinforcement learning might be the way to go, since I already have a dataset and is mandatory to use it (this is a college project). And that RNN are a good idea too. And that wouldn&amp;#39;t be a problem if coding in python, since it has all it&amp;#39;s wonderful libraries. But what would you choose if you had to do this using C++ and some libraries for it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggt84m,True,,ATiredRedHead,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggt84m/what_would_be_the_best_approach_for_a_ml/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggt84m/what_would_be_the_best_approach_for_a_ml/,155203,1589078163.0,0,,False,,,,
,learnmachinelearning,"Does anyone know of a dataset (preferably large-scale) for distinguishing programming code from text? I know this would be easy to put together, but just curious if one already exists out there in the wild (I couldn't find anything on Google Dataset Search)",t2_6f7z197p,False,,0,False,Datasets for code detection?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggry04,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589101984.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know of a dataset (preferably large-scale) for distinguishing programming code from text? I know this would be easy to put together, but just curious if one already exists out there in the wild (I couldn&amp;#39;t find anything on Google Dataset Search)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggry04,True,,awaythrow9508,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggry04/datasets_for_code_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggry04/datasets_for_code_detection/,155203,1589073184.0,0,,False,,,,
,learnmachinelearning,"I'm a student studying computer engineering and I'm planning on teaching myself machine learning concepts and Tensorflow over the summer with several Coursera courses. At the moment my plan of action is to do a theory of machine learning course in tandem with an intro to Tensorflow course. My question is, should I wait to do the Tensorflow course until after I've learned the theory of ML?",t2_38jhws92,False,,0,False,Learning theory in tandem with practice?,[],r/learnmachinelearning,False,6,,0,,False,t3_gglfoi,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589079852.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a student studying computer engineering and I&amp;#39;m planning on teaching myself machine learning concepts and Tensorflow over the summer with several Coursera courses. At the moment my plan of action is to do a theory of machine learning course in tandem with an intro to Tensorflow course. My question is, should I wait to do the Tensorflow course until after I&amp;#39;ve learned the theory of ML?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gglfoi,True,,Magma_not_Lava,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gglfoi/learning_theory_in_tandem_with_practice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gglfoi/learning_theory_in_tandem_with_practice/,155203,1589051052.0,0,,False,,,,
,learnmachinelearning,"Applying LSTMs to stock prices seems to be a fairly popular exercise, at least for learning purposes. As far as I could tell from googling, the fairly broad consensus is that it doesn't really work, as there's no much information to be gleened beyond the current stock price.

However, when looking for papers to back this up, the few I found seemed to show 'promising results'.  E.g.

[https://www.researchgate.net/profile/Adriano\_Pereira3/publication/318329563\_Stock\_market%27s\_price\_movement\_prediction\_with\_LSTM\_neural\_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf](https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf)

Which has a fair number of citations (however 55% accuracy doesn't seem particularly impressive if the dataset is skewed). Just wanted to double check my initial impressions were correct and the papers I've found are just poorly conducted, or if I'm wrong and LSTMs are actually used in stock price prediction (and some credible papers that show this).",t2_6dilbrzl,False,,0,False,LSTMs in stock price prediction,[],r/learnmachinelearning,False,6,,0,,False,t3_ggjkew,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1589073920.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Applying LSTMs to stock prices seems to be a fairly popular exercise, at least for learning purposes. As far as I could tell from googling, the fairly broad consensus is that it doesn&amp;#39;t really work, as there&amp;#39;s no much information to be gleened beyond the current stock price.&lt;/p&gt;

&lt;p&gt;However, when looking for papers to back this up, the few I found seemed to show &amp;#39;promising results&amp;#39;.  E.g.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf""&gt;https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Which has a fair number of citations (however 55% accuracy doesn&amp;#39;t seem particularly impressive if the dataset is skewed). Just wanted to double check my initial impressions were correct and the papers I&amp;#39;ve found are just poorly conducted, or if I&amp;#39;m wrong and LSTMs are actually used in stock price prediction (and some credible papers that show this).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggjkew,True,,econtextthrow,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggjkew/lstms_in_stock_price_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggjkew/lstms_in_stock_price_prediction/,155203,1589045120.0,0,,False,,,,
,learnmachinelearning,,t2_1jyhaoq,False,,0,False,LSTM and back propagation with numpy,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggdp8u,False,light,0.92,,public,10,0,{},,,False,[],,False,False,,{},Project,False,10,,False,default,False,,[],{},,,False,,1589051629.0,richtext,6,,,text,blog.varunajayasiri.com,False,,,,,,False,False,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggdp8u,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdp8u/lstm_and_back_propagation_with_numpy/,all_ads,False,http://blog.varunajayasiri.com/numpy_lstm.html,155203,1589022829.0,0,,False,,,,
,learnmachinelearning,,t2_5yd6bcu6,False,,0,False,5 CORE Data Science Skills You Should Master,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggpabf,False,light,0.4,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '5 CORE Data Science Skills You Should Master', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lfn6u2K4oHE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggpabf', 'height': 338}",Discussion,False,0,,False,https://b.thumbs.redditmedia.com/KSft4rolzdRmNGpfPaFJxSKlKfjpwQpdn92-e0JX3VE.jpg,False,,[],{},rich:video,,False,,1589092461.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?auto=webp&amp;s=bb84a9141cd85cde01f4e63a5be70bc129b4acbf', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e59deb112d8c3e60da8cb4147d7ae06c1f0658b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59a45d92b637e87cbbbe5b18e19915bd8143a4c5', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d8f4435bf0b6cdf655ed57e8465d75800c47bca', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'HGwIi2xzEWiaik-fE8Ih8Y2NovJck78ghH4hShArPss'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggpabf,True,,JK_Bielan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggpabf/5_core_data_science_skills_you_should_master/,all_ads,False,https://www.youtube.com/watch?v=lfn6u2K4oHE,155203,1589063661.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '5 CORE Data Science Skills You Should Master', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lfn6u2K4oHE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}}",False,,,,
,learnmachinelearning,"I started with Andrew Ng's Machine Learning course on Coursera last year but quit by week 5 due to school and not putting in enough time. I have decided to start again and have enrolled in the course again. 

I was thinking that having a discord to discuss each week's content would be helpful since that would be a study group apart from the discussions forum on Coursera which I don't really like. Let me know if any of you are interested and we can discuss on connecting through discord and setting up regular meetings. We can set goals and be accountable for the group's progress, trying to reach deadlines for discussions.",t2_67ndk8lg,False,,0,False,Discord for Ng's ML Course (Coursera),[],r/learnmachinelearning,False,6,,0,,False,t3_ggo29g,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589088376.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I started with Andrew Ng&amp;#39;s Machine Learning course on Coursera last year but quit by week 5 due to school and not putting in enough time. I have decided to start again and have enrolled in the course again. &lt;/p&gt;

&lt;p&gt;I was thinking that having a discord to discuss each week&amp;#39;s content would be helpful since that would be a study group apart from the discussions forum on Coursera which I don&amp;#39;t really like. Let me know if any of you are interested and we can discuss on connecting through discord and setting up regular meetings. We can set goals and be accountable for the group&amp;#39;s progress, trying to reach deadlines for discussions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggo29g,True,,newtonseitz,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggo29g/discord_for_ngs_ml_course_coursera/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggo29g/discord_for_ngs_ml_course_coursera/,155203,1589059576.0,0,,False,,,,
,learnmachinelearning,,t2_2xz6764h,False,,0,False,Why is reinforcement learning often used with neural networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggnddn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589086100.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggnddn,True,,User1377420,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggnddn/why_is_reinforcement_learning_often_used_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggnddn/why_is_reinforcement_learning_often_used_with/,155203,1589057300.0,0,,False,,,,
,learnmachinelearning,"I want to increase the output class size of a pre-trained YOLOV3 model, to detect an object (crosswalk) not originally existing in the COCO dataset it was trained with. I also want to keep the pre-existing detection accuracy for most of the original COCO objects.

The specific object I'm targeting are crosswalks, and my network code is an adaptation of [https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3\_tf2/models.py#L204](https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204)

How I think I want to accomplish this is via [transfer learning](https://medium.com/starschema-blog/transfer-learning-the-dos-and-donts-165729d66625). What I understand training requirements are:

* The new target object needs resemblance to original dataset objects the pre-trained model worked with. Crosswalks are primarily simple shapes with slight warping. The weight's I'm reusing were trained under COCO.
* I only reuse the n-1 layers of the pre-trained model and freeze those to avoid adjustments.
* Because I'm only fine tunning the final layer, my learning rate can be larger than 10\^-6 (used learning is 10\^-4)

# Problems

The general problem is after training, the model can't detect sidewalks nor the original objects. But this could be a symptom of the following:

# Sub Issue 1: Force Fit Layer Weights

The original model was built for 80 class outputs. Since I want to keep those as classification outputs, The model I train is built for 81 classes. This results in slight layer weights shape mismatch.

To resolve this I force set the weights by iterating through the problematic layer and identify which weights have mismatching shape. Those specific subset weights get adjusted using `np.resize(weights, target_size)`. This results in padding randomized weights of similar values to the respective weight set.

This ""Force Fit"" does not happen at ever layer, nor at every weight set in the respective layer. Here's an example log that might help paint the situation:

* a layer weights is a tuple of Nd-arrays. Depth is the index within that respective tuple
* `Shape at depth 5: (1, 1, 512, 255) | expected: (1, 1, 512, 258)`translates to `model.get_layers(""yolo_output_1"").get_weights[4]`

&amp;#x200B;

    Network consists of layers [['input', 'yolo_darknet', 'yolo_conv_0', 'yolo_conv_1', 'yolo_conv_2', 'yolo_output_0', 'yolo_output_1', 'yolo_output_2']]
    Transfering weights for layer [input]
    Transfer success
    Freezing layer [input]
    Transfering weights for layer [yolo_darknet]
    Transfer success
    Freezing layer [yolo_darknet]
    Transfering weights for layer [yolo_conv_0]
    Transfer success
    Freezing layer [yolo_conv_0]
    Transfering weights for layer [yolo_conv_1]
    Transfer success
    Freezing layer [yolo_conv_1]
    Transfering weights for layer [yolo_conv_2]
    Transfer success
    Freezing layer [yolo_conv_2]
    Transfering weights for layer [yolo_output_0]
    Reshaping layer [yolo_output_0] source weights to fit expected size
    Mismatch found at layer depth 5
    Shape at depth 5: (1, 1, 1024, 255)	| expected: (1, 1, 1024, 258)
    Mismatch found at layer depth 6
    Shape at depth 6: (255,)	| expected: (258,)
    Attempting to set reshaped weights to layer [yolo_output_0]
    Transfer success
    Freezing layer [yolo_output_0]
    Transfering weights for layer [yolo_output_1]
    Reshaping layer [yolo_output_1] source weights to fit expected size
    Mismatch found at layer depth 5
    Shape at depth 5: (1, 1, 512, 255)	| expected: (1, 1, 512, 258)
    Mismatch found at layer depth 6
    Shape at depth 6: (255,)	| expected: (258,)
    Attempting to set reshaped weights to layer [yolo_output_1]
    Transfer success
    Freezing layer [yolo_output_1]

# Questions (Part 1)

1. Is this ""Force Fit"" approach stupid? should I just train everything from scratch if I want to increase the possible classification output while inheriting the pre-trained models accuracy?
2. If the ""Force Fit"" isn't an issue, do I need to reconsider how many layers I leave unfrozen for training? What I understand of DNN layers is the bottom most builds up simple-generic understanding of structures. So if I shave off the top most on a pre-trained model that detects traffic-lights/people, to now work with crosswalks, then I'm far too deep in the network layers and should consider a smaller architecture?

# Sub Issue 2: Not Enough Data

My current annotated data of crosswalks images is about 400 images. I partition the dataset as 300 to train, and 100 to validate. This [Intel case study of transfer learning](https://software.intel.com/content/www/us/en/develop/articles/traffic-light-detection-using-the-tensorflow-object-detection-api.html) used around 600 images for training.

# Questions (Part 2)

1. Is the minimum training data requirements to build on-top of COCO dataset typically 600-1k sized?
2. My goal is to have my final model inherit the detection accuracy of some COCO objects. So is it correct to consider that I need to provide training data for those respective objects, in addition to my new target object (cross walks)? If I do does each object also require 600-1k sized samples each?
3. If I need to retrain the model with the COCO objects I want to retain in my final model, this can potentially reduce my classification class output to less than 80. Should I force fit the loaded weights to the smaller network size if there's a mismatch? Or should I still keep network size equipped to predict 80 output types, as they'll default to zeros due to not having representation in the training set?",t2_2a11d7b5,False,,0,False,"How to expand detection class output in a pre-trained model, and retrain original objects classification success?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggncn6,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,1589057408.0,,[],{},self,,True,,1589086026.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to increase the output class size of a pre-trained YOLOV3 model, to detect an object (crosswalk) not originally existing in the COCO dataset it was trained with. I also want to keep the pre-existing detection accuracy for most of the original COCO objects.&lt;/p&gt;

&lt;p&gt;The specific object I&amp;#39;m targeting are crosswalks, and my network code is an adaptation of &lt;a href=""https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204""&gt;https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How I think I want to accomplish this is via &lt;a href=""https://medium.com/starschema-blog/transfer-learning-the-dos-and-donts-165729d66625""&gt;transfer learning&lt;/a&gt;. What I understand training requirements are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The new target object needs resemblance to original dataset objects the pre-trained model worked with. Crosswalks are primarily simple shapes with slight warping. The weight&amp;#39;s I&amp;#39;m reusing were trained under COCO.&lt;/li&gt;
&lt;li&gt;I only reuse the n-1 layers of the pre-trained model and freeze those to avoid adjustments.&lt;/li&gt;
&lt;li&gt;Because I&amp;#39;m only fine tunning the final layer, my learning rate can be larger than 10^-6 (used learning is 10^-4)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Problems&lt;/h1&gt;

&lt;p&gt;The general problem is after training, the model can&amp;#39;t detect sidewalks nor the original objects. But this could be a symptom of the following:&lt;/p&gt;

&lt;h1&gt;Sub Issue 1: Force Fit Layer Weights&lt;/h1&gt;

&lt;p&gt;The original model was built for 80 class outputs. Since I want to keep those as classification outputs, The model I train is built for 81 classes. This results in slight layer weights shape mismatch.&lt;/p&gt;

&lt;p&gt;To resolve this I force set the weights by iterating through the problematic layer and identify which weights have mismatching shape. Those specific subset weights get adjusted using &lt;code&gt;np.resize(weights, target_size)&lt;/code&gt;. This results in padding randomized weights of similar values to the respective weight set.&lt;/p&gt;

&lt;p&gt;This &amp;quot;Force Fit&amp;quot; does not happen at ever layer, nor at every weight set in the respective layer. Here&amp;#39;s an example log that might help paint the situation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a layer weights is a tuple of Nd-arrays. Depth is the index within that respective tuple&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shape at depth 5: (1, 1, 512, 255) | expected: (1, 1, 512, 258)&lt;/code&gt;translates to &lt;code&gt;model.get_layers(&amp;quot;yolo_output_1&amp;quot;).get_weights[4]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Network consists of layers [[&amp;#39;input&amp;#39;, &amp;#39;yolo_darknet&amp;#39;, &amp;#39;yolo_conv_0&amp;#39;, &amp;#39;yolo_conv_1&amp;#39;, &amp;#39;yolo_conv_2&amp;#39;, &amp;#39;yolo_output_0&amp;#39;, &amp;#39;yolo_output_1&amp;#39;, &amp;#39;yolo_output_2&amp;#39;]]
Transfering weights for layer [input]
Transfer success
Freezing layer [input]
Transfering weights for layer [yolo_darknet]
Transfer success
Freezing layer [yolo_darknet]
Transfering weights for layer [yolo_conv_0]
Transfer success
Freezing layer [yolo_conv_0]
Transfering weights for layer [yolo_conv_1]
Transfer success
Freezing layer [yolo_conv_1]
Transfering weights for layer [yolo_conv_2]
Transfer success
Freezing layer [yolo_conv_2]
Transfering weights for layer [yolo_output_0]
Reshaping layer [yolo_output_0] source weights to fit expected size
Mismatch found at layer depth 5
Shape at depth 5: (1, 1, 1024, 255) | expected: (1, 1, 1024, 258)
Mismatch found at layer depth 6
Shape at depth 6: (255,)    | expected: (258,)
Attempting to set reshaped weights to layer [yolo_output_0]
Transfer success
Freezing layer [yolo_output_0]
Transfering weights for layer [yolo_output_1]
Reshaping layer [yolo_output_1] source weights to fit expected size
Mismatch found at layer depth 5
Shape at depth 5: (1, 1, 512, 255)  | expected: (1, 1, 512, 258)
Mismatch found at layer depth 6
Shape at depth 6: (255,)    | expected: (258,)
Attempting to set reshaped weights to layer [yolo_output_1]
Transfer success
Freezing layer [yolo_output_1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Questions (Part 1)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Is this &amp;quot;Force Fit&amp;quot; approach stupid? should I just train everything from scratch if I want to increase the possible classification output while inheriting the pre-trained models accuracy?&lt;/li&gt;
&lt;li&gt;If the &amp;quot;Force Fit&amp;quot; isn&amp;#39;t an issue, do I need to reconsider how many layers I leave unfrozen for training? What I understand of DNN layers is the bottom most builds up simple-generic understanding of structures. So if I shave off the top most on a pre-trained model that detects traffic-lights/people, to now work with crosswalks, then I&amp;#39;m far too deep in the network layers and should consider a smaller architecture?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;Sub Issue 2: Not Enough Data&lt;/h1&gt;

&lt;p&gt;My current annotated data of crosswalks images is about 400 images. I partition the dataset as 300 to train, and 100 to validate. This &lt;a href=""https://software.intel.com/content/www/us/en/develop/articles/traffic-light-detection-using-the-tensorflow-object-detection-api.html""&gt;Intel case study of transfer learning&lt;/a&gt; used around 600 images for training.&lt;/p&gt;

&lt;h1&gt;Questions (Part 2)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Is the minimum training data requirements to build on-top of COCO dataset typically 600-1k sized?&lt;/li&gt;
&lt;li&gt;My goal is to have my final model inherit the detection accuracy of some COCO objects. So is it correct to consider that I need to provide training data for those respective objects, in addition to my new target object (cross walks)? If I do does each object also require 600-1k sized samples each?&lt;/li&gt;
&lt;li&gt;If I need to retrain the model with the COCO objects I want to retain in my final model, this can potentially reduce my classification class output to less than 80. Should I force fit the loaded weights to the smaller network size if there&amp;#39;s a mismatch? Or should I still keep network size equipped to predict 80 output types, as they&amp;#39;ll default to zeros due to not having representation in the training set?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?auto=webp&amp;s=29c6af9777795fc4a07c9bd886cf006df323b2e5', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ac66d19f565defbcb4ef10a1c1f7703091a3cbb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e4d60edea4e2c5bd0bbf274ec777b04ee35040', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'hiozEMqFCp1Wb4R27LrsnnDsknwoNS55yOsGQVAZ4_o'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggncn6,True,,SuspiciousSimple,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggncn6/how_to_expand_detection_class_output_in_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggncn6/how_to_expand_detection_class_output_in_a/,155203,1589057226.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

[Accuracy as a function of epoch and batch sizes in one specific case](https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;format=pjpg&amp;auto=webp&amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040)

I'm just trying to understand machine learning. I was working a simple example and the author gave certain values for epoch and batch size, and said that one just develops 'a feel' for what  the right numbers are. With 0 experience, I wanted to understand what considerations go into that guesstimate. I trained the same model on the same dataset, varying batch size and number of epochs, trying to visualize where the best accuracy lies. I'm sure it varies per dataset, per model, per run on the same machine, but I was hoping for something to develop an intuition on. I have to say I'm still in the dark on what to use as numbers for these two factors. How does one know where to set these?",t2_3ybjnaub,False,,0,False,How do you intuit epoch and batch sizes?,[],r/learnmachinelearning,False,6,,0,139.0,False,t3_ggiyzf,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/ukLAHqYSaYf9rwiL6OHn7lJqztQYS8Hpdo3jmGx33is.jpg,False,,[],{},,,True,,1589072056.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040""&gt;Accuracy as a function of epoch and batch sizes in one specific case&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m just trying to understand machine learning. I was working a simple example and the author gave certain values for epoch and batch size, and said that one just develops &amp;#39;a feel&amp;#39; for what  the right numbers are. With 0 experience, I wanted to understand what considerations go into that guesstimate. I trained the same model on the same dataset, varying batch size and number of epochs, trying to visualize where the best accuracy lies. I&amp;#39;m sure it varies per dataset, per model, per run on the same machine, but I was hoping for something to develop an intuition on. I have to say I&amp;#39;m still in the dark on what to use as numbers for these two factors. How does one know where to set these?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggiyzf,True,,TechIsSoCool,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggiyzf/how_do_you_intuit_epoch_and_batch_sizes/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggiyzf/how_do_you_intuit_epoch_and_batch_sizes/,155203,1589043256.0,0,,False,,,"{'3dpyb9fcqrx41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 107, 'x': 108, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77c0079044709e68e25ede361f5590b90382f246'}, {'y': 215, 'x': 216, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6a5bd5cf71e94cac334bb652de74717ea577582'}, {'y': 319, 'x': 320, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d63db89943f5c45b8bd28ff5cd3c325a28645a26'}, {'y': 638, 'x': 640, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73d1e20bd272730449bdf303e2634343bef86350'}, {'y': 957, 'x': 960, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73d4b210c2084a3975716e98e093cdbb5b51a620'}, {'y': 1076, 'x': 1080, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28987acbd1ec4a96bf0af7b9f0e8c1946b74558e'}], 's': {'y': 2577, 'x': 2585, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;format=pjpg&amp;auto=webp&amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040'}, 'id': '3dpyb9fcqrx41'}}",
,learnmachinelearning,Transformer i have seen on papers typically has 12 heads and 12 layers. Does it suffer from vanishing gradient and how is the problem overcome normally?,t2_3qui5vln,False,,0,False,Does transformer suffer from vanishing gradient problem?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggggaj,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589063754.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Transformer i have seen on papers typically has 12 heads and 12 layers. Does it suffer from vanishing gradient and how is the problem overcome normally?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggggaj,True,,lifesaboxofchoco,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggggaj/does_transformer_suffer_from_vanishing_gradient/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggggaj/does_transformer_suffer_from_vanishing_gradient/,155203,1589034954.0,0,,False,,,,True
,learnmachinelearning,"I wanna start learning AI /ML.
I learnt java (2+ years) + now python(6+ months) 
I learned flask/numpy/pillow..etc. 

Now kinda lost where to start? 
In my CS Bachelor course at Uni we took Intro to AI needless to say (Agent Env model and three jar problems) didnâ€™t really tell me what next to do and how AI and ML works. 

So I have zero idea now at what next ?
Do I jump to sci-kit and NLTK or maybe tenserflow ?  
Or maybe there are some theoretical knowledge first ? Am too lost right now .
Would ohevif there is a road map just how web dev have one.

Am kinda scared that I need to go under heavy math courses for a year and then start learn another year AI then another year ML.

(Sry for my bad English)",t2_6cacicc9,False,,0,False,Lost at how to start learning ML/AI as CS student.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggmmb7,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,1589060122.0,,[],{},,,True,,1589083641.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wanna start learning AI /ML.
I learnt java (2+ years) + now python(6+ months) 
I learned flask/numpy/pillow..etc. &lt;/p&gt;

&lt;p&gt;Now kinda lost where to start? 
In my CS Bachelor course at Uni we took Intro to AI needless to say (Agent Env model and three jar problems) didnâ€™t really tell me what next to do and how AI and ML works. &lt;/p&gt;

&lt;p&gt;So I have zero idea now at what next ?
Do I jump to sci-kit and NLTK or maybe tenserflow ?&lt;br/&gt;
Or maybe there are some theoretical knowledge first ? Am too lost right now .
Would ohevif there is a road map just how web dev have one.&lt;/p&gt;

&lt;p&gt;Am kinda scared that I need to go under heavy math courses for a year and then start learn another year AI then another year ML.&lt;/p&gt;

&lt;p&gt;(Sry for my bad English)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggmmb7,True,,r-_-mark,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggmmb7/lost_at_how_to_start_learning_mlai_as_cs_student/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggmmb7/lost_at_how_to_start_learning_mlai_as_cs_student/,155203,1589054841.0,0,,False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,TensorFlow 2.0 Tutorial : Deploying Machine Learning models with REST API,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggdbeh,False,dark,0.81,,public,7,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Deploying Tensorflow 2.0 with REST API', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zpYAQIA1z4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggdbeh', 'height': 338}",,False,7,,False,https://b.thumbs.redditmedia.com/IFPqEBuGC5Ud78kuKEcInYmhebcC-mZ9KLK87NbRx_Q.jpg,False,,[],{},rich:video,,False,,1589049730.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?auto=webp&amp;s=1c0815185cb5762085f76a69dda22b922d882686', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f95e26d6b21270b0ccda5fd3a61b5f9a47691198', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75be5c3ec52cc7057a0423bb885aeef3248dbff2', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26a712e955ad607fc8f537b84c12300174a2b7bb', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'XxlnRd7qWzk9yKonMvAHgTxuCHRGbObr__dvBol8G-Q'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdbeh,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdbeh/tensorflow_20_tutorial_deploying_machine_learning/,all_ads,False,https://youtu.be/zpYAQIA1z4Y,155203,1589020930.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Deploying Tensorflow 2.0 with REST API', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zpYAQIA1z4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,,,,
,learnmachinelearning,"Hi guys,

I am working on the FIFA 19 data set for a project in college using R.
I was wondering if there was a algorithm which could help me select the best team from the dataset of all the players under different parameters. Such as age, nation, or for a given budget. 

Any and all help would be appreciated.

Thanks.",t2_5cmgmwoi,False,,0,False,Suggestions for algorithms for selecting teams from FIFA 19 dataset from kaggel,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggli77,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1589080063.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I am working on the FIFA 19 data set for a project in college using R.
I was wondering if there was a algorithm which could help me select the best team from the dataset of all the players under different parameters. Such as age, nation, or for a given budget. &lt;/p&gt;

&lt;p&gt;Any and all help would be appreciated.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggli77,True,,imyaash,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggli77/suggestions_for_algorithms_for_selecting_teams/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggli77/suggestions_for_algorithms_for_selecting_teams/,155203,1589051263.0,0,,False,,,,
,learnmachinelearning,"I have been working on a video series that uses Python to build a variety of cool projects in Machine Learning using just Python and recently started a tutorial series on Python. I would love to have constructive feedback in order to improvise on any particular front that you want to suggest.

Some of the features of both the series are these:

1. Linear Regression Project using Python (we work with a dataset)
2. Implementation of Multiple Linear Regression using Gradient Descent Algorithm (Working with a dataset)
3. Intuition and Conceptual Videos
4. As a pre-requisite, I have posted some Python Tutorial Series (both are in progress and ongoing series)

This is what we will be covering from absolute scratch in the ongoing series. I have added some videos already (12+) so that would be enough for you to know how the content is. 

https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274

I have already put up around 13 videos  on Python and more than 10 videos on Machine Learning in the respective YouTube Playlists : [Python Tutorials with Projects](https://www.youtube.com/watch?v=q6V0cBzQ7bc&amp;list=PLXgqhtspYCM8eUX94Ng4SQ-3kWMTZ7zFM)  &amp; [Machine Learning Tutorials with Projects](https://www.youtube.com/playlist?list=PLXgqhtspYCM9-eMFw31mJZnQFYjj2SQLO) and  will be uploading more content on a regular basis soon.",t2_51mclnu7,False,,0,False,FREE MACHINE LEARNING TUTORIAL SERIES ALONG WITH PYTHON (FROM SCRATCH),[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfpypp,False,dark,0.94,,public,390,0,{},140.0,,False,[],,False,False,,{},,False,390,,False,https://b.thumbs.redditmedia.com/bl9FBWcj_JAfgMWb6rBptoCaIYXcKMCVUOzqdyg3jfE.jpg,False,,[],{},self,,True,,1588958841.0,text,6,,,text,self.learnmachinelearning,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working on a video series that uses Python to build a variety of cool projects in Machine Learning using just Python and recently started a tutorial series on Python. I would love to have constructive feedback in order to improvise on any particular front that you want to suggest.&lt;/p&gt;

&lt;p&gt;Some of the features of both the series are these:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Linear Regression Project using Python (we work with a dataset)&lt;/li&gt;
&lt;li&gt;Implementation of Multiple Linear Regression using Gradient Descent Algorithm (Working with a dataset)&lt;/li&gt;
&lt;li&gt;Intuition and Conceptual Videos&lt;/li&gt;
&lt;li&gt;As a pre-requisite, I have posted some Python Tutorial Series (both are in progress and ongoing series)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is what we will be covering from absolute scratch in the ongoing series. I have added some videos already (12+) so that would be enough for you to know how the content is. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274""&gt;https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have already put up around 13 videos  on Python and more than 10 videos on Machine Learning in the respective YouTube Playlists : &lt;a href=""https://www.youtube.com/watch?v=q6V0cBzQ7bc&amp;amp;list=PLXgqhtspYCM8eUX94Ng4SQ-3kWMTZ7zFM""&gt;Python Tutorials with Projects&lt;/a&gt;  &amp;amp; &lt;a href=""https://www.youtube.com/playlist?list=PLXgqhtspYCM9-eMFw31mJZnQFYjj2SQLO""&gt;Machine Learning Tutorials with Projects&lt;/a&gt; and  will be uploading more content on a regular basis soon.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?auto=webp&amp;s=71171898179a0e65790a09a05886abc30af3be34', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7688ac46afc0b87978baa7f6c2f22e68572767f6', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa7d86a201fa49f7d6ad1dfec22a921704cd2b83', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90b80a83a872775c1c925d01c846b6b64377ae16', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zq0x9-UdUCMi4WdCmIecPstk_D-SEz9vNP1XVsIu850'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfpypp,True,,TheNerdyDevYT,,48,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfpypp/free_machine_learning_tutorial_series_along_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfpypp/free_machine_learning_tutorial_series_along_with/,155203,1588930041.0,1,,False,,,"{'mgyugbuweix41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=472ca82dfe9c9d5a0e1afbd92f952df4f2bdb152'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fdc98131b22dcb15eea21cabfa5d341fa5ba8124'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44cfd139f2cf2a75e32367c8f1adec4a7fd1771f'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c85919667399855f3d7701de4dbac3cb60f9015'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2023164290504f8ee447a709d6ac8c780724394'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3009d5e669f7676f59adb581d0415785598ec89'}], 's': {'y': 720, 'x': 1280, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274'}, 'id': 'mgyugbuweix41'}}",
,learnmachinelearning,,t2_16diqth,False,,0,False,A Commit History of BERT and its Forks,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggf5r8,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,default,False,,[],{},,,False,,1589058547.0,richtext,6,,,text,amitness.com,False,,,,,,False,False,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggf5r8,True,,amitness,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggf5r8/a_commit_history_of_bert_and_its_forks/,all_ads,False,https://amitness.com/2020/05/git-log-of-bert/,155203,1589029747.0,0,,False,,,,
,learnmachinelearning,"I'm interested in creating a crude working demo of this: [https://arxiv.org/pdf/2004.02349.pdf](https://arxiv.org/pdf/2004.02349.pdf)

Essentially, it's using state of the art NLP (specifically, BERT) to map statements like:

&amp;#x200B;

&gt;How many world champions are there with only one reign?

to

    select count(*) where column(""No. of reigns"") == 1;

I have a solid background in python, and an intermediate level of experience with ML (mostly sklearn and fast.ai) on tabular data.  But I am completely self-taught, and have a very weak background in both math and NLP, i'm stronger on the practical/coding side.  As such, the 'mathy' explanation in that paper is tough for me to translate into code intuitively.

Any advice?  Anybody want to collaborate?  give me breadcrumbs and I'll do the heavy lifting...",t2_702gf,False,,0,False,how to turn an ARVIX paper into working code example?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggixcz,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1589043413.0,,[],{},,,True,,1589071919.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m interested in creating a crude working demo of this: &lt;a href=""https://arxiv.org/pdf/2004.02349.pdf""&gt;https://arxiv.org/pdf/2004.02349.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Essentially, it&amp;#39;s using state of the art NLP (specifically, BERT) to map statements like:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How many world champions are there with only one reign?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select count(*) where column(&amp;quot;No. of reigns&amp;quot;) == 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have a solid background in python, and an intermediate level of experience with ML (mostly sklearn and fast.ai) on tabular data.  But I am completely self-taught, and have a very weak background in both math and NLP, i&amp;#39;m stronger on the practical/coding side.  As such, the &amp;#39;mathy&amp;#39; explanation in that paper is tough for me to translate into code intuitively.&lt;/p&gt;

&lt;p&gt;Any advice?  Anybody want to collaborate?  give me breadcrumbs and I&amp;#39;ll do the heavy lifting...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggixcz,True,,ezeeetm,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggixcz/how_to_turn_an_arvix_paper_into_working_code/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggixcz/how_to_turn_an_arvix_paper_into_working_code/,155203,1589043119.0,1,,False,,,,
,learnmachinelearning,"After a lot of work I've created a machine learning api for asking questions, but I'm having quite some trouble publishing it.  
I've tried using the Google App Engine but it crashes due to my large model which I'm uploading.   
Heroku didn't work neither because the storage to low.

Any suggestions on which cloud provider I could use would be really helpful (not too complicated).",t2_4cohyf78,False,,0,False,Can someone help me publish my api to production,[],r/learnmachinelearning,False,6,,0,,False,t3_ggeyfi,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589057688.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After a lot of work I&amp;#39;ve created a machine learning api for asking questions, but I&amp;#39;m having quite some trouble publishing it.&lt;br/&gt;
I&amp;#39;ve tried using the Google App Engine but it crashes due to my large model which I&amp;#39;m uploading.&lt;br/&gt;
Heroku didn&amp;#39;t work neither because the storage to low.&lt;/p&gt;

&lt;p&gt;Any suggestions on which cloud provider I could use would be really helpful (not too complicated).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggeyfi,True,,mariusjohan,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggeyfi/can_someone_help_me_publish_my_api_to_production/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggeyfi/can_someone_help_me_publish_my_api_to_production/,155203,1589028888.0,0,,False,,,,
,learnmachinelearning,,t2_xf2t5,False,,0,False,Data Science. Probability distributions,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_gg62za,False,dark,0.86,,public,15,0,{},140.0,,False,[],,False,False,,{},,False,15,,False,https://b.thumbs.redditmedia.com/MUCTC_5KCGeUgULClVsSjViN5JVoAPIjpWKDL_7UiAc.jpg,False,,[],{},link,,False,,1589016282.0,text,6,,,text,luminousmen.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?auto=webp&amp;s=051606fa10335d9dbcd97728c06b2894853312c4', 'width': 1400, 'height': 1049}, 'resolutions': [{'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf775e8b7ad6e70779fab043c940ad7226a8c45c', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b80dc192efe7f6e7ab7f8cae88a23b840e37636', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88d021c095649ac06f5445e28baf70edb9f06c8a', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b25959d4294aae27aa2a70f758d5b3bbc75fb7fc', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc918eb45ea0de5293b6f953bae4776e7b2447b5', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=116ddd61c5f9de2d0da4a6a15b0db04c3c797452', 'width': 1080, 'height': 809}], 'variants': {}, 'id': '1jYcsUcnKCWVynpCtNXuF3QYUAsWpKbDM4oRP8DxhXc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg62za,True,,luminoumen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg62za/data_science_probability_distributions/,all_ads,False,https://luminousmen.com/post/data-science-probability-distributions,155203,1588987482.0,0,,False,,,,
,learnmachinelearning,"Lets consider [this image](https://imgur.com/a/du3qznX) with some notation on a [3 layer neural network](https://i.imgur.com/VE8FcI4.png) with 2 inputs, 3 hidden neurons and 1 out put.

&amp;#x200B;

Is this math basically telling me that to back propagate my error through the network I need to:

1. Find  âˆ‚ J/ âˆ‚ z^((3))  =  âˆ‚ J/ âˆ‚ Å·  \*  âˆ‚Å·/âˆ‚z^((3)) =  ð›¿^((3))
2. Then multiply my error through W^((2))
3. Take the weights multiplied by the error and transpose it so that when the weights are cross multiplied by the vector gradient of the derivative of the activation function, ð›¿^((2)) , becomes a scalar value.
4. Finally update the weights(W^((1))) with ð›¿^((2))X^(T)

The last part I am confused about because it seems like ð›¿^((2))X^(T) becomes a 1x2 matrix and W^((1)) is a 2x3 matrix. How does that work out?

I have read multiple resources on BP including [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html) (multiple times). I just need some conversation about it please.",t2_7jjem,False,,0,False,I'd like help understanding the maths behind Back propagation,[],r/learnmachinelearning,False,6,,0,,False,t3_ggig40,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1589070445.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lets consider &lt;a href=""https://imgur.com/a/du3qznX""&gt;this image&lt;/a&gt; with some notation on a &lt;a href=""https://i.imgur.com/VE8FcI4.png""&gt;3 layer neural network&lt;/a&gt; with 2 inputs, 3 hidden neurons and 1 out put.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is this math basically telling me that to back propagate my error through the network I need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Find  âˆ‚ J/ âˆ‚ z&lt;sup&gt;(3&lt;/sup&gt;)  =  âˆ‚ J/ âˆ‚ Å·  *  âˆ‚Å·/âˆ‚z&lt;sup&gt;(3&lt;/sup&gt;) =  ð›¿&lt;sup&gt;(3&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;Then multiply my error through W&lt;sup&gt;(2&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;Take the weights multiplied by the error and transpose it so that when the weights are cross multiplied by the vector gradient of the derivative of the activation function, ð›¿&lt;sup&gt;(2&lt;/sup&gt;) , becomes a scalar value.&lt;/li&gt;
&lt;li&gt;Finally update the weights(W&lt;sup&gt;(1&lt;/sup&gt;)) with ð›¿&lt;sup&gt;(2&lt;/sup&gt;)X&lt;sup&gt;T&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The last part I am confused about because it seems like ð›¿&lt;sup&gt;(2&lt;/sup&gt;)X&lt;sup&gt;T&lt;/sup&gt; becomes a 1x2 matrix and W&lt;sup&gt;(1&lt;/sup&gt;) is a 2x3 matrix. How does that work out?&lt;/p&gt;

&lt;p&gt;I have read multiple resources on BP including &lt;a href=""http://neuralnetworksanddeeplearning.com/chap2.html""&gt;http://neuralnetworksanddeeplearning.com/chap2.html&lt;/a&gt; (multiple times). I just need some conversation about it please.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?auto=webp&amp;s=5dcba14cfa9a087ba0c5abd12bcc149210f6f04b', 'width': 1160, 'height': 1036}, 'resolutions': [{'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a89fca42af5a62be0e7de1815ae5e59a71f6647', 'width': 108, 'height': 96}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f22d48c638c5b05a94b9ebef97bba758bc2d1e9', 'width': 216, 'height': 192}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d33a332c8b9e00d89798644e4f37dc1a5b77bc3a', 'width': 320, 'height': 285}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc30a8196453fd108b0deb90c8ca29707d2d0d1b', 'width': 640, 'height': 571}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8d6f7099ccb00f1965a189aa74208f76026531d', 'width': 960, 'height': 857}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c22aa875d9a568bebe07590320f2e59e578e7336', 'width': 1080, 'height': 964}], 'variants': {}, 'id': 'ryYx1suE-qF2PtBO9I6gK0yoP448CugMTk3sww4kjKM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggig40,True,,raidicy,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggig40/id_like_help_understanding_the_maths_behind_back/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggig40/id_like_help_understanding_the_maths_behind_back/,155203,1589041645.0,0,,False,,,,
,learnmachinelearning,"Hey guys.. I've been on this for the past few days and couldn't figure it out. Posted on various groups,  StackOverflow etc and got suggestions from many users. I implemented these suggestions into the code shown below, but still having the same issue. Sorry for the lengthy post, but I want to be as clear as possible. All relevant code snippets are shown below:

Setting up image paths:

    imagepaths = []
    
    for root, dirs, files in os.walk(""."", topdown=False): 
      for name in files:
        path = os.path.join(root, name)
        if path.endswith(""jpg""): # We want only the images
          imagepaths.append(path)

Loading into arrays, preprocessing:

    X = [] # Image data
    y = [] # Labels
    
    datagen = ImageDataGenerator(rescale=1./255, samplewise_center=True)
    
    # Loops through imagepaths to load images and labels into arrays
    for path in imagepaths:
      img = cv2.imread(path) # Reads image and returns np.array
      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY) #find rgb 
      img = cv2.resize(img, (75, 75)) # Reduce image size so training can be faster
      img = image.img_to_array(img)
      img = datagen.standardize(img)
      X.append(img)
      
      # Processing label in image path
      category = path.split(""\\"")[1]
      #print(category)
      split = (category.split(""_""))     
      if int(split[0]) == 0:
        label = int(split[1])
      else:
        label = int(split[0])
      y.append(label)
    
    # Turn X and y into np.array to speed up train_test_split
    
    X = np.array(X, dtype=""float32"") #ORIGINAL uint8
    X = X.reshape(len(imagepaths), 75, 75, 1) # Needed to reshape so CNN knows it's different images, 1 for bw change to 3 for rgb
    y = np.array(y)
    tf.keras.utils.to_categorical(X, num_classes=None, dtype=""float32"")
    tf.keras.utils.to_categorical(y, num_classes=None, dtype=""float32"")

Creating test set:

    ts = 0.3 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)

Creating model. Yes I know its super small, just 1 layer, but I was suggested to cut down to start from the base and build up. Originally it was 5 layers, but the results are still the same.

    model = Sequential()
    
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(75, 75, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(26, activation='softmax'))

Compiling the model. And fitting. I was told that the gradient could be exploding, so was suggested to add the first line with the clipnorm..

    adam = keras.optimizers.Adam(clipnorm=1.)
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  
    
    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1, validation_data=(X_test, y_test))   

The final training can be seen here. The issues are, losses are NAN and accuracies are 0.

    Train on 54600 samples, validate on 23400 samples
    Epoch 1/5
    54600/54600 [==============================] - 14s 265us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 2/5
    54600/54600 [==============================] - 15s 269us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 3/5
    54600/54600 [==============================] - 15s 273us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 4/5
    54600/54600 [==============================] - 15s 267us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 5/5
    54600/54600 [==============================] - 14s 263us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00

Here are a list of things which I did wrong and was suggested to do by others:

1. I didn't standardize the data originally - So, I did ImageDataGenerator, rescaled, and standardized it.
2. I was suggested to turn the data to categorical, which I did use the to\_categorical function (I think i did that right) but I'm not sure if there's anything else required.
3. Reduce model complexity. I did that brought it to only one layer to debug.
4. Possible exploding gradient - so changed the adam optimizer with clipnorm = 1

BACKGROUND: This model trains and recognizes the 26 letters of the alphabet. I know the dataset is fine because when I use it to train a model for 10 letters at a time (A-J) for example it works fine. The issue is only when I go from 10-26. Yes, I did try to change the dense to 26 on the original code but that did not work.

I've been staring at this and trying everything for the past two days...

ANY HELP IS APPRECIATED",t2_7ajy1,False,,0,False,"Loss of NAN, Accuracy of 0 - Any idea why? Full code provided..",[],r/learnmachinelearning,False,6,,0,,False,t3_ggi8eg,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589069802.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys.. I&amp;#39;ve been on this for the past few days and couldn&amp;#39;t figure it out. Posted on various groups,  StackOverflow etc and got suggestions from many users. I implemented these suggestions into the code shown below, but still having the same issue. Sorry for the lengthy post, but I want to be as clear as possible. All relevant code snippets are shown below:&lt;/p&gt;

&lt;p&gt;Setting up image paths:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;imagepaths = []

for root, dirs, files in os.walk(&amp;quot;.&amp;quot;, topdown=False): 
  for name in files:
    path = os.path.join(root, name)
    if path.endswith(&amp;quot;jpg&amp;quot;): # We want only the images
      imagepaths.append(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loading into arrays, preprocessing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = [] # Image data
y = [] # Labels

datagen = ImageDataGenerator(rescale=1./255, samplewise_center=True)

# Loops through imagepaths to load images and labels into arrays
for path in imagepaths:
  img = cv2.imread(path) # Reads image and returns np.array
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY) #find rgb 
  img = cv2.resize(img, (75, 75)) # Reduce image size so training can be faster
  img = image.img_to_array(img)
  img = datagen.standardize(img)
  X.append(img)

  # Processing label in image path
  category = path.split(&amp;quot;\\&amp;quot;)[1]
  #print(category)
  split = (category.split(&amp;quot;_&amp;quot;))     
  if int(split[0]) == 0:
    label = int(split[1])
  else:
    label = int(split[0])
  y.append(label)

# Turn X and y into np.array to speed up train_test_split

X = np.array(X, dtype=&amp;quot;float32&amp;quot;) #ORIGINAL uint8
X = X.reshape(len(imagepaths), 75, 75, 1) # Needed to reshape so CNN knows it&amp;#39;s different images, 1 for bw change to 3 for rgb
y = np.array(y)
tf.keras.utils.to_categorical(X, num_classes=None, dtype=&amp;quot;float32&amp;quot;)
tf.keras.utils.to_categorical(y, num_classes=None, dtype=&amp;quot;float32&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating test set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ts = 0.3 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating model. Yes I know its super small, just 1 layer, but I was suggested to cut down to start from the base and build up. Originally it was 5 layers, but the results are still the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()

model.add(Conv2D(32, (3, 3), activation=&amp;#39;relu&amp;#39;, input_shape=(75, 75, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(26, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling the model. And fitting. I was told that the gradient could be exploding, so was suggested to add the first line with the clipnorm..&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adam = keras.optimizers.Adam(clipnorm=1.)

model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])  

model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1, validation_data=(X_test, y_test))   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final training can be seen here. The issues are, losses are NAN and accuracies are 0.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train on 54600 samples, validate on 23400 samples
Epoch 1/5
54600/54600 [==============================] - 14s 265us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/5
54600/54600 [==============================] - 15s 269us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/5
54600/54600 [==============================] - 15s 273us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 4/5
54600/54600 [==============================] - 15s 267us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 5/5
54600/54600 [==============================] - 14s 263us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are a list of things which I did wrong and was suggested to do by others:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I didn&amp;#39;t standardize the data originally - So, I did ImageDataGenerator, rescaled, and standardized it.&lt;/li&gt;
&lt;li&gt;I was suggested to turn the data to categorical, which I did use the to_categorical function (I think i did that right) but I&amp;#39;m not sure if there&amp;#39;s anything else required.&lt;/li&gt;
&lt;li&gt;Reduce model complexity. I did that brought it to only one layer to debug.&lt;/li&gt;
&lt;li&gt;Possible exploding gradient - so changed the adam optimizer with clipnorm = 1&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BACKGROUND: This model trains and recognizes the 26 letters of the alphabet. I know the dataset is fine because when I use it to train a model for 10 letters at a time (A-J) for example it works fine. The issue is only when I go from 10-26. Yes, I did try to change the dense to 26 on the original code but that did not work.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been staring at this and trying everything for the past two days...&lt;/p&gt;

&lt;p&gt;ANY HELP IS APPRECIATED&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggi8eg,True,,MrMegaGamerz,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggi8eg/loss_of_nan_accuracy_of_0_any_idea_why_full_code/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggi8eg/loss_of_nan_accuracy_of_0_any_idea_why_full_code/,155203,1589041002.0,0,,False,,,,
,learnmachinelearning,I  am beginner in ML. And sometimes I feel lost as I don't know someone personally who is studying ML. It would be awesome If I had someone as mentor or study buddy whom I could DM without any hesitation  .,t2_4q46suxl,False,,0,False,Need study buddy or mentor.,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg9ohu,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Request,False,6,,False,self,False,,[],{},,,True,,1589031374.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  am beginner in ML. And sometimes I feel lost as I don&amp;#39;t know someone personally who is studying ML. It would be awesome If I had someone as mentor or study buddy whom I could DM without any hesitation  .&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gg9ohu,True,,dark_--knight,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9ohu/need_study_buddy_or_mentor/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg9ohu/need_study_buddy_or_mentor/,155203,1589002574.0,0,,False,,,,
,learnmachinelearning," I was looking into some things on the internet and I stumbled across [https://nptel.ac.in/courses/106/103/106103205/](https://nptel.ac.in/courses/106/103/106103205/). Should I go ahead and utilise this course, considering I have an interest in ML and this could help me cover some of the mathematics behind it? (And probably help me learn other data science topics later)?",t2_43a4imr8,False,,0,False,Discrete mathematics as a prerequisite to Machine learning?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gghrxg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589068379.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was looking into some things on the internet and I stumbled across &lt;a href=""https://nptel.ac.in/courses/106/103/106103205/""&gt;https://nptel.ac.in/courses/106/103/106103205/&lt;/a&gt;. Should I go ahead and utilise this course, considering I have an interest in ML and this could help me cover some of the mathematics behind it? (And probably help me learn other data science topics later)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gghrxg,True,,AficionadoDS,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gghrxg/discrete_mathematics_as_a_prerequisite_to_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gghrxg/discrete_mathematics_as_a_prerequisite_to_machine/,155203,1589039579.0,0,,False,,,,
,learnmachinelearning,"I am doing a master's degree in Machine Learning and I would like to gather more resources regarding the subject. For me ML has a steep learning curve but I really enjoy. So what I would like to do is to add ML in my daily life from ""unofficial"" sources (eg. podcasts on ML, books, websites etc) 

Could you suggest such resources? Thank you in advance for your time",t2_mmqygks,False,,0,False,Machine Learning everyday resources,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggho5q,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1589068011.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing a master&amp;#39;s degree in Machine Learning and I would like to gather more resources regarding the subject. For me ML has a steep learning curve but I really enjoy. So what I would like to do is to add ML in my daily life from &amp;quot;unofficial&amp;quot; sources (eg. podcasts on ML, books, websites etc) &lt;/p&gt;

&lt;p&gt;Could you suggest such resources? Thank you in advance for your time&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggho5q,True,,platico_dev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggho5q/machine_learning_everyday_resources/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggho5q/machine_learning_everyday_resources/,155203,1589039211.0,0,,False,,,,
,learnmachinelearning,"In this project tutorial, youâ€™ll learn how to use machine learning to develop a stock trading robot. Youâ€™ll gain all the essential skills to create a full-fledged stock trading algorithm that investors and traders can utilize in their trading. 

 [https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/](https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/)",t2_3ovm77le,False,,0,False,Machine Learning for Stock Trading,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gghcqs,False,light,0.43,,public,0,0,{},,,False,[],,False,False,,{},Project,False,0,,False,self,False,,[],{},self,,True,,1589066947.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this project tutorial, youâ€™ll learn how to use machine learning to develop a stock trading robot. Youâ€™ll gain all the essential skills to create a full-fledged stock trading algorithm that investors and traders can utilize in their trading. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/""&gt;https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?auto=webp&amp;s=9e43ee8c006d992808d8c071cfd5a99a66707ea3', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d92fb3ccc07110d0511be9542e29e56f2b471ef', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c83730b89e16d0391d3a5a0c3a3b56010ffe9313', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'NjWIsUOwVenqDHwTtkeo-58F5m1HUc2DwROccUE0-Cs'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gghcqs,True,,juancarlospro,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gghcqs/machine_learning_for_stock_trading/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gghcqs/machine_learning_for_stock_trading/,155203,1589038147.0,0,,False,,,,
,learnmachinelearning,"This is the situation. I'm training a model to recognize letters of the Alphabet. There are, 26 classes. When writing the code for 26 classes, and loading nearly 100,000 images to train, I'm having a lot of issues. I could however successfully train the model to work on 10 letter increments. As in, A-J, K-T, and then U-Z. These three work perfectly fine (A-Z does not).   


Question: Can I train my A-J model, save the H5. Then, train the K-T, and U-Z after and then MERGE the H5 files together? I understand it's possible to train all A-Z using less images and then retrain the model with a different image set, but the issues are coming when I'm doing a 26 class system - hence I'm asking if I can do it in increments of 10 and merge it after.",t2_7ajy1,False,,0,False,Can I add categories to a dataset after and retrain an H5 file?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggh2vu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589066001.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is the situation. I&amp;#39;m training a model to recognize letters of the Alphabet. There are, 26 classes. When writing the code for 26 classes, and loading nearly 100,000 images to train, I&amp;#39;m having a lot of issues. I could however successfully train the model to work on 10 letter increments. As in, A-J, K-T, and then U-Z. These three work perfectly fine (A-Z does not).   &lt;/p&gt;

&lt;p&gt;Question: Can I train my A-J model, save the H5. Then, train the K-T, and U-Z after and then MERGE the H5 files together? I understand it&amp;#39;s possible to train all A-Z using less images and then retrain the model with a different image set, but the issues are coming when I&amp;#39;m doing a 26 class system - hence I&amp;#39;m asking if I can do it in increments of 10 and merge it after.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggh2vu,True,,MrMegaGamerz,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggh2vu/can_i_add_categories_to_a_dataset_after_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggh2vu/can_i_add_categories_to_a_dataset_after_and/,155203,1589037201.0,0,,False,,,,
,learnmachinelearning," 

Starting from the fact that handwritten documents style are  gender-dependent (male and female have different writing styles), I'm  trying to predict writer's gender from its handwritten scripts using  Convolutional Neural Network (CNN). I choose the IAM and KHATT datasets  for English and Arabic respectively.  After I read research articles  related to this problem, I realized that few of them used deep learning  (with handwritten word and/or sentence as input to CNN) and the majority  used classic methods like (LBP, HOG, GLCM, SFTA), the state of the art  accuracy is 80.79% and 85% (for CNN and classic methods respectively), I  also read this [Article](https://www.etsmtl.ca/ETS/media/ImagesETS/Labo/LIVIA/Publications/2012/Hanusiak_IJDAR_2012.pdf)  about identifying/verifying writer from its handwritten scripts which  uses texture blocks from the written documents that give good results.  As data preprocessing I used the method of texture blocks, after line  and word segmentation, i constructed texture image from handwriting  words, and segment the texture image to texture blocks of size  100\*100px.

I used 60 writers per gender for training (which give 42,000 texture  blocks), and 7 writers per gender for validation (which give 4,000  texture blocks) and 7 writers per gender for testing (which give 4,000  texture blocks).

I'm using TensorFlow and Keras as framework, I started with simple  (LeNet-like architectures) for base line, I got 60% test accuracy. As a  second approach, I used different state of the art architecture in image  classification like (VGG16, VGG19, RESNET34, RESNET50) and I got 64%  test accuracy.

So, my questions are:

&amp;#x200B;

1. How to reduce overfitting though I tried regularization methods (Dropout, L1, L2, Batch Norm) to reduce overfitting ?
2. I observed high variance in accuracy, if I re-shuffle the data (e.g from 64% to 50%) ?
3. How to modify CNN architectures to (multi-input CNN) to make the decision on multiple texture blocks, instead of one?

Texture blocks examples : 

https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;format=png&amp;auto=webp&amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de",t2_3i7gc514,False,,0,False,Gender Prediction from Offline Handwriting Using Convolutional Neural Networks,[],r/learnmachinelearning,False,6,,0,72.0,False,t3_gggtve,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/29_S0BvJBfrLJkf2qCgmPyY317IaRiQLd01xyQefTDI.jpg,False,,[],{},,,True,,1589065124.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Starting from the fact that handwritten documents style are  gender-dependent (male and female have different writing styles), I&amp;#39;m  trying to predict writer&amp;#39;s gender from its handwritten scripts using  Convolutional Neural Network (CNN). I choose the IAM and KHATT datasets  for English and Arabic respectively.  After I read research articles  related to this problem, I realized that few of them used deep learning  (with handwritten word and/or sentence as input to CNN) and the majority  used classic methods like (LBP, HOG, GLCM, SFTA), the state of the art  accuracy is 80.79% and 85% (for CNN and classic methods respectively), I  also read this &lt;a href=""https://www.etsmtl.ca/ETS/media/ImagesETS/Labo/LIVIA/Publications/2012/Hanusiak_IJDAR_2012.pdf""&gt;Article&lt;/a&gt;  about identifying/verifying writer from its handwritten scripts which  uses texture blocks from the written documents that give good results.  As data preprocessing I used the method of texture blocks, after line  and word segmentation, i constructed texture image from handwriting  words, and segment the texture image to texture blocks of size  100*100px.&lt;/p&gt;

&lt;p&gt;I used 60 writers per gender for training (which give 42,000 texture  blocks), and 7 writers per gender for validation (which give 4,000  texture blocks) and 7 writers per gender for testing (which give 4,000  texture blocks).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using TensorFlow and Keras as framework, I started with simple  (LeNet-like architectures) for base line, I got 60% test accuracy. As a  second approach, I used different state of the art architecture in image  classification like (VGG16, VGG19, RESNET34, RESNET50) and I got 64%  test accuracy.&lt;/p&gt;

&lt;p&gt;So, my questions are:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to reduce overfitting though I tried regularization methods (Dropout, L1, L2, Batch Norm) to reduce overfitting ?&lt;/li&gt;
&lt;li&gt;I observed high variance in accuracy, if I re-shuffle the data (e.g from 64% to 50%) ?&lt;/li&gt;
&lt;li&gt;How to modify CNN architectures to (multi-input CNN) to make the decision on multiple texture blocks, instead of one?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Texture blocks examples : &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de""&gt;https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gggtve,True,,khalilmeftah,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gggtve/gender_prediction_from_offline_handwriting_using/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gggtve/gender_prediction_from_offline_handwriting_using/,155203,1589036324.0,0,,False,,,"{'lp3ekbfa7rx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d124ec402061952e70ec2431eff2bb65af367e'}, {'y': 111, 'x': 216, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c64cc4d841f8e26b385e64883fb0a80d1f47c2d8'}, {'y': 165, 'x': 320, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3f6ddbcb49cbfc6280466872314fb85b5edb400'}], 's': {'y': 289, 'x': 558, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;format=png&amp;auto=webp&amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de'}, 'id': 'lp3ekbfa7rx41'}}",
,learnmachinelearning,"Hi all,

I'm developing some models for image classification and object detection, and at the very end i understood I don't have good enough grasps on statistics for model evaluation. For instance, I wanted to plot Precision/Recall plot to learn value for optimal threshold for given class. I know how to do it but I have a feeling that i don't fully understand it. Also, I want to use some Platt's scaling ( [https://en.wikipedia.org/wiki/Platt\_scaling](https://en.wikipedia.org/wiki/Platt_scaling) ) to map models scores to probability and again I think i can do it, but i want someone to patiently explain to me what exactly I should do and what does it do.  

Do you have any idea for a coursera courses (or other websites, books, materials) that will let easily understand these concepts?",t2_1owxkdl1,False,,0,False,Appropriate courses to learn model evaluation,[],r/learnmachinelearning,False,6,,0,,False,t3_ggg20d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1589062249.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m developing some models for image classification and object detection, and at the very end i understood I don&amp;#39;t have good enough grasps on statistics for model evaluation. For instance, I wanted to plot Precision/Recall plot to learn value for optimal threshold for given class. I know how to do it but I have a feeling that i don&amp;#39;t fully understand it. Also, I want to use some Platt&amp;#39;s scaling ( &lt;a href=""https://en.wikipedia.org/wiki/Platt_scaling""&gt;https://en.wikipedia.org/wiki/Platt_scaling&lt;/a&gt; ) to map models scores to probability and again I think i can do it, but i want someone to patiently explain to me what exactly I should do and what does it do.  &lt;/p&gt;

&lt;p&gt;Do you have any idea for a coursera courses (or other websites, books, materials) that will let easily understand these concepts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?auto=webp&amp;s=78838a66125bf7aa03489f673313601508eae5ad', 'width': 1200, 'height': 545}, 'resolutions': [{'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=648d3b34a2e5f5e6a07f9d274c6223f9e0e611a4', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3483de6dc00e14dac35b27240761b96b09da7496', 'width': 216, 'height': 98}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dba4ab2e8503305436507058815213788d1b53a', 'width': 320, 'height': 145}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=070b91604de8ab8a73274edcc276a23ed194d1e3', 'width': 640, 'height': 290}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c8f5a11df21ad558dbb7aa3d50aa0d478be7321b', 'width': 960, 'height': 436}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21959f74b60bfbf454e05276edb49e37d32f3ccb', 'width': 1080, 'height': 490}], 'variants': {}, 'id': 'othF90FfeIQC66ekAeqIYR_nvXmSV79Jb3-4hkfnoVg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggg20d,True,,buniosmieci,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggg20d/appropriate_courses_to_learn_model_evaluation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggg20d/appropriate_courses_to_learn_model_evaluation/,155203,1589033449.0,0,,False,,,,
,learnmachinelearning,"Hey, I'm an ML enthusiast who's trying to learn ML the mathematical way. One of the things that gave me a lot of intuition into Neural Networks and other ML algos was implementing them myself  on Python, using NumPy. 

You can check these out on [my GitHub](https://github.com/Vikhyat2603/Machine-Learning).

As of now, I've uploaded :

1. A complete feed-forward Neural Network implementation
2. Using Genetic Algorithms to train Neural Networks on Reinforcement Learning tasks
3. A visualisation for Gradient Descent using linear regression (named 'Perceptron Model')
4. A visualisation for K-Means Clustering

I've tried to make these projects understandable and modular. Feel free to reach out to me for any questions, and I'd love to get some suggestions/feedback on this!

GitHub -  [https://github.com/Vikhyat2603/Machine-Learning](https://github.com/Vikhyat2603/Machine-Learning) 

Email - [vikhyat2603@gmail.com](mailto:vikhyat2603@gmail.com)

Linkedin - [https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/](https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/)",t2_2y9k6ek4,False,,0,False,Check out these NumPy Implementations for Machine Learning algorithms,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg0e1a,False,light,0.91,,public,28,0,{},,,False,[],,False,False,,{},Project,False,28,,False,self,False,,[],{},self,,True,,1588996746.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;m an ML enthusiast who&amp;#39;s trying to learn ML the mathematical way. One of the things that gave me a lot of intuition into Neural Networks and other ML algos was implementing them myself  on Python, using NumPy. &lt;/p&gt;

&lt;p&gt;You can check these out on &lt;a href=""https://github.com/Vikhyat2603/Machine-Learning""&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As of now, I&amp;#39;ve uploaded :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A complete feed-forward Neural Network implementation&lt;/li&gt;
&lt;li&gt;Using Genetic Algorithms to train Neural Networks on Reinforcement Learning tasks&lt;/li&gt;
&lt;li&gt;A visualisation for Gradient Descent using linear regression (named &amp;#39;Perceptron Model&amp;#39;)&lt;/li&gt;
&lt;li&gt;A visualisation for K-Means Clustering&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;ve tried to make these projects understandable and modular. Feel free to reach out to me for any questions, and I&amp;#39;d love to get some suggestions/feedback on this!&lt;/p&gt;

&lt;p&gt;GitHub -  &lt;a href=""https://github.com/Vikhyat2603/Machine-Learning""&gt;https://github.com/Vikhyat2603/Machine-Learning&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Email - [&lt;a href=""mailto:vikhyat2603@gmail.com""&gt;vikhyat2603@gmail.com&lt;/a&gt;](mailto:&lt;a href=""mailto:vikhyat2603@gmail.com""&gt;vikhyat2603@gmail.com&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Linkedin - &lt;a href=""https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/""&gt;https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?auto=webp&amp;s=7b5c198d1f8a5f492e7baeead69c994c93b2a2bf', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c237e6b3f9608a9faf28ff948516e22fccb9bc41', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fa01f7df346e5680c13191d74df6d9990e43b9e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bd1d7aa2ac1b7a6e51dad32bfbc31b34801e970', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Blxa0lT7ynyYVYTvJarl813SqGKxe_nB7Hr5nidACVc'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gg0e1a,True,,Vikhyat333,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg0e1a/check_out_these_numpy_implementations_for_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg0e1a/check_out_these_numpy_implementations_for_machine/,155203,1588967946.0,0,,False,,,,
,learnmachinelearning,,t2_64cu1j8e,False,,0,False,Suggestions for first ML project for resume building and overall development.,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg55x0,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},Request,False,10,,False,self,False,,[],{},,,True,,1589012775.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gg55x0,True,,redeyetree,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg55x0/suggestions_for_first_ml_project_for_resume/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg55x0/suggestions_for_first_ml_project_for_resume/,155203,1588983975.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,How to monitor boiling milk - Homemade AI recipe,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gfrv28,False,light,0.96,,public,86,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How to monitor boiling milk - Homemade AI recipe', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_CiOCvD1--Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfrv28', 'height': 338}",Discussion,False,86,,False,https://b.thumbs.redditmedia.com/mUad4fceyad_fXkH1qYss1f1gJfQcJ7fXf1DXygzdRQ.jpg,False,,[],{},rich:video,,False,,1588967844.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?auto=webp&amp;s=479b55d98a9e1c30270a70cf8709fabfe5949beb', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68bcc4222ebec2364fbf2219e6ee9dfec210cc9c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be7434b6716d90e95f1903424f1540b0fe674045', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4734d8bfa8a68291e6a9be462c536d99d9dc776e', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'IsINZF1zS5Hd6b1lkm1e6eXa_IaJBhGCgV3Y1gTWdf8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfrv28,True,,cmillionaire9,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrv28/how_to_monitor_boiling_milk_homemade_ai_recipe/,all_ads,False,https://youtu.be/_CiOCvD1--Q,155203,1588939044.0,2,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How to monitor boiling milk - Homemade AI recipe', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_CiOCvD1--Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,,,,
,learnmachinelearning,"I have  compiled a video file using yolo and i want to upload it directly to google drive.

I searched online but can't find any command which works.

This is what I'm doing after mounting drive:

!cpÂ out.aviÂ ""/content/drive/MyÂ Drive/images/out1.avi""

And the error i get is:

 Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(""/content/gdrive"", force\_remount=True). cp: failed to access '/content/drive/My Drive/images/out1.avi': Transport endpoint is not connected .

Can anyone help me how to solve this.",t2_6dqc4uv8,False,,0,False,[HELP] Upload file from colab straight to google drive,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg9qb8,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1589031615.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have  compiled a video file using yolo and i want to upload it directly to google drive.&lt;/p&gt;

&lt;p&gt;I searched online but can&amp;#39;t find any command which works.&lt;/p&gt;

&lt;p&gt;This is what I&amp;#39;m doing after mounting drive:&lt;/p&gt;

&lt;p&gt;!cpÂ out.aviÂ &amp;quot;/content/drive/MyÂ Drive/images/out1.avi&amp;quot;&lt;/p&gt;

&lt;p&gt;And the error i get is:&lt;/p&gt;

&lt;p&gt;Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&amp;quot;/content/gdrive&amp;quot;, force_remount=True). cp: failed to access &amp;#39;/content/drive/My Drive/images/out1.avi&amp;#39;: Transport endpoint is not connected .&lt;/p&gt;

&lt;p&gt;Can anyone help me how to solve this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg9qb8,True,,RayS0l0,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9qb8/help_upload_file_from_colab_straight_to_google/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg9qb8/help_upload_file_from_colab_straight_to_google/,155203,1589002815.0,0,,False,,,,
,learnmachinelearning,Expecting replies from people in industry.,t2_11uipv0u,False,,0,False,Put down your top 3 favorite machine learning algorithms.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggdl13,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589051031.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Expecting replies from people in industry.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdl13,True,,k_anu7,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdl13/put_down_your_top_3_favorite_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggdl13/put_down_your_top_3_favorite_machine_learning/,155203,1589022231.0,0,,False,,,,
,learnmachinelearning,"What if instead of Humans, machines are controlling the Financial System of the Country. With this idea in mind created a small project which used Reinforcement learning to control this simulated Environment called 'Chair The Fed'. This simulation is created by the Federal Reserve Bank of San Francisco to teach about the effects of external factors like news and how manipulating the fed funds rate can control the inflation and unemployment rates. The link to code and demo video is below.

Project - https://github.com/lucky630/Chair_The_Fed_Rl

Demo - https://www.youtube.com/watch?v=vDVLj1d361A",t2_1084g6,False,,0,False,Controlling Unemployment in a simulated environment using Reinforcement learning,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggav83,False,light,0.67,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},self,,True,,1589037247.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What if instead of Humans, machines are controlling the Financial System of the Country. With this idea in mind created a small project which used Reinforcement learning to control this simulated Environment called &amp;#39;Chair The Fed&amp;#39;. This simulation is created by the Federal Reserve Bank of San Francisco to teach about the effects of external factors like news and how manipulating the fed funds rate can control the inflation and unemployment rates. The link to code and demo video is below.&lt;/p&gt;

&lt;p&gt;Project - &lt;a href=""https://github.com/lucky630/Chair_The_Fed_Rl""&gt;https://github.com/lucky630/Chair_The_Fed_Rl&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Demo - &lt;a href=""https://www.youtube.com/watch?v=vDVLj1d361A""&gt;https://www.youtube.com/watch?v=vDVLj1d361A&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?auto=webp&amp;s=5212052bfff458308e142ae6bb980a9ff4963f0b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d768890f2c3a54a148b167073df5a4b9117ec8ae', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ba35ec24ac399217e9cc435f56ccfcfed538b97', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b904b957ae053574684aa8a5b60d3997f3919674', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_PSC4biw4O45uAH1nz6aApKjE1sS06yocyrxCBhbdzI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggav83,True,,rednivrug,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggav83/controlling_unemployment_in_a_simulated/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggav83/controlling_unemployment_in_a_simulated/,155203,1589008447.0,0,,False,,,,
,learnmachinelearning,"Let's have a  meeting!

1. What have you accomplished since last week?
2. What are your goals for next week?
3. Do you have any blockers that need helps from the /r/LearnMachineLearning community?

Don't be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.",t2_6l4z3,False,,0,False,"Weekly Status Check Meeting - Share your progress, your goals, and whatever is stopping you from achieving those goals!",[],r/learnmachinelearning,False,6,,0,,False,t3_ggar50,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,True,self,False,,[],{},,,True,,1589036674.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s have a  meeting!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What have you accomplished since last week?&lt;/li&gt;
&lt;li&gt;What are your goals for next week?&lt;/li&gt;
&lt;li&gt;Do you have any blockers that need helps from the &lt;a href=""/r/LearnMachineLearning""&gt;/r/LearnMachineLearning&lt;/a&gt; community?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Don&amp;#39;t be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,ggar50,True,,AutoModerator,,4,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggar50/weekly_status_check_meeting_share_your_progress/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggar50/weekly_status_check_meeting_share_your_progress/,155203,1589007874.0,0,,False,,,,
,learnmachinelearning,"Hey everyone, I am new to the world of machine learning and a hobbyist. I am thinking of a fun side project that would involve making predictions based on file structures, and wondering if anyone had any papers on it or reading material. This seems like a pretty straight forward classification problem if I'm not mistaken and any thoughts or advice would be greatly appreciated.",t2_7z7uy,False,,0,False,File Structure Prediction?,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg919h,False,light,0.67,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},,,True,,1589028473.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I am new to the world of machine learning and a hobbyist. I am thinking of a fun side project that would involve making predictions based on file structures, and wondering if anyone had any papers on it or reading material. This seems like a pretty straight forward classification problem if I&amp;#39;m not mistaken and any thoughts or advice would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gg919h,True,,Phizy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg919h/file_structure_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg919h/file_structure_prediction/,155203,1588999673.0,0,,False,,,,
,learnmachinelearning,"In logistic regression one of the first ideas is to equate the logit(p) = a straight line, how can we equate the logit of probability to a straight line?

&amp;#x200B;

Thanks, in advance",t2_3rsotruc,False,,0,False,Logistic Regression Help,[],r/learnmachinelearning,False,6,,0,,False,t3_ggdcfh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589049876.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In logistic regression one of the first ideas is to equate the logit(p) = a straight line, how can we equate the logit of probability to a straight line?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks, in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdcfh,True,,Jamhead2000,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdcfh/logistic_regression_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggdcfh/logistic_regression_help/,155203,1589021076.0,0,,False,,,,
,learnmachinelearning,"Hi all, 

Could you recommend some resources for learning Graphical Neural Networks? My attempts to learn the concepts from review papers didn't go well. 

Thanks for your time and consideration.",t2_61xh5qps,False,,0,False,Resources for learning Graphical Neural Networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg64k7,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1589016452.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, &lt;/p&gt;

&lt;p&gt;Could you recommend some resources for learning Graphical Neural Networks? My attempts to learn the concepts from review papers didn&amp;#39;t go well. &lt;/p&gt;

&lt;p&gt;Thanks for your time and consideration.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg64k7,True,,ppsrs,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg64k7/resources_for_learning_graphical_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg64k7/resources_for_learning_graphical_neural_networks/,155203,1588987652.0,0,,False,,,,
,learnmachinelearning,"Hi! Iâ€™m new to python and the instructions they put up donâ€™t really make a lot of sense to me, would someone be able to give an easy walkthrough for setting it up? I tried using the instructions provided but it didnâ€™t work.",t2_x05qd,False,,0,False,Is there an easy way to set up Jukebox AI?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8jvm,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1589026359.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! Iâ€™m new to python and the instructions they put up donâ€™t really make a lot of sense to me, would someone be able to give an easy walkthrough for setting it up? I tried using the instructions provided but it didnâ€™t work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg8jvm,True,,WAFFLED_II,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8jvm/is_there_an_easy_way_to_set_up_jukebox_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8jvm/is_there_an_easy_way_to_set_up_jukebox_ai/,155203,1588997559.0,0,,False,,,,
,learnmachinelearning,"\[Actual problem statement\]

For example and account posts about the same topic for at least 10 posts, shifts to a new topic for at least 10 posts.

How can I deal with the statement above?",t2_tabm4,False,,0,False,How to detect if the user is talking about the same topic in his/her posts?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggcdid,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589044936.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[Actual problem statement]&lt;/p&gt;

&lt;p&gt;For example and account posts about the same topic for at least 10 posts, shifts to a new topic for at least 10 posts.&lt;/p&gt;

&lt;p&gt;How can I deal with the statement above?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggcdid,True,,waheed0332,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggcdid/how_to_detect_if_the_user_is_talking_about_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggcdid/how_to_detect_if_the_user_is_talking_about_the/,155203,1589016136.0,0,,False,,,,
,learnmachinelearning,I finished Andrew's MOOC on Coursera and i've been wasting 1 day and half without anything to do lol.What do you guys suggest i do now? I want to learn more theory and Implementation of Machine Learning before moving on to deep learning. I plan on taking the deep learning specialization course offered by [deeplearning.ai](https://deeplearning.ai) on coursera. But i want to cover more machine learning and master it before  I move on . What do i do now? Please drop some suggestions for me,t2_2ov8dfiu,False,,0,False,I just finished Andrew Ng's Machine Learning MOOC on coursera and i have no idea what to do now.Any suggestions?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gftmrs,False,light,0.87,,public,31,0,{},,,False,[],,False,False,,{},Discussion,False,31,,False,self,False,,[],{},self,,True,,1588974790.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I finished Andrew&amp;#39;s MOOC on Coursera and i&amp;#39;ve been wasting 1 day and half without anything to do lol.What do you guys suggest i do now? I want to learn more theory and Implementation of Machine Learning before moving on to deep learning. I plan on taking the deep learning specialization course offered by &lt;a href=""https://deeplearning.ai""&gt;deeplearning.ai&lt;/a&gt; on coursera. But i want to cover more machine learning and master it before  I move on . What do i do now? Please drop some suggestions for me&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?auto=webp&amp;s=f2427c47bea6d58850d6911cf474069a18cc9b62', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b07bfe98cdf9df387c029da8df49b5de69c706a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b5db315d49d86527b77a361b9a2c28d75a3b90b', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47549465daed6c2193229dd479b317c23b543247', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d398aae35768742b8943fa84352b11665c8d238', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=56bea95f704ddfd895b407e0e304fb0df53866e9', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df0ba01e8a1c6732cd9ae449a51e95d19f18cedf', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '5zNMs90HSxmwgVyfBrk-wTVOjvtEek3FpmHWS_2ZKP4'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gftmrs,True,,FaizRahim,,20,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gftmrs/i_just_finished_andrew_ngs_machine_learning_mooc/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gftmrs/i_just_finished_andrew_ngs_machine_learning_mooc/,155203,1588945990.0,0,,False,,,,
,learnmachinelearning,"Looks like Andrew Ng's ML course on Coursera is being offered for free (there are charges if you want the certificate), with classes starting May 11.

Is this a big deal? Has this already been discussed here? Cheers.",t2_rlnpqcu,False,,0,False,Andrew Ng's Coursera course.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggbrer,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1589013279.0,,[],{},,,True,,1589041787.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looks like Andrew Ng&amp;#39;s ML course on Coursera is being offered for free (there are charges if you want the certificate), with classes starting May 11.&lt;/p&gt;

&lt;p&gt;Is this a big deal? Has this already been discussed here? Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggbrer,True,,cosmictypist,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggbrer/andrew_ngs_coursera_course/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggbrer/andrew_ngs_coursera_course/,155203,1589012987.0,0,,False,,,,
,learnmachinelearning,"Hello, my summer plans were screwed because of the current pandemic, so I'm trying to spend some of my time this summer learning some machine learning. Given the current situation, I'd like to see if I can use some coronavirus related datasets, as a beginner I obviously don't expect to find out amything useful, but I still think it would be a good idea. Are there any coronavirus related datasets online?
Maybe like something were doctors post patient responses to different treatments and info about the patients, or maybe something more related to the spread of the virus. If there isn't anything like this, do you guys think, something like that would be useful in any way?",t2_1zkqj8ti,False,,0,False,Public datasets on coronavirus information?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg8s7n,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589027370.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, my summer plans were screwed because of the current pandemic, so I&amp;#39;m trying to spend some of my time this summer learning some machine learning. Given the current situation, I&amp;#39;d like to see if I can use some coronavirus related datasets, as a beginner I obviously don&amp;#39;t expect to find out amything useful, but I still think it would be a good idea. Are there any coronavirus related datasets online?
Maybe like something were doctors post patient responses to different treatments and info about the patients, or maybe something more related to the spread of the virus. If there isn&amp;#39;t anything like this, do you guys think, something like that would be useful in any way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8s7n,True,,flyingwizard1,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8s7n/public_datasets_on_coronavirus_information/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8s7n/public_datasets_on_coronavirus_information/,155203,1588998570.0,0,,False,,,,
,learnmachinelearning,Should i really go in depth into regression/classification and do some projects before diving into deep learning? What would you guys recommend?,t2_5dk1rkk2,False,,0,False,When should i start deep learning?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg8nfl,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589026794.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Should i really go in depth into regression/classification and do some projects before diving into deep learning? What would you guys recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8nfl,True,,shawn2james,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8nfl/when_should_i_start_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8nfl/when_should_i_start_deep_learning/,155203,1588997994.0,0,,False,,,,
,learnmachinelearning,,t2_djtutca,False,,0,False,"How do you choose between the different Pooling layers (mean, max...) in a CNN ?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg535t,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Question,False,5,,False,self,False,,[],{},,,True,,1589012495.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg535t,True,,Avditvs,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg535t/how_do_you_choose_between_the_different_pooling/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg535t/how_do_you_choose_between_the_different_pooling/,155203,1588983695.0,0,,False,,,,
,learnmachinelearning,,t2_3baoayyx,False,,0,False,Help with linear alegbra projections question,[],r/learnmachinelearning,False,6,,0,26.0,False,t3_ggb78z,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/_s6B9cdgqTCQE-gE52ilDoXdUBzHhxfQ2hj3Gg2Uaxo.jpg,False,,[],{},,,False,,1589038966.0,text,6,,,text,self.askmath,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggb78z,True,,gimlidorf,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggb78z/help_with_linear_alegbra_projections_question/,all_ads,False,/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/,155203,1589010166.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'askmath', 'selftext': ""Can someone tell me what's wrong with my logic in answering this question from strang. Problem set 4.4 q 37\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pt3qux47wfx41.png?width=499&amp;format=png&amp;auto=webp&amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6\n\ni think you should subtract *QQ**^(T)**a*    but the answer is subtract *Q**^(T)**a*. But *Q**^(T)**a* would be a would be a (*n by m)* matrix times (*m* by *n*) col which would leave a (*n* by 1)  column but a is a column of length *n* rather than *m*"", 'author_fullname': 't2_3baoayyx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help with linear alegbra projections question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/askmath', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 26, 'hide_score': False, 'media_metadata': {'pt3qux47wfx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 20, 'x': 108, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2327f28593718cff795743a1c60dadb9b2c7e7b3'}, {'y': 40, 'x': 216, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0db109ae37d2b2c08c2af12f50399aefbd3c7e0'}, {'y': 59, 'x': 320, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=33cd3db602c03a465560c29cc2d056c4ea812c70'}], 's': {'y': 93, 'x': 499, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;format=png&amp;auto=webp&amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6'}, 'id': 'pt3qux47wfx41'}}, 'name': 't3_gfjb3s', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588928483.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.askmath', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone tell me what&amp;#39;s wrong with my logic in answering this question from strang. Problem set 4.4 q 37&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6""&gt;https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;i think you should subtract &lt;em&gt;QQ&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;    but the answer is subtract &lt;em&gt;Q&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;. But &lt;em&gt;Q&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt; would be a would be a (&lt;em&gt;n by m)&lt;/em&gt; matrix times (&lt;em&gt;m&lt;/em&gt; by &lt;em&gt;n&lt;/em&gt;) col which would leave a (&lt;em&gt;n&lt;/em&gt; by 1)  column but a is a column of length &lt;em&gt;n&lt;/em&gt; rather than &lt;em&gt;m&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qm4f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfjb3s', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'gimlidorf', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/', 'subreddit_subscribers': 20697, 'created_utc': 1588899683.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_gfjb3s,,
,learnmachinelearning,"Hello! I'm trying to create a NN which can recognize the letters of the alphabet (26 classes). I apologize for the lengthy post, but I've included all my relevant code to be as clear as possible. In the end I've explained the issue.

The following block is where I name the paths correctly, and standardize/normalize the image and get it ready for training.

    X = [] # Image data
    y = [] # Labels
    
    datagen = ImageDataGenerator(samplewise_center=True)
    
    for path in imagepaths:
      img = cv2.imread(path)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
      img = cv2.resize(img, (200, 200))
      img = image.img_to_array(img)
      img = datagen.standardize(img)
      X.append(img)
    
      # Processing label in image path
      category = path.split(""\\"")[1]
      #print(category)
      split = (category.split(""_""))     
      if int(split[0]) == 0:
        label = int(split[1])
      else:
        label = int(split[0])
      y.append(label)
    
    # Turn X and y into np.array to speed up train_test_split
    X = np.array(X, dtype=""uint8"")
    X = X.reshape(len(imagepaths), 200, 200, 1) 
    y = np.array(y)

Creating the test set.

    ts = 0.3 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)

Creating a model. Dense of 26, one output for each letter, and size 200,200,1 to match input:

    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(26, activation='softmax'))

Model compiler and fit:

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 
    model.fit(X_train, y_train, epochs=1, batch_size=64, verbose=1, validation_data=(X_test, y_test))

The issue comes here where the output of my [model.fit](https://model.fit/) is:

    Train on 54600 samples, validate on 23400 samples
    Epoch 1/1
    54600/54600 [==============================] - 79s 1ms/step - loss: nan - accuracy: 1.8315e-05 - val_loss: nan - val_accuracy: 0.0000e+00

I understand that it may not be high accuracy or anything from the get-go, but why are the losses nan? I posted elsewhere and I was first told to normalize my data (which I fixed for this post). Then I was told that it was possible that my dataset is corrupt or leaking - this is not the case because when I don't do 26 letters at once, it works perfectly fine. (Meaning I tested the code, using letters A-J, dense = 10, etc) and got a high accuracy of about 95%.

Any help is appreciated, I've been scratching my had at this for hours!",t2_7ajy1,False,,0,False,"Losses of NAN, Accuracy of 0, I tried everything! (CNN)",[],r/learnmachinelearning,False,6,,0,,False,t3_gg4qs7,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589011206.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I&amp;#39;m trying to create a NN which can recognize the letters of the alphabet (26 classes). I apologize for the lengthy post, but I&amp;#39;ve included all my relevant code to be as clear as possible. In the end I&amp;#39;ve explained the issue.&lt;/p&gt;

&lt;p&gt;The following block is where I name the paths correctly, and standardize/normalize the image and get it ready for training.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = [] # Image data
y = [] # Labels

datagen = ImageDataGenerator(samplewise_center=True)

for path in imagepaths:
  img = cv2.imread(path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
  img = cv2.resize(img, (200, 200))
  img = image.img_to_array(img)
  img = datagen.standardize(img)
  X.append(img)

  # Processing label in image path
  category = path.split(&amp;quot;\\&amp;quot;)[1]
  #print(category)
  split = (category.split(&amp;quot;_&amp;quot;))     
  if int(split[0]) == 0:
    label = int(split[1])
  else:
    label = int(split[0])
  y.append(label)

# Turn X and y into np.array to speed up train_test_split
X = np.array(X, dtype=&amp;quot;uint8&amp;quot;)
X = X.reshape(len(imagepaths), 200, 200, 1) 
y = np.array(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ts = 0.3 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating a model. Dense of 26, one output for each letter, and size 200,200,1 to match input:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (3, 3), activation=&amp;#39;relu&amp;#39;, input_shape=(200, 200, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(26, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Model compiler and fit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) 
model.fit(X_train, y_train, epochs=1, batch_size=64, verbose=1, validation_data=(X_test, y_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The issue comes here where the output of my &lt;a href=""https://model.fit/""&gt;model.fit&lt;/a&gt; is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train on 54600 samples, validate on 23400 samples
Epoch 1/1
54600/54600 [==============================] - 79s 1ms/step - loss: nan - accuracy: 1.8315e-05 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I understand that it may not be high accuracy or anything from the get-go, but why are the losses nan? I posted elsewhere and I was first told to normalize my data (which I fixed for this post). Then I was told that it was possible that my dataset is corrupt or leaking - this is not the case because when I don&amp;#39;t do 26 letters at once, it works perfectly fine. (Meaning I tested the code, using letters A-J, dense = 10, etc) and got a high accuracy of about 95%.&lt;/p&gt;

&lt;p&gt;Any help is appreciated, I&amp;#39;ve been scratching my had at this for hours!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg4qs7,True,,MrMegaGamerz,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg4qs7/losses_of_nan_accuracy_of_0_i_tried_everything_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg4qs7/losses_of_nan_accuracy_of_0_i_tried_everything_cnn/,155203,1588982406.0,0,,False,,,,
,learnmachinelearning,,t2_2f64gmd6,False,,0,False,Is it natural that an image captioning model becomes spammy and a lot slower to train when adding attention?!,[],r/learnmachinelearning,False,6,,0,80.0,False,t3_gg9lqd,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/HUgFuLB4-WJGHg0Qad2yj_4Oc-Cgu-hkjid7QzIhUcQ.jpg,False,,[],{},image,,False,,1589031013.0,text,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?auto=webp&amp;s=f5cb39273bdf3155ca33a4ca55629f92be1221f5', 'width': 432, 'height': 248}, 'resolutions': [{'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=31ddd4f4a0de50eaa0a6924ea0e426dafe675241', 'width': 108, 'height': 62}, {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6ca58168f2859318f4e11e19fa352a0f4fca09c', 'width': 216, 'height': 124}, {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5e860f086c5e5947e43000a859e27aa795754a1', 'width': 320, 'height': 183}], 'variants': {}, 'id': 'WZHFZ8UPbXH2W9JQZqdfgFT-kWYXzoeSIqjI385EfrY'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg9lqd,True,,Abdalrahman12,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9lqd/is_it_natural_that_an_image_captioning_model/,all_ads,False,https://i.redd.it/nhrbd6kvdox41.png,155203,1589002213.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying - How a good memory makes you wealthy $$$,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8x18,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1589027951.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?auto=webp&amp;s=66e486c81869feaad60df238edaa128388f3110a', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e642309c726659985affc46be1040421897ce8d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9453b1ab1344ef827e5e7bc71f6e87fc973649b6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dcded9a7211251b822a8a446cbaf8c647b0f52a', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef3d77ac395bf51c23ea92070a0e6a5320802e4e', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=666bfe10bbef0f8584a838c844d4c1c5c9d639ef', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a77cb1ce3d021ab0e49f1235bbe929c8254ab0a6', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'JhDjM1_cc6F88dvY5zq9T1V-OXjf34-ySM2rRo5JObg'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg8x18,True,,jianfa-ben-tsai,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8x18/studying_how_a_good_memory_makes_you_wealthy/,all_ads,False,/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/,155203,1588999151.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Benefits of a Good Memory\n\nâ€œGreat memory at work leads to be a star = promotion &amp; $$$. Not forgetting your partnerâ€™s birthday or your wedding anniversary leads to a happier life.â€\n\nIMAGINATION\n\nAlbert Einstein once said: Knowledge takes you from A to Z but imagination takes you everywhere.\n\nPay attention and convert the words of a book page into a story or an image.\n\nMake the learning of personally meaningful material easy by leveraging on your imagination.\n\nLook for meaning and relationship between data points to extract action points that translate into money or benefits. See it in your mind like a movie as clearly as you can. Hear &amp; feel it. Use your entire being and emotions to invest in making the material part of who you are and the knowledge will never leave you. Look for alternative words that sound like the difficult word that I am learning. This is associative thinking in play.\n\nWhen you have trouble making a mental connection in your mind about the material that you are trying to remember, that means that you failed to understand the material fully. Go back and look at learning this with a different strategy or ask for help from a friend, colleague, expert or stranger.\n\nSPACED REPETITION\n\nThe art of memorisation lies in practice recalling - not repeat reading\n\nSpaced repetition helps to retain and reinforce what you have learnt.\n\nAsk: how is the information I am learning going to help me right now and in the future?\n\nHow is the new information move me a step towards my goals?\n\nHow is the new material going to tell me something that not many people know?\n\nFind action steps from the new material.\n\nLearn and remember by applying creativity to everything that they know, i.e. produce images in our minds. Use your five senses. Manipulate the size of the image and make the image ridiculous. Colour the images with the rainbow shades of life. Use action, singing, talking and dancing to animate the images. Donâ€™t let the rules of inhibition prevent you from playing with your images.\n\nExtract a familiar word or phrase from familiar and link it with the sound that is the same or similar to the abstract word.\n\nPlant images at markers along mind palace journey or in mindâ€™s buildings.\n\nTurn the abstract concept into something tangible.\n\nReview journey/building backwards &amp; forwards a few times.\n\nPeriodic self-test and continued use.\n\nGiving personal meaning to numbers.\n\nInterest level is measured by how much you remember.\n\nMake an image vocabulary for key subject concepts.\n\nReview images 1 hr later, 1 day, 2 days, 3 days, 1 week, 2 weeks, 3 weeks, | month, 2 months, 3 months.\n\nOTHER\n\nSelectively take 10% of a book page out and rephrase into a single sentence in your own words and covert that single sentence into an image and story. Add sound to that image.\n\nForgetting irrelevant information helps us to prioritise.\n\nThe art of thinking is different from having eidetic memory or being able to Google well.\n\nTeach others.\n\nAsk for help when you are stuck.\n\nPeople often study a field when they are young and naive and simply goes with what is popular and will get a job even if they are disinterested in the field. It makes the learning curve steep and learning unpleasant. They wonâ€™t succeed in their job would have wasted decades of their life.\n\nHard Facts that are hard to derive meaning needs to be learned by rote. Even if learning by rote, you could adopt spaced timed interval repetition for review and revision.\n\nFind the easiest way to learn the material. Work hard but think smart, find out how it applies to learn and to work too.\n\nFind out and appreciate the reason behind each point. When you gain an appreciation of the underlying principles behind why something exists, you will be emotionally moved by its beauty and it will be effortless to remember the information. Bridge new information to old information. By making association from old information to new information it makes it easy to develop neural pathways in your both left and right hemispheres of your brain.\n\nUse your family and friends body parts to remember your material. Make it intimate.\n\nHow to remember numbers:\n\nZero for (Z,S)\n\nOne for One-way street (t)\n\nTwo for On/Off Switch (n)\n\nThree for Three Blind Mice (m)\n\nFour for 4ingers chicken (R)\n\nFive for Five Star Hotel (L)\n\nSix for Sixth Sense (J)\n\nSeven for 7 Eleven (K)\n\nEight for 8 Ball (G)\n\nNine for Cat with 9 Lives (P)\n\nTen for Ten Fingers &amp; Toes\n\nSummarise what you have written. People are busy. Deliver 70% of value in 3 minutes is better than 0% of the value with wordy text.\n\nWhy should students put in so much effort to memorize equations and facts, when they could easily find that information at the click of a mouse button?\n\nJianfa: â€œWhy didnâ€™t schools articulate specifically how does learning this subject help student make money in the near future directly other than some vague â€“ this graduation paper will get you a job which we know that getting a good job requires referrals and human connections which you get from the dark arts and partying.\n\nFive minutes of light physical exercise after any learning session can give your memory a boost.\n\nWhen you have to put something in an unfamiliar place, say what you are doing out loud: ""I am putting my sunglasses on the table by the door."" By letting your ears register the information, you increase your chances of remembering it later.\n\nMake up a story - break the information down and make up a story linking together each piece of information.\n\nAsk yourself Who, What, When, Why, Where, How.\n\nCreate mind maps, colour-code your notes, rely on written instructions for assignments and projects, use visual aids such as flashcards, diagrams, charts and pictures, watch a short clip or video that explains your topic.\n\nRecord your lectures and class notes, attend group study sessions, participate in class discussions, read the material out loud, make up a rhyme or song about the topic, use word association.\n\nTake detailed notes in class, rewrite your notes, turn diagrams and charts into words, ask your teacher for handouts, write regular summaries.\n\nAdd a physical activity while you study, act out concepts and theories, build 3D models to apply complex information, study in short blocks, study with others, including plenty of examples in your notes, attend field trips to make information more memorable.\n\nAvoid over-studying and filling your brain with useless information. Many people have a tendency to focus on unnecessary facts\n\nTo remember any piece of information, begin the review/rehearsal process as soon as possible after the information enters your short-term memory.\n\nIt is easier to remember information that has been grouped, organized, or clustered than information that has not.\n\nIndividuals remember more of what was learned at the beginning and end of a learning session.\n\nmake a conscious decision and effort to remember something.\n\nAvoid heavy meals before exams. Drink water earlier in the day to avoid dehydration.\n\nIn order to remember something thoroughly, we must be interested in it.\n\nBe positive about remembering -\n\nMost academic subjects can be classified into 3 categories,\n\na) problem solving;\n\nb) concept-driven\n\nc) interpretation\n\nSegregate ""core material"" from ""elaborative material"".\n\nDistil notes, repeat and write. Read ideas out loud in a dramatic manner. Re-organize ideas (think)\n\nGet an overview. Limit the amount to learn. Visualise, or draw an image. Link concepts. Test yourself repeatedly. Explain ideas to someone. Develop or use photographic memory. Move while repeating the main points, walk, dance, kick a ball, then write them. Develop a mnemonic for the most important points.\n\nWe cannot write our notes down logical because we tend to forget logical stuff more than creative stuff. Jianfa Thesis: Typing your notes are fast, but you should turn your typed notes in your own words into questions and answers. Take the Q&amp;A into Anki free flashcards app, as you test yourself with the flashcards, handwrite the answer down with pen and paper and draw simple images to illustrate the answer. Voice record the Q&amp;A into a voice memo app and listen to your recording while travelling on public transport.\n\nWhat feelings must you employ to improve your memory?\n\n\\- Emotion - (If you love what you do, you will remember what you love) &gt; Be Present - (Center yourself in the present and be aware) &gt; Pay Attention - (Not paying attention leads to costly mistakes)&gt; Do 1 thing at a time (Unless you are breathing and talking, humans can realistically do 1 thing well at a time.) &gt; Interesting - (If you love your job, you will find even the most mundane of tasks interesting?) &gt; Attractive - (Your mind remembers beautiful people? Capitalise that strength) &gt; Ridiculous - (What was your first memory of a silly thing that you or someone else did decades ago?\n\nHow come you still remember that without initial effortful thinking?\n\nWhat are the Associative Techniques to improve your memory?\n\nRemembering what you read\n\nDraw diagrams to see the structure of key ideas.â€\n\nIngrid Spielman recommends interacting with the text by asking yourself questions as you go along.\n\nIf youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€\n\nIf youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€\n\nskim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.\n\nThe first part is the impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.\n\nThe second part is an association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.\n\nThe third part is the repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.\n\nExperts say that, if you want to remember what you experience, itâ€™s important to do something with that information.\n\nTwo Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.\n\nPlus, if you find that you canâ€™t explain it, you might want to go back and reread.\n\nReadout loud to myself most of the time to understand and remember what I just read.â€\n\nStart by reading a Wikipedia article on the subject as preparation\n\nThe key is to see, connect, and then repeat.\n\nThe more you actively engage with the content that you are consuming, the more readily youâ€™ll remember it.\n\nTo remember something, you need to apply it. Instead of passively taking in information or actively trying to memorize it by rote, itâ€™s important to make connections. If you can apply what youâ€™ve learned, get feedback, and re-apply a concept with feedback, itâ€™s much more likely to stick.\n\nFor example, reading a recipe alone wonâ€™t help you learn to cook. Cooking a meal and having the combined feedback of your taste-buds and the comments of others will stand out in your mind. Watching someone do an exercise never has the same impact as doing it yourself. A framework is all but useless unless you apply it.\n\nWhen you apply a concept or practise to your life, it becomes easier to internalize the information.\n\nWhen you pick up a book or sit down to watch a movie, have a purpose in mind.\n\nWhen you spot related chapters or ideas in books, find ways to connect them. Highlight them, write notes, or clip the sections that are related. Taking notes by hand is an especially valuable way to help you remember important concepts.\n\nPeople who watch lots of movies or read lots of books, but canâ€™t remember them, waste a lot of time. They havenâ€™t taken in any information that will actually help them. To avoid forgetting everything you see, apply it immediately after you see it, and revisit the concepts often.\n\nWatching movies and reading mindlessly is a waste of time. Make the most of everything that you see and read by finding ways to engage with the content. Think of what youâ€™ll be missing if you allow these learning opportunities to pass you by.\n\nWhat are the mental traps that lead to learning difficulties? Avoid being duped that it is easy to remember so you donâ€™t put the effort in repetitive recall thus you quickly forget.\n\nThink in both images and words.\n\nHow do u recall information u have learnt? Give your mind a command to recall\n\nHow do you store information that you have read into your long term memory? Taking notes by hand is more effective than typing for memory retention\n\nPause after each page of reading, close your eyes to rest it and reflect on what you have read\n\nDonâ€™t read over and over, write a one-page summary. Recall information from memory\n\nSelf-test using flashcards using spaced repetition. Reflect &amp; repeat over &amp; over.\n\nDraw it out. Learn the difficult stuff in the morning\n\nIdentify the order in which to remember\n\nUse Colour, Sexuality, Humour, Outrageous Images, Exaggeration, Size Manipulation, Quantity Manipulation, 3D, time and space, to lock information into your long term memory - draw it out to play with images from Google.  \n \n\n8 tricks for remembering everything you read\n\nShana Lebowitz, Business Insider US\n\nReference: Public Accessed: 09. May 2020 [https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;IR=T](https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;IR=T)\n\n1. Take notes on the page.\n\nâ€œNever read without a pencil,â€ says Quora user Deniz AteÅŸ. â€œUnderline sentences you find confusing, interesting, or important. Draw lines along the side of important paragraphs. Draw diagrams to see the structure of key ideas.â€\n\n   \n \n\n2. Ask yourself questions about the material.\n\nIngrid Spielman recommends interacting with the text by asking yourself questions as you go along.\n\n   \n \n\nIf youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€\n\n   \n \n\nIf youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€\n\n   \n \n\n3. Skim the text first.\n\nAn anonymous user cites an article by Bill Klemm, Ph.D., a professor of neuroscience, which highlights skimming as a key strategy for retaining information.\n\n   \n \n\nThe idea here isnâ€™t to skip the whole reading process. Instead, youâ€™ll want to skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.\n\n   \n \n\nMake associations between the information youâ€™re reading and facts you already know. Francisco Osorio/Flickr\n\n4. Impress, associate, repeat.\n\nStack Exchange user TRdH says that memory is a three-pronged process. (His answer was reproduced on Lifehacker.)\n\n   \n \n\nThe first part is impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.\n\n   \n \n\nThe second part is association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.\n\n   \n \n\nThe third part is repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.\n\n   \n \n\n5. Introduce the information to others.\n\nExperts say that, if you want to remember what you experience, itâ€™s important to do something with that information.\n\n   \n \n\nTwo Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.\n\n   \n \n\nPlus, if you find that you canâ€™t explain it, you might want to go back and reread.\n\n   \n \n\n6. Read out loud.\n\nAnother anonymous Quora user says, â€œI actually have to read out loud to myself most of the time to understand and remember what I just read.â€\n\n   \n \n\nWriting in Psychology Today, psychologist Art Markman, Ph.D., says this strategy might work best when there are a few key items you need to remember. Thatâ€™s because the sentences you speak (or even whisper) out loud take on a distinctiveness. You remember producing and hearing the items and so your memory for them is different from the memory of the words you read silently.\n\n   \n \n\nResearch suggests reading on Kindle, instead of on paper, hurts your ability to remember a storyâ€™s plot. Flickr/Rich Mitchell\n\n7. Read on paper.\n\nE-readers are convenient tools for when you want to bring a ton of books on vacation and for downloading stories in an instant.\n\n   \n \n\nBut research suggests that they could also undermine the strength of your memories. One study found that, when people read the same short story in a paperback or on a Kindle, the paperback readers were better able to remember the storyâ€™s chronology.\n\n   \n \n\nLead study author Anne Mangen, Ph.D., says thatâ€™s possibly because the piles of pages in your hands creates a â€œtactile sense of progressâ€ that you donâ€™t get from a Kindle. (Of course, itâ€™s possible that people who are more accustomed to reading online may not have this problem.)\n\n   \n \n\nMeanwhile, Mangenâ€™s other research found that high-school students performed better on a test of reading comprehension when they read a text in print instead of on a computer screen.\n\n   \n \n\n8. Become familiar with the topic first.\n\nBlogger Ryan Battles recommends gaining some background knowledge before you dive into a particular text.\n\n   \n \n\nâ€œThe more you understand about a particular subject,â€ he writes, â€œthe more â€˜hooksâ€™ keep the facts in there.â€ Presumably, thatâ€™s because youâ€™re able to make more associations between the new information and what you already know.\n\n   \n \n\nYou can even start by reading a Wikipedia article on the subject as preparation.\n\nInformation + Emotion = Long-Term Memory\n\nMemory is the power of association.\n\nYour memory is not fixed. You donâ€™t have a memory, you do a memory.\n\nThere is a learning curve, but also a forgetting curve. Within 48 hours of learning something new, 80% of it can be gone.\n\nThe art of memory is the art of attention.\n\nAn incredible memory and a powerful presence come from being powerfully present.\n\nUse Spaced Repetition as a way to review the information to consolidate it from short to long-term memory.\n\nMake the information memorable. Make it silly, shocking or different.\n\nUse visualization and emotion when remembering things. What do you see and how it makes you feel.\n\nRemember: Information + Emotion = Long-Term Memory.\n\nProper sleep is very important for your memory and your brain.\n\nIt consolidates your short to long-term memory.\n\nDuring sleep, you clear the metabolic waste in your brain that leads to dementia and Alzheimer.\n\nWhen you are dreaming is when you come up with new solutions and ideas.\n\nMemory Principles:\n\nFirst or Primacy\n\nLast or Recency\n\nOrganized or Chunked\n\nEmotional\n\nDifferent or Unique\n\nFamiliar\n\nWe can visualize\n\nConnected\n\nAssociated\n\nPictures are a universal language.\n\nT.I.P. - Turn each element Into a Picture.\n\nConnect each picture to the next.\n\nUse emotion, visualization, action, and exaggeration.\n\nUse how your memory works, so that you can work your memory.\n\nBrain Bites from this lesson:\n\nThe Sun List provides you with 20 pegs. Use them!\n\nThe right answer is whatever works for you.\n\nThe best practice is teaching someone.\n\nT.I.P. - Turn Into Picture.\n\nThe challenge is not your retention, itâ€™s your attention.\n\nM.O.M. - Motivation, Observation, and Mechanics.\n\nBE SUAVE - Believe, Exercise, Say, Use, Ask, Visualize, and End.\n\nPractice makes permanent.\n\nMaking pictures helps you remember better.\n\nPIE: Place, Imagine, and Entwine.\n\nThe PIE Method works like this: Find a place on the person that pops out, imagine the personâ€™s name turned into a picture, then entwine or link the place and the image.\n\nGenius leaves clues. There is always a method behind what appears to be magical.\n\nVisual - Write the name on the person\'s forehead. You can use your favorite colour.\n\nAuditory - Repeat the name 2-3 times. \\[Remember also the S in BE SUAVE\\]\n\nKinaesthetic - Use micro-movements to write the name with your hand on the side of your body.\n\nT.I.P. - Turn Into Picture.\n\nLink images using the Vowels.\n\nThe Vowels: Action, Emotion, Illogical, Outstanding, Unusual.\n\nYour memory has three parts: Encode, Store, Retrieve.\n\nTake a picture and substitute it for the word.\n\nTurn the ordinary into extraordinary using intensifiers. \\[imagination, visualization, emotion, association, etc.\\]\n\nYou can learn using frequency, duration, or (the best way) intensity.\n\nTurn the words you want to memorize into pictures and link them via intensifiers. \\[imagination, visualization, emotion, association, etc.\\]\n\nMemory has 3 parts: Encode, Store, Retrieve.\n\nP.I.E. - Place, Imagine, Entwine.\n\nThe Location Method\n\nFind 5 places\n\nGo clockwise\n\nPick unique items\n\nPick large items\n\nNo empty spaces\n\nJimâ€™s Morning Routine\n\nRemember dreams\n\nMake the bed\n\nDrink water\n\nPhysical exercise\n\nBreathing techniques\n\nCold shower\n\nBrush teeth with the opposite hand\n\nSuperbrain Smoothie\n\nJournaling\n\nNew learnings\n\nT.I.P. - Turn Into Picture.\n\nIf you can clearly imagine it, you\'ll clearly remember it.\n\nTake the Sun List images and use Chain Linking.\n\nUse visual or auditory Basic Association to build your own lists.\n\nAuditory Basic Association\n\n1 - Bun\n\n2 - Shoe\n\n3 - Tree\n\n4 - Door\n\n5 - Hive\n\n6 - Sticks\n\n7 - Heaven\n\n8 - Gate\n\n9 - Wine\n\n10 - Zen\n\nNumbers are abstract, words are easier to remember.\n\nUse the numbers 0 to 9 and assign a consonant sound to each number.\n\nAlphanumeric Code Of Memory\n\n1 = T, D, Th (strokes)\n\n2 = N (upside down N)\n\n3 = M (3M)\n\n4 = R (Four)\n\n5 = L (hand)\n\n6 = J, G (soft), Sh, Ch (Mirror Image)\n\n7 = C (hard), K, G (hard) (top bottom K)\n\n8 = F, V (V8)\n\n9 = B, P (Mirror Image)\n\n0 = S, C (soft), Z (Zorro)\n\nRules\n\nVowels have no value.\n\nSilent letters have no value.\n\nW, H, Y have no value.\n\nDouble letters count once.\n\nNumbers are abstract, words are easier to remember.\n\nPick the word that is the easiest to picture.\n\nExample of 1 to 10\n\n1 = T = Tie\n\n2 = N = Noah\n\n3 = M = Ma (Mother)\n\n4 = R = Rye\n\n5 = L = Law\n\n6 = Sh = Shoe\n\n7 = K = Key\n\n8 = V = Ivy\n\n9 = B = Bee\n\n10 = T, S = Toes\n\nA single number can create multiple words\n\n72 = K, N = Can | Cone | Gone\n\n72 = G (hard), N = Gone | Gun | Goon\n\n72 = C (hard), N = Cane\n\nOther examples\n\n33 = M, M = Mummy\n\n47 = R, K = Rock\n\n49 = R, P = Rope\n\n51 = L, T = Lite\n\n60 = Ch, S = Cheese\n\n80 = F, C = Face\n\n97 = B, K = Book\n\nWords transformed into numbers\n\nTable = T, B, L = 195\n\nCat = K, T = 71\n\nCarpet = K, R, P, T = 7491\n\nButter = B, T, R = 914\n\nBody Folders in Numeric Code\n\n\\*\\*\\*We use just the 1st sound of each element in the Body Folders.\n\nTop = T = 1\n\nNose = N = 2\n\nMouth = M = 3\n\nEars = R = 4\n\nLarynx = L = 5\n\nShoulders = Sh = 6\n\nCollar = C = 7\n\nFingers = F = 8\n\nBelly = B = 9\n\nSeat = S = 10\n\nThere is no learning without memory.\n\nThe champion pushes past the pain period.\n\nAll behaviour is belief-driven.\n\nThe key to better comprehension is by asking better questions.\n\nBehaviour what, Capability how, beliefs and values why, identity who, environment where &amp; when.\n\nAsk a question, reticular activation system.\n\nThe 8 C\'s To Muscle Memory\n\nCompetency\n\nChunking\n\nCombining\n\nConsequences\n\nCharacter\n\nConsistency\n\nCommit by burning the bridges. Make a decision by cutting from other possibilities.\n\nHave a Coach that challenge you.\n\nClose my eyes and use my imagination and imagine I am that person.\n\nYou will never need to memorise a book word for word. \n\nTake your textbook, and take a good look at it:\n\nLook at the front cover.\n\nLook at the back cover.\n\nLook over the introduction.\n\nRead the conclusion, and\n\nBe sure to scan through the index, if your book has one.\n\nAnd read information about the bookâ€™s publication, like the place of publication, the publisher, and the publication date\n\nRead the table of contents, introduction and conclusion. \n\nTurn the memorised information into knowledge that you can use over and over â€” not just for this single test or exam.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying - How a good memory makes you wealthy $$$', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg7vxh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.64, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': 1588999068.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589023618.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Benefits of a Good Memory&lt;/p&gt;\n\n&lt;p&gt;â€œGreat memory at work leads to be a star = promotion &amp;amp; $$$. Not forgetting your partnerâ€™s birthday or your wedding anniversary leads to a happier life.â€&lt;/p&gt;\n\n&lt;p&gt;IMAGINATION&lt;/p&gt;\n\n&lt;p&gt;Albert Einstein once said: Knowledge takes you from A to Z but imagination takes you everywhere.&lt;/p&gt;\n\n&lt;p&gt;Pay attention and convert the words of a book page into a story or an image.&lt;/p&gt;\n\n&lt;p&gt;Make the learning of personally meaningful material easy by leveraging on your imagination.&lt;/p&gt;\n\n&lt;p&gt;Look for meaning and relationship between data points to extract action points that translate into money or benefits. See it in your mind like a movie as clearly as you can. Hear &amp;amp; feel it. Use your entire being and emotions to invest in making the material part of who you are and the knowledge will never leave you. Look for alternative words that sound like the difficult word that I am learning. This is associative thinking in play.&lt;/p&gt;\n\n&lt;p&gt;When you have trouble making a mental connection in your mind about the material that you are trying to remember, that means that you failed to understand the material fully. Go back and look at learning this with a different strategy or ask for help from a friend, colleague, expert or stranger.&lt;/p&gt;\n\n&lt;p&gt;SPACED REPETITION&lt;/p&gt;\n\n&lt;p&gt;The art of memorisation lies in practice recalling - not repeat reading&lt;/p&gt;\n\n&lt;p&gt;Spaced repetition helps to retain and reinforce what you have learnt.&lt;/p&gt;\n\n&lt;p&gt;Ask: how is the information I am learning going to help me right now and in the future?&lt;/p&gt;\n\n&lt;p&gt;How is the new information move me a step towards my goals?&lt;/p&gt;\n\n&lt;p&gt;How is the new material going to tell me something that not many people know?&lt;/p&gt;\n\n&lt;p&gt;Find action steps from the new material.&lt;/p&gt;\n\n&lt;p&gt;Learn and remember by applying creativity to everything that they know, i.e. produce images in our minds. Use your five senses. Manipulate the size of the image and make the image ridiculous. Colour the images with the rainbow shades of life. Use action, singing, talking and dancing to animate the images. Donâ€™t let the rules of inhibition prevent you from playing with your images.&lt;/p&gt;\n\n&lt;p&gt;Extract a familiar word or phrase from familiar and link it with the sound that is the same or similar to the abstract word.&lt;/p&gt;\n\n&lt;p&gt;Plant images at markers along mind palace journey or in mindâ€™s buildings.&lt;/p&gt;\n\n&lt;p&gt;Turn the abstract concept into something tangible.&lt;/p&gt;\n\n&lt;p&gt;Review journey/building backwards &amp;amp; forwards a few times.&lt;/p&gt;\n\n&lt;p&gt;Periodic self-test and continued use.&lt;/p&gt;\n\n&lt;p&gt;Giving personal meaning to numbers.&lt;/p&gt;\n\n&lt;p&gt;Interest level is measured by how much you remember.&lt;/p&gt;\n\n&lt;p&gt;Make an image vocabulary for key subject concepts.&lt;/p&gt;\n\n&lt;p&gt;Review images 1 hr later, 1 day, 2 days, 3 days, 1 week, 2 weeks, 3 weeks, | month, 2 months, 3 months.&lt;/p&gt;\n\n&lt;p&gt;OTHER&lt;/p&gt;\n\n&lt;p&gt;Selectively take 10% of a book page out and rephrase into a single sentence in your own words and covert that single sentence into an image and story. Add sound to that image.&lt;/p&gt;\n\n&lt;p&gt;Forgetting irrelevant information helps us to prioritise.&lt;/p&gt;\n\n&lt;p&gt;The art of thinking is different from having eidetic memory or being able to Google well.&lt;/p&gt;\n\n&lt;p&gt;Teach others.&lt;/p&gt;\n\n&lt;p&gt;Ask for help when you are stuck.&lt;/p&gt;\n\n&lt;p&gt;People often study a field when they are young and naive and simply goes with what is popular and will get a job even if they are disinterested in the field. It makes the learning curve steep and learning unpleasant. They wonâ€™t succeed in their job would have wasted decades of their life.&lt;/p&gt;\n\n&lt;p&gt;Hard Facts that are hard to derive meaning needs to be learned by rote. Even if learning by rote, you could adopt spaced timed interval repetition for review and revision.&lt;/p&gt;\n\n&lt;p&gt;Find the easiest way to learn the material. Work hard but think smart, find out how it applies to learn and to work too.&lt;/p&gt;\n\n&lt;p&gt;Find out and appreciate the reason behind each point. When you gain an appreciation of the underlying principles behind why something exists, you will be emotionally moved by its beauty and it will be effortless to remember the information. Bridge new information to old information. By making association from old information to new information it makes it easy to develop neural pathways in your both left and right hemispheres of your brain.&lt;/p&gt;\n\n&lt;p&gt;Use your family and friends body parts to remember your material. Make it intimate.&lt;/p&gt;\n\n&lt;p&gt;How to remember numbers:&lt;/p&gt;\n\n&lt;p&gt;Zero for (Z,S)&lt;/p&gt;\n\n&lt;p&gt;One for One-way street (t)&lt;/p&gt;\n\n&lt;p&gt;Two for On/Off Switch (n)&lt;/p&gt;\n\n&lt;p&gt;Three for Three Blind Mice (m)&lt;/p&gt;\n\n&lt;p&gt;Four for 4ingers chicken (R)&lt;/p&gt;\n\n&lt;p&gt;Five for Five Star Hotel (L)&lt;/p&gt;\n\n&lt;p&gt;Six for Sixth Sense (J)&lt;/p&gt;\n\n&lt;p&gt;Seven for 7 Eleven (K)&lt;/p&gt;\n\n&lt;p&gt;Eight for 8 Ball (G)&lt;/p&gt;\n\n&lt;p&gt;Nine for Cat with 9 Lives (P)&lt;/p&gt;\n\n&lt;p&gt;Ten for Ten Fingers &amp;amp; Toes&lt;/p&gt;\n\n&lt;p&gt;Summarise what you have written. People are busy. Deliver 70% of value in 3 minutes is better than 0% of the value with wordy text.&lt;/p&gt;\n\n&lt;p&gt;Why should students put in so much effort to memorize equations and facts, when they could easily find that information at the click of a mouse button?&lt;/p&gt;\n\n&lt;p&gt;Jianfa: â€œWhy didnâ€™t schools articulate specifically how does learning this subject help student make money in the near future directly other than some vague â€“ this graduation paper will get you a job which we know that getting a good job requires referrals and human connections which you get from the dark arts and partying.&lt;/p&gt;\n\n&lt;p&gt;Five minutes of light physical exercise after any learning session can give your memory a boost.&lt;/p&gt;\n\n&lt;p&gt;When you have to put something in an unfamiliar place, say what you are doing out loud: &amp;quot;I am putting my sunglasses on the table by the door.&amp;quot; By letting your ears register the information, you increase your chances of remembering it later.&lt;/p&gt;\n\n&lt;p&gt;Make up a story - break the information down and make up a story linking together each piece of information.&lt;/p&gt;\n\n&lt;p&gt;Ask yourself Who, What, When, Why, Where, How.&lt;/p&gt;\n\n&lt;p&gt;Create mind maps, colour-code your notes, rely on written instructions for assignments and projects, use visual aids such as flashcards, diagrams, charts and pictures, watch a short clip or video that explains your topic.&lt;/p&gt;\n\n&lt;p&gt;Record your lectures and class notes, attend group study sessions, participate in class discussions, read the material out loud, make up a rhyme or song about the topic, use word association.&lt;/p&gt;\n\n&lt;p&gt;Take detailed notes in class, rewrite your notes, turn diagrams and charts into words, ask your teacher for handouts, write regular summaries.&lt;/p&gt;\n\n&lt;p&gt;Add a physical activity while you study, act out concepts and theories, build 3D models to apply complex information, study in short blocks, study with others, including plenty of examples in your notes, attend field trips to make information more memorable.&lt;/p&gt;\n\n&lt;p&gt;Avoid over-studying and filling your brain with useless information. Many people have a tendency to focus on unnecessary facts&lt;/p&gt;\n\n&lt;p&gt;To remember any piece of information, begin the review/rehearsal process as soon as possible after the information enters your short-term memory.&lt;/p&gt;\n\n&lt;p&gt;It is easier to remember information that has been grouped, organized, or clustered than information that has not.&lt;/p&gt;\n\n&lt;p&gt;Individuals remember more of what was learned at the beginning and end of a learning session.&lt;/p&gt;\n\n&lt;p&gt;make a conscious decision and effort to remember something.&lt;/p&gt;\n\n&lt;p&gt;Avoid heavy meals before exams. Drink water earlier in the day to avoid dehydration.&lt;/p&gt;\n\n&lt;p&gt;In order to remember something thoroughly, we must be interested in it.&lt;/p&gt;\n\n&lt;p&gt;Be positive about remembering -&lt;/p&gt;\n\n&lt;p&gt;Most academic subjects can be classified into 3 categories,&lt;/p&gt;\n\n&lt;p&gt;a) problem solving;&lt;/p&gt;\n\n&lt;p&gt;b) concept-driven&lt;/p&gt;\n\n&lt;p&gt;c) interpretation&lt;/p&gt;\n\n&lt;p&gt;Segregate &amp;quot;core material&amp;quot; from &amp;quot;elaborative material&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Distil notes, repeat and write. Read ideas out loud in a dramatic manner. Re-organize ideas (think)&lt;/p&gt;\n\n&lt;p&gt;Get an overview. Limit the amount to learn. Visualise, or draw an image. Link concepts. Test yourself repeatedly. Explain ideas to someone. Develop or use photographic memory. Move while repeating the main points, walk, dance, kick a ball, then write them. Develop a mnemonic for the most important points.&lt;/p&gt;\n\n&lt;p&gt;We cannot write our notes down logical because we tend to forget logical stuff more than creative stuff. Jianfa Thesis: Typing your notes are fast, but you should turn your typed notes in your own words into questions and answers. Take the Q&amp;amp;A into Anki free flashcards app, as you test yourself with the flashcards, handwrite the answer down with pen and paper and draw simple images to illustrate the answer. Voice record the Q&amp;amp;A into a voice memo app and listen to your recording while travelling on public transport.&lt;/p&gt;\n\n&lt;p&gt;What feelings must you employ to improve your memory?&lt;/p&gt;\n\n&lt;p&gt;- Emotion - (If you love what you do, you will remember what you love) &amp;gt; Be Present - (Center yourself in the present and be aware) &amp;gt; Pay Attention - (Not paying attention leads to costly mistakes)&amp;gt; Do 1 thing at a time (Unless you are breathing and talking, humans can realistically do 1 thing well at a time.) &amp;gt; Interesting - (If you love your job, you will find even the most mundane of tasks interesting?) &amp;gt; Attractive - (Your mind remembers beautiful people? Capitalise that strength) &amp;gt; Ridiculous - (What was your first memory of a silly thing that you or someone else did decades ago?&lt;/p&gt;\n\n&lt;p&gt;How come you still remember that without initial effortful thinking?&lt;/p&gt;\n\n&lt;p&gt;What are the Associative Techniques to improve your memory?&lt;/p&gt;\n\n&lt;p&gt;Remembering what you read&lt;/p&gt;\n\n&lt;p&gt;Draw diagrams to see the structure of key ideas.â€&lt;/p&gt;\n\n&lt;p&gt;Ingrid Spielman recommends interacting with the text by asking yourself questions as you go along.&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€&lt;/p&gt;\n\n&lt;p&gt;skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.&lt;/p&gt;\n\n&lt;p&gt;The first part is the impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.&lt;/p&gt;\n\n&lt;p&gt;The second part is an association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.&lt;/p&gt;\n\n&lt;p&gt;The third part is the repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.&lt;/p&gt;\n\n&lt;p&gt;Experts say that, if you want to remember what you experience, itâ€™s important to do something with that information.&lt;/p&gt;\n\n&lt;p&gt;Two Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.&lt;/p&gt;\n\n&lt;p&gt;Plus, if you find that you canâ€™t explain it, you might want to go back and reread.&lt;/p&gt;\n\n&lt;p&gt;Readout loud to myself most of the time to understand and remember what I just read.â€&lt;/p&gt;\n\n&lt;p&gt;Start by reading a Wikipedia article on the subject as preparation&lt;/p&gt;\n\n&lt;p&gt;The key is to see, connect, and then repeat.&lt;/p&gt;\n\n&lt;p&gt;The more you actively engage with the content that you are consuming, the more readily youâ€™ll remember it.&lt;/p&gt;\n\n&lt;p&gt;To remember something, you need to apply it. Instead of passively taking in information or actively trying to memorize it by rote, itâ€™s important to make connections. If you can apply what youâ€™ve learned, get feedback, and re-apply a concept with feedback, itâ€™s much more likely to stick.&lt;/p&gt;\n\n&lt;p&gt;For example, reading a recipe alone wonâ€™t help you learn to cook. Cooking a meal and having the combined feedback of your taste-buds and the comments of others will stand out in your mind. Watching someone do an exercise never has the same impact as doing it yourself. A framework is all but useless unless you apply it.&lt;/p&gt;\n\n&lt;p&gt;When you apply a concept or practise to your life, it becomes easier to internalize the information.&lt;/p&gt;\n\n&lt;p&gt;When you pick up a book or sit down to watch a movie, have a purpose in mind.&lt;/p&gt;\n\n&lt;p&gt;When you spot related chapters or ideas in books, find ways to connect them. Highlight them, write notes, or clip the sections that are related. Taking notes by hand is an especially valuable way to help you remember important concepts.&lt;/p&gt;\n\n&lt;p&gt;People who watch lots of movies or read lots of books, but canâ€™t remember them, waste a lot of time. They havenâ€™t taken in any information that will actually help them. To avoid forgetting everything you see, apply it immediately after you see it, and revisit the concepts often.&lt;/p&gt;\n\n&lt;p&gt;Watching movies and reading mindlessly is a waste of time. Make the most of everything that you see and read by finding ways to engage with the content. Think of what youâ€™ll be missing if you allow these learning opportunities to pass you by.&lt;/p&gt;\n\n&lt;p&gt;What are the mental traps that lead to learning difficulties? Avoid being duped that it is easy to remember so you donâ€™t put the effort in repetitive recall thus you quickly forget.&lt;/p&gt;\n\n&lt;p&gt;Think in both images and words.&lt;/p&gt;\n\n&lt;p&gt;How do u recall information u have learnt? Give your mind a command to recall&lt;/p&gt;\n\n&lt;p&gt;How do you store information that you have read into your long term memory? Taking notes by hand is more effective than typing for memory retention&lt;/p&gt;\n\n&lt;p&gt;Pause after each page of reading, close your eyes to rest it and reflect on what you have read&lt;/p&gt;\n\n&lt;p&gt;Donâ€™t read over and over, write a one-page summary. Recall information from memory&lt;/p&gt;\n\n&lt;p&gt;Self-test using flashcards using spaced repetition. Reflect &amp;amp; repeat over &amp;amp; over.&lt;/p&gt;\n\n&lt;p&gt;Draw it out. Learn the difficult stuff in the morning&lt;/p&gt;\n\n&lt;p&gt;Identify the order in which to remember&lt;/p&gt;\n\n&lt;p&gt;Use Colour, Sexuality, Humour, Outrageous Images, Exaggeration, Size Manipulation, Quantity Manipulation, 3D, time and space, to lock information into your long term memory - draw it out to play with images from Google.  &lt;/p&gt;\n\n&lt;p&gt;8 tricks for remembering everything you read&lt;/p&gt;\n\n&lt;p&gt;Shana Lebowitz, Business Insider US&lt;/p&gt;\n\n&lt;p&gt;Reference: Public Accessed: 09. May 2020 &lt;a href=""https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;amp;IR=T""&gt;https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;amp;IR=T&lt;/a&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Take notes on the page.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;â€œNever read without a pencil,â€ says Quora user Deniz AteÅŸ. â€œUnderline sentences you find confusing, interesting, or important. Draw lines along the side of important paragraphs. Draw diagrams to see the structure of key ideas.â€&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ask yourself questions about the material.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ingrid Spielman recommends interacting with the text by asking yourself questions as you go along.&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Skim the text first.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;An anonymous user cites an article by Bill Klemm, Ph.D., a professor of neuroscience, which highlights skimming as a key strategy for retaining information.&lt;/p&gt;\n\n&lt;p&gt;The idea here isnâ€™t to skip the whole reading process. Instead, youâ€™ll want to skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.&lt;/p&gt;\n\n&lt;p&gt;Make associations between the information youâ€™re reading and facts you already know. Francisco Osorio/Flickr&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Impress, associate, repeat.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Stack Exchange user TRdH says that memory is a three-pronged process. (His answer was reproduced on Lifehacker.)&lt;/p&gt;\n\n&lt;p&gt;The first part is impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.&lt;/p&gt;\n\n&lt;p&gt;The second part is association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.&lt;/p&gt;\n\n&lt;p&gt;The third part is repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Introduce the information to others.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Experts say that, if you want to remember what you experience, itâ€™s important to do something with that information.&lt;/p&gt;\n\n&lt;p&gt;Two Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.&lt;/p&gt;\n\n&lt;p&gt;Plus, if you find that you canâ€™t explain it, you might want to go back and reread.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read out loud.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Another anonymous Quora user says, â€œI actually have to read out loud to myself most of the time to understand and remember what I just read.â€&lt;/p&gt;\n\n&lt;p&gt;Writing in Psychology Today, psychologist Art Markman, Ph.D., says this strategy might work best when there are a few key items you need to remember. Thatâ€™s because the sentences you speak (or even whisper) out loud take on a distinctiveness. You remember producing and hearing the items and so your memory for them is different from the memory of the words you read silently.&lt;/p&gt;\n\n&lt;p&gt;Research suggests reading on Kindle, instead of on paper, hurts your ability to remember a storyâ€™s plot. Flickr/Rich Mitchell&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read on paper.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;E-readers are convenient tools for when you want to bring a ton of books on vacation and for downloading stories in an instant.&lt;/p&gt;\n\n&lt;p&gt;But research suggests that they could also undermine the strength of your memories. One study found that, when people read the same short story in a paperback or on a Kindle, the paperback readers were better able to remember the storyâ€™s chronology.&lt;/p&gt;\n\n&lt;p&gt;Lead study author Anne Mangen, Ph.D., says thatâ€™s possibly because the piles of pages in your hands creates a â€œtactile sense of progressâ€ that you donâ€™t get from a Kindle. (Of course, itâ€™s possible that people who are more accustomed to reading online may not have this problem.)&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Mangenâ€™s other research found that high-school students performed better on a test of reading comprehension when they read a text in print instead of on a computer screen.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Become familiar with the topic first.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Blogger Ryan Battles recommends gaining some background knowledge before you dive into a particular text.&lt;/p&gt;\n\n&lt;p&gt;â€œThe more you understand about a particular subject,â€ he writes, â€œthe more â€˜hooksâ€™ keep the facts in there.â€ Presumably, thatâ€™s because youâ€™re able to make more associations between the new information and what you already know.&lt;/p&gt;\n\n&lt;p&gt;You can even start by reading a Wikipedia article on the subject as preparation.&lt;/p&gt;\n\n&lt;p&gt;Information + Emotion = Long-Term Memory&lt;/p&gt;\n\n&lt;p&gt;Memory is the power of association.&lt;/p&gt;\n\n&lt;p&gt;Your memory is not fixed. You donâ€™t have a memory, you do a memory.&lt;/p&gt;\n\n&lt;p&gt;There is a learning curve, but also a forgetting curve. Within 48 hours of learning something new, 80% of it can be gone.&lt;/p&gt;\n\n&lt;p&gt;The art of memory is the art of attention.&lt;/p&gt;\n\n&lt;p&gt;An incredible memory and a powerful presence come from being powerfully present.&lt;/p&gt;\n\n&lt;p&gt;Use Spaced Repetition as a way to review the information to consolidate it from short to long-term memory.&lt;/p&gt;\n\n&lt;p&gt;Make the information memorable. Make it silly, shocking or different.&lt;/p&gt;\n\n&lt;p&gt;Use visualization and emotion when remembering things. What do you see and how it makes you feel.&lt;/p&gt;\n\n&lt;p&gt;Remember: Information + Emotion = Long-Term Memory.&lt;/p&gt;\n\n&lt;p&gt;Proper sleep is very important for your memory and your brain.&lt;/p&gt;\n\n&lt;p&gt;It consolidates your short to long-term memory.&lt;/p&gt;\n\n&lt;p&gt;During sleep, you clear the metabolic waste in your brain that leads to dementia and Alzheimer.&lt;/p&gt;\n\n&lt;p&gt;When you are dreaming is when you come up with new solutions and ideas.&lt;/p&gt;\n\n&lt;p&gt;Memory Principles:&lt;/p&gt;\n\n&lt;p&gt;First or Primacy&lt;/p&gt;\n\n&lt;p&gt;Last or Recency&lt;/p&gt;\n\n&lt;p&gt;Organized or Chunked&lt;/p&gt;\n\n&lt;p&gt;Emotional&lt;/p&gt;\n\n&lt;p&gt;Different or Unique&lt;/p&gt;\n\n&lt;p&gt;Familiar&lt;/p&gt;\n\n&lt;p&gt;We can visualize&lt;/p&gt;\n\n&lt;p&gt;Connected&lt;/p&gt;\n\n&lt;p&gt;Associated&lt;/p&gt;\n\n&lt;p&gt;Pictures are a universal language.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn each element Into a Picture.&lt;/p&gt;\n\n&lt;p&gt;Connect each picture to the next.&lt;/p&gt;\n\n&lt;p&gt;Use emotion, visualization, action, and exaggeration.&lt;/p&gt;\n\n&lt;p&gt;Use how your memory works, so that you can work your memory.&lt;/p&gt;\n\n&lt;p&gt;Brain Bites from this lesson:&lt;/p&gt;\n\n&lt;p&gt;The Sun List provides you with 20 pegs. Use them!&lt;/p&gt;\n\n&lt;p&gt;The right answer is whatever works for you.&lt;/p&gt;\n\n&lt;p&gt;The best practice is teaching someone.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;The challenge is not your retention, itâ€™s your attention.&lt;/p&gt;\n\n&lt;p&gt;M.O.M. - Motivation, Observation, and Mechanics.&lt;/p&gt;\n\n&lt;p&gt;BE SUAVE - Believe, Exercise, Say, Use, Ask, Visualize, and End.&lt;/p&gt;\n\n&lt;p&gt;Practice makes permanent.&lt;/p&gt;\n\n&lt;p&gt;Making pictures helps you remember better.&lt;/p&gt;\n\n&lt;p&gt;PIE: Place, Imagine, and Entwine.&lt;/p&gt;\n\n&lt;p&gt;The PIE Method works like this: Find a place on the person that pops out, imagine the personâ€™s name turned into a picture, then entwine or link the place and the image.&lt;/p&gt;\n\n&lt;p&gt;Genius leaves clues. There is always a method behind what appears to be magical.&lt;/p&gt;\n\n&lt;p&gt;Visual - Write the name on the person&amp;#39;s forehead. You can use your favorite colour.&lt;/p&gt;\n\n&lt;p&gt;Auditory - Repeat the name 2-3 times. [Remember also the S in BE SUAVE]&lt;/p&gt;\n\n&lt;p&gt;Kinaesthetic - Use micro-movements to write the name with your hand on the side of your body.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;Link images using the Vowels.&lt;/p&gt;\n\n&lt;p&gt;The Vowels: Action, Emotion, Illogical, Outstanding, Unusual.&lt;/p&gt;\n\n&lt;p&gt;Your memory has three parts: Encode, Store, Retrieve.&lt;/p&gt;\n\n&lt;p&gt;Take a picture and substitute it for the word.&lt;/p&gt;\n\n&lt;p&gt;Turn the ordinary into extraordinary using intensifiers. [imagination, visualization, emotion, association, etc.]&lt;/p&gt;\n\n&lt;p&gt;You can learn using frequency, duration, or (the best way) intensity.&lt;/p&gt;\n\n&lt;p&gt;Turn the words you want to memorize into pictures and link them via intensifiers. [imagination, visualization, emotion, association, etc.]&lt;/p&gt;\n\n&lt;p&gt;Memory has 3 parts: Encode, Store, Retrieve.&lt;/p&gt;\n\n&lt;p&gt;P.I.E. - Place, Imagine, Entwine.&lt;/p&gt;\n\n&lt;p&gt;The Location Method&lt;/p&gt;\n\n&lt;p&gt;Find 5 places&lt;/p&gt;\n\n&lt;p&gt;Go clockwise&lt;/p&gt;\n\n&lt;p&gt;Pick unique items&lt;/p&gt;\n\n&lt;p&gt;Pick large items&lt;/p&gt;\n\n&lt;p&gt;No empty spaces&lt;/p&gt;\n\n&lt;p&gt;Jimâ€™s Morning Routine&lt;/p&gt;\n\n&lt;p&gt;Remember dreams&lt;/p&gt;\n\n&lt;p&gt;Make the bed&lt;/p&gt;\n\n&lt;p&gt;Drink water&lt;/p&gt;\n\n&lt;p&gt;Physical exercise&lt;/p&gt;\n\n&lt;p&gt;Breathing techniques&lt;/p&gt;\n\n&lt;p&gt;Cold shower&lt;/p&gt;\n\n&lt;p&gt;Brush teeth with the opposite hand&lt;/p&gt;\n\n&lt;p&gt;Superbrain Smoothie&lt;/p&gt;\n\n&lt;p&gt;Journaling&lt;/p&gt;\n\n&lt;p&gt;New learnings&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;If you can clearly imagine it, you&amp;#39;ll clearly remember it.&lt;/p&gt;\n\n&lt;p&gt;Take the Sun List images and use Chain Linking.&lt;/p&gt;\n\n&lt;p&gt;Use visual or auditory Basic Association to build your own lists.&lt;/p&gt;\n\n&lt;p&gt;Auditory Basic Association&lt;/p&gt;\n\n&lt;p&gt;1 - Bun&lt;/p&gt;\n\n&lt;p&gt;2 - Shoe&lt;/p&gt;\n\n&lt;p&gt;3 - Tree&lt;/p&gt;\n\n&lt;p&gt;4 - Door&lt;/p&gt;\n\n&lt;p&gt;5 - Hive&lt;/p&gt;\n\n&lt;p&gt;6 - Sticks&lt;/p&gt;\n\n&lt;p&gt;7 - Heaven&lt;/p&gt;\n\n&lt;p&gt;8 - Gate&lt;/p&gt;\n\n&lt;p&gt;9 - Wine&lt;/p&gt;\n\n&lt;p&gt;10 - Zen&lt;/p&gt;\n\n&lt;p&gt;Numbers are abstract, words are easier to remember.&lt;/p&gt;\n\n&lt;p&gt;Use the numbers 0 to 9 and assign a consonant sound to each number.&lt;/p&gt;\n\n&lt;p&gt;Alphanumeric Code Of Memory&lt;/p&gt;\n\n&lt;p&gt;1 = T, D, Th (strokes)&lt;/p&gt;\n\n&lt;p&gt;2 = N (upside down N)&lt;/p&gt;\n\n&lt;p&gt;3 = M (3M)&lt;/p&gt;\n\n&lt;p&gt;4 = R (Four)&lt;/p&gt;\n\n&lt;p&gt;5 = L (hand)&lt;/p&gt;\n\n&lt;p&gt;6 = J, G (soft), Sh, Ch (Mirror Image)&lt;/p&gt;\n\n&lt;p&gt;7 = C (hard), K, G (hard) (top bottom K)&lt;/p&gt;\n\n&lt;p&gt;8 = F, V (V8)&lt;/p&gt;\n\n&lt;p&gt;9 = B, P (Mirror Image)&lt;/p&gt;\n\n&lt;p&gt;0 = S, C (soft), Z (Zorro)&lt;/p&gt;\n\n&lt;p&gt;Rules&lt;/p&gt;\n\n&lt;p&gt;Vowels have no value.&lt;/p&gt;\n\n&lt;p&gt;Silent letters have no value.&lt;/p&gt;\n\n&lt;p&gt;W, H, Y have no value.&lt;/p&gt;\n\n&lt;p&gt;Double letters count once.&lt;/p&gt;\n\n&lt;p&gt;Numbers are abstract, words are easier to remember.&lt;/p&gt;\n\n&lt;p&gt;Pick the word that is the easiest to picture.&lt;/p&gt;\n\n&lt;p&gt;Example of 1 to 10&lt;/p&gt;\n\n&lt;p&gt;1 = T = Tie&lt;/p&gt;\n\n&lt;p&gt;2 = N = Noah&lt;/p&gt;\n\n&lt;p&gt;3 = M = Ma (Mother)&lt;/p&gt;\n\n&lt;p&gt;4 = R = Rye&lt;/p&gt;\n\n&lt;p&gt;5 = L = Law&lt;/p&gt;\n\n&lt;p&gt;6 = Sh = Shoe&lt;/p&gt;\n\n&lt;p&gt;7 = K = Key&lt;/p&gt;\n\n&lt;p&gt;8 = V = Ivy&lt;/p&gt;\n\n&lt;p&gt;9 = B = Bee&lt;/p&gt;\n\n&lt;p&gt;10 = T, S = Toes&lt;/p&gt;\n\n&lt;p&gt;A single number can create multiple words&lt;/p&gt;\n\n&lt;p&gt;72 = K, N = Can | Cone | Gone&lt;/p&gt;\n\n&lt;p&gt;72 = G (hard), N = Gone | Gun | Goon&lt;/p&gt;\n\n&lt;p&gt;72 = C (hard), N = Cane&lt;/p&gt;\n\n&lt;p&gt;Other examples&lt;/p&gt;\n\n&lt;p&gt;33 = M, M = Mummy&lt;/p&gt;\n\n&lt;p&gt;47 = R, K = Rock&lt;/p&gt;\n\n&lt;p&gt;49 = R, P = Rope&lt;/p&gt;\n\n&lt;p&gt;51 = L, T = Lite&lt;/p&gt;\n\n&lt;p&gt;60 = Ch, S = Cheese&lt;/p&gt;\n\n&lt;p&gt;80 = F, C = Face&lt;/p&gt;\n\n&lt;p&gt;97 = B, K = Book&lt;/p&gt;\n\n&lt;p&gt;Words transformed into numbers&lt;/p&gt;\n\n&lt;p&gt;Table = T, B, L = 195&lt;/p&gt;\n\n&lt;p&gt;Cat = K, T = 71&lt;/p&gt;\n\n&lt;p&gt;Carpet = K, R, P, T = 7491&lt;/p&gt;\n\n&lt;p&gt;Butter = B, T, R = 914&lt;/p&gt;\n\n&lt;p&gt;Body Folders in Numeric Code&lt;/p&gt;\n\n&lt;p&gt;***We use just the 1st sound of each element in the Body Folders.&lt;/p&gt;\n\n&lt;p&gt;Top = T = 1&lt;/p&gt;\n\n&lt;p&gt;Nose = N = 2&lt;/p&gt;\n\n&lt;p&gt;Mouth = M = 3&lt;/p&gt;\n\n&lt;p&gt;Ears = R = 4&lt;/p&gt;\n\n&lt;p&gt;Larynx = L = 5&lt;/p&gt;\n\n&lt;p&gt;Shoulders = Sh = 6&lt;/p&gt;\n\n&lt;p&gt;Collar = C = 7&lt;/p&gt;\n\n&lt;p&gt;Fingers = F = 8&lt;/p&gt;\n\n&lt;p&gt;Belly = B = 9&lt;/p&gt;\n\n&lt;p&gt;Seat = S = 10&lt;/p&gt;\n\n&lt;p&gt;There is no learning without memory.&lt;/p&gt;\n\n&lt;p&gt;The champion pushes past the pain period.&lt;/p&gt;\n\n&lt;p&gt;All behaviour is belief-driven.&lt;/p&gt;\n\n&lt;p&gt;The key to better comprehension is by asking better questions.&lt;/p&gt;\n\n&lt;p&gt;Behaviour what, Capability how, beliefs and values why, identity who, environment where &amp;amp; when.&lt;/p&gt;\n\n&lt;p&gt;Ask a question, reticular activation system.&lt;/p&gt;\n\n&lt;p&gt;The 8 C&amp;#39;s To Muscle Memory&lt;/p&gt;\n\n&lt;p&gt;Competency&lt;/p&gt;\n\n&lt;p&gt;Chunking&lt;/p&gt;\n\n&lt;p&gt;Combining&lt;/p&gt;\n\n&lt;p&gt;Consequences&lt;/p&gt;\n\n&lt;p&gt;Character&lt;/p&gt;\n\n&lt;p&gt;Consistency&lt;/p&gt;\n\n&lt;p&gt;Commit by burning the bridges. Make a decision by cutting from other possibilities.&lt;/p&gt;\n\n&lt;p&gt;Have a Coach that challenge you.&lt;/p&gt;\n\n&lt;p&gt;Close my eyes and use my imagination and imagine I am that person.&lt;/p&gt;\n\n&lt;p&gt;You will never need to memorise a book word for word. &lt;/p&gt;\n\n&lt;p&gt;Take your textbook, and take a good look at it:&lt;/p&gt;\n\n&lt;p&gt;Look at the front cover.&lt;/p&gt;\n\n&lt;p&gt;Look at the back cover.&lt;/p&gt;\n\n&lt;p&gt;Look over the introduction.&lt;/p&gt;\n\n&lt;p&gt;Read the conclusion, and&lt;/p&gt;\n\n&lt;p&gt;Be sure to scan through the index, if your book has one.&lt;/p&gt;\n\n&lt;p&gt;And read information about the bookâ€™s publication, like the place of publication, the publisher, and the publication date&lt;/p&gt;\n\n&lt;p&gt;Read the table of contents, introduction and conclusion. &lt;/p&gt;\n\n&lt;p&gt;Turn the memorised information into knowledge that you can use over and over â€” not just for this single test or exam.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?auto=webp&amp;s=66e486c81869feaad60df238edaa128388f3110a', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e642309c726659985affc46be1040421897ce8d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9453b1ab1344ef827e5e7bc71f6e87fc973649b6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dcded9a7211251b822a8a446cbaf8c647b0f52a', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef3d77ac395bf51c23ea92070a0e6a5320802e4e', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=666bfe10bbef0f8584a838c844d4c1c5c9d639ef', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a77cb1ce3d021ab0e49f1235bbe929c8254ab0a6', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'JhDjM1_cc6F88dvY5zq9T1V-OXjf34-ySM2rRo5JObg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gg7vxh', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/', 'subreddit_subscribers': 33, 'created_utc': 1588994818.0, 'num_crossposts': 27, 'media': None, 'is_video': False}]",t3_gg7vxh,,
,learnmachinelearning,Iâ€™m new to python and Iâ€™m wondering if thereâ€™s a place where people created models to work with different genres. Is there a place to find them?,t2_x05qd,False,,0,False,Spleeter - where would I find and download other pre-made models?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8tpz,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589027533.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m new to python and Iâ€™m wondering if thereâ€™s a place where people created models to work with different genres. Is there a place to find them?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg8tpz,True,,WAFFLED_II,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8tpz/spleeter_where_would_i_find_and_download_other/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8tpz/spleeter_where_would_i_find_and_download_other/,155203,1588998733.0,0,,False,,,,
,learnmachinelearning,"Hello as part of my subjects assignment, my task is to work on a sentiment analysis related project on Yelp review data. Iâ€™ve never done sentiment analysis before, would anyone here whoâ€™s had a go at it be able to guide me to some useful resources for a first timer? 

Thank you very much x",t2_120a94wr,False,,0,False,Sentiment analysis resources request,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8tor,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1589027530.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello as part of my subjects assignment, my task is to work on a sentiment analysis related project on Yelp review data. Iâ€™ve never done sentiment analysis before, would anyone here whoâ€™s had a go at it be able to guide me to some useful resources for a first timer? &lt;/p&gt;

&lt;p&gt;Thank you very much x&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg8tor,True,,Khoobsuratt,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8tor/sentiment_analysis_resources_request/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8tor/sentiment_analysis_resources_request/,155203,1588998730.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: High-Fidelity 3D Face Reconstruction,[],r/learnmachinelearning,False,6,,0,55.0,False,t3_gg8rvn,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg,False,,[],{},link,,False,,1589027327.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8rvn,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8rvn/from_cvpr_20_highfidelity_3d_face_reconstruction/,all_ads,False,/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/,155203,1588998527.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI)\n\nhttps://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e\n\nmain idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: High-Fidelity 3D Face Reconstruction"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'hide_score': False, 'media_metadata': {'81ta0o3c2ox41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55c32367cf95f7e6fa654cbf786e32d01ecd2dcc'}, {'y': 86, 'x': 216, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f885fc37a7b8df2c81d03bc56b51edb2a591a7ab'}, {'y': 127, 'x': 320, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7b2695c338c17666bf413cccda5684a106c7efa'}, {'y': 255, 'x': 640, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46fba7b645462a6a2fcbd3bd624b5df9e9aefdec'}, {'y': 383, 'x': 960, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b8d855cfc2c0ccc5d012b462f5e9d1d7fe03c2c'}, {'y': 431, 'x': 1080, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21026136b7f8cca8faf9058e0a868ef6c6a1143d'}], 's': {'y': 758, 'x': 1898, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e'}, 'id': '81ta0o3c2ox41'}}, 'name': 't3_gg8qay', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589027133.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e""&gt;https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;main idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg8qay', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'subreddit_subscribers': 3386, 'created_utc': 1588998333.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gg8qay,,
,learnmachinelearning,"I'm a newbie at ml, never finished any course that I have ever started. 
I was doing fastai's deep learning course and in that, the instructor recommended that we check out Andrew Ng's course on Coursera. 
Now, that course looks old and I found a new version on YouTube (from 2018) [here](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) 

Which one should I do? The Coursera one or this one? And should I continue fastai's course side by side?


Edit: Thank you everyone for your answers. I have started the ML course on Coursera today!",t2_qhggx,False,,0,False,Andrew Ng Coursera or CS229 YouTube 2018?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg2zh7,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,1589088323.0,,[],{},self,,True,,1589005047.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a newbie at ml, never finished any course that I have ever started. 
I was doing fastai&amp;#39;s deep learning course and in that, the instructor recommended that we check out Andrew Ng&amp;#39;s course on Coursera. 
Now, that course looks old and I found a new version on YouTube (from 2018) &lt;a href=""https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;here&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Which one should I do? The Coursera one or this one? And should I continue fastai&amp;#39;s course side by side?&lt;/p&gt;

&lt;p&gt;Edit: Thank you everyone for your answers. I have started the ML course on Coursera today!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?auto=webp&amp;s=7dad7e69084fe17dc70d1948b6f58fc13d288f0c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54a29aa3b0894b9f5f031e6ec93d6896ab4450', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'WdNtCcCE5kkWeUtaFzPEmvl-T9SFUg0GJCbpJncoGqQ'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg2zh7,True,,RKRohk,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg2zh7/andrew_ng_coursera_or_cs229_youtube_2018/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg2zh7/andrew_ng_coursera_or_cs229_youtube_2018/,155203,1588976247.0,0,,False,,,,
,learnmachinelearning,"Could anyone help to derive the following [CIoU loss equations](https://arxiv.org/abs/1911.08287) ?

Note: I understood the concept behind *RDIoU*, but not *RCIoU*  


https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1",t2_bpftl,False,,0,False,Deriving CIoU equations,[],r/learnmachinelearning,False,6,,0,80.0,False,t3_gg84t2,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/RdmkT3sxrnN6_60s1ykCRtj16zGr7o9ZIihH2jcPvQ0.jpg,False,,[],{},,,True,,1589024632.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could anyone help to derive the following &lt;a href=""https://arxiv.org/abs/1911.08287""&gt;CIoU loss equations&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;Note: I understood the concept behind &lt;em&gt;RDIoU&lt;/em&gt;, but not &lt;em&gt;RCIoU&lt;/em&gt;  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1""&gt;https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg84t2,True,,promach,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg84t2/deriving_ciou_equations/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg84t2/deriving_ciou_equations/,155203,1588995832.0,0,,False,,,"{'7yvmjetkunx41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 61, 'x': 108, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ed3a6dd261d1734d87c20561bb50141115febbd'}, {'y': 123, 'x': 216, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75b5fe0b87153e939c64385e46f776b5cd97e530'}, {'y': 182, 'x': 320, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8026a93a220ac791f86a1346a3deb9ec1a7bb231'}, {'y': 365, 'x': 640, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb38f866a9a8faeb079e133ca3d45902c868b4f7'}, {'y': 548, 'x': 960, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36f79913a2e7858856997fde74e450703facaeb7'}, {'y': 617, 'x': 1080, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90bb0632f84a1f16c411a6a6e3595772ae263e7'}], 's': {'y': 800, 'x': 1400, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1'}, 'id': '7yvmjetkunx41'}}",
,learnmachinelearning,"Has anyone worked on PU-GAN or PU-Net? I was trying to give colour point cloud as input. I am not getting idea, how that can be done. If someone knows, give suggestions.",t2_3zkrpw14,False,,0,False,How can PU-GAN be changed to take input of xyzrgb,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg81w5,False,dark,0.66,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589024303.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone worked on PU-GAN or PU-Net? I was trying to give colour point cloud as input. I am not getting idea, how that can be done. If someone knows, give suggestions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg81w5,True,,Shutthefrontdoooor,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg81w5/how_can_pugan_be_changed_to_take_input_of_xyzrgb/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg81w5/how_can_pugan_be_changed_to_take_input_of_xyzrgb/,155203,1588995503.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying - How to learn effectively 09. May 2020,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg7wxq,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},,,False,,1589023730.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg7wxq,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg7wxq/studying_how_to_learn_effectively_09_may_2020/,all_ads,False,/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/,155203,1588994930.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': '**SELF**\n\nIdentify where are your weaknesses are in learning and capitalise on your strengths by being honest with yourself.\n\nDevelop a love of learning\n\nSpend 20% time to chat with others and 80% time to think deeply\n\nAn expert is someone who can practically apply the books knowledge to the real-life situation.  \n\nThe practice is not how long you have studied or worked at a job, but your performance within a year. After the performance review, what you have done the previous year is erased and forgotten.\n\nIf you need to choose between learning technical skills or soft skills, what should you do?\n\nReduce entertainment hours, and focus 80% time and energy to master technical course content and selectively find people and resources to practice your soft skills with 20% of your time\n\nTell me and I forget\n\nTeach me &amp; I remember\n\nInvolve me &amp; I will learn\n\nThe secret to choosing the most suitable self-improvement book for you is to understand your current situation â€“ and to have a clear vision of what you hope to achieve in the future. \n\nThe ideal self-improvement book will be one that fits your current needs and will be easy and enjoyable for you to read from start to finish. \n\nAsk yourself at the end of every day, what did you learn today\n\nSeek progress, not perfection\n\nTake a step a day\n\nWhen you meet with hard stuff to learn, what must you think? This is fun - enjoy the process\n\nWhen you fail to understand a piece of information, what must you do? Tell yourself not to blame others - be grateful for the little things in life and things that you have learnt\n\nTake ownership of your mistakes but donâ€™t be a scapegoat. \n\n**CHALLENGE** \n\nIf the challenge is too hard, we give up. If the challenge is too easy, we get bored. \n\nAlways seek feedback loop on your progress from people around you. \n\nIf you want to improve yourself, seek out opponents that are better than you.\n\nPeople often lose concentration resulting in demotivation and giving up. \n\nTrain yourself to relax by taking multiple short breaks between intense work during each day.\n\n**BUSINESS IDEA**\n\nFrom an entrepreneurship point of view, why not hire humans with a sexy and husky voice as a selling point to read aloud and record the study notes of lonely men and women (customers) as a paid service to help enhance the memory retention of their notes? This helps to reduce crime and empower sex workers out of poverty. This saves time for time poor corporate executives to learn new knowledge which becomes a win-win solution.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying - How to learn effectively 09. May 2020', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg6spe', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589019086.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;SELF&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Identify where are your weaknesses are in learning and capitalise on your strengths by being honest with yourself.&lt;/p&gt;\n\n&lt;p&gt;Develop a love of learning&lt;/p&gt;\n\n&lt;p&gt;Spend 20% time to chat with others and 80% time to think deeply&lt;/p&gt;\n\n&lt;p&gt;An expert is someone who can practically apply the books knowledge to the real-life situation.  &lt;/p&gt;\n\n&lt;p&gt;The practice is not how long you have studied or worked at a job, but your performance within a year. After the performance review, what you have done the previous year is erased and forgotten.&lt;/p&gt;\n\n&lt;p&gt;If you need to choose between learning technical skills or soft skills, what should you do?&lt;/p&gt;\n\n&lt;p&gt;Reduce entertainment hours, and focus 80% time and energy to master technical course content and selectively find people and resources to practice your soft skills with 20% of your time&lt;/p&gt;\n\n&lt;p&gt;Tell me and I forget&lt;/p&gt;\n\n&lt;p&gt;Teach me &amp;amp; I remember&lt;/p&gt;\n\n&lt;p&gt;Involve me &amp;amp; I will learn&lt;/p&gt;\n\n&lt;p&gt;The secret to choosing the most suitable self-improvement book for you is to understand your current situation â€“ and to have a clear vision of what you hope to achieve in the future. &lt;/p&gt;\n\n&lt;p&gt;The ideal self-improvement book will be one that fits your current needs and will be easy and enjoyable for you to read from start to finish. &lt;/p&gt;\n\n&lt;p&gt;Ask yourself at the end of every day, what did you learn today&lt;/p&gt;\n\n&lt;p&gt;Seek progress, not perfection&lt;/p&gt;\n\n&lt;p&gt;Take a step a day&lt;/p&gt;\n\n&lt;p&gt;When you meet with hard stuff to learn, what must you think? This is fun - enjoy the process&lt;/p&gt;\n\n&lt;p&gt;When you fail to understand a piece of information, what must you do? Tell yourself not to blame others - be grateful for the little things in life and things that you have learnt&lt;/p&gt;\n\n&lt;p&gt;Take ownership of your mistakes but donâ€™t be a scapegoat. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CHALLENGE&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;If the challenge is too hard, we give up. If the challenge is too easy, we get bored. &lt;/p&gt;\n\n&lt;p&gt;Always seek feedback loop on your progress from people around you. &lt;/p&gt;\n\n&lt;p&gt;If you want to improve yourself, seek out opponents that are better than you.&lt;/p&gt;\n\n&lt;p&gt;People often lose concentration resulting in demotivation and giving up. &lt;/p&gt;\n\n&lt;p&gt;Train yourself to relax by taking multiple short breaks between intense work during each day.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BUSINESS IDEA&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;From an entrepreneurship point of view, why not hire humans with a sexy and husky voice as a selling point to read aloud and record the study notes of lonely men and women (customers) as a paid service to help enhance the memory retention of their notes? This helps to reduce crime and empower sex workers out of poverty. This saves time for time poor corporate executives to learn new knowledge which becomes a win-win solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gg6spe', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/', 'subreddit_subscribers': 33, 'created_utc': 1588990286.0, 'num_crossposts': 29, 'media': None, 'is_video': False}]",t3_gg6spe,,
,learnmachinelearning,,t2_17rz8f84,False,,0,False,LSTM stock market prediction exercise. Some help needed please,[],r/learnmachinelearning,False,6,,0,,False,t3_gg79vh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1589021059.0,text,6,,,text,self.NeuralNetwork,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg79vh,True,,edenmannh,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg79vh/lstm_stock_market_prediction_exercise_some_help/,all_ads,False,/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/,155203,1588992259.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'NeuralNetwork', 'selftext': ""I have made a model which attempts to predict the next five days of closing price for a given stock (KMD.NZ for example). The training data is fetched from Yahoo Finance.  Depending on whether I download 10 years or 10.3 years, the 2 month trend completely changes (like from positive 30% to -5%). I'm programming in python using keras. Is there any way to place more weighting on recent data than data 10yrs ago? Any other reason why this small snippet of data would completely change the prediction?\n\nThanks."", 'author_fullname': 't2_17rz8f84', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'LSTM stock market prediction exercise. Some help needed please', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/NeuralNetwork', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg76yr', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 4, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1588993522.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589020727.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.NeuralNetwork', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have made a model which attempts to predict the next five days of closing price for a given stock (KMD.NZ for example). The training data is fetched from Yahoo Finance.  Depending on whether I download 10 years or 10.3 years, the 2 month trend completely changes (like from positive 30% to -5%). I&amp;#39;m programming in python using keras. Is there any way to place more weighting on recent data than data 10yrs ago? Any other reason why this small snippet of data would completely change the prediction?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2s3sq', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg76yr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'edenmannh', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/', 'subreddit_subscribers': 2651, 'created_utc': 1588991927.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_gg76yr,,
,learnmachinelearning,,t2_5lflrdo8,False,,0,False,AI basketball analysis web App and API,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,46.0,False,t3_gf6zl8,False,light,1.0,,public,813,0,{},140.0,,False,[],,True,False,,{},Project,False,813,,False,https://a.thumbs.redditmedia.com/ivNOFyoEI-Mr2MSlv5hZlAPjua8n4B8x7aIqAB2gNr8.jpg,False,,[],{},image,,False,,1588888539.0,richtext,6,,,text,i.redd.it,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?format=png8&amp;s=3ebce6dd0db0d14061b50af6df99d478955b0686', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3d16e2d3a7d07ca46522f1d592d3b2f1e6487dc2', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=f905d9c859ed4af2c01f74ff6ccd046c79bdc3c8', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=7744aacfa1e46ec86d4ff396e8eee310fee5c5be', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=388e231701849acee50ba5accde0e13d1654f892', 'width': 640, 'height': 212}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?s=88c4904a289fdff3e755a6c41b167453976a3287', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;crop=smart&amp;s=d190fcc9ea29b77f782e29781a3d4a5e9c9a26b6', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;crop=smart&amp;s=79acfd768b2776afad15db91827819e719038053', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;crop=smart&amp;s=fa7947c378eb72a749934b5e76e50c2fb780ec61', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;crop=smart&amp;s=ea1faf5da973c35fb72d1d8a853a96778ec2594e', 'width': 640, 'height': 212}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?format=mp4&amp;s=bb5ef88a30acb882c8efb78276d90df60225e742', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;format=mp4&amp;s=0479dda0d419f23dc563b55f4bdf4a427f4d9cf4', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;format=mp4&amp;s=c0af8dc4f5ed6b19fbd937ff4085f75a3d652621', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;format=mp4&amp;s=a45cab424209d45d5e13096fcd19954c18305eeb', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;format=mp4&amp;s=22c50849835802c26e3a2ba27499975aa28aa568', 'width': 640, 'height': 212}]}}, 'id': 'ZP0v8R5rkp0Vai3Cr-z4BXKLCDv9sg9mwNzj5oVYffw'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf6zl8,True,,chonyyy,,42,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6zl8/ai_basketball_analysis_web_app_and_api/,all_ads,False,https://i.redd.it/ddymeguslcx41.gif,155203,1588859739.0,0,,False,,,,
,learnmachinelearning,,t2_34e5wzr6,False,,0,False,"Phd students and Drs, how much did you know about AI/ML before embarking on your PhD?",[],r/learnmachinelearning,False,6,,0,,False,t3_gfstro,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1588971798.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfstro,True,,AssumeSmallAngle,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfstro/phd_students_and_drs_how_much_did_you_know_about/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfstro/phd_students_and_drs_how_much_did_you_know_about/,155203,1588942998.0,0,,False,,,,
,learnmachinelearning,"A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)
 
Haven't taken the GRE yet. 
GPA 9.36/10

So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)",t2_4ih25pre,False,,0,False,How do I go about learning TensorFlow?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfo7cw,False,light,0.93,,public,23,0,{},,,False,[],,False,False,,{},HELP,False,23,,False,self,False,,[],{},,,True,,1588950050.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)&lt;/p&gt;

&lt;p&gt;Haven&amp;#39;t taken the GRE yet. 
GPA 9.36/10&lt;/p&gt;

&lt;p&gt;So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfo7cw,True,,SuccMyStrangerThings,,11,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo7cw/how_do_i_go_about_learning_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo7cw/how_do_i_go_about_learning_tensorflow/,155203,1588921250.0,0,,False,,,,
,learnmachinelearning,"I am looking to further build my mathematical foundation for machine learning, and I came across this book at https://mml-book.github.io. I am currently a software engineer who uses python as my main language, and mostly focus on data engineering work. My company would like me to move more into a data scientist role and wants me to delve into machine learning. I have a decent math background (could certainly be better). I took the full calc series, linear algebra, and the upper division courses such as real analysis and several other proof based classes. I would say probability and statistics are my weakest areas. So, is this book a good catch-all? Or should I focus on specific books for specific subjects? Thanks!",t2_mhvcd,False,,0,False,How is the book Mathematics for Machine Learning? Are there better resources for building a solid mathematical foundation?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfvjpg,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},self,,True,,1588981590.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to further build my mathematical foundation for machine learning, and I came across this book at &lt;a href=""https://mml-book.github.io""&gt;https://mml-book.github.io&lt;/a&gt;. I am currently a software engineer who uses python as my main language, and mostly focus on data engineering work. My company would like me to move more into a data scientist role and wants me to delve into machine learning. I have a decent math background (could certainly be better). I took the full calc series, linear algebra, and the upper division courses such as real analysis and several other proof based classes. I would say probability and statistics are my weakest areas. So, is this book a good catch-all? Or should I focus on specific books for specific subjects? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?auto=webp&amp;s=a5a066e892cd1a887ba10e324183d3020e0906fe', 'width': 180, 'height': 261}, 'resolutions': [{'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=324d98b2afe974c9923a90b7fece00bbb8c2357f', 'width': 108, 'height': 156}], 'variants': {}, 'id': 'XzFfeCJvwXSOhVXOWBpN_GqQftnCRE5pJg-f7VJIAm8'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfvjpg,True,,RawCS,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfvjpg/how_is_the_book_mathematics_for_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfvjpg/how_is_the_book_mathematics_for_machine_learning/,155203,1588952790.0,0,,False,,,,
,learnmachinelearning,"Hey there, I decided to take CS230, and my AI isnâ€™t working properly, Itâ€™s supposed to track faces because it would be easy for me to make videos, can anyone help with this? Thanks.",t2_3kvgiyr1,False,,0,False,First ML Project,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg3zp9,False,light,0.33,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1589008529.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, I decided to take CS230, and my AI isnâ€™t working properly, Itâ€™s supposed to track faces because it would be easy for me to make videos, can anyone help with this? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg3zp9,True,,silverfoxreddits,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3zp9/first_ml_project/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3zp9/first_ml_project/,155203,1588979729.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,Bring Old Photos Back to Life,[],r/learnmachinelearning,False,6,,0,57.0,False,t3_gg3z53,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg,False,,[],{},link,,False,,1589008475.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3z53,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3z53/bring_old_photos_back_to_life/,all_ads,False,/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/,155203,1588979675.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:\n\n**Bring Old Photos Back to Life**       \nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2004.09484)\n\nhttps://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b\n\nThis new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bring Old Photos Back to Life', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'hide_score': False, 'media_metadata': {'q9cz19sjgmx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 44, 'x': 108, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5a0301ab1954fe92d14162ef010d5594c2163ac'}, {'y': 88, 'x': 216, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15b420753d4d84a5c4adede4fabd0ca0ee41f8b'}, {'y': 130, 'x': 320, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571432baf4e3158e45084882f8b87bc44d31b982'}, {'y': 261, 'x': 640, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56dce9805fac99c032f0a6dcd382a6575a08ee5b'}, {'y': 391, 'x': 960, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c3f3cd35f64b66f1c381fe8a349e4ba144e069f'}, {'y': 440, 'x': 1080, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f9e19d9f978923cc491fa3295836356d1888206'}], 's': {'y': 552, 'x': 1352, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b'}, 'id': 'q9cz19sjgmx41'}}, 'name': 't3_gg3rac', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 40, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 40, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589007729.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bring Old Photos Back to Life&lt;/strong&gt;&lt;br/&gt;\nFor project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.09484""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b""&gt;https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg3rac', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'subreddit_subscribers': 3386, 'created_utc': 1588978929.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_gg3rac,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#learning #studying Quick Study Guides - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg3fmn,False,light,0.4,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},,,False,,1589006602.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg3fmn,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3fmn/learning_studying_quick_study_guides_a_university/,all_ads,False,/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/,155203,1588977802.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Public Accessed: 07. May 2020  [https://www.monash.edu/rlo/quick-study-guides](https://www.monash.edu/rlo/quick-study-guides)  Monash University, RLO Study Skills, 2020\n\nAssignment direction words\n\nBelow is an explanation of the keywords commonly used in assignment questions.\n\nThese explanations are intended as a guide only. There are not always hard distinctions between the words, and different academics may use them to mean slightly different things.\n\n&amp;#x200B;\n\nANALYSE. Identify the elements of an argument, text, issue, process or event and show how they are related.ARGUE. Present the case for and/or against a particular proposition. \n\n&amp;#x200B;\n\nCOMMENT ON. Point out the important features, Criticise. \n\n&amp;#x200B;\n\nCOMPARE. Identify and explain similarities and differences. \n\n&amp;#x200B;\n\nCONTRAST. Stress the differences between two or more things. \n\n&amp;#x200B;\n\nCRITICISE. Judge the merit or truth of the factors or views mentioned, including both strengths and weaknesses.CRITICALLYâ€¦Used to add direction to another direction word (e.g. â€œcritically analyseâ€), this means approaching the task in a **questioning** way: â€œHow does this work?â€ â€œWhy is it like this?â€ â€œWhat are its strengths and weaknesses?â€\n\n \n\nDEFINE. Provide concise, clear, and authoritative meanings. Give the limits of the definition, but omit detailed explanations. Show how the item defined differs from items in other classes. \n\n&amp;#x200B;\n\nDESCRIBE. Recount, characterise, outline, and relate in sequence.\n\n&amp;#x200B;\n\nDIAGRAM. A drawing, chart, plan, or graph. Diagrams should be labelled and there should be an accompanying explanation.\n\n&amp;#x200B;\n\nDISCUSS. Examine, analyse carefully and give reasons for and against. Be complete and give details, usually with a view to assessing how satisfactory something is.\n\n&amp;#x200B;\n\nEVALUATE. Appraise in relation to some standard, referring to advantages, limitations, and costs and benefits as Appropriate.\n\n&amp;#x200B;\n\nEXAMINE. Investigate critically appraises a subject in detail.\n\n&amp;#x200B;\n\nEXPLAIN. Clarify, interpret and elaborate on the material presented. Give reasons for differences of opinion or results, and try to analyse causes.\n\n&amp;#x200B;\n\nILLUSTRATE. Use a concrete example, diagram, or figure to explain or clarify a problem.\n\n&amp;#x200B;\n\nINDICATE. Identify, then focuses attention so as to clarify.\n\n&amp;#x200B;\n\nJUSTIFY. Prove or give reasons for conclusions or decisions.\n\n&amp;#x200B;\n\nOUTLINE. Present the essential features, showing the main points and subordinate points. Omit minor details.\n\n&amp;#x200B;\n\nREVIEW. Examine a subject critically, analysing and commenting on the important or controversial statements.\n\n&amp;#x200B;\n\nSTATE. Present the main points in a brief and clear sequence, usually omitting details or examples.\n\n&amp;#x200B;\n\nSUMMARISE. Give the main points or facts in condensed form. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nBrainstorming: Mind mapping\n\nWhy mind map?\n\nOne effective form of brainstorming is mind mapping. A mind map is a visual representation of your ideas, consisting of words, images and colours, and can help you to:\n\nfocus on your research topic/question\n\nstructure and plan your assignment\n\ncombine one or more types of major thought relationships\n\nidentify relationships between ideas/concepts.\n\nStage 1\n\nYou can create a mind map on the paper, whiteboard or digitally, using visual mapping software such as [FreeMind](http://freemind.sourceforge.net/wiki/index.php/Main_Page). To begin:\n\nwrite your topic in the centre of a blank page\n\nassociate your ideas freely anywhere on the page and do not filter out ideas.\n\nStage 2\n\nWhen you have run out of ideas:\n\nconsider each item and determine how this point is related to other points and to your topic\n\nmap relationships with lines, arrows, colours, images and bold type.\n\nStage 3\n\nUse the relationships you have identified to reorganise your ideas.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSignposting\n\nWhy signpost?\n\nSignposting helps you to:\n\ncreate a navigation path to guide your reader,\n\nindicate to your reader the direction your writing will take, and\n\nremember your key points.\n\nMajor signposts\n\nMajor signposts indicate to the reader the important elements of your writing such as the purpose, connection between points, and the conclusion.\n\nExamples:\n\n&amp;#x200B;\n\nThis study argues that â€¦\n\nThis paper examinesâ€¦\n\nThis essay begins byâ€¦ it goes on to â€¦\n\nIn conclusion â€¦\n\nTransition sentences\n\nTransition sentences inform the reader when your writing is moving from one idea to another, and how those ideas are connected.\n\n&amp;#x200B;\n\nExamples:\n\n&amp;#x200B;\n\nHaving examined; It is important to\n\nThe discussion highlights; It is also essential, however, to explore;\n\nA significant factor inâ€¦ isâ€¦\n\nLinking words\n\nLinking words signify to the reader the connections between your ideas. Linking words convey what you will be discussing next, and how the reader should interpret it.\n\n&amp;#x200B;\n\nSome types of linking words\n\n&amp;#x200B;\n\nExamples\n\nAddition\tin addition; also\n\nCause and effect\tdue to; as a result\n\nComparison\tsimilarly; likewise\n\nContrast\talternatively; in contrast\n\nExamples\tfor instance; to show this\n\nException\thowever; although\n\nTime and sequence\tinitially; to being; previously; finally\n\nReminders\n\nReminders can help the reader recall what you have previously discussed.\n\n&amp;#x200B;\n\nExamples:\n\n&amp;#x200B;\n\nAs discussed aboveâ€¦\n\nSection 2 outlinesâ€¦\n\nSo farâ€¦\n\nIn conclusionâ€¦\n\nIn briefâ€¦\n\nThusâ€¦\n\n&amp;#x200B;\n\nWriting essays\n\nAnalysing essay topics\n\nUnderstand the essay question\n\nMake sure you know the precise meaning of every word in the essay question. Use:\n\n&amp;#x200B;\n\na) your general dictionary for unfamiliar words such as intrinsic, core values\n\n&amp;#x200B;\n\nb) a subject-specific dictionary, for example, the APA Dictionary of Psychology, for academic words such as proof, random sample, significance level\n\n&amp;#x200B;\n\nThink about the different parts of the question\n\nDecide how many parts the question has.\n\n&amp;#x200B;\n\nList areas you will probably have to research. It may help to write yourself a brief task description: â€œFirst find out what a market niche is, then see what significance this has for marketing. Nextâ€¦â€\n\n&amp;#x200B;\n\nNote any direction words\n\nThese might include:\n\n&amp;#x200B;\n\ndiscuss, discuss critically, discuss the importance of, assess, justify, evaluate, analyse.\n\n&amp;#x200B;\n\nThe structure of an essay\n\nIntroduction\n\nAbout 10% of the total length. May be one paragraph or several, depending on essay length.\n\n&amp;#x200B;\n\nIntroduce the topic.\n\nProvide background information.\n\nLimit the scope of the discussion.\n\nDefine or state the topic.\n\nPresent the plan of coverage including your viewpoint and line of reasoning.\n\nMove from general background information to your specific topic. You can set your own agenda to avoid too broad a focus.\n\n&amp;#x200B;\n\nBody\n\nThis is a series of linked paragraphs.\n\n&amp;#x200B;\n\nEach paragraph should have one main point.\n\nThe topic sentence of each paragraph carries the theme or argument.\n\nConclusion\n\nHere you are moving from the specifics of your essay to the more general background of the topic.\n\n&amp;#x200B;\n\nSum up your argument and information with reference to the essay question.\n\nPerhaps mention wider implications or future directions.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nFeatures of a critical review\n\nA critical review requires you to evaluate an academic text and make your own judgement about it based on various criteria.\n\n&amp;#x200B;\n\nThis guide points out the common features of critical reviews. Always check with your lecturer to confirm the exact requirements of your assignment.\n\n&amp;#x200B;\n\nCritical reviews can present positive and/or negative judgements.\n\n&amp;#x200B;\n\nCritical reviews involve two main tasks:\n\n&amp;#x200B;\n\nsummary\n\nevaluation.\n\nThe evaluation criteria can vary depending on the discipline.\n\n&amp;#x200B;\n\nThe aim of a critical review is to evaluate the text. This means you must be very familiar with the text, and your writing needs to clearly present your judgement.\n\n&amp;#x200B;\n\nTitle\n\nUsually looks like an entry in a bibliography, and includes full bibliographic details of the text.\n\nIntroduction\n\nGives an overview of the text including the importance of the topic or question.\n\nBriefly states your evaluation of the merits of the text.\n\nOutlines your reviewâ€™s approach and structure.\n\nSummary (may be combined with evaluation)\n\nDescribes the key points from the text, including the authorâ€™s intentions and findings.\n\nEvaluation (may be combined with summary)\n\nPresents strengths and weaknesses.\n\nFocuses on the evaluation criteria to present your judgement of the text.\n\nConclusion\n\nUsually quite short, so can be included at the end of the evaluation or as a separate section.\n\nRestates your overall evaluation.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nFeatures of reports\n\nReferencing and quotations in reports follow the same guidelines required for essays.\n\nA system of\xa0numbered sections with headings is typically used.\n\nJust as in the essay, a good report will describe, analyse and evaluate a problem or issue. Unlike an essay, it will describe the method used to investigate the problem and formulate a set of recommendations based on the findings of the report.\n\nReport\n\nVS\n\nEssay\n\nOften a problem or case study which sets up a hypothetical situation\n\nBased on the reading, fieldwork or practical work\n\nTopic\n\nResponds to a question or a proposition\n\nIs based on research\n\nTo investigate, analyse and present information\n\nUsually to make recommendations to solve a problem\n\nPurpose\n\nTo articulate a well-argued response to a question or proposition\n\nEstablished in the topic and is often a client or manager\n\nAudience\n\nAn academic audience\n\nContains an executive summary or abstract\n\nComprises sections with headings\n\nMay use bullet points, tables, graphs to convey information\n\nFormat\n\nDoes not typically include sections or headings\n\nDoes not typically include use bullet points, tables, graphs\n\nThird-person\n\nFormal language\n\nStyle\n\nThird or first person\n\nFormal language\n\nSuccess depends on:\n\nthe demonstration of good research skills\n\nthe quality of the recommendations to respond to an issue\n\nthe presentation and analysis of relevant information\n\nAssessment\n\nSuccess depends on:\n\nthe demonstration of good research skills\n\nthe identification of a cogent argument\n\nthe quality of reasoning and evidence\n\nhow well it analyses and evaluates the issue\n\nDifferent types of reports typically include different sections.  \nFor the requirements for reports in Business and Economics, see the Q Manual. For all other disciplines, look at the Faculty examples in\xa0[Assignment Structures and Samples](https://www.monash.edu/rlo/assignment-samples).\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nWriting a case study\n\nThere are two different approaches to case studies. This guide focuses on the problem-oriented method. Always check with your lecturer to confirm if this is the type required.\n\nA successful case study analyses a real-life situation where existing problems need to be solved. It should relate the theory to a practical situation; for example, apply the ideas and knowledge discussed in the coursework to the practical situation at hand in the case study.\n\nIdentify the problems.\n\nSelect the major problems in the case.\n\nSuggest solutions to these major problems.\n\nRecommend the best solution to be implemented.\n\nDetail how this solution should be implemented.\n\nThere are usually eight sections in a case study:\n\n1. Synopsis/Executive Summary\n\nOutline the purpose of the case study.\n\nDescribe the field of research.\n\nOutline the issues and findings of the case study without specific details.\n\nIdentify the theory that will be used.\n\nNote any assumptions made (you may not have all the information you\'d like so some assumptions may be necessary e.g.: ""It has been assumed thatâ€¦"", ""Assuming that it takes half an hour to read one documentâ€¦"").\n\n2. Findings\n\nIdentify the problems found in the case by:\n\nanalysing the problem, supporting your findings with facts given in the case, the relevant theory and course concepts.\n\nsearching for the underlying problems\n\nThis section is often divided into sub-sections.\n\n3. Discussion\n\nSummarise the major problem/s.\n\nIdentify alternative solutions to this/these major problem/s.\n\nBriefly outline each alternative solution and evaluate its advantages and disadvantages.\n\nThere is no need to refer to theory or coursework here.\n\n4. Conclusion\n\nSum up the main points from the findings and discussion.\n\n5. Recommendations\n\nChoose which of the alternative solutions should be adopted.\n\nBriefly justify your choice explaining how it will solve the major problem/s.\n\nThis should be written in a forceful style as this section is intended to be persuasive.\n\nHere integration of theory and coursework is appropriate.\n\n6. Implementation\n\nExplain what should be done, by whom and by when.\n\nIf appropriate include a rough estimate of costs (both financial and time).\n\n7. References\n\nMake sure all references are cited correctly.\n\n8. Appendices (if any)\n\nAttach any original data that relates to the study but which would have interrupted the flow of the main body\n\n&amp;#x200B;\n\nWriting a critical review\n\nCritical reviews require careful planning and drafting just like any other assignment. This guide suggests what to focus on at each stage of the process.\n\n&amp;#x200B;\n\n1. Plan your approach\n\nRead your assignment instructions carefully in order to:\n\n&amp;#x200B;\n\ndetermine your exact criteria;\n\nfind out what proportion of your review you should dedicate to summary and evaluation; and\n\nknow whether the summary and evaluation should be presented as separate sections or a combined section.\n\n2. Make notes\n\nSkim read the text and make notes about:\n\n&amp;#x200B;\n\nthe main question or questions;\n\nthe authorâ€™s aim;\n\nthe methods used;\n\nthe evidence provided;\n\nthe key findings or answers; and\n\nthe implications and significance of the findings.\n\n3. Evaluate the text\n\nJudge the quality or value of the text (for other researchers, or to practitioners in the field, or to students).\n\nConsider the merits of the text in the short term as well as the long term.\n\nConsider the merits of the text in comparison to other related text.\n\nWhen evaluating the text you could answer some of the following questions:\n\n&amp;#x200B;\n\nIs the question the text tries to answer relevant, interesting, new, or useful? To who, and why?\n\nDoes the text give new answers or interpretations to an old question?\n\nIs the text detailed, or brief? Simple or complex?\n\nIs the evidence presented to support the answer extensive? Strong? Weak? Relevant? Persuasive? Contradictory?\n\nAre the conclusions reached final, limited, qualified or preliminary?\n\n4. Write it up\n\nWhen writing and proofreading your critical review:\n\n&amp;#x200B;\n\nStay focused on your evaluation criteria.\n\nRead the text you are reviewing again to check that you have covered everything.\n\n&amp;#x200B;\n\n \n\nEditing and proofreading your work\n\nRefining your own work is an essential skill, and an excellent way to continually improve your writing.\n\nThis guide covers the differences between editing and proofreading and provides checklists you can use to review your work before submission.\n\nWhat is the difference?\n\nEditing focuses on improving the \'big picture\' of your assignment. It is how you ensure you have fully addressed the task requirements, and involves making structural changes to your writing and checking the logic and flow.\n\nProofreading focuses on specific details like spelling, sentence structure, and referencing.\n\nStep 1: Reread your instructions, question and rubric, so you can approach the task with clarity about your aims and purpose.\n\nStep 2: Use the checklists below as a starting point to refine your work.\n\nTip: To spot errors more easily, read your text aloud, and take breaks between writing, editing, and proofreading.\n\nEditing\n\nStructural aspects\n\nThe introduction clearly states the topic and how it will be covered.\n\nParagraphs have clear topic sentences and present information in a logical order.\n\nThe conclusion sums up the main points and has a takeaway message.\n\nTopic coverage\n\nAll aspects of the question are answered.\n\nAll key terms and concepts are defined.\n\nEvery point on the rubric is fully addressed.\n\nAnalysis and argument\n\nThe analysis presents an evaluation (not just description).\n\nThe argument is supported by sufficient evidence and a range of sources.\n\nQuoting, paraphrasing\n\nQuotations are applied to your specific context, and their significance is clearly discussed.\n\nParaphrased content retains the same meaning as the original.\n\nProofreading\n\nFormatting\n\nCheck for consistency of:\n\nHeading levels\n\nDiagrams and tables\n\nMargins and indentation\n\nFootnotes and block quotes (if used)\n\nReferencing, citations\n\nAll quotes and paraphrases are cited.\n\nAll sources are in the reference list.\n\nAll reference details are complete.\n\nAll in-text citations and reference list entries are in the required style.\n\nLanguage use\n\nSentences are complete and separated by appropriate punctuation.\n\nSpelling is accurate and consistent, in Australian English.\n\nThe academic tone used throughout (formal, objective, impersonal, concise and precise).\n\nPersonal checklist\n\nAdd your own items to each of these checklists based on the feedback you have received in the past.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nA guide to oral presentations\n\nThe ability to undertake an oral presentation is a valuable skill for assessment tasks, interviews and your future career. This skill can be developed by everyone and is not reserved for those who are ""naturally"" confident at public speaking. This guide will provide you with some tips and techniques for ensuring your presentation is well planned, structured and delivered.\n\n1.\xa0\xa0Plan\n\nAnalyse your audience\n\nSome questions to consider include:\n\nWho is your audience?\n\nWhat do they know about the subject? What terminology will they know?\n\nWhat do they want to know?\n\nHow can you engage this audience? What matters to them?\n\nDetermine the purpose\n\nThe purpose of a presentation may be to provide information, persuade the audience to accept a point of view, or encourage them to take action. Knowing your purpose will help you decide what to include and how to structure your presentation.\n\nSelect effective information\n\nWhat kind of information will best support the presentation?\n\nWhat kind of information will appeal to the audience?\n\nAre there some useful examples or case studies to illustrate an idea?\n\n2.\xa0\xa0Prepare\n\nThe structure provides a framework for your presentation.\n\nIntroduction - an overview of the issue and the main ideas to be considered. Explain the issue, the background and key terms.\n\nBody\xa0- the main ideas, reasoning, evidence and explanation provided. Avoid overloading your audience with too much information. Categorise your information into key ideas.\n\nConclusion - a summary of what you have considered with repetition of key ideas.\n\nConsider how long you can spend on each section, given the time available.\n\nSelect appropriate visual aids\n\nRemember that the visuals are not the presentation. Their purpose is to enhance what you are saying by providing a visual link.\n\n3.\xa0\xa0Practise and present\n\nThe key to a good delivery is to practise your speech and your body language. Here are some tips to assist you:\n\nPractise your presentation several times, aloud and standing up.\n\nTime the presentation. If it is too long, remove and/or simplify information, rather than speaking more quickly.\n\nStand straight with your feet ""planted"" in the ground. This will eliminate swaying and nervous movements in the legs. You can move but do so with purpose.\n\nEstablish a ""resting place"" for your hands at the front of your body, such as cupped at waist level.\n\nEye contact is a powerful means to engage your audience so look at your audience when you speak.\n\nSpeak more slowly and clearly than you normally would. Provide emphasis through voice intonation, volume and pausing.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nPlanning your assignment\n\nAnalyse the topic\n\nWhat is the purpose of the task/topic?\n\nWhat is it asking?\n\nBrainstorm the topic\n\nWhat do I already know?\n\nWhat have I read?\n\nWhat ideas/questions do I want to research?\n\nWho are the key authors on my topic?\n\nPlan your time\n\nWork back from the due date to plan time for research, drafts and editing.\n\nTake time to account for other work due\n\nConstruct a rough plan\n\nWhat are my main points?\n\nWhat other ideas are worth including?\n\nPreliminary research\n\nRevise with lecture notes and prescribed or recommended textbooks.\n\nBegin researching\n\nKeep your topic in mind.\n\nFocus your research on relevant journal articles and texts.\n\nRecord your findings.\n\nRevise your plan\n\nConsider the relationships between your ideas.\n\nSelect\xa0key points.\n\nStart writing\n\nKeep your topic in mind when writing.\n\nFollow formatting requirements.\n\nCheck for grammar, coherence, flow.\n\nFollow relevant citation style.\n\nSubmit\n\nSubmit your assignment.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#learning #studying Quick Study Guides - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf1nhp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588863496.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Public Accessed: 07. May 2020  &lt;a href=""https://www.monash.edu/rlo/quick-study-guides""&gt;https://www.monash.edu/rlo/quick-study-guides&lt;/a&gt;  Monash University, RLO Study Skills, 2020&lt;/p&gt;\n\n&lt;p&gt;Assignment direction words&lt;/p&gt;\n\n&lt;p&gt;Below is an explanation of the keywords commonly used in assignment questions.&lt;/p&gt;\n\n&lt;p&gt;These explanations are intended as a guide only. There are not always hard distinctions between the words, and different academics may use them to mean slightly different things.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ANALYSE. Identify the elements of an argument, text, issue, process or event and show how they are related.ARGUE. Present the case for and/or against a particular proposition. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;COMMENT ON. Point out the important features, Criticise. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;COMPARE. Identify and explain similarities and differences. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;CONTRAST. Stress the differences between two or more things. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;CRITICISE. Judge the merit or truth of the factors or views mentioned, including both strengths and weaknesses.CRITICALLYâ€¦Used to add direction to another direction word (e.g. â€œcritically analyseâ€), this means approaching the task in a &lt;strong&gt;questioning&lt;/strong&gt; way: â€œHow does this work?â€ â€œWhy is it like this?â€ â€œWhat are its strengths and weaknesses?â€&lt;/p&gt;\n\n&lt;p&gt;DEFINE. Provide concise, clear, and authoritative meanings. Give the limits of the definition, but omit detailed explanations. Show how the item defined differs from items in other classes. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DESCRIBE. Recount, characterise, outline, and relate in sequence.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DIAGRAM. A drawing, chart, plan, or graph. Diagrams should be labelled and there should be an accompanying explanation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DISCUSS. Examine, analyse carefully and give reasons for and against. Be complete and give details, usually with a view to assessing how satisfactory something is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EVALUATE. Appraise in relation to some standard, referring to advantages, limitations, and costs and benefits as Appropriate.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EXAMINE. Investigate critically appraises a subject in detail.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EXPLAIN. Clarify, interpret and elaborate on the material presented. Give reasons for differences of opinion or results, and try to analyse causes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ILLUSTRATE. Use a concrete example, diagram, or figure to explain or clarify a problem.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;INDICATE. Identify, then focuses attention so as to clarify.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;JUSTIFY. Prove or give reasons for conclusions or decisions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;OUTLINE. Present the essential features, showing the main points and subordinate points. Omit minor details.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;REVIEW. Examine a subject critically, analysing and commenting on the important or controversial statements.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;STATE. Present the main points in a brief and clear sequence, usually omitting details or examples.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SUMMARISE. Give the main points or facts in condensed form. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Brainstorming: Mind mapping&lt;/p&gt;\n\n&lt;p&gt;Why mind map?&lt;/p&gt;\n\n&lt;p&gt;One effective form of brainstorming is mind mapping. A mind map is a visual representation of your ideas, consisting of words, images and colours, and can help you to:&lt;/p&gt;\n\n&lt;p&gt;focus on your research topic/question&lt;/p&gt;\n\n&lt;p&gt;structure and plan your assignment&lt;/p&gt;\n\n&lt;p&gt;combine one or more types of major thought relationships&lt;/p&gt;\n\n&lt;p&gt;identify relationships between ideas/concepts.&lt;/p&gt;\n\n&lt;p&gt;Stage 1&lt;/p&gt;\n\n&lt;p&gt;You can create a mind map on the paper, whiteboard or digitally, using visual mapping software such as &lt;a href=""http://freemind.sourceforge.net/wiki/index.php/Main_Page""&gt;FreeMind&lt;/a&gt;. To begin:&lt;/p&gt;\n\n&lt;p&gt;write your topic in the centre of a blank page&lt;/p&gt;\n\n&lt;p&gt;associate your ideas freely anywhere on the page and do not filter out ideas.&lt;/p&gt;\n\n&lt;p&gt;Stage 2&lt;/p&gt;\n\n&lt;p&gt;When you have run out of ideas:&lt;/p&gt;\n\n&lt;p&gt;consider each item and determine how this point is related to other points and to your topic&lt;/p&gt;\n\n&lt;p&gt;map relationships with lines, arrows, colours, images and bold type.&lt;/p&gt;\n\n&lt;p&gt;Stage 3&lt;/p&gt;\n\n&lt;p&gt;Use the relationships you have identified to reorganise your ideas.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Signposting&lt;/p&gt;\n\n&lt;p&gt;Why signpost?&lt;/p&gt;\n\n&lt;p&gt;Signposting helps you to:&lt;/p&gt;\n\n&lt;p&gt;create a navigation path to guide your reader,&lt;/p&gt;\n\n&lt;p&gt;indicate to your reader the direction your writing will take, and&lt;/p&gt;\n\n&lt;p&gt;remember your key points.&lt;/p&gt;\n\n&lt;p&gt;Major signposts&lt;/p&gt;\n\n&lt;p&gt;Major signposts indicate to the reader the important elements of your writing such as the purpose, connection between points, and the conclusion.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This study argues that â€¦&lt;/p&gt;\n\n&lt;p&gt;This paper examinesâ€¦&lt;/p&gt;\n\n&lt;p&gt;This essay begins byâ€¦ it goes on to â€¦&lt;/p&gt;\n\n&lt;p&gt;In conclusion â€¦&lt;/p&gt;\n\n&lt;p&gt;Transition sentences&lt;/p&gt;\n\n&lt;p&gt;Transition sentences inform the reader when your writing is moving from one idea to another, and how those ideas are connected.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Having examined; It is important to&lt;/p&gt;\n\n&lt;p&gt;The discussion highlights; It is also essential, however, to explore;&lt;/p&gt;\n\n&lt;p&gt;A significant factor inâ€¦ isâ€¦&lt;/p&gt;\n\n&lt;p&gt;Linking words&lt;/p&gt;\n\n&lt;p&gt;Linking words signify to the reader the connections between your ideas. Linking words convey what you will be discussing next, and how the reader should interpret it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some types of linking words&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples&lt;/p&gt;\n\n&lt;p&gt;Addition    in addition; also&lt;/p&gt;\n\n&lt;p&gt;Cause and effect    due to; as a result&lt;/p&gt;\n\n&lt;p&gt;Comparison  similarly; likewise&lt;/p&gt;\n\n&lt;p&gt;Contrast    alternatively; in contrast&lt;/p&gt;\n\n&lt;p&gt;Examples    for instance; to show this&lt;/p&gt;\n\n&lt;p&gt;Exception   however; although&lt;/p&gt;\n\n&lt;p&gt;Time and sequence   initially; to being; previously; finally&lt;/p&gt;\n\n&lt;p&gt;Reminders&lt;/p&gt;\n\n&lt;p&gt;Reminders can help the reader recall what you have previously discussed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As discussed aboveâ€¦&lt;/p&gt;\n\n&lt;p&gt;Section 2 outlinesâ€¦&lt;/p&gt;\n\n&lt;p&gt;So farâ€¦&lt;/p&gt;\n\n&lt;p&gt;In conclusionâ€¦&lt;/p&gt;\n\n&lt;p&gt;In briefâ€¦&lt;/p&gt;\n\n&lt;p&gt;Thusâ€¦&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing essays&lt;/p&gt;\n\n&lt;p&gt;Analysing essay topics&lt;/p&gt;\n\n&lt;p&gt;Understand the essay question&lt;/p&gt;\n\n&lt;p&gt;Make sure you know the precise meaning of every word in the essay question. Use:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;a) your general dictionary for unfamiliar words such as intrinsic, core values&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;b) a subject-specific dictionary, for example, the APA Dictionary of Psychology, for academic words such as proof, random sample, significance level&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Think about the different parts of the question&lt;/p&gt;\n\n&lt;p&gt;Decide how many parts the question has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;List areas you will probably have to research. It may help to write yourself a brief task description: â€œFirst find out what a market niche is, then see what significance this has for marketing. Nextâ€¦â€&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note any direction words&lt;/p&gt;\n\n&lt;p&gt;These might include:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;discuss, discuss critically, discuss the importance of, assess, justify, evaluate, analyse.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The structure of an essay&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;About 10% of the total length. May be one paragraph or several, depending on essay length.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Introduce the topic.&lt;/p&gt;\n\n&lt;p&gt;Provide background information.&lt;/p&gt;\n\n&lt;p&gt;Limit the scope of the discussion.&lt;/p&gt;\n\n&lt;p&gt;Define or state the topic.&lt;/p&gt;\n\n&lt;p&gt;Present the plan of coverage including your viewpoint and line of reasoning.&lt;/p&gt;\n\n&lt;p&gt;Move from general background information to your specific topic. You can set your own agenda to avoid too broad a focus.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Body&lt;/p&gt;\n\n&lt;p&gt;This is a series of linked paragraphs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each paragraph should have one main point.&lt;/p&gt;\n\n&lt;p&gt;The topic sentence of each paragraph carries the theme or argument.&lt;/p&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Here you are moving from the specifics of your essay to the more general background of the topic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sum up your argument and information with reference to the essay question.&lt;/p&gt;\n\n&lt;p&gt;Perhaps mention wider implications or future directions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Features of a critical review&lt;/p&gt;\n\n&lt;p&gt;A critical review requires you to evaluate an academic text and make your own judgement about it based on various criteria.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This guide points out the common features of critical reviews. Always check with your lecturer to confirm the exact requirements of your assignment.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Critical reviews can present positive and/or negative judgements.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Critical reviews involve two main tasks:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;summary&lt;/p&gt;\n\n&lt;p&gt;evaluation.&lt;/p&gt;\n\n&lt;p&gt;The evaluation criteria can vary depending on the discipline.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The aim of a critical review is to evaluate the text. This means you must be very familiar with the text, and your writing needs to clearly present your judgement.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;Usually looks like an entry in a bibliography, and includes full bibliographic details of the text.&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;Gives an overview of the text including the importance of the topic or question.&lt;/p&gt;\n\n&lt;p&gt;Briefly states your evaluation of the merits of the text.&lt;/p&gt;\n\n&lt;p&gt;Outlines your reviewâ€™s approach and structure.&lt;/p&gt;\n\n&lt;p&gt;Summary (may be combined with evaluation)&lt;/p&gt;\n\n&lt;p&gt;Describes the key points from the text, including the authorâ€™s intentions and findings.&lt;/p&gt;\n\n&lt;p&gt;Evaluation (may be combined with summary)&lt;/p&gt;\n\n&lt;p&gt;Presents strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;Focuses on the evaluation criteria to present your judgement of the text.&lt;/p&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Usually quite short, so can be included at the end of the evaluation or as a separate section.&lt;/p&gt;\n\n&lt;p&gt;Restates your overall evaluation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Features of reports&lt;/p&gt;\n\n&lt;p&gt;Referencing and quotations in reports follow the same guidelines required for essays.&lt;/p&gt;\n\n&lt;p&gt;A system of\xa0numbered sections with headings is typically used.&lt;/p&gt;\n\n&lt;p&gt;Just as in the essay, a good report will describe, analyse and evaluate a problem or issue. Unlike an essay, it will describe the method used to investigate the problem and formulate a set of recommendations based on the findings of the report.&lt;/p&gt;\n\n&lt;p&gt;Report&lt;/p&gt;\n\n&lt;p&gt;VS&lt;/p&gt;\n\n&lt;p&gt;Essay&lt;/p&gt;\n\n&lt;p&gt;Often a problem or case study which sets up a hypothetical situation&lt;/p&gt;\n\n&lt;p&gt;Based on the reading, fieldwork or practical work&lt;/p&gt;\n\n&lt;p&gt;Topic&lt;/p&gt;\n\n&lt;p&gt;Responds to a question or a proposition&lt;/p&gt;\n\n&lt;p&gt;Is based on research&lt;/p&gt;\n\n&lt;p&gt;To investigate, analyse and present information&lt;/p&gt;\n\n&lt;p&gt;Usually to make recommendations to solve a problem&lt;/p&gt;\n\n&lt;p&gt;Purpose&lt;/p&gt;\n\n&lt;p&gt;To articulate a well-argued response to a question or proposition&lt;/p&gt;\n\n&lt;p&gt;Established in the topic and is often a client or manager&lt;/p&gt;\n\n&lt;p&gt;Audience&lt;/p&gt;\n\n&lt;p&gt;An academic audience&lt;/p&gt;\n\n&lt;p&gt;Contains an executive summary or abstract&lt;/p&gt;\n\n&lt;p&gt;Comprises sections with headings&lt;/p&gt;\n\n&lt;p&gt;May use bullet points, tables, graphs to convey information&lt;/p&gt;\n\n&lt;p&gt;Format&lt;/p&gt;\n\n&lt;p&gt;Does not typically include sections or headings&lt;/p&gt;\n\n&lt;p&gt;Does not typically include use bullet points, tables, graphs&lt;/p&gt;\n\n&lt;p&gt;Third-person&lt;/p&gt;\n\n&lt;p&gt;Formal language&lt;/p&gt;\n\n&lt;p&gt;Style&lt;/p&gt;\n\n&lt;p&gt;Third or first person&lt;/p&gt;\n\n&lt;p&gt;Formal language&lt;/p&gt;\n\n&lt;p&gt;Success depends on:&lt;/p&gt;\n\n&lt;p&gt;the demonstration of good research skills&lt;/p&gt;\n\n&lt;p&gt;the quality of the recommendations to respond to an issue&lt;/p&gt;\n\n&lt;p&gt;the presentation and analysis of relevant information&lt;/p&gt;\n\n&lt;p&gt;Assessment&lt;/p&gt;\n\n&lt;p&gt;Success depends on:&lt;/p&gt;\n\n&lt;p&gt;the demonstration of good research skills&lt;/p&gt;\n\n&lt;p&gt;the identification of a cogent argument&lt;/p&gt;\n\n&lt;p&gt;the quality of reasoning and evidence&lt;/p&gt;\n\n&lt;p&gt;how well it analyses and evaluates the issue&lt;/p&gt;\n\n&lt;p&gt;Different types of reports typically include different sections.&lt;br/&gt;\nFor the requirements for reports in Business and Economics, see the Q Manual. For all other disciplines, look at the Faculty examples in\xa0&lt;a href=""https://www.monash.edu/rlo/assignment-samples""&gt;Assignment Structures and Samples&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing a case study&lt;/p&gt;\n\n&lt;p&gt;There are two different approaches to case studies. This guide focuses on the problem-oriented method. Always check with your lecturer to confirm if this is the type required.&lt;/p&gt;\n\n&lt;p&gt;A successful case study analyses a real-life situation where existing problems need to be solved. It should relate the theory to a practical situation; for example, apply the ideas and knowledge discussed in the coursework to the practical situation at hand in the case study.&lt;/p&gt;\n\n&lt;p&gt;Identify the problems.&lt;/p&gt;\n\n&lt;p&gt;Select the major problems in the case.&lt;/p&gt;\n\n&lt;p&gt;Suggest solutions to these major problems.&lt;/p&gt;\n\n&lt;p&gt;Recommend the best solution to be implemented.&lt;/p&gt;\n\n&lt;p&gt;Detail how this solution should be implemented.&lt;/p&gt;\n\n&lt;p&gt;There are usually eight sections in a case study:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Synopsis/Executive Summary&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Outline the purpose of the case study.&lt;/p&gt;\n\n&lt;p&gt;Describe the field of research.&lt;/p&gt;\n\n&lt;p&gt;Outline the issues and findings of the case study without specific details.&lt;/p&gt;\n\n&lt;p&gt;Identify the theory that will be used.&lt;/p&gt;\n\n&lt;p&gt;Note any assumptions made (you may not have all the information you&amp;#39;d like so some assumptions may be necessary e.g.: &amp;quot;It has been assumed thatâ€¦&amp;quot;, &amp;quot;Assuming that it takes half an hour to read one documentâ€¦&amp;quot;).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Findings&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Identify the problems found in the case by:&lt;/p&gt;\n\n&lt;p&gt;analysing the problem, supporting your findings with facts given in the case, the relevant theory and course concepts.&lt;/p&gt;\n\n&lt;p&gt;searching for the underlying problems&lt;/p&gt;\n\n&lt;p&gt;This section is often divided into sub-sections.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Discussion&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Summarise the major problem/s.&lt;/p&gt;\n\n&lt;p&gt;Identify alternative solutions to this/these major problem/s.&lt;/p&gt;\n\n&lt;p&gt;Briefly outline each alternative solution and evaluate its advantages and disadvantages.&lt;/p&gt;\n\n&lt;p&gt;There is no need to refer to theory or coursework here.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Conclusion&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Sum up the main points from the findings and discussion.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Recommendations&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Choose which of the alternative solutions should be adopted.&lt;/p&gt;\n\n&lt;p&gt;Briefly justify your choice explaining how it will solve the major problem/s.&lt;/p&gt;\n\n&lt;p&gt;This should be written in a forceful style as this section is intended to be persuasive.&lt;/p&gt;\n\n&lt;p&gt;Here integration of theory and coursework is appropriate.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Implementation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Explain what should be done, by whom and by when.&lt;/p&gt;\n\n&lt;p&gt;If appropriate include a rough estimate of costs (both financial and time).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;References&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Make sure all references are cited correctly.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Appendices (if any)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Attach any original data that relates to the study but which would have interrupted the flow of the main body&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing a critical review&lt;/p&gt;\n\n&lt;p&gt;Critical reviews require careful planning and drafting just like any other assignment. This guide suggests what to focus on at each stage of the process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Plan your approach&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Read your assignment instructions carefully in order to:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;determine your exact criteria;&lt;/p&gt;\n\n&lt;p&gt;find out what proportion of your review you should dedicate to summary and evaluation; and&lt;/p&gt;\n\n&lt;p&gt;know whether the summary and evaluation should be presented as separate sections or a combined section.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Make notes&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Skim read the text and make notes about:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;the main question or questions;&lt;/p&gt;\n\n&lt;p&gt;the authorâ€™s aim;&lt;/p&gt;\n\n&lt;p&gt;the methods used;&lt;/p&gt;\n\n&lt;p&gt;the evidence provided;&lt;/p&gt;\n\n&lt;p&gt;the key findings or answers; and&lt;/p&gt;\n\n&lt;p&gt;the implications and significance of the findings.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Evaluate the text&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Judge the quality or value of the text (for other researchers, or to practitioners in the field, or to students).&lt;/p&gt;\n\n&lt;p&gt;Consider the merits of the text in the short term as well as the long term.&lt;/p&gt;\n\n&lt;p&gt;Consider the merits of the text in comparison to other related text.&lt;/p&gt;\n\n&lt;p&gt;When evaluating the text you could answer some of the following questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is the question the text tries to answer relevant, interesting, new, or useful? To who, and why?&lt;/p&gt;\n\n&lt;p&gt;Does the text give new answers or interpretations to an old question?&lt;/p&gt;\n\n&lt;p&gt;Is the text detailed, or brief? Simple or complex?&lt;/p&gt;\n\n&lt;p&gt;Is the evidence presented to support the answer extensive? Strong? Weak? Relevant? Persuasive? Contradictory?&lt;/p&gt;\n\n&lt;p&gt;Are the conclusions reached final, limited, qualified or preliminary?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write it up&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;When writing and proofreading your critical review:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Stay focused on your evaluation criteria.&lt;/p&gt;\n\n&lt;p&gt;Read the text you are reviewing again to check that you have covered everything.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Editing and proofreading your work&lt;/p&gt;\n\n&lt;p&gt;Refining your own work is an essential skill, and an excellent way to continually improve your writing.&lt;/p&gt;\n\n&lt;p&gt;This guide covers the differences between editing and proofreading and provides checklists you can use to review your work before submission.&lt;/p&gt;\n\n&lt;p&gt;What is the difference?&lt;/p&gt;\n\n&lt;p&gt;Editing focuses on improving the &amp;#39;big picture&amp;#39; of your assignment. It is how you ensure you have fully addressed the task requirements, and involves making structural changes to your writing and checking the logic and flow.&lt;/p&gt;\n\n&lt;p&gt;Proofreading focuses on specific details like spelling, sentence structure, and referencing.&lt;/p&gt;\n\n&lt;p&gt;Step 1: Reread your instructions, question and rubric, so you can approach the task with clarity about your aims and purpose.&lt;/p&gt;\n\n&lt;p&gt;Step 2: Use the checklists below as a starting point to refine your work.&lt;/p&gt;\n\n&lt;p&gt;Tip: To spot errors more easily, read your text aloud, and take breaks between writing, editing, and proofreading.&lt;/p&gt;\n\n&lt;p&gt;Editing&lt;/p&gt;\n\n&lt;p&gt;Structural aspects&lt;/p&gt;\n\n&lt;p&gt;The introduction clearly states the topic and how it will be covered.&lt;/p&gt;\n\n&lt;p&gt;Paragraphs have clear topic sentences and present information in a logical order.&lt;/p&gt;\n\n&lt;p&gt;The conclusion sums up the main points and has a takeaway message.&lt;/p&gt;\n\n&lt;p&gt;Topic coverage&lt;/p&gt;\n\n&lt;p&gt;All aspects of the question are answered.&lt;/p&gt;\n\n&lt;p&gt;All key terms and concepts are defined.&lt;/p&gt;\n\n&lt;p&gt;Every point on the rubric is fully addressed.&lt;/p&gt;\n\n&lt;p&gt;Analysis and argument&lt;/p&gt;\n\n&lt;p&gt;The analysis presents an evaluation (not just description).&lt;/p&gt;\n\n&lt;p&gt;The argument is supported by sufficient evidence and a range of sources.&lt;/p&gt;\n\n&lt;p&gt;Quoting, paraphrasing&lt;/p&gt;\n\n&lt;p&gt;Quotations are applied to your specific context, and their significance is clearly discussed.&lt;/p&gt;\n\n&lt;p&gt;Paraphrased content retains the same meaning as the original.&lt;/p&gt;\n\n&lt;p&gt;Proofreading&lt;/p&gt;\n\n&lt;p&gt;Formatting&lt;/p&gt;\n\n&lt;p&gt;Check for consistency of:&lt;/p&gt;\n\n&lt;p&gt;Heading levels&lt;/p&gt;\n\n&lt;p&gt;Diagrams and tables&lt;/p&gt;\n\n&lt;p&gt;Margins and indentation&lt;/p&gt;\n\n&lt;p&gt;Footnotes and block quotes (if used)&lt;/p&gt;\n\n&lt;p&gt;Referencing, citations&lt;/p&gt;\n\n&lt;p&gt;All quotes and paraphrases are cited.&lt;/p&gt;\n\n&lt;p&gt;All sources are in the reference list.&lt;/p&gt;\n\n&lt;p&gt;All reference details are complete.&lt;/p&gt;\n\n&lt;p&gt;All in-text citations and reference list entries are in the required style.&lt;/p&gt;\n\n&lt;p&gt;Language use&lt;/p&gt;\n\n&lt;p&gt;Sentences are complete and separated by appropriate punctuation.&lt;/p&gt;\n\n&lt;p&gt;Spelling is accurate and consistent, in Australian English.&lt;/p&gt;\n\n&lt;p&gt;The academic tone used throughout (formal, objective, impersonal, concise and precise).&lt;/p&gt;\n\n&lt;p&gt;Personal checklist&lt;/p&gt;\n\n&lt;p&gt;Add your own items to each of these checklists based on the feedback you have received in the past.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A guide to oral presentations&lt;/p&gt;\n\n&lt;p&gt;The ability to undertake an oral presentation is a valuable skill for assessment tasks, interviews and your future career. This skill can be developed by everyone and is not reserved for those who are &amp;quot;naturally&amp;quot; confident at public speaking. This guide will provide you with some tips and techniques for ensuring your presentation is well planned, structured and delivered.&lt;/p&gt;\n\n&lt;p&gt;1.\xa0\xa0Plan&lt;/p&gt;\n\n&lt;p&gt;Analyse your audience&lt;/p&gt;\n\n&lt;p&gt;Some questions to consider include:&lt;/p&gt;\n\n&lt;p&gt;Who is your audience?&lt;/p&gt;\n\n&lt;p&gt;What do they know about the subject? What terminology will they know?&lt;/p&gt;\n\n&lt;p&gt;What do they want to know?&lt;/p&gt;\n\n&lt;p&gt;How can you engage this audience? What matters to them?&lt;/p&gt;\n\n&lt;p&gt;Determine the purpose&lt;/p&gt;\n\n&lt;p&gt;The purpose of a presentation may be to provide information, persuade the audience to accept a point of view, or encourage them to take action. Knowing your purpose will help you decide what to include and how to structure your presentation.&lt;/p&gt;\n\n&lt;p&gt;Select effective information&lt;/p&gt;\n\n&lt;p&gt;What kind of information will best support the presentation?&lt;/p&gt;\n\n&lt;p&gt;What kind of information will appeal to the audience?&lt;/p&gt;\n\n&lt;p&gt;Are there some useful examples or case studies to illustrate an idea?&lt;/p&gt;\n\n&lt;p&gt;2.\xa0\xa0Prepare&lt;/p&gt;\n\n&lt;p&gt;The structure provides a framework for your presentation.&lt;/p&gt;\n\n&lt;p&gt;Introduction - an overview of the issue and the main ideas to be considered. Explain the issue, the background and key terms.&lt;/p&gt;\n\n&lt;p&gt;Body\xa0- the main ideas, reasoning, evidence and explanation provided. Avoid overloading your audience with too much information. Categorise your information into key ideas.&lt;/p&gt;\n\n&lt;p&gt;Conclusion - a summary of what you have considered with repetition of key ideas.&lt;/p&gt;\n\n&lt;p&gt;Consider how long you can spend on each section, given the time available.&lt;/p&gt;\n\n&lt;p&gt;Select appropriate visual aids&lt;/p&gt;\n\n&lt;p&gt;Remember that the visuals are not the presentation. Their purpose is to enhance what you are saying by providing a visual link.&lt;/p&gt;\n\n&lt;p&gt;3.\xa0\xa0Practise and present&lt;/p&gt;\n\n&lt;p&gt;The key to a good delivery is to practise your speech and your body language. Here are some tips to assist you:&lt;/p&gt;\n\n&lt;p&gt;Practise your presentation several times, aloud and standing up.&lt;/p&gt;\n\n&lt;p&gt;Time the presentation. If it is too long, remove and/or simplify information, rather than speaking more quickly.&lt;/p&gt;\n\n&lt;p&gt;Stand straight with your feet &amp;quot;planted&amp;quot; in the ground. This will eliminate swaying and nervous movements in the legs. You can move but do so with purpose.&lt;/p&gt;\n\n&lt;p&gt;Establish a &amp;quot;resting place&amp;quot; for your hands at the front of your body, such as cupped at waist level.&lt;/p&gt;\n\n&lt;p&gt;Eye contact is a powerful means to engage your audience so look at your audience when you speak.&lt;/p&gt;\n\n&lt;p&gt;Speak more slowly and clearly than you normally would. Provide emphasis through voice intonation, volume and pausing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Planning your assignment&lt;/p&gt;\n\n&lt;p&gt;Analyse the topic&lt;/p&gt;\n\n&lt;p&gt;What is the purpose of the task/topic?&lt;/p&gt;\n\n&lt;p&gt;What is it asking?&lt;/p&gt;\n\n&lt;p&gt;Brainstorm the topic&lt;/p&gt;\n\n&lt;p&gt;What do I already know?&lt;/p&gt;\n\n&lt;p&gt;What have I read?&lt;/p&gt;\n\n&lt;p&gt;What ideas/questions do I want to research?&lt;/p&gt;\n\n&lt;p&gt;Who are the key authors on my topic?&lt;/p&gt;\n\n&lt;p&gt;Plan your time&lt;/p&gt;\n\n&lt;p&gt;Work back from the due date to plan time for research, drafts and editing.&lt;/p&gt;\n\n&lt;p&gt;Take time to account for other work due&lt;/p&gt;\n\n&lt;p&gt;Construct a rough plan&lt;/p&gt;\n\n&lt;p&gt;What are my main points?&lt;/p&gt;\n\n&lt;p&gt;What other ideas are worth including?&lt;/p&gt;\n\n&lt;p&gt;Preliminary research&lt;/p&gt;\n\n&lt;p&gt;Revise with lecture notes and prescribed or recommended textbooks.&lt;/p&gt;\n\n&lt;p&gt;Begin researching&lt;/p&gt;\n\n&lt;p&gt;Keep your topic in mind.&lt;/p&gt;\n\n&lt;p&gt;Focus your research on relevant journal articles and texts.&lt;/p&gt;\n\n&lt;p&gt;Record your findings.&lt;/p&gt;\n\n&lt;p&gt;Revise your plan&lt;/p&gt;\n\n&lt;p&gt;Consider the relationships between your ideas.&lt;/p&gt;\n\n&lt;p&gt;Select\xa0key points.&lt;/p&gt;\n\n&lt;p&gt;Start writing&lt;/p&gt;\n\n&lt;p&gt;Keep your topic in mind when writing.&lt;/p&gt;\n\n&lt;p&gt;Follow formatting requirements.&lt;/p&gt;\n\n&lt;p&gt;Check for grammar, coherence, flow.&lt;/p&gt;\n\n&lt;p&gt;Follow relevant citation style.&lt;/p&gt;\n\n&lt;p&gt;Submit&lt;/p&gt;\n\n&lt;p&gt;Submit your assignment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf1nhp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/', 'subreddit_subscribers': 33, 'created_utc': 1588834696.0, 'num_crossposts': 30, 'media': None, 'is_video': False}]",t3_gf1nhp,,
,learnmachinelearning,"How do people find the github implementation of different models?

How do you even know some approach exists?

Are people finding new approaches through searching for papers? How do you find those papers?",t2_b3oz3,False,,0,False,How do you find the right method for a particular application?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg3f3s,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589006552.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do people find the github implementation of different models?&lt;/p&gt;

&lt;p&gt;How do you even know some approach exists?&lt;/p&gt;

&lt;p&gt;Are people finding new approaches through searching for papers? How do you find those papers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3f3s,True,,sinefine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3f3s/how_do_you_find_the_right_method_for_a_particular/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3f3s/how_do_you_find_the_right_method_for_a_particular/,155203,1588977752.0,0,,False,,,,
,learnmachinelearning,"I'm working my way through Andrew Ng's ML class, but I have a question regarding theta in linear regression. From what I understand, the theta value is the weight associated with the X values in a machine learning problem, which are often set arbitrarily when the model is created. The purpose is to find the best theta value that will produce the best result on unknown data, right? Once a model is created and we test it, how do we find the actual theta value? For example, the model below is using the body mass index to predict whether someone is obese. A simple example, but how can we determine the theta value? 

&amp;#x200B;

    X = df['bmi']
    y = ['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
    
    reg = LinearRegression()
    reg.fit(X_train, y_train)
    y_pred = reg.predict(y_test)

&amp;#x200B;

From here how can we find the theta value? Is it just under the hood, or is there a way to find the actual theta value? I might be misunderstanding what exactly theta is.",t2_3pnizflv,False,,0,False,How to get the theta value for Linear Regression?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg3bu5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589006244.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working my way through Andrew Ng&amp;#39;s ML class, but I have a question regarding theta in linear regression. From what I understand, the theta value is the weight associated with the X values in a machine learning problem, which are often set arbitrarily when the model is created. The purpose is to find the best theta value that will produce the best result on unknown data, right? Once a model is created and we test it, how do we find the actual theta value? For example, the model below is using the body mass index to predict whether someone is obese. A simple example, but how can we determine the theta value? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = df[&amp;#39;bmi&amp;#39;]
y = [&amp;#39;target&amp;#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;From here how can we find the theta value? Is it just under the hood, or is there a way to find the actual theta value? I might be misunderstanding what exactly theta is.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3bu5,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3bu5/how_to_get_the_theta_value_for_linear_regression/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3bu5/how_to_get_the_theta_value_for_linear_regression/,155203,1588977444.0,0,,False,,,,
,learnmachinelearning,"Greetings,

I'm working on uploading a few machine learning projects to GitHub as a way to build a portfolio of my skills. Recently I completed a machine learning project predicting heart disease; however, a few sample projects on Github look very similar to the one I worked on independently. Is this a strike against my project? I don't want someone to look at my project and think I just copied the one on GitHub. Is this a common issue with ML projects? Granted, my project isn't new, but it was a topic that's hits close to home, so I felt compelled to study it. Should I still add this project to my portfolio on GutHub?",t2_3pnizflv,False,,0,False,Project on Github similar to one I completed. Should I still use my project as a professional example?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfyyom,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588992375.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on uploading a few machine learning projects to GitHub as a way to build a portfolio of my skills. Recently I completed a machine learning project predicting heart disease; however, a few sample projects on Github look very similar to the one I worked on independently. Is this a strike against my project? I don&amp;#39;t want someone to look at my project and think I just copied the one on GitHub. Is this a common issue with ML projects? Granted, my project isn&amp;#39;t new, but it was a topic that&amp;#39;s hits close to home, so I felt compelled to study it. Should I still add this project to my portfolio on GutHub?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfyyom,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfyyom/project_on_github_similar_to_one_i_completed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfyyom/project_on_github_similar_to_one_i_completed/,155203,1588963575.0,0,,False,,,,
,learnmachinelearning,"Hi Folks, a few months ago I taught an intro to Python course for data science students (my first teaching experience). I wrote a little Jupyter magic command which posts code executed in that cell to a server where I could see, in real-time what students were submitting. Once everyone submitted marked exercises, I would share the results with the class so students could see how their classmates solved the same problem. We would have a short discussion about the various solutions.

I found this real-time feedback to be pretty productive and students seemed to have liked it as well. It kept students engaged, provided them with immediate feedback and helped them see that their classmates were struggling with same issues (and how to solve them).

Iâ€™ve worked on the app for the past few weeks and converted it to something people other than myself can use. Please keep in mind that this is very basic and I have spent very little time around user interface niceties. I still need to implement some important features, but will wait until I get some feedback. Please consider this a â€œbetaâ€ (perhaps even a functional â€œalphaâ€) version.

Please check it out. The main url is:
https://postcell.io/

Iâ€™ve written a basic help page at:
https://postcell.io/help.html

There is a tiny skeleton project at:
https://github.com/falconair/postcell_example

TLDR:
I'd love to get feedback from instructors on my jupyter extension at https://postcell.io",t2_rep0j,False,,0,False,My quarantine project for instructors who use Jupyter to teach: real-time feedback,[],r/learnmachinelearning,False,6,,0,,False,t3_gfyvzp,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588992137.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Folks, a few months ago I taught an intro to Python course for data science students (my first teaching experience). I wrote a little Jupyter magic command which posts code executed in that cell to a server where I could see, in real-time what students were submitting. Once everyone submitted marked exercises, I would share the results with the class so students could see how their classmates solved the same problem. We would have a short discussion about the various solutions.&lt;/p&gt;

&lt;p&gt;I found this real-time feedback to be pretty productive and students seemed to have liked it as well. It kept students engaged, provided them with immediate feedback and helped them see that their classmates were struggling with same issues (and how to solve them).&lt;/p&gt;

&lt;p&gt;Iâ€™ve worked on the app for the past few weeks and converted it to something people other than myself can use. Please keep in mind that this is very basic and I have spent very little time around user interface niceties. I still need to implement some important features, but will wait until I get some feedback. Please consider this a â€œbetaâ€ (perhaps even a functional â€œalphaâ€) version.&lt;/p&gt;

&lt;p&gt;Please check it out. The main url is:
&lt;a href=""https://postcell.io/""&gt;https://postcell.io/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Iâ€™ve written a basic help page at:
&lt;a href=""https://postcell.io/help.html""&gt;https://postcell.io/help.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is a tiny skeleton project at:
&lt;a href=""https://github.com/falconair/postcell_example""&gt;https://github.com/falconair/postcell_example&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TLDR:
I&amp;#39;d love to get feedback from instructors on my jupyter extension at &lt;a href=""https://postcell.io""&gt;https://postcell.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?auto=webp&amp;s=8759f660f293b041e021b14bd931cb50f5b81785', 'width': 2143, 'height': 1576}, 'resolutions': [{'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f06824a4e8347aa01feb74b7348b4225755a68b9', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=efb1b865c3fc7e1453ee0f1a5e5d99b600ddfe1e', 'width': 216, 'height': 158}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3498879e4ef836de3e7c6205bf989a8d905a5142', 'width': 320, 'height': 235}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35c171ce19ed33dcad0369d350f9d1790b2c38c4', 'width': 640, 'height': 470}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55b18cc3dbff4c4e4e926ff3beb4da252438a0fb', 'width': 960, 'height': 706}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50e0f7d240a3de3cbe824d0c098116474c57824f', 'width': 1080, 'height': 794}], 'variants': {}, 'id': 'sJA7EnzpmJkrkhC5jLhRyqgDTTwvpwOm7dSJLkkFPgE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfyvzp,True,,shahbazac,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfyvzp/my_quarantine_project_for_instructors_who_use/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfyvzp/my_quarantine_project_for_instructors_who_use/,155203,1588963337.0,0,,False,,,,
,learnmachinelearning,"Hello,

Do it unusual &amp; pandemic situations, I got let go of my last job as a UI Tools Engineer and landed a *temporary* job at an old company dealing with machine learning. Right now I'm improving their data preparation for machine learning, and various other things as it's a small company. This is my first time dealing with ML professionally; I got a BS in CS but as for ML I'm self taught.

My question is how many years of experience does one need in order to make a jump to machine learning as a career? Many job listings don't seem to specify years of experience exactly. Technically I only have 0.5 years of experience, yet I'm in the industry now so I can't tell if I would be able to make the jump to another company (any size) any time soon; I got this role because of connections.",t2_fwcx8,False,,0,False,Typical years of experience needed?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfykh7,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588991146.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;Do it unusual &amp;amp; pandemic situations, I got let go of my last job as a UI Tools Engineer and landed a &lt;em&gt;temporary&lt;/em&gt; job at an old company dealing with machine learning. Right now I&amp;#39;m improving their data preparation for machine learning, and various other things as it&amp;#39;s a small company. This is my first time dealing with ML professionally; I got a BS in CS but as for ML I&amp;#39;m self taught.&lt;/p&gt;

&lt;p&gt;My question is how many years of experience does one need in order to make a jump to machine learning as a career? Many job listings don&amp;#39;t seem to specify years of experience exactly. Technically I only have 0.5 years of experience, yet I&amp;#39;m in the industry now so I can&amp;#39;t tell if I would be able to make the jump to another company (any size) any time soon; I got this role because of connections.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfykh7,True,,TIL_this_shit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfykh7/typical_years_of_experience_needed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfykh7/typical_years_of_experience_needed/,155203,1588962346.0,0,,False,,,,
,learnmachinelearning,,t2_ipugc,False,,0,False,Becoming a full-stack data scientist,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfy33s,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/6G_zB9Uxd1-vfL9ZxYm-dyA9Q7jor8VRyW9tHtftVWM.jpg,False,,[],{},link,,False,,1588989679.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?auto=webp&amp;s=b0f6b3d205fde07ec0665abeb23c5d444e88f77c', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aab802192379e5f1e9ad830693840133c038d273', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77c2844e7d41513083d8b9254057e2dffd84b477', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=827ecbf79d80f037ea7a87892a558e43ed060326', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9eca57892e195e21d82fb4e05dfdc4e8aad9514c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=413034b19f1f54d396eb310263cad5c6b6764601', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b04c8ac29c62fe283ce2d5ead0fc1e6a1b5a440', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'y4E11f4IhVqXKgt5Hmw5FvvjfZru1XjOaXMkeTrW9Po'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfy33s,True,,stolzen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfy33s/becoming_a_fullstack_data_scientist/,all_ads,False,https://medium.com/data-science-insider/becoming-a-full-stack-data-scientist-d6514ee2e34a,155203,1588960879.0,0,,False,,,,
,learnmachinelearning,"I'm trying to work with IIC by [u/xuj1](https://www.reddit.com/u/xuj1/):

[Invariant Information Clustering for Unsupervised Image Classification and Segmentation](https://arxiv.org/abs/1807.06653)

I've tried 3 different GitHub repositories and none work.  The main one isn't runnable at all and is hard to follow:

xu-ji/IIC

This one runs but it's only showing losses and the accuracy isn't working :

RuABraun/phone-clustering

And this one after a bunch of tweaks I got to run but it doesn't get higher than 30% with a full VGG net after 200 iterations and stays random with a smaller net:

DuaneNielsen/iic

Trying all with MNIST.

Thanks to anyone that can help.",t2_5mz3he12,False,,0,False,Invariant Information Clustering - has anyone actually gotten this to work?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfxa4y,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588987182.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to work with IIC by &lt;a href=""https://www.reddit.com/u/xuj1/""&gt;u/xuj1&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1807.06653""&gt;Invariant Information Clustering for Unsupervised Image Classification and Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve tried 3 different GitHub repositories and none work.  The main one isn&amp;#39;t runnable at all and is hard to follow:&lt;/p&gt;

&lt;p&gt;xu-ji/IIC&lt;/p&gt;

&lt;p&gt;This one runs but it&amp;#39;s only showing losses and the accuracy isn&amp;#39;t working :&lt;/p&gt;

&lt;p&gt;RuABraun/phone-clustering&lt;/p&gt;

&lt;p&gt;And this one after a bunch of tweaks I got to run but it doesn&amp;#39;t get higher than 30% with a full VGG net after 200 iterations and stays random with a smaller net:&lt;/p&gt;

&lt;p&gt;DuaneNielsen/iic&lt;/p&gt;

&lt;p&gt;Trying all with MNIST.&lt;/p&gt;

&lt;p&gt;Thanks to anyone that can help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfxa4y,True,,mustgoplay,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfxa4y/invariant_information_clustering_has_anyone/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfxa4y/invariant_information_clustering_has_anyone/,155203,1588958382.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,Improve your #listening skills by anonymous - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg17bc,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1588999275.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?auto=webp&amp;s=228f714a4a874584faa511221d802f251ab528c4', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49315ce2396d3bd5881f9d8ab90637e2b2287b77', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005821dd56c5a268ef920f17f78ffca32344a95f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2224c7f2357d31b2898207bd5706effe92ee46b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2e4b198bf81b99a9a8a17c62554e72f54649d45', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b0964a7af8ddbef28c2be4929b17c53c07f9621', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=489247a3390e8d864a3539cba508e0f347a03851', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'CpHs7vQlxKRbpx3n1C8dTgV4xAyoMHI4qj_xxMUYdFk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg17bc,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg17bc/improve_your_listening_skills_by_anonymous_a/,all_ads,False,/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/,155203,1588970475.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': ""Reference: Public Accessed: 07. May 2020  [http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo](http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo)  Monash University, RLO, Study Skills, 2020\n\n*Lectures can be overwhelming no matter what you're studying. There's so much content! Do you need to write everything down? Romany Manuell, a subject librarian at Monash, offers a few tips to help you with your listening and note-taking skills.*  \nLectures- with a chalkboard!  \n\n\nWe all wish we had a photographic memory - with an audio component - so we can capture everything our lecturer says... These days, that wish is a reality! Many lecturers at Monash Uni make use of Learning Capture to record lectures, and then make the content available on Moodle through your unit site. But whether you're attending the lecture in person, or reviewing the lecture via Learning Capture, listening just isn't enough. You'll remember much more if you adopt some of these approaches:\n\n**1. Prepare to listen with purpose**  \n\n\nA good way to prepare for lectures is to try to read relevant readings beforehand and come to the lecture with a series of questions youâ€™d like the answers to. Listen out for the answers, and youâ€™ll be listening with a purpose! You donâ€™t actually have to ask the questions out loud, but if they arenâ€™t answered during the lecture, look for opportunities to discuss your questions with your lecturer, tutor or fellow students.\n\n**2. Practice your handwriting**  \n\n\nYes, itâ€™s old school, but according to studies such as [this one](http://journals.sagepub.com/doi/abs/10.1177/0956797614524581), writing by hand can actually help you remember. Researchers believe thereâ€™s something about handwriting that helps you to reframe content in your own words. So leave that laptop at home (it might help you stay off Facebook tooâ€¦ gulp!).\n\n**3. Listen out for signalling words**\n\nYou may find that words such as *first, second, also, furthermore, moreover, therefore* and *finally* indicate stages in the lecturer's argument. Listen out for those words in order to grab the main points. There are more useful signalling words and other tips available on [Research and Learning Online](https://www.monash.edu/rlo/study-skills/learning-in-university-classes/listening-and-notetaking-in-lectures).\n\nAs you can see, listening and note-taking really work hand-in-hand. So if you need to brush up on your note-taking skills, watch the video:  [**https://youtu.be/XELOxGx\\_ZZg**](https://youtu.be/XELOxGx_ZZg)"", 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Improve your #listening skills by anonymous - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf0q4i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588858834.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Public Accessed: 07. May 2020  &lt;a href=""http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo""&gt;http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo&lt;/a&gt;  Monash University, RLO, Study Skills, 2020&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Lectures can be overwhelming no matter what you&amp;#39;re studying. There&amp;#39;s so much content! Do you need to write everything down? Romany Manuell, a subject librarian at Monash, offers a few tips to help you with your listening and note-taking skills.&lt;/em&gt;&lt;br/&gt;\nLectures- with a chalkboard!  &lt;/p&gt;\n\n&lt;p&gt;We all wish we had a photographic memory - with an audio component - so we can capture everything our lecturer says... These days, that wish is a reality! Many lecturers at Monash Uni make use of Learning Capture to record lectures, and then make the content available on Moodle through your unit site. But whether you&amp;#39;re attending the lecture in person, or reviewing the lecture via Learning Capture, listening just isn&amp;#39;t enough. You&amp;#39;ll remember much more if you adopt some of these approaches:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Prepare to listen with purpose&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;A good way to prepare for lectures is to try to read relevant readings beforehand and come to the lecture with a series of questions youâ€™d like the answers to. Listen out for the answers, and youâ€™ll be listening with a purpose! You donâ€™t actually have to ask the questions out loud, but if they arenâ€™t answered during the lecture, look for opportunities to discuss your questions with your lecturer, tutor or fellow students.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Practice your handwriting&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Yes, itâ€™s old school, but according to studies such as &lt;a href=""http://journals.sagepub.com/doi/abs/10.1177/0956797614524581""&gt;this one&lt;/a&gt;, writing by hand can actually help you remember. Researchers believe thereâ€™s something about handwriting that helps you to reframe content in your own words. So leave that laptop at home (it might help you stay off Facebook tooâ€¦ gulp!).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Listen out for signalling words&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You may find that words such as &lt;em&gt;first, second, also, furthermore, moreover, therefore&lt;/em&gt; and &lt;em&gt;finally&lt;/em&gt; indicate stages in the lecturer&amp;#39;s argument. Listen out for those words in order to grab the main points. There are more useful signalling words and other tips available on &lt;a href=""https://www.monash.edu/rlo/study-skills/learning-in-university-classes/listening-and-notetaking-in-lectures""&gt;Research and Learning Online&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;As you can see, listening and note-taking really work hand-in-hand. So if you need to brush up on your note-taking skills, watch the video:  &lt;a href=""https://youtu.be/XELOxGx_ZZg""&gt;&lt;strong&gt;https://youtu.be/XELOxGx_ZZg&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?auto=webp&amp;s=228f714a4a874584faa511221d802f251ab528c4', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49315ce2396d3bd5881f9d8ab90637e2b2287b77', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005821dd56c5a268ef920f17f78ffca32344a95f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2224c7f2357d31b2898207bd5706effe92ee46b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2e4b198bf81b99a9a8a17c62554e72f54649d45', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b0964a7af8ddbef28c2be4929b17c53c07f9621', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=489247a3390e8d864a3539cba508e0f347a03851', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'CpHs7vQlxKRbpx3n1C8dTgV4xAyoMHI4qj_xxMUYdFk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf0q4i', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/', 'subreddit_subscribers': 33, 'created_utc': 1588830034.0, 'num_crossposts': 31, 'media': None, 'is_video': False}]",t3_gf0q4i,,
,learnmachinelearning,"Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups.

I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I'm unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I'm trying to see how this would be changed to go to 25 groups.

    model = Sequential()
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(100, 100, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))

I tried to change the input scale to input\_shape=(200, 200, 1) to match my input image size, and I also changed the Final Dense layer from 10 to 25, however, I am still getting 0% accuracy. Any advice is appreciated!

EDIT: My [model.fit](https://model.fit) is and model.compile is: 

    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',     
    metrics=['accuracy']) 
    
    model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, validation_data=(X_test, y_test))

&amp;#x200B;",t2_7ajy1,False,,0,False,Looking for advice on how to set up parameters for CNN (sequential),[],r/learnmachinelearning,False,6,,0,,False,t3_gg0m3x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588970659.0,,[],{},,,True,,1588997436.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups.&lt;/p&gt;

&lt;p&gt;I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I&amp;#39;m unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I&amp;#39;m trying to see how this would be changed to go to 25 groups.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (5, 5), activation=&amp;#39;relu&amp;#39;, input_shape=(100, 100, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(10, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried to change the input scale to input_shape=(200, 200, 1) to match my input image size, and I also changed the Final Dense layer from 10 to 25, however, I am still getting 0% accuracy. Any advice is appreciated!&lt;/p&gt;

&lt;p&gt;EDIT: My &lt;a href=""https://model.fit""&gt;model.fit&lt;/a&gt; is and model.compile is: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.compile(optimizer=&amp;#39;adam&amp;#39;,loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;,     
metrics=[&amp;#39;accuracy&amp;#39;]) 

model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, validation_data=(X_test, y_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg0m3x,True,,MrMegaGamerz,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg0m3x/looking_for_advice_on_how_to_set_up_parameters/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg0m3x/looking_for_advice_on_how_to_set_up_parameters/,155203,1588968636.0,0,,False,,,,
,learnmachinelearning,"I've been wanting to try making a model that would be able to tell if an object is of a specific class or not (for ex. Dog and non-dogs). When, it came to thinking of implementation, I wasn't particularly sure what would be the right way of going about this problem.  


If I were to think of it as a multi-class classification, the number of classes would be limited to the number of classes under non-dog objects. On the other hand if I'll limit it to one class, when I think of compiling everything into a single class for non-dog objects, the objects in question are to varied to be classified as such.  


Since essentially we're checking the likelihood if it's a Dog or not, it's only reliant on the class of the dog? So is it really just a neural network trained under a single class then??  


Am i overthinking this?",t2_6cw729z7,False,,0,False,A question about the implementation of object discrimination,[],r/learnmachinelearning,False,6,,0,,False,t3_gfwk0z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588984862.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been wanting to try making a model that would be able to tell if an object is of a specific class or not (for ex. Dog and non-dogs). When, it came to thinking of implementation, I wasn&amp;#39;t particularly sure what would be the right way of going about this problem.  &lt;/p&gt;

&lt;p&gt;If I were to think of it as a multi-class classification, the number of classes would be limited to the number of classes under non-dog objects. On the other hand if I&amp;#39;ll limit it to one class, when I think of compiling everything into a single class for non-dog objects, the objects in question are to varied to be classified as such.  &lt;/p&gt;

&lt;p&gt;Since essentially we&amp;#39;re checking the likelihood if it&amp;#39;s a Dog or not, it&amp;#39;s only reliant on the class of the dog? So is it really just a neural network trained under a single class then??  &lt;/p&gt;

&lt;p&gt;Am i overthinking this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfwk0z,True,,iloveuandre3000,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfwk0z/a_question_about_the_implementation_of_object/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfwk0z/a_question_about_the_implementation_of_object/,155203,1588956062.0,0,,False,,,,
,learnmachinelearning,"Hi, everyone I have been trying to train a custom a custom object detector. So far the in the official documentation I have found how to use preexisting models to do object detection and online there is a convoluted way of doing it with the object detection API but there is no depth in the tutorials concerning the API  it and it doesn't as fa as I know work with tf2. Where can I find a good resource for object detection?",t2_4q4a2isq,False,,0,False,The documentation on Object Detection with TensorFlow is Confusing.,[],r/learnmachinelearning,False,6,,0,,False,t3_gfw7l7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588983740.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone I have been trying to train a custom a custom object detector. So far the in the official documentation I have found how to use preexisting models to do object detection and online there is a convoluted way of doing it with the object detection API but there is no depth in the tutorials concerning the API  it and it doesn&amp;#39;t as fa as I know work with tf2. Where can I find a good resource for object detection?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfw7l7,True,,hassankamran689,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfw7l7/the_documentation_on_object_detection_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfw7l7/the_documentation_on_object_detection_with/,155203,1588954940.0,0,,False,,,,
,learnmachinelearning,"Hi!

My project basically involves predicting the relative ratios of Reactions (Love 0.3, Haha 0.4 etc. summing to 1) for a Facebook post.

I'm confused as to what loss function to use - Categorical Cross Entropy or MSE loss. I originally trained all my models using MSE, but am wondering if Categorical Cross Entropy would be more appropriate, as it seems to deal directly with probability distributions. However in most articles online, it is only used for classification, and where labels are encoded as one-hot vectors (as opposed to a distribution). I'm therefore concerned as to what the implications of using Categorical Cross Entropy over MSE are, and whether there is a 'right' choice.

Hope what I'm asking is clear - thanks!",t2_6d2jn3fg,False,,0,False,Predicting Facebook Reaction Ratios - MSE or Categorical Cross Entropy,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfsdsk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588970058.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;My project basically involves predicting the relative ratios of Reactions (Love 0.3, Haha 0.4 etc. summing to 1) for a Facebook post.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m confused as to what loss function to use - Categorical Cross Entropy or MSE loss. I originally trained all my models using MSE, but am wondering if Categorical Cross Entropy would be more appropriate, as it seems to deal directly with probability distributions. However in most articles online, it is only used for classification, and where labels are encoded as one-hot vectors (as opposed to a distribution). I&amp;#39;m therefore concerned as to what the implications of using Categorical Cross Entropy over MSE are, and whether there is a &amp;#39;right&amp;#39; choice.&lt;/p&gt;

&lt;p&gt;Hope what I&amp;#39;m asking is clear - thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfsdsk,True,,sbh116,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfsdsk/predicting_facebook_reaction_ratios_mse_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfsdsk/predicting_facebook_reaction_ratios_mse_or/,155203,1588941258.0,0,,False,,,,
,learnmachinelearning,"We were learning recommender systems in class this week, and we got into distance measures like cosine similarity.  It reminded me a lot like our lectures on KNN where it uses Euclidean distance to find its neighbors. So are recommender systems just another form of KNN? From what we were taught, the whole concept of KNN is just a wrapper for different algorithms like KDtree and Balltree.",t2_5xavqe1z,False,,0,False,Difference between KNN and recommender systems?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfv5ka,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588980273.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We were learning recommender systems in class this week, and we got into distance measures like cosine similarity.  It reminded me a lot like our lectures on KNN where it uses Euclidean distance to find its neighbors. So are recommender systems just another form of KNN? From what we were taught, the whole concept of KNN is just a wrapper for different algorithms like KDtree and Balltree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfv5ka,True,,phi_beta_kappa,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfv5ka/difference_between_knn_and_recommender_systems/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfv5ka/difference_between_knn_and_recommender_systems/,155203,1588951473.0,0,,False,,,,
,learnmachinelearning," Hi,

I had gotten the yearly membership but I am not using it anymore. Let me know if anyone wants it.

Thanks",t2_10vapm,False,,0,False,Selling my Dataquest account valid till Jan 26th. $130,[],r/learnmachinelearning,False,6,,0,,False,t3_gg4q7j,False,dark,0.13,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589011149.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I had gotten the yearly membership but I am not using it anymore. Let me know if anyone wants it.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg4q7j,True,,stupidarg,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg4q7j/selling_my_dataquest_account_valid_till_jan_26th/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg4q7j/selling_my_dataquest_account_valid_till_jan_26th/,155203,1588982349.0,0,,False,,,,
,learnmachinelearning,,t2_42dg2rb1,False,,0,False,COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfuoh3,False,dark,0.33,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases | DS Meetup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Oc4NB1qpPZE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfuoh3', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/LjdfqmcozyAh4jTjvBP3Z3klGC74i5UAtfO2jNVnubw.jpg,False,,[],{},rich:video,,False,,1588978652.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?auto=webp&amp;s=6b63e72ee7efc07aa876eeda3846e62d47e26d2f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e216bd855eb0e80860fe1153b68cd557deb67ae', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e04f5b822bd9e3b28a4a61ffa201e2562fdda33', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413c1f5302f343054a35238209dcc3f319dbcac4', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'EMtppusbcMDi1FLQh6piI8f_JAU-PUB6KD_D9bnd1rs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfuoh3,True,,reddit_data_guy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfuoh3/covid19_ultrasound_detection_impact_of/,all_ads,False,https://youtu.be/Oc4NB1qpPZE?t=1826,155203,1588949852.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases | DS Meetup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Oc4NB1qpPZE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}}",False,,,,
,learnmachinelearning,"You can google similar images by uploading an image, I want to replicate this technology for my own project. Maybe somebody knows where to find a course or an article on how to do that?  
Thanks in advance.",t2_1bodl8op,False,,0,False,ASK: Maybe anyone knows how to implement a search by an image algorithm?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfuaik,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588977160.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;You can google similar images by uploading an image, I want to replicate this technology for my own project. Maybe somebody knows where to find a course or an article on how to do that?&lt;br/&gt;
Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfuaik,True,,lekorotkov,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfuaik/ask_maybe_anyone_knows_how_to_implement_a_search/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfuaik/ask_maybe_anyone_knows_how_to_implement_a_search/,155203,1588948360.0,0,,False,,,,
,learnmachinelearning,,t2_5jtt9adc,False,,0,False,How to handle clients which can be pain sometimes,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfqjwq,False,light,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,default,False,,[],{},,,False,,1588961710.0,richtext,6,,,text,link.medium.com,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfqjwq,True,,kaputasf,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqjwq/how_to_handle_clients_which_can_be_pain_sometimes/,all_ads,False,https://link.medium.com/3BNLkotLj6,155203,1588932910.0,0,,False,,,,
,learnmachinelearning,"I dont like following video courses much and I also get bored reading long books. Any short book/online resource which is fairly practical, but also involves  coneptual understanding and some mathematics!

I am a beginner!",t2_5n3pjk3w,False,,0,False,Resource suggestions for beginner,[],r/learnmachinelearning,False,6,,0,,False,t3_gfwt5x,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588985680.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I dont like following video courses much and I also get bored reading long books. Any short book/online resource which is fairly practical, but also involves  coneptual understanding and some mathematics!&lt;/p&gt;

&lt;p&gt;I am a beginner!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfwt5x,True,,utm99,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfwt5x/resource_suggestions_for_beginner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfwt5x/resource_suggestions_for_beginner/,155203,1588956880.0,0,,False,,,,
,learnmachinelearning,"A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)
 
Haven't taken the GRE yet. 
GPA 9.36/10

So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)",t2_4ih25pre,False,,0,False,How do I go about learning TensorFlow?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfo7cu,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588950049.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)&lt;/p&gt;

&lt;p&gt;Haven&amp;#39;t taken the GRE yet. 
GPA 9.36/10&lt;/p&gt;

&lt;p&gt;So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfo7cu,True,,SuccMyStrangerThings,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo7cu/how_do_i_go_about_learning_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo7cu/how_do_i_go_about_learning_tensorflow/,155203,1588921249.0,0,,False,,,,
,learnmachinelearning,,t2_jqdv3,False,,0,False,What are Concept Drifts in Time Series Data?,[],r/learnmachinelearning,False,6,,0,61.0,False,t3_gfs9bk,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Wr1SW3lqLCHepDiIGnXTq7b45RrfRFr7npnkJ47zTUs.jpg,False,,[],{},link,,False,,1588969546.0,text,6,,,text,iunera.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?auto=webp&amp;s=aed92538a0b3812d688044e55aea1b1c589ea66d', 'width': 1280, 'height': 558}, 'resolutions': [{'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b9b35c2b419189706c5ea2b8ad3c163aeed1e5d', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2566f9f254dd3139bb9e60d2e32fffc8694dc427', 'width': 216, 'height': 94}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78792c89d249fe5c3122e10fcfcb5f38d6ab6cba', 'width': 320, 'height': 139}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a99c62beb811f107e7cfcb9f37f5b70ee6753a36', 'width': 640, 'height': 279}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc8386edace9513aeaa3e409a4220811217e22a3', 'width': 960, 'height': 418}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09db4dcc8bd6f32a1c67f880ffcadd6c8b5ea6a0', 'width': 1080, 'height': 470}], 'variants': {}, 'id': '9IikYfauu55Db_94Pp22GuabwDNjWai_xXq3St9JLb0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfs9bk,True,,Timbo2020,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfs9bk/what_are_concept_drifts_in_time_series_data/,all_ads,False,https://www.iunera.com/kraken/fabric/concept-drifts/,155203,1588940746.0,0,,False,,,,
,learnmachinelearning,,t2_ftjjtyo,False,,0,False,Anybody knows how to solve this?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfs23x,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,default,False,,[],{},,,False,,1588968689.0,text,6,,,text,self.ArtificialInteligence,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfs23x,True,,AdaptiveNarc,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfs23x/anybody_knows_how_to_solve_this/,all_ads,False,/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/,155203,1588939889.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ArtificialInteligence', 'selftext': 'I have this exercise:\n\nYou have a training set composed of grades from 6 classes for a population of 100 students from 20 countries. Can you use a K-nearest neighbor classifier to determine most likely nationality for an unknown student from their grades?  If it IS possible give the formula for P( X(vector) |Ï‰k ) . If it is not possible, explain why.', 'author_fullname': 't2_6et4j6li', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'K-nearest neighbor', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ArtificialInteligence', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gfcimg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588905955.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ArtificialInteligence', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have this exercise:&lt;/p&gt;\n\n&lt;p&gt;You have a training set composed of grades from 6 classes for a population of 100 students from 20 countries. Can you use a K-nearest neighbor classifier to determine most likely nationality for an unknown student from their grades?  If it IS possible give the formula for P( X(vector) |Ï‰k ) . If it is not possible, explain why.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3crzr', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfcimg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'tasian123', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/', 'subreddit_subscribers': 34102, 'created_utc': 1588877155.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_gfcimg,,
,learnmachinelearning,"Hi everyone!

Currently, I'm working on my first Convolutional Neural Network for a project in university. I have to create a model that can recognize if a cable has a defect by only using images of ""good"" and ""defect"" cables.

Right now, I'm struggling with the quantity of my data:

I have 224 images of ""good"" cables in my ""train""-folder.

My ""test""-folder consists of 58 ""good""-cable images and 92 ""defect""-cable images. So my test-data is unbalanced. Our professor advised us to transfer 34 of the train images into the ""good""-cable class of the test folder so that we'll have a ratio of 92/92.

I wanted to load the data with the ""flow\_from\_directory""-function of keras. I already know how to only load the images from the ""good""-cable class of the test-folder. But now I'm drawing a blank about how to transfer the 34 pictures from the training folder into the test folder so that the folder has a balanced distribution of data.  I need to do it via code but can't come up with an idea. Can somebody maybe help me or give me a hint?

I would appreciate every input. :)",t2_1dzrzji,False,,0,False,[CNN] Loading and separating images with Keras and/or Tensorflow,[],r/learnmachinelearning,False,6,,0,,False,t3_gfrtkb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588967655.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;Currently, I&amp;#39;m working on my first Convolutional Neural Network for a project in university. I have to create a model that can recognize if a cable has a defect by only using images of &amp;quot;good&amp;quot; and &amp;quot;defect&amp;quot; cables.&lt;/p&gt;

&lt;p&gt;Right now, I&amp;#39;m struggling with the quantity of my data:&lt;/p&gt;

&lt;p&gt;I have 224 images of &amp;quot;good&amp;quot; cables in my &amp;quot;train&amp;quot;-folder.&lt;/p&gt;

&lt;p&gt;My &amp;quot;test&amp;quot;-folder consists of 58 &amp;quot;good&amp;quot;-cable images and 92 &amp;quot;defect&amp;quot;-cable images. So my test-data is unbalanced. Our professor advised us to transfer 34 of the train images into the &amp;quot;good&amp;quot;-cable class of the test folder so that we&amp;#39;ll have a ratio of 92/92.&lt;/p&gt;

&lt;p&gt;I wanted to load the data with the &amp;quot;flow_from_directory&amp;quot;-function of keras. I already know how to only load the images from the &amp;quot;good&amp;quot;-cable class of the test-folder. But now I&amp;#39;m drawing a blank about how to transfer the 34 pictures from the training folder into the test folder so that the folder has a balanced distribution of data.  I need to do it via code but can&amp;#39;t come up with an idea. Can somebody maybe help me or give me a hint?&lt;/p&gt;

&lt;p&gt;I would appreciate every input. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfrtkb,True,,Ruffybeo,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrtkb/cnn_loading_and_separating_images_with_keras/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfrtkb/cnn_loading_and_separating_images_with_keras/,155203,1588938855.0,0,,False,,,,
,learnmachinelearning,"I'm trying to create a network that sorts through data and classifies lists as either a sine or cosine. I've made the data and added noise to the sines and cosines and trained the network. I used an MNIST tutorial as a base and I was expecting my network to give an integer output based on whether the signal is a sine (0) or cosine (1) but instead I get a float.

    i = Input(shape=(100,))
    x = Dense(128, activation='relu')(i)
    x = Dropout(0.2)(x)
    x = Dense(1)(x)
    
    model = Model(i, x)
    model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])
    r = model.fit(X, Y, epochs=50)

This is my model. Can anyone tell me if a part of this is the problem? I've tried changing the loss to 'sparse\_categorical\_crossentropy' but this gives the following error

    InvalidArgumentError:  Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0

This goes away if I change the last Dense layer to Dense(2) but I still don't get an integer.",t2_4di36zz,False,,0,False,Create classification network with integer output,[],r/learnmachinelearning,False,6,,0,,False,t3_gfrgms,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588966018.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to create a network that sorts through data and classifies lists as either a sine or cosine. I&amp;#39;ve made the data and added noise to the sines and cosines and trained the network. I used an MNIST tutorial as a base and I was expecting my network to give an integer output based on whether the signal is a sine (0) or cosine (1) but instead I get a float.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;i = Input(shape=(100,))
x = Dense(128, activation=&amp;#39;relu&amp;#39;)(i)
x = Dropout(0.2)(x)
x = Dense(1)(x)

model = Model(i, x)
model.compile(loss=&amp;#39;mse&amp;#39;, optimizer=&amp;#39;Adam&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])
r = model.fit(X, Y, epochs=50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is my model. Can anyone tell me if a part of this is the problem? I&amp;#39;ve tried changing the loss to &amp;#39;sparse_categorical_crossentropy&amp;#39; but this gives the following error&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InvalidArgumentError:  Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This goes away if I change the last Dense layer to Dense(2) but I still don&amp;#39;t get an integer.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfrgms,True,,Competitive_Mongoose,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrgms/create_classification_network_with_integer_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfrgms/create_classification_network_with_integer_output/,155203,1588937218.0,0,,False,,,,
,learnmachinelearning,Is writing a function correct way to do ?. Or is there any alternative way to do that,t2_59797uro,False,,0,False,I have built a model after a lot of data cleaning. Now how do I do the same data cleaning steps for unseen data?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfmfc6,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588941363.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is writing a function correct way to do ?. Or is there any alternative way to do that&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfmfc6,True,,FoolishlyPainful,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfmfc6/i_have_built_a_model_after_a_lot_of_data_cleaning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfmfc6/i_have_built_a_model_after_a_lot_of_data_cleaning/,155203,1588912563.0,0,,False,,,,
,learnmachinelearning,"I'm currently an academic at a UK university. I primarily teach web and mobile development to undergraduate students. I'd like to pursue learning ML. However, I'm find it particularly overwhelming. I have a good understanding of Python and OOP, advanced knowledge of SQL and I'm  pretty good with AWS and GCP. So I think as far programming goes I'm okay. It's learning the ML theory that I'm finding overwhelming.

Any tips? I'd appreciate hearing how others have managed to overcome a similar situation.

Thanks in adv!",t2_6ewfalng,False,,0,False,Any advice.. beginning to feel somewhat overwhelmed,[],r/learnmachinelearning,False,6,,0,,False,t3_gfj3km,False,dark,0.8,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},,,True,,1588927685.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently an academic at a UK university. I primarily teach web and mobile development to undergraduate students. I&amp;#39;d like to pursue learning ML. However, I&amp;#39;m find it particularly overwhelming. I have a good understanding of Python and OOP, advanced knowledge of SQL and I&amp;#39;m  pretty good with AWS and GCP. So I think as far programming goes I&amp;#39;m okay. It&amp;#39;s learning the ML theory that I&amp;#39;m finding overwhelming.&lt;/p&gt;

&lt;p&gt;Any tips? I&amp;#39;d appreciate hearing how others have managed to overcome a similar situation.&lt;/p&gt;

&lt;p&gt;Thanks in adv!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfj3km,True,,datadroiduk,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfj3km/any_advice_beginning_to_feel_somewhat_overwhelmed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfj3km/any_advice_beginning_to_feel_somewhat_overwhelmed/,155203,1588898885.0,0,,False,,,,
,learnmachinelearning,,t2_1k4nqnsa,False,,0,False,"[GitHub] a simple text autoencoder in Jupyter Notebook. As simple as it can be, so it is good for beginners.","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_gfnyvw,False,light,0.63,,public,2,0,{},140.0,,False,[],,False,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/0dSqFVTaNBElSulyRQ7ULSdZdWViRmONWnwTl37fyJk.jpg,False,,[],{},link,,False,,1588948849.0,richtext,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SpSOngL8ZZ2R4phXD-49_WuN8ccjeYgcII0LzrKbtoM.jpg?auto=webp&amp;s=96a67decf19f6bea2685973d7e241fb32a10373f', 'width': 215, 'height': 215}, 'resolutions': [{'url': 'https://external-preview.redd.it/SpSOngL8ZZ2R4phXD-49_WuN8ccjeYgcII0LzrKbtoM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b88cb1857a765fdec41923ee862b0c11fbcd26b3', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'zGIrxBhitFmy_opFh2espjqeKyetnqWE6n_OyIYfLWY'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gfnyvw,True,,kiasari,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfnyvw/github_a_simple_text_autoencoder_in_jupyter/,all_ads,False,https://github.com/kiasar/text_autoencoder/blob/master/Simple_Autoencoder_for_text.ipynb,155203,1588920049.0,0,,False,,,,
,learnmachinelearning,"Hey guys, I'm new to machine learning and have completed a udemy course talking about the basics and stuff. It would be great if someone suggested more material or sources that I can keep learning from. And any more suggestions that I need to be aware of.",t2_28m0i5zb,False,,0,False,Need help in learning ML,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfqji5,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588961653.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I&amp;#39;m new to machine learning and have completed a udemy course talking about the basics and stuff. It would be great if someone suggested more material or sources that I can keep learning from. And any more suggestions that I need to be aware of.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfqji5,True,,_skullcrusher1_,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqji5/need_help_in_learning_ml/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfqji5/need_help_in_learning_ml/,155203,1588932853.0,0,,False,,,,
,learnmachinelearning," 

Hi.

I am exploring Sutton and Barto's textbook on reinforcement learning. I think I need to learn some more of the underlying maths first.

I have a high-school level understanding of calculus, probability and statistics. I have taken a college course on linear algebra.

Real analysis, \_serious\_ probability and statistics are a weak spot. On the bright side, I am more interested in (as of now) implementing the key algorithms and techniques, more than proofs (though I will definitely revisit RL in a more rigorous way at a later time).

What all mathematics should I learn, and how (textbooks, courses etc)?

Thanks,

Pakodanomics",t2_zpl4l3o,False,,0,False,Maths behind Sutton and Barto's textbook?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfqf7h,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588961078.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;I am exploring Sutton and Barto&amp;#39;s textbook on reinforcement learning. I think I need to learn some more of the underlying maths first.&lt;/p&gt;

&lt;p&gt;I have a high-school level understanding of calculus, probability and statistics. I have taken a college course on linear algebra.&lt;/p&gt;

&lt;p&gt;Real analysis, _serious_ probability and statistics are a weak spot. On the bright side, I am more interested in (as of now) implementing the key algorithms and techniques, more than proofs (though I will definitely revisit RL in a more rigorous way at a later time).&lt;/p&gt;

&lt;p&gt;What all mathematics should I learn, and how (textbooks, courses etc)?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Pakodanomics&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfqf7h,True,,pakodanomics,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqf7h/maths_behind_sutton_and_bartos_textbook/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfqf7h/maths_behind_sutton_and_bartos_textbook/,155203,1588932278.0,0,,False,,,,
,learnmachinelearning,,t2_6bo5rgdf,False,,0,False,Intro to machine learning and data science with Python using the Iris dataset. 7 video course on YouTube.,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfn0lo,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Intro to Machine Learning with Python 1: Welcome and Project Setup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Project Data Science', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rdaG53khzv0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCM7_uh02Xqv4PFKbyIasP4g'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfn0lo', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/V190C7orjwFldbSPPS4S-Ga7ldr3wHhsdC3nG0H9TiA.jpg,False,,[],{},rich:video,,False,,1588944092.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?auto=webp&amp;s=cdc7f84812a2900a8da97ba4c38ffa52b2711e55', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=867b27a273dacb6c04953f7933b69ad11bb264ce', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e01515d7b43ae45a570b24c02f086fd8a7582c81', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33bd815a00a2703543c19824dd53d671d889a7f2', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'GQTyjtkHObDFTs_Juf_KzBRV40TJJVE7QJECPt_XhEQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn0lo,True,,projectdatascience,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn0lo/intro_to_machine_learning_and_data_science_with/,all_ads,False,https://www.youtube.com/watch?v=rdaG53khzv0&amp;list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr,155203,1588915292.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Intro to Machine Learning with Python 1: Welcome and Project Setup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Project Data Science', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rdaG53khzv0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCM7_uh02Xqv4PFKbyIasP4g'}}",False,,,,
,learnmachinelearning,"I'm trying to use some python 3D visualization tools to visualize 3D meshes, but non of the python libraries support ray-tracing.

However, all these cool videos that you can see online have really nice renderings with ray-tracing and everything with animation.

What kind of tools to graphics researchers use? and is there a good tutorial on learning how to make one?",t2_6ag000ku,False,,0,False,How do Siggraph authors make such a cool visualization and what tools do they use?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfppfp,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588957569.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to use some python 3D visualization tools to visualize 3D meshes, but non of the python libraries support ray-tracing.&lt;/p&gt;

&lt;p&gt;However, all these cool videos that you can see online have really nice renderings with ray-tracing and everything with animation.&lt;/p&gt;

&lt;p&gt;What kind of tools to graphics researchers use? and is there a good tutorial on learning how to make one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfppfp,True,,MiniMongMari,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfppfp/how_do_siggraph_authors_make_such_a_cool/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfppfp/how_do_siggraph_authors_make_such_a_cool/,155203,1588928769.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#TimeManagement tips: How to get organised by Rosemary Miller - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfpnna,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1588957330.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?auto=webp&amp;s=26b0feeed9db05e72415f41913a12bd8507b380a', 'width': 900, 'height': 473}, 'resolutions': [{'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bdb51f655c42d9bead3fe93ac306fdb2d72c770', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8be55f1ce4ded6d3dd9871571b3f1ce4e0dde0f9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2e5e47ae0ed547c115e33adaea67bc210d55a56', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e1b8e64f48e7304c6e0c0e10529dbc66ddb697d', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'vo8-QmQle6mXAT3LtitgtE8QPRljYtZGXBPMwS4RLnA'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfpnna,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfpnna/timemanagement_tips_how_to_get_organised_by/,all_ads,False,/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/,155203,1588928530.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University RLO Study Skills - Time Management, Publicly Accessed 07. May 2020,  [http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo](http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo) \n\n *Juggling readings, assignments and revision can be one of the most challenging parts of the university. Hereâ€™s how to get organised and make the most out of your time! By Clinton Bell*  \n\n\nYou probably already know procrastination is a bad idea. If you put off doing assignments or donâ€™t revise regularly, itâ€™s easy to fall behind and end up with way too much stuff to catch up on. Unfortunately, even if you know you should study, it can be difficult to make yourself do it - especially if youâ€™re busy with other things.  \n\n\nIf you find yourself struggling to make time for study, or you feel like you just have way too much going on, try planning your time with a study schedule! Thereâ€™s [an example of how to make one](http://www.monash.edu/rlo/study-skills/studying-effectively/managing-your-time) on the library website.  \n\n\nMaking a schedule has several benefits:\n\n* It helps you work out how much time you have, and plan your study around your work, social life, and other commitments\n* Itâ€™s easier to keep track of tasks and due dates if you have them all written down in one place\n* Youâ€™re less likely to procrastinate if the study is a regular part of your routine. Scheduling study in advance can also make you feel more committed to actually doing it\n* Having a plan can help you feel less stressed and more in control of your study.\n\nWhen making your schedule itâ€™s important to prioritise. Consider how important things are as well as when theyâ€™re due - if an assignment is worth a lot of marks youâ€™ll probably need to spend more time on it. If you need to do something which requires other people, special facilities or equipment, you may also need to work around when those things are available.  \n\n\nFor large assignments, it can be helpful to split the task into smaller goals. For example, you might aim to write one paragraph of an essay each night. Splitting the task into chunks can make it less intimidating to get started, and can also help you stress less - if youâ€™re meeting your goals you know youâ€™re on track to get the assignment done.  \n\n\nAs well as planning your time, itâ€™s important to use it effectively. Using good study methods and improving your skills can give you better results in less time:  \n\n\n* [Listen carefully in lectures, and take good notes](https://www.monash.edu/rlo/quick-study-guides?a=388983). This will reduce the time you need to spend revising later\n* [Read assignment instructions carefully](http://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/analysing-the-task-requirements), and if youâ€™re not clear on something ask your lecturer or tutor. If you donâ€™t understand the task, you can waste time and lose marks by doing the wrong thing\n* [Learn strategies that help you read more efficiently](http://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies).\n* Keep track of what youâ€™ve read. Youâ€™ll need [to reference your sources later](http://www.monash.edu/rlo/research-writing-assignments/referencing-and-academic-integrity), and if you didnâ€™t make a note of the information you need for referencing, youâ€™ll have to go back and find it\n* Get advice from the library. We can help you improve your study skills, and learn how to find the resources you need for your assignments more quickly. If you canâ€™t see us in person, [we also have a lot of helpful information online](http://www.monash.edu/rlo)!\n* [Get help from English Connect](http://www.monash.edu/english-connect) if you have difficulty with English language skills.\n* Get some sleep! You donâ€™t work or learn as efficiently if you donâ€™t get enough sleep, so staying up too late to study can be counterproductive.\n\nTime management can be challenging, but with good planning and study skills, you can get everything done on time. So best of luck with your study this semester - and remember, [come see us](http://www.monash.edu/library/skills/resources/programs/drop-in)\xa0at a drop-in session\xa0if you need help!', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#TimeManagement tips: How to get organised by Rosemary Miller - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf0oal', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.77, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588858591.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University RLO Study Skills - Time Management, Publicly Accessed 07. May 2020,  &lt;a href=""http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo""&gt;http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Juggling readings, assignments and revision can be one of the most challenging parts of the university. Hereâ€™s how to get organised and make the most out of your time! By Clinton Bell&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;You probably already know procrastination is a bad idea. If you put off doing assignments or donâ€™t revise regularly, itâ€™s easy to fall behind and end up with way too much stuff to catch up on. Unfortunately, even if you know you should study, it can be difficult to make yourself do it - especially if youâ€™re busy with other things.  &lt;/p&gt;\n\n&lt;p&gt;If you find yourself struggling to make time for study, or you feel like you just have way too much going on, try planning your time with a study schedule! Thereâ€™s &lt;a href=""http://www.monash.edu/rlo/study-skills/studying-effectively/managing-your-time""&gt;an example of how to make one&lt;/a&gt; on the library website.  &lt;/p&gt;\n\n&lt;p&gt;Making a schedule has several benefits:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It helps you work out how much time you have, and plan your study around your work, social life, and other commitments&lt;/li&gt;\n&lt;li&gt;Itâ€™s easier to keep track of tasks and due dates if you have them all written down in one place&lt;/li&gt;\n&lt;li&gt;Youâ€™re less likely to procrastinate if the study is a regular part of your routine. Scheduling study in advance can also make you feel more committed to actually doing it&lt;/li&gt;\n&lt;li&gt;Having a plan can help you feel less stressed and more in control of your study.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When making your schedule itâ€™s important to prioritise. Consider how important things are as well as when theyâ€™re due - if an assignment is worth a lot of marks youâ€™ll probably need to spend more time on it. If you need to do something which requires other people, special facilities or equipment, you may also need to work around when those things are available.  &lt;/p&gt;\n\n&lt;p&gt;For large assignments, it can be helpful to split the task into smaller goals. For example, you might aim to write one paragraph of an essay each night. Splitting the task into chunks can make it less intimidating to get started, and can also help you stress less - if youâ€™re meeting your goals you know youâ€™re on track to get the assignment done.  &lt;/p&gt;\n\n&lt;p&gt;As well as planning your time, itâ€™s important to use it effectively. Using good study methods and improving your skills can give you better results in less time:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=""https://www.monash.edu/rlo/quick-study-guides?a=388983""&gt;Listen carefully in lectures, and take good notes&lt;/a&gt;. This will reduce the time you need to spend revising later&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/analysing-the-task-requirements""&gt;Read assignment instructions carefully&lt;/a&gt;, and if youâ€™re not clear on something ask your lecturer or tutor. If you donâ€™t understand the task, you can waste time and lose marks by doing the wrong thing&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies""&gt;Learn strategies that help you read more efficiently&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;Keep track of what youâ€™ve read. Youâ€™ll need &lt;a href=""http://www.monash.edu/rlo/research-writing-assignments/referencing-and-academic-integrity""&gt;to reference your sources later&lt;/a&gt;, and if you didnâ€™t make a note of the information you need for referencing, youâ€™ll have to go back and find it&lt;/li&gt;\n&lt;li&gt;Get advice from the library. We can help you improve your study skills, and learn how to find the resources you need for your assignments more quickly. If you canâ€™t see us in person, &lt;a href=""http://www.monash.edu/rlo""&gt;we also have a lot of helpful information online&lt;/a&gt;!&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/english-connect""&gt;Get help from English Connect&lt;/a&gt; if you have difficulty with English language skills.&lt;/li&gt;\n&lt;li&gt;Get some sleep! You donâ€™t work or learn as efficiently if you donâ€™t get enough sleep, so staying up too late to study can be counterproductive.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Time management can be challenging, but with good planning and study skills, you can get everything done on time. So best of luck with your study this semester - and remember, &lt;a href=""http://www.monash.edu/library/skills/resources/programs/drop-in""&gt;come see us&lt;/a&gt;\xa0at a drop-in session\xa0if you need help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?auto=webp&amp;s=26b0feeed9db05e72415f41913a12bd8507b380a', 'width': 900, 'height': 473}, 'resolutions': [{'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bdb51f655c42d9bead3fe93ac306fdb2d72c770', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8be55f1ce4ded6d3dd9871571b3f1ce4e0dde0f9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2e5e47ae0ed547c115e33adaea67bc210d55a56', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e1b8e64f48e7304c6e0c0e10529dbc66ddb697d', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'vo8-QmQle6mXAT3LtitgtE8QPRljYtZGXBPMwS4RLnA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf0oal', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/', 'subreddit_subscribers': 33, 'created_utc': 1588829791.0, 'num_crossposts': 28, 'media': None, 'is_video': False}]",t3_gf0oal,,
,learnmachinelearning,"I am going through some of the tensorflow tutorials here: 

[https://www.tensorflow.org/tutorials/images/cnn](https://www.tensorflow.org/tutorials/images/cnn) 

&amp;#x200B;

After the dataset is downloaded, the dataset is then divided by 255 so that we may normalize the values to be between 0 and 1

As seen here:  [https://i.imgur.com/nwbqDcT.png](https://i.imgur.com/nwbqDcT.png) 

This step does not seem to be necessary, as we can still train and validate the model without doing this part. But it does improve the accuracy of the model. Can anyone explain as to why this is?",t2_o9tnr,False,,0,False,Going through Tensorflow tutorials. How and why does normalizing data help this model?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfkmfd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588933625.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am going through some of the tensorflow tutorials here: &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.tensorflow.org/tutorials/images/cnn""&gt;https://www.tensorflow.org/tutorials/images/cnn&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;After the dataset is downloaded, the dataset is then divided by 255 so that we may normalize the values to be between 0 and 1&lt;/p&gt;

&lt;p&gt;As seen here:  &lt;a href=""https://i.imgur.com/nwbqDcT.png""&gt;https://i.imgur.com/nwbqDcT.png&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;This step does not seem to be necessary, as we can still train and validate the model without doing this part. But it does improve the accuracy of the model. Can anyone explain as to why this is?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?auto=webp&amp;s=a597266c297a2130a3826c64bb3418582ff46ad2', 'width': 769, 'height': 119}, 'resolutions': [{'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f5407c8f959e5304ae46b92e96c1902294a9785', 'width': 108, 'height': 16}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3d847552013d0c7aaa9c8d5c9ff85d6b663c0b8', 'width': 216, 'height': 33}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=34c120ef08662aed1c9a0632ae0b1fca4e274720', 'width': 320, 'height': 49}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b8c4c295444eba260414e78fcead30689943f61', 'width': 640, 'height': 99}], 'variants': {}, 'id': 't3zr2gBQMCJosgxrDFP6LhG4I2ZsodxSNnRgtP1iUQ4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfkmfd,True,,Moo3247,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkmfd/going_through_tensorflow_tutorials_how_and_why/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfkmfd/going_through_tensorflow_tutorials_how_and_why/,155203,1588904825.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #17 | Rubik's Code,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfot5l,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/q40TYZVgvDuVIhiCMY2LGrrwPV64Jh7cItGv36aVymY.jpg,False,,[],{},link,,False,,1588953119.0,text,6,,,text,rubikscode.net,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?auto=webp&amp;s=aba15a8e08da24b0f366ff7e94f7f49c652884e3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19f794df176d742445e70a834f02073618a034a8', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbc2caf7263a43928c2f96facf6709c61c560994', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba4603038973138eb62c7d4c3e483b55662d27a9', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5118555c9f824b549676e01773b5ef1341a3b0c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bdb1652f7804251c04d9290f584d7a00cd93d38', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=238aef893d35150be90c75468911d2d706a4e494', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'A1f3svXfhPHnYTvqZdu1PaAFyL18pTPA-Iq6T1_JU48'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfot5l,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfot5l/this_week_in_ai_issue_17_rubiks_code/,all_ads,False,https://rubikscode.net/2020/05/08/this-week-in-ai-issue-17/,155203,1588924319.0,0,,False,,,,
,learnmachinelearning,"Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.",t2_6l4z3,False,,0,False,Weekly Show-off!,[],r/learnmachinelearning,False,6,,0,,False,t3_gfo90z,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,True,self,False,,[],{},,,True,,1588950286.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gfo90z,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo90z/weekly_showoff/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo90z/weekly_showoff/,155203,1588921486.0,0,,False,,,,
,learnmachinelearning,"Hi everyone, my nameâ€™s Chris! Iâ€™m a research software engineer with a focus on applying machine learning techniques to a variety of new inventions. Iâ€™m also the co-founder and current co-lead of my companyâ€™s machine learning and artificial intelligence (ML/AI) community of practice.

In my free time, I apply ML to games in many different ways. My projects so far mainly have to do with Old School Runescape (examples below but thatâ€™s not what Iâ€™m here for today). Iâ€™m here today because a few people have reached out to me recently about starting their own projects applying ML to various games and I think thatâ€™s a great idea!! After all, the best way to learn is through implementation and why not implement something you care about? Not to mention, personal projects look great on any resume!

Throughout my journey, Iâ€™ve found that Iâ€™m happiest when helping people out by thinking through their process with them and figuring out what comes next. I would really like to share my experience and expertise gained with anyone who is interested in making their own projects and learning much more about ML and its applications in the process. I also have a bunch of ideas for ML-game projects that Iâ€™ll never have the time to implement myself which someone else might find invigorating.

That said, Iâ€™m thinking of starting what Iâ€™ll call the Chronic Coder Academy. Iâ€™ll start with a few students who I personally call on a weekly basis for about 30 minutes to an hour to discuss their new or ongoing ML projects - weekly todos, potential resources and next steps. I personally think itâ€™d help keep people accountable on their personal journey learning ML and also keep them motivated if I direct them well instead of being lost in the rough sea of ML/AI.

This is targeted at people who care about games and also want to learn more about ML through application. Your age, race, gender, ethnicity, etc mean nothing to me but you have to be passionate about what you do and hopefully converse well enough in English. Everything you make is completely your own, no matter how much advice or guidance I end up giving. Oh and also, itâ€™s completely free (before anyone asks). Iâ€™m really in this to better expose myself to different applications and ideas plus I truly enjoy watching people learn especially from interesting use cases like these. If they will allow me, I might use their projects as examples for teaching people more about specific ML concepts in the future. All that said, please fill out this survey if youâ€™re interested: [https://forms.gle/W37tzNFsTCFXGEwMA](https://forms.gle/W37tzNFsTCFXGEwMA)

If this isnâ€™t for you at this point in your life, feel free to join our discord! This applies regardless of who you are, what you do or what your experience level is. Itâ€™s an amazing community of ML experts and curious students alike who also happen to share a love for games. We could always use more experts to help anyone in need but also itâ€™s a place to learn if youâ€™re doing your own applied ML project and arenâ€™t sure who to go to. Join us here: [https://discord.gg/ZummSXK](https://discord.gg/ZummSXK)

Oh and if youâ€™re looking for a job sometime soon and worried about a project involving games not being a series enough topic to get hired, I previously did some asking around: [https://docs.google.com/document/d/1tU2GxQ3SZJVjV8\_A\_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing](https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing). Looks like most professionals agree that ML applied to games can be as serious a topic as any other within the ML field.

Whew that was a long piece... Thank you for reading if you've made it this far!

or **TL;DR** \- I want to help people learn ML by applying it to games. I intend to personally call them on a weekly basis to discuss progress and next steps. Fill out the form above if youâ€™re interested!

My Personal Projects:

**Bot Classification** \- currently in progress, classification techniques for the automated detection and reporting of bots: [https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp](https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp)

**Grand Exchange Prediction** \- time series analysis to predict the future prices of items in the grand exchange: [https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45](https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45)

**Cow Compliments** \- computer vision to detect cows on the screen in real time, walk over to them and compliment them for the lulz: [https://www.youtube.com/watch?v=7oW7jDyIufE](https://www.youtube.com/watch?v=7oW7jDyIufE)

&amp;#x200B;

**EDIT**: Firstly, THANK YOU FOR MY FIRST GOLD.

On the topic of when you can expect a response, I'll read through every submission by the end of the week and I'll let you guys know who I decide to start off working with by this weekend (05/10/2020). Probably a max of 5 individuals who I will reach out to once chosen.

The goal is to have a variety of different projects each from different games the individuals are passionate about. This is to hopefully allow for a broader scope of projects that ALL of you can contribute to as well. I plan to make a discord text channel for each game that we're going to be working on. There, we can all discuss ideas, issues and breakthroughs of applying ML to the chosen game. To be clear, these channels are not just for the main project that we'll be working on with the CCA Fellows (which you can contribute as well) but also for your own project ideas that you can work through alongside other ongoing projects that are different but still focused on that specific game.  So please join the discord if you're interested with working on these projects!

I know I will not be able to reach all of you due to the overwhelming response (once again, thank you so much!). However, I still definitely believe in the power of learning through application. Therefore, even if I can't get to you all personally, I hope you still get started on your own by either contributing to the main projects or with your own ML+Game projects. If you do, please keep us up to date as well on the channels we'll create for the games. We'll make sure to check in and, even if it's not a long personal call, that'll definitely still be a way to keep you accountable and continuously learning!",t2_3tqetf9o,False,,1,False,Want to learn ML by applying it to Games? I'll personally call you weekly and make sure you do it!,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_geuukb,False,light,0.96,,public,322,1,{},,,False,[],,False,False,,{},Project,False,322,,True,self,1588864050.0,,[],{'gid_2': 1},self,,True,,1588835404.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, my nameâ€™s Chris! Iâ€™m a research software engineer with a focus on applying machine learning techniques to a variety of new inventions. Iâ€™m also the co-founder and current co-lead of my companyâ€™s machine learning and artificial intelligence (ML/AI) community of practice.&lt;/p&gt;

&lt;p&gt;In my free time, I apply ML to games in many different ways. My projects so far mainly have to do with Old School Runescape (examples below but thatâ€™s not what Iâ€™m here for today). Iâ€™m here today because a few people have reached out to me recently about starting their own projects applying ML to various games and I think thatâ€™s a great idea!! After all, the best way to learn is through implementation and why not implement something you care about? Not to mention, personal projects look great on any resume!&lt;/p&gt;

&lt;p&gt;Throughout my journey, Iâ€™ve found that Iâ€™m happiest when helping people out by thinking through their process with them and figuring out what comes next. I would really like to share my experience and expertise gained with anyone who is interested in making their own projects and learning much more about ML and its applications in the process. I also have a bunch of ideas for ML-game projects that Iâ€™ll never have the time to implement myself which someone else might find invigorating.&lt;/p&gt;

&lt;p&gt;That said, Iâ€™m thinking of starting what Iâ€™ll call the Chronic Coder Academy. Iâ€™ll start with a few students who I personally call on a weekly basis for about 30 minutes to an hour to discuss their new or ongoing ML projects - weekly todos, potential resources and next steps. I personally think itâ€™d help keep people accountable on their personal journey learning ML and also keep them motivated if I direct them well instead of being lost in the rough sea of ML/AI.&lt;/p&gt;

&lt;p&gt;This is targeted at people who care about games and also want to learn more about ML through application. Your age, race, gender, ethnicity, etc mean nothing to me but you have to be passionate about what you do and hopefully converse well enough in English. Everything you make is completely your own, no matter how much advice or guidance I end up giving. Oh and also, itâ€™s completely free (before anyone asks). Iâ€™m really in this to better expose myself to different applications and ideas plus I truly enjoy watching people learn especially from interesting use cases like these. If they will allow me, I might use their projects as examples for teaching people more about specific ML concepts in the future. All that said, please fill out this survey if youâ€™re interested: &lt;a href=""https://forms.gle/W37tzNFsTCFXGEwMA""&gt;https://forms.gle/W37tzNFsTCFXGEwMA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If this isnâ€™t for you at this point in your life, feel free to join our discord! This applies regardless of who you are, what you do or what your experience level is. Itâ€™s an amazing community of ML experts and curious students alike who also happen to share a love for games. We could always use more experts to help anyone in need but also itâ€™s a place to learn if youâ€™re doing your own applied ML project and arenâ€™t sure who to go to. Join us here: &lt;a href=""https://discord.gg/ZummSXK""&gt;https://discord.gg/ZummSXK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Oh and if youâ€™re looking for a job sometime soon and worried about a project involving games not being a series enough topic to get hired, I previously did some asking around: &lt;a href=""https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing""&gt;https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing&lt;/a&gt;. Looks like most professionals agree that ML applied to games can be as serious a topic as any other within the ML field.&lt;/p&gt;

&lt;p&gt;Whew that was a long piece... Thank you for reading if you&amp;#39;ve made it this far!&lt;/p&gt;

&lt;p&gt;or &lt;strong&gt;TL;DR&lt;/strong&gt; - I want to help people learn ML by applying it to games. I intend to personally call them on a weekly basis to discuss progress and next steps. Fill out the form above if youâ€™re interested!&lt;/p&gt;

&lt;p&gt;My Personal Projects:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bot Classification&lt;/strong&gt; - currently in progress, classification techniques for the automated detection and reporting of bots: &lt;a href=""https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp""&gt;https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Grand Exchange Prediction&lt;/strong&gt; - time series analysis to predict the future prices of items in the grand exchange: &lt;a href=""https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45""&gt;https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cow Compliments&lt;/strong&gt; - computer vision to detect cows on the screen in real time, walk over to them and compliment them for the lulz: &lt;a href=""https://www.youtube.com/watch?v=7oW7jDyIufE""&gt;https://www.youtube.com/watch?v=7oW7jDyIufE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Firstly, THANK YOU FOR MY FIRST GOLD.&lt;/p&gt;

&lt;p&gt;On the topic of when you can expect a response, I&amp;#39;ll read through every submission by the end of the week and I&amp;#39;ll let you guys know who I decide to start off working with by this weekend (05/10/2020). Probably a max of 5 individuals who I will reach out to once chosen.&lt;/p&gt;

&lt;p&gt;The goal is to have a variety of different projects each from different games the individuals are passionate about. This is to hopefully allow for a broader scope of projects that ALL of you can contribute to as well. I plan to make a discord text channel for each game that we&amp;#39;re going to be working on. There, we can all discuss ideas, issues and breakthroughs of applying ML to the chosen game. To be clear, these channels are not just for the main project that we&amp;#39;ll be working on with the CCA Fellows (which you can contribute as well) but also for your own project ideas that you can work through alongside other ongoing projects that are different but still focused on that specific game.  So please join the discord if you&amp;#39;re interested with working on these projects!&lt;/p&gt;

&lt;p&gt;I know I will not be able to reach all of you due to the overwhelming response (once again, thank you so much!). However, I still definitely believe in the power of learning through application. Therefore, even if I can&amp;#39;t get to you all personally, I hope you still get started on your own by either contributing to the main projects or with your own ML+Game projects. If you do, please keep us up to date as well on the channels we&amp;#39;ll create for the games. We&amp;#39;ll make sure to check in and, even if it&amp;#39;s not a long personal call, that&amp;#39;ll definitely still be a way to keep you accountable and continuously learning!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?auto=webp&amp;s=ceeb466dab89c19b28d7c1a705c120ab083492c7', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63c56cac71cca6eb0ed38496f3be93af748929f9', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f51db1d17542846e83cc1ab922f1cb853d3a084', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7c65bdec2e163e3ecb46b11cc9164edba8c7a36', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a126016c4897032f5ee69cda7573a9fcfa044088', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac65fb8c06828aba0b8ea3ccca6671fef5b4cf21', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d00c2b2a25880883bb0dd5fa4c0e6e523d27a59f', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'u5mV1tvG7-5CDubQL7omAhHKFY2GHhm2sfnyhKj4sMA'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'icon_height': 512, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'start_date': None, 'is_enabled': True, 'description': 'Gives the author a week of Reddit Premium, %{coin_symbol}100 Coins to do with as they please, and shows a Gold Award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'name': 'Gold', 'icon_format': None, 'award_sub_type': 'GLOBAL', 'penny_price': None, 'award_type': 'global'}]",[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geuukb,True,,chriskok1337,,27,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuukb/want_to_learn_ml_by_applying_it_to_games_ill/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geuukb/want_to_learn_ml_by_applying_it_to_games_ill/,155203,1588806604.0,0,,False,,,,
,learnmachinelearning,"I have a set of data in a csv file, roughly 1700 rows by 35 columns, that gets added to every day and my goal is to take that data and use it to predict one number per row. The problem that I am running into is that I don't know where go from here. All of the machine learning software I have looked at is focused on image recognition and not text data problems. If I do find a guide for one it's super basic and uses downloaded data that I can't easily replace with a csv import. So I was wondering if you guys have any recommendations for software I could use to solve this problem and if you know any tutorials that go along with them.",t2_57emyy01,False,,0,False,What to use to run a regression problem in python?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfnujz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Request,False,1,,False,self,False,,[],{},,,True,,1588948254.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a set of data in a csv file, roughly 1700 rows by 35 columns, that gets added to every day and my goal is to take that data and use it to predict one number per row. The problem that I am running into is that I don&amp;#39;t know where go from here. All of the machine learning software I have looked at is focused on image recognition and not text data problems. If I do find a guide for one it&amp;#39;s super basic and uses downloaded data that I can&amp;#39;t easily replace with a csv import. So I was wondering if you guys have any recommendations for software I could use to solve this problem and if you know any tutorials that go along with them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gfnujz,True,,Void-Nut,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfnujz/what_to_use_to_run_a_regression_problem_in_python/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfnujz/what_to_use_to_run_a_regression_problem_in_python/,155203,1588919454.0,0,,False,,,,
,learnmachinelearning,,t2_o0pbd,False,,0,False,"22 worked examples in machine learning (energy, medicine, banking, retail...)",[],r/learnmachinelearning,False,6,,0,73.0,False,t3_gf2coi,False,dark,0.9,,public,66,0,{},140.0,,False,[],,False,False,,{},,False,66,,False,https://b.thumbs.redditmedia.com/AtNCeUtZZotZem202EB0jbArWcXkaKK6S5i3Sa9o6iE.jpg,False,,[],{},link,,False,,1588867158.0,text,6,,,text,neuraldesigner.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?auto=webp&amp;s=d100079d234b6b84dba6f38614d575af623b1d3e', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8109e511182067447f3ff3b0fc99d8c07f798df', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=39122191eb7fb9bed7ca1924828fb21e7cb6ab3f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acbb3f60a8963b1b096f874c06457e6dded7e691', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66874fccb85f1c12b0b0dbaf3ddaf3957fb21ef2', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=34a2144239c51c4bed947b72af8f19c88655a513', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faf980c3422664e2d56565b03e1416d12b54bd17', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'lDWE1FUKuv1DyfNkkPXhMigVGDiektzN0SUFmFD3RTw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf2coi,True,,datapablo,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2coi/22_worked_examples_in_machine_learning_energy/,all_ads,False,https://www.neuraldesigner.com/learning/examples,155203,1588838358.0,0,,False,,,,
,learnmachinelearning,"Hello.  I would like to create a custom real-time object detection app, preferably in Pytorch, that people can download to Mac, PC, or Linux, without having to install Python or anything else except for that one app.  I would like this to work on very old and slow computers too, so I'm hoping for a program that is fast but also small on disk space and easy to use.  Any advice for me on which ML packages I should use?  

Thank you in advance for your help.",t2_57i0bime,False,,0,False,Advice on standalone object detection app please!,[],r/learnmachinelearning,False,6,,0,,False,t3_gfn2mu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588944364.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello.  I would like to create a custom real-time object detection app, preferably in Pytorch, that people can download to Mac, PC, or Linux, without having to install Python or anything else except for that one app.  I would like this to work on very old and slow computers too, so I&amp;#39;m hoping for a program that is fast but also small on disk space and easy to use.  Any advice for me on which ML packages I should use?  &lt;/p&gt;

&lt;p&gt;Thank you in advance for your help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn2mu,True,,tylersuard,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn2mu/advice_on_standalone_object_detection_app_please/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfn2mu/advice_on_standalone_object_detection_app_please/,155203,1588915564.0,0,,False,,,,
,learnmachinelearning,"I have a decent understanding of Python (OOP, Pandas, NumPy) and a solid foundation in Statistics and Linear Algebra. I want to learn predictive modeling and machine learning. As I have a decent understanding of the pre-requisites, what resources do you recommend I learn?",t2_6dx8f99v,False,,0,False,"I know most of the pre-reqs for Machine Learning, where to start?",[],r/learnmachinelearning,False,6,,0,,False,t3_gfn0c9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588944055.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a decent understanding of Python (OOP, Pandas, NumPy) and a solid foundation in Statistics and Linear Algebra. I want to learn predictive modeling and machine learning. As I have a decent understanding of the pre-requisites, what resources do you recommend I learn?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn0c9,True,,whyamisosmart,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn0c9/i_know_most_of_the_prereqs_for_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfn0c9/i_know_most_of_the_prereqs_for_machine_learning/,155203,1588915255.0,0,,False,,,,
,learnmachinelearning,,t2_4th82f0f,False,,0,False,Funny tweet,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_gfmtsf,False,dark,1.0,,public,1,0,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'height': 796}",140.0,,False,[],"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/richard_droste/status/1258096722657579008', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Richard Droste', 'height': 796, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/richard_droste', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,False,,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfmtsf', 'height': 796}",,False,1,,False,https://b.thumbs.redditmedia.com/wtOPds_RRS9_Q5rz0QI7h5_OpMJC34_rXqfaeCFKyOw.jpg,False,,[],{},link,,False,,1588943212.0,text,6,,,text,mobile.twitter.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?auto=webp&amp;s=a200776f1543e11890960170ad1b09247277d60a', 'width': 616, 'height': 1272}, 'resolutions': [{'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=909b8a123d9b6f8d1a69c873cf1ce907c725366a', 'width': 108, 'height': 216}, {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dbdcae36b58ab83f6a0f585e79d105dc56a66a39', 'width': 216, 'height': 432}, {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99bf40978b5349d442655598351d3a5095194339', 'width': 320, 'height': 640}], 'variants': {}, 'id': 'ShGOari78UprytI0ZwV-IMroCLmhfmrrzgfTJKp91TE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfmtsf,True,,Jolly-Theory,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfmtsf/funny_tweet/,all_ads,False,https://mobile.twitter.com/richard_droste/status/1258096722657579008,155203,1588914412.0,0,"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/richard_droste/status/1258096722657579008', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Richard Droste', 'height': 796, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/richard_droste', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,,,,
,learnmachinelearning,,t2_50tfqv5x,False,,0,False,Top Machine Learning Companies in India - 2020,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,93.0,False,t3_gft84z,False,light,0.14,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/OELzze_HNOwQrm2KcM_Lje29mcy0cYZfruX2IO9LbgU.jpg,False,,[],{},link,,False,,1588973285.0,richtext,6,,,text,mygreatlearning.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?auto=webp&amp;s=5b626a5b6565a84ef8db6cdebe0737409e6d9571', 'width': 1254, 'height': 837}, 'resolutions': [{'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=315dbf5e94e11aa8d0eff69b0d3ed384226085b7', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c07a8aa61da54a837ae1ab49737f85fa3d36c1b', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a65015fe36f76b8066e9217db0d06dc11e8aceb', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=89f078bb5a516bb3d7fdf7ba5200c311f1c6aa9a', 'width': 640, 'height': 427}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=21c8109a5ea1d4b60a307f2282c82fb3cc01609a', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96ceba3b0bccc7226332a715672a55980d15d767', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 't0IxogoYjl-0vChEZJ0MWwVXNi1hmRlisO8c6eb_y8U'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gft84z,True,,agarwalsimran,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gft84z/top_machine_learning_companies_in_india_2020/,all_ads,False,https://www.mygreatlearning.com/blog/top-machine-learning-companies-in-india/,155203,1588944485.0,0,,False,,,,
,learnmachinelearning,"Hello! 

Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups. 

I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I'm unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I'm trying to see how this would be changed to go to 25 groups. Any help or advice on where to look would be appreciated! Thanks 

    model = Sequential()
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))",t2_7ajy1,False,,0,False,Looking for advice on how to set up parameters for CNN (sequential),"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfm7bn,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588940306.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! &lt;/p&gt;

&lt;p&gt;Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups. &lt;/p&gt;

&lt;p&gt;I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I&amp;#39;m unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I&amp;#39;m trying to see how this would be changed to go to 25 groups. Any help or advice on where to look would be appreciated! Thanks &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (5, 5), activation=&amp;#39;relu&amp;#39;, input_shape=(120, 320, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(10, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfm7bn,True,,MrMegaGamerz,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfm7bn/looking_for_advice_on_how_to_set_up_parameters/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfm7bn/looking_for_advice_on_how_to_set_up_parameters/,155203,1588911506.0,0,,False,,,,
,learnmachinelearning,"I'm working on a project to predict the likelihood of having heart disease based on 8 features from a dataset found on kaggle. The results are binary, between 0 and 1. After trying logistic regression and SVM, the best model has been random forest classifier, but I get suspiciously good results--model round 98% accuray. I added a cross\_val\_score, which I thought would give me more accurate results, but I still get in the high 90s. There's no way this can be true, as the dataset is very small, around 1025 rows. Is this high because the dataset is so small, or am I making an error in any of the steps that would be give suspicious results? Is overfitting a potention issue with random forest?

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
    
    from sklearn.ensemble import RandomForestClassifier
    classifier2 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')
    classifier2.fit(X_train, y_train)
    
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(classifier2, X, y, cv=5)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    
    ypredd = classifier2.predict(X_test)
    
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, ypredd)
    
    from sklearn.metrics import classification_report
    
    print(classification_report(y_test,ypredd))",t2_3pnizflv,False,,0,False,Random Forest Classifier too good--I'm suspicious,[],r/learnmachinelearning,False,6,,0,,False,t3_gflxig,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588911462.0,,[],{},,,True,,1588939100.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project to predict the likelihood of having heart disease based on 8 features from a dataset found on kaggle. The results are binary, between 0 and 1. After trying logistic regression and SVM, the best model has been random forest classifier, but I get suspiciously good results--model round 98% accuray. I added a cross_val_score, which I thought would give me more accurate results, but I still get in the high 90s. There&amp;#39;s no way this can be true, as the dataset is very small, around 1025 rows. Is this high because the dataset is so small, or am I making an error in any of the steps that would be give suspicious results? Is overfitting a potention issue with random forest?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

from sklearn.ensemble import RandomForestClassifier
classifier2 = RandomForestClassifier(n_estimators = 10, criterion = &amp;#39;entropy&amp;#39;)
classifier2.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(classifier2, X, y, cv=5)
print(&amp;quot;Accuracy: %0.2f (+/- %0.2f)&amp;quot; % (scores.mean(), scores.std() * 2))

ypredd = classifier2.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, ypredd)

from sklearn.metrics import classification_report

print(classification_report(y_test,ypredd))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gflxig,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gflxig/random_forest_classifier_too_goodim_suspicious/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gflxig/random_forest_classifier_too_goodim_suspicious/,155203,1588910300.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#Studying and #Learning Effectively - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,110.0,False,t3_gfkuq9,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,True,https://b.thumbs.redditmedia.com/pgwfxutWY-G_dXN34KAKVpuEzp1Y6ibnE32S8enW7Tg.jpg,False,,[],{},,,False,,1588934561.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfkuq9,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkuq9/studying_and_learning_effectively_a_university/,all_ads,False,/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/,155203,1588905761.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University, Studying Effectively, 2020  [https://www.monash.edu/rlo/study-skills/studying-effectively](https://www.monash.edu/rlo/study-skills/studying-effectively) \n\nNew study patterns\n\nLectures and tutorials take up part of the day. You must plan your own long and short term timetables.\n\nFor every one-hour lecture or tutorial, about two hours of private study will be necessary.\n\nAssignments tend to be long and not frequent. They are usually set many weeks ahead.\n\nWide reading is essential. You may be given a reading list, but you should read other sources as well.\n\nLecture groups may be large. It is up to you to approach your lecturer or tutor if you are having difficulties.\n\nYou will have to identify and make notes on the main points in lectures and texts.\n\nYou must acknowledge all your sources. To avoid plagiarism, you will need to learn referencing skills.\n\nYou are expected to develop independent thinking by:\n\nmemorising information,\n\nasking questions,\n\nexamining evidence, and\n\nthinking critically.\n\n&amp;#x200B;\n\n \n\nManaging your time\n\nPlanning your semester\n\nIt is not uncommon to have several assignments due at the same time, and it is easy to run out of time without careful planning. All assignments due dates are listed in your unit guides, so you can make at least a draft version of this activity at the end of week 1.\n\nIt will help you plan ahead realistically so that you don\'t miss any deadlines.\n\n1. Write the name or code of each unit in the spaces across the top of the planner.\n\n2. For each unit, fill in the due dates of all tests and assignments in the appropriate week.\n\n3. For each assignment or test:  \n\n\na. Break down the preparation into a series of steps.  \nb. Consider how much time will be needed for each step and record your time estimates next to the tasks.  \nc. Working backwards from the due date, distribute the tasks in a logical order through the weeks available.\n\n4. Calculate the average number of hours you need to allocate each week to prepare for assignments and tests:\n\na. Add up the total number of hours needed in the first 6 weeks of the semester.  \nb. Divide the sum by the number of weeks.\n\nFor example:\n\n6 + 16 + 16 = 38 hours needed for assignments and test\n\n38 hours / 6 weeks = 6 hours and 20 min per week\n\nPlanning your week\n\nThis weekly schedule should be used flexibly; every week will be different. However, this example will give you an idea of what your \'average\' week should look like. If you can\'t fit everything in, you may need to consider reducing your extra-curricular activities during the semester.\n\n1. Enter all your classes, recording the unit code or name and class type.\n\n2. Enter any other commitments with set hours, e.g. club meetings, team sports practice, work.\n\n3. Block in at least 30 minutes preparation time for each class, recording the unit code or name and class type. Some units will need longer. Classes should be reviewed on the same day as much as possible.\n\n4. Allocate time to work on assignments or to study for tests. Consider your preferences: short bursts or long stretches?\n\n5. Block in your travel time (to and from uni, sports practice, work, etc.) and meal times. Allow time for cooking and cleaning up if necessary. Give yourself a lunch break even on busy days.\n\n6. Block in time for regular chores: shopping, washing, cleaning, etc. Be realistic: they take time.\n\n7. Now add time to relax at the end of the day and socialise with family and friends.\n\n8. Finally, make sure you have some free \'catch-up\' hours to allow for the unexpected.\n\n&amp;#x200B;\n\n Reference: Monash University, RLO Study Skills, 2020  [https://www.monash.edu/rlo/study-skills/reading-and-note-taking](https://www.monash.edu/rlo/study-skills/reading-and-note-taking) \n\nReading and note-taking\n\n Getting started\n\nThe amount of reading that you are expected to do at university can be daunting.\n\nHowever, with some preparation and adjustment to your reading techniques, you will soon be able to read effectively and efficiently.\n\n \n\nTop tip\n\nThe key to managing your reading load is to become an active reader - that is, you need to:\n\nask questions about what you need to know,\n\nselect readings that relate to your questions and tasks,\n\nand read them efficiently.\n\n \n\nActivity: Academic Reading\n\nTake this quiz to recognise your current level of interpreting academic reading.\n\n I have to read every book or article on my subject reading list. \n\nOption 1: True\n\nWrong answer.\n\n \n\nOption 2:  False\n\nCorrect answer.\n\nLarge reading lists can be very intimidating. You are not expected to read absolutely everything on your subject. University study requires a lot of reading within a limited time, so it is important to be selective about what you read. You need to make decisions about what is essential.\n\n&amp;#x200B;\n\n You can spend many hours reading, and feel as if you are being productive, but actually not get very far with learning, or achieving your study goal. \n\n Four essential pre-reading tips:  \n\n1. Know your purpose\n\nAlways read with a purpose in mind.\n\nFor example, are you reading to:\n\ngain an overview of the area of study?\n\nlocate specific information?\n\nunderstand difficult concepts?\n\nrevise for exams?\n\ncomplete a specific assignment?\n\nWhen you know why you are reading, you will be better equipped to decide how you are going to read (ie. Which reading strategies you need to use â€“ more on that later!)\n\n&amp;#x200B;\n\n \n\n2. Be selective with what you read and focus on the question or task\n\nOnce you know what you are looking for (i.e. have a purpose), you can start making some decisions about what is essential reading, and what can be saved for later.\n\nAsk yourself what you need to find out. Identify:\n\nKey topic words or phrases\n\nQuestions that you want to answer\n\nThen actively look for answers and evidence to support these answers.\n\nTip:\xa0\n\nhave a copy of your assignment question with you and read with it nearby, so you donâ€™t waste time reading irrelevant material.\n\n&amp;#x200B;\n\n \n\n3. Break reading into segments\n\nBreak things down to manageable chunks (e.g. number of pages, articles or chapters). Identify how much time you have and aim to read a certain number of pages, or for a set length of time.\n\n&amp;#x200B;\n\n \n\n4. Keep track of what you have read\n\nAlways note down where information and ideas have come from. This will also help when you need to reference the text. \xa0\n\nKeep track of details such as: \xa0\xa0\n\nAuthor\n\nTitle\n\nPlace of Publication\n\nDate\n\nPage numbers\n\n&amp;#x200B;\n\nReference: Monash University, RLO Study Skill, 2020,  [https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text) \n\n&amp;#x200B;\n\n \n\nApproaching and evaluating a text\n\n \n\nAs you become more familiar with the text of your specific discipline area (economics, history, engineering, etc.), you will become better at predicting the kinds of questions you might find answers to in the text.\n\nBeing able to make predictions is important. Creating certain expectations will increase your alertness to the text, and you will be better at noticing whether or not your expectations are met.\n\nThe accuracy of your prediction is not usually the most important thing. What is important is recognising what the text does or does not deal with.\n\n&amp;#x200B;\n\n \n\nActivity: Predicting\n\nSelect any of the subject areas below by clicking on the ðŸ“·.\n\nLook at the title of the book. Then consider what you already know about the topic. Think of some questions to which the book might supply answers.\n\n Agriculture in semi-arid environments\n\n... I don\'t have any specialised knowledge about agriculture, and especially agriculture in semi-arid environments, but I do have everyday knowledge. I know agriculture needs water, and very often lots of it. Knowing that leads me to ask the following questions:\n\nHow can agriculture survive in such dry places?\n\nIs it possible to develop agricultural practices that use less water, or is it possible to recycle water, or do farmers in such environments have to pipe in large amounts of water?\n\nThis leads to a further question:\n\nIf farmers have to pipe in water, and perhaps have to use lots of fertilisers to make the semi-arid soils nutritious and agriculturally productive, how can such agriculture be economically viable?\n\nThat makes me wonder:\n\nAre there other ways of naturally enriching soils through changing farming practices?\n\nNB. It is possible that none of these questions helps to focus the reader\'s attention and increase his/her ability to make sense of the text. As the reader looks for answers, more precise ideas will come to his/her attention, and as a result, more specific questions can then be asked...\n\n&amp;#x200B;\n\nCivil Engineering - Soil plasticity\n\n""I know nothing about soil plasticity, and therefore my questions are very basic. I need to know:\n\nIn what ways are soils \'plastic\'? What does plasticity mean here?\n\nWhat is the significance of soil plasticity?\n\nHow does soil plasticity affect engineering design or practices?\n\nIf I were knowledgeable about the subject, I could ask more specific questions based on that knowledge.""\n\n&amp;#x200B;\n\nLiterature -  Madness and sexual politics in the feminist novel\n\n... I have some background knowledge on this topic. I know for instance that sexual politics is about power relations between the two sexes. But I also know there are different theories about these power relations. So my first question might be:\n\nWhich theorists and theoretical approaches do this text draw on?\n\nI also know that there is no clear definition of what we mean by a feminist novel. So I might ask:\n\nHow does this writer define a feminist novel? What novels does s/he discuss?\n\nI know madness is usually the concern of medicine, but I also know that madness is of interest in humanities subjects. Some people argue that those who are mad do not necessarily have anything wrong with them, but that they are categorised as mad by society because they do not conform in certain ways. I suspect the writer of this article takes up this view of madness. I predict therefore that s/he will look at how women in novels become mad, and that s/he will link this to their resistance or nonconformity to struggle with male-dominated practices. Therefore I might ask:\n\nWhat is meant by madness in this text?\n\nIs female madness the result of male-oriented social relations? ...\n\n&amp;#x200B;\n\nEducation -  Gender issues in physics education\n\n""I know there has been a lot of concern about girls taking an interest in physics at school. I also know that while some people think this is because girls are naturally not interested, others argue it is because physics is seen as a masculine subject, and so girls do not become interested, even though they could actually do as well as boys. I suspect this article adopts this last view and therefore sees the way we create masculinity and femininity in our society (gender issues) as a central problem in physics education. Therefore I might ask:\n\nIs the gender issue discussed here the one I have predicted - that girls do not take up physics because it is not feminine? Or is it another issue?\n\nWhat exactly are the issues to be discussed? (Is there a list of them?)\n\nDoes the author only describe the issues, or does s/he also suggest ways of overcoming the gender problems?\n\nIf s/he offers solutions, do these lie in changing teaching methodologies, or changing the physics curriculum, or something else?""\n\n&amp;#x200B;\n\nPhilosophy -  Is it good to make people happy?\n\n... This looks like a strange topic. Everybody knows it is good to make people happy! But I know that philosophy wants to understand precisely why it might be good, and ethics wants to distinguish between good and bad actions. So a definition of good seems essential. Therefore, my first questions might be:\n\nWhat is meant by good in this context? Can I quickly find a definition?\n\nWhy is it good to make people happy, and not merely nice?\n\nI might also want to know about the link between goodness and happiness. If happiness is good, then presumably it should always be an aim of ours. But what happens if happiness is in conflict with justice? Pursuing justice can make some people very unhappy. So I might also ask:\n\nWhat happens if the principle of happiness conflicts with another principle, such as justice? If happiness gives way to justice, does this mean it is not good, but merely desirable?\n\nThe more we think about this topic, the more questions will come to mind, and so we can get deeper and deeper into the text ...\n\n&amp;#x200B;\n\nPsychology -  A theory of cultural values and some implications for work\n\nWhat is the theory of cultural values?\n\nHow can theory have implications for work?\n\nThese questions immediately come to my mind. A further question follows:\n\nWhat kind of theory does this article present?\n\nIt seems to me that cultural values include many things. Another question comes to mind:\n\nWhat cultural values exactly will this text discuss?\n\nThis leads to further questions:\n\nIs this text talking about cultural values that have a direct bearing on work?\n\nIf so, what aspects of work? Does it discuss the ways cultural values affect attitudes to work, or work efficiency, or the kinds of works that people will want to do?\n\nI find very quickly that I have a lot of questions. This title seems so general it is hard to predict what it is likely to be about. Therefore my questions are more to do with trying to define the scope of this text. Once I have some sense of its scope, then I might be able to make predictions and ask questions about the content ...\n\n&amp;#x200B;\n\n \n\nStrategies for effective reading\n\n \n\nThere are different strategies you can apply to your reading, depending on your goal. You might need to choose one or a few of these strategies for each text that you read.\n\nIn this section:\n\n[Previewing](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#previewing)\n\n[Skimming](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#skimming)\n\n[Scanning](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#scanning)\n\n[Detailed reading](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#detailed_reading)\n\nPreviewing\n\nWhat is previewing?\n\nPreviewing is getting a sense of what\'s in a given piece of work without reading the body of the text\n\nWhen should you preview?\n\nPreviewing helps you decide whether a book or article is useful for your purpose. \xa0It gives you a general sense of the content so you can see if you want to read in more detail, and it helps you locate sections that you need to read, and sections you don\'t.\n\nHow to preview:\n\nRead the title and author details\n\nRead the abstract (if available)\n\nRead main headings, chapter summaries, and anything that \'jumps out\' at you\n\nLook at any diagrams, graphs, tables. \xa0These usually summarise the content of large written paragraphs.\n\n&amp;#x200B;\n\n \n\nWhat is skimming?\n\nSkimming is reading small amounts from throughout the text.\xa0It is different from previewing because you\'re reading the body of the text. The chief benefit of skimming is in being able to pick up the key ideas quickly. \xa0\n\nHow to skim\n\nIf the introduction is short read it in full. If long, read the first sentence of each paragraph. Then read the first sentence of each subsequent paragraph, or until you find the topic sentence (usually the first or second sentence).\xa0This will give you an overview of the content of the passage.\xa0It can also be useful to read the concluding paragraph in full.\n\nFor a report or research paper,\xa0first of all, read the Abstract. Then look over the section headings and subheadings and any figures or tables before skimming the text. It may also be useful to read the Conclusion.\n\nDon\'t get bogged down. \xa0This is a fast process.\n\n&amp;#x200B;\n\n \n\nWhen should you skim?\n\nWhen you want to get an overview or the gist of a text. This can help you decide whether or not to read the full text.\n\nSkimming adds to the information that you picked up in previewing.\n\n&amp;#x200B;\n\n \n\nActivity: Test your skimming skills\n\nIn the excerpt from a book chapter below, the first paragraph is presented in full. In the following paragraphs (2-6), only the topic sentences appear. You should still be able to get the gist of the full passage.\n\nRead the first paragraph and the topic sentence of each subsequent paragraph.\n\nYou should find you can easily answer the three questions which follow.\n\nUse the blue arrows to move through the quiz.\n\nChapter Twelve: Trade routes and rituals  \n1. Trade between distant people is often seen as a mark of a more advanced economic life. If this insight is valid, many groups of aboriginals must have been far from backward because their raw materials and manufactures were traded to people hundreds of miles away. It is probable that every tribe in Australia traded with its neighbours, and a few commodities were involved in such a sequence of transactions that they crossed from the tropical coast almost to the Southern Ocean.  \n2. Pearl shell travelled further perhaps than any other Item. (13 lines deleted)  \n3. In eastern Australia, the axe-stone also moved over a wide area. (6 lines deleted)  \n4. A quarry which provided stone fit for stronger, sharper axes was likely to supply trade routes stretching in every direction. (6 lines deleted)  \n5. As the written records were thin in tracing the trade-in stone axes from the Tamworth district; other ways of reconstructing the extent of the trade were needed. The petrological analysis was one promising technique.  \n6. This kind of archaeological jigsaw - the exact matching of axe and quarry - can be solved only when every likely source of stone has been discovered and described. (3 lines deleted) â€¦ axes had gone overland through a chain of tribal territories to Cobar, Bourke, Wilcannia, and other points on the plains as remote as 500 miles from the home quarries. (2 lines deleted)  \n(Blainey, G. (1975). Triumph of the nomads: A History of Ancient Australia. South Melbourne, Vic.: Macmillan. p. 203-204.)\n\n&amp;#x200B;\n\n What is the topic of the passage? \n\n \n\n1. The distribution of axe stone in eastern Australia.\n\nWrong answer.\n\nThis topic was used to illustrate the topic.\n\n&amp;#x200B;\n\n \n\n2. Aboriginal trade routes.\n\nWrong answer.\n\nThe focus is not so much on the routes themselves as their far-reaching range.\n\n&amp;#x200B;\n\n3.  Trade among indigenous populations.\n\nWrong answer.\n\nThis is true, but the scope in the passage is limited to Australia.\n\n&amp;#x200B;\n\n \n\n4.  The extent of trade among Australian aborigines \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n What trade items were discussed?  Axe Stone\n\n&amp;#x200B;\n\n \n\nWhat is scanning?\n\nYou skim-read material to get the general picture. \xa0\n\nYou scan when looking for specific information.\n\n&amp;#x200B;\n\n \n\nWhen should you scan?\n\nYou may need to find specific details on a topic for an assignment or a task that your lecturer has set. \xa0There is little point in skimming a whole book for this purpose. \xa0You should scan the text for words related to the topic. \xa0You can run your eyes down the page looking for these expressions - in chapter headings or sub-headings, or in the text itself.\n\n&amp;#x200B;\n\n \n\nActivity: Test your scanning skills\n\nYou need to find the definition of â€˜postmodernityâ€™ for an assignment.\n\nYour first step might be to look in the index for the word, and see if it can direct you to some definitions. However, if the whole book is about postmodernism, then you might have too many references to check. Another obvious place to look is in the Introduction.\n\nBelow is a section from the Introduction to Intimations of postmodernity.\n\nScan through to see if you can locate where in the text the author defines what he means by the term.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[ Adapted from Bauman, Z. \\(1992\\). Intimations of postmodernity. London: Routledge ](https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;format=png&amp;auto=webp&amp;s=bd823975e2c04aa78391017c3ab7eff261046b70)\n\n \n\nWhich sentence in the text does the author define postmodernity?\n\n7 Wrong answer.Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(\'perhaps more than anything else.\')\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n11 Correct answer. RIGHT ANSWER! In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(\'perhaps more than anything else.\')\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n3 Wrong answer. Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(perhaps more than anything else)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n&amp;#x200B;\n\n \n\nWhat is a detailed reading?\n\nDetailed reading is intensive reading that focuses on the important parts - key chapters, books, poems, pages. This is when you are likely to take detailed notes.\n\n&amp;#x200B;\n\n \n\nWhen should you read in detail?\n\nIntensive reading is usually the final step - after you\'ve previewed, scanned and skimmed when you know that it is worth reading the full text.\xa0\n\nIn every unit of study, there will be key sources or texts that you will need to read carefully. \xa0\n\nExpect to read these more than once and to take notes on important material.\n\n You will often need to take notes while reading. This video on [Efficient note taking strategies](https://www.youtube.com/watch?v=XELOxGx_ZZg) will help you take better notes. \n\n&amp;#x200B;\n\n \n\nDeveloping your critical reading skills\n\n \n\nIn university study you will frequently hear your lecturer or tutor encouraging you to take a critical approach to reading and researching. A critical approach to reading does not mean criticising or \'finding fault\'. It means asking yourself why this particular author has written this particular text, and evaluating their strengths and weaknesses.\n\nYou can ask yourself many questions about the text - the more the better. These are shaped by what you already understand about the text, and what you need to get from it.\n\nðŸ“·\n\nConsider the following\n\nThere are several questions to consider when approaching your reading. The following examples will give you a few ideas on the types of questions you should be asking when reading:\n\nThe author:\n\n \n\nWhat is the author\'s main theme or point?\n\nIs the author making any particular assumptions? On what authority?\n\nWhich aspects does the author focus on and why? Does he/she omit any important points?\n\nAre there additional clues about the author\'s attitude or stance (e.g. from his/her position/qualifications, country of origin,\xa0the text\'s date of publication or publisher,\xa0the type of text)?\n\nWhat theoretical perspective has the author taken (e.g. which writers does she/he cite most often or most approvingly)?\n\nWhat basis or criteria is the author using to make judgements?\n\n&amp;#x200B;\n\nThe content:\n\n \n\nWhat is the main point, thesis or argument?\n\nWhat is the text really about (i.e. special agenda, underlying themes)?\n\nWhat explanations or supporting evidence are drawn on? Do they seem adequate, completely relevant?\n\nIs all the factual information correct as far as you know?\n\nWhat aspect of the topic has the author chosen to focus on? What has s/he omitted?\n\nWhat are the authorâ€™s assumptions? Are they explicitly or implicitly stated?\n\nIs there any evidence of deliberate bias?\n\nIs there any particular philosophy that influence the authorâ€™s view?\n\nDoes any idea/information interest, confuse or intrigue you?\n\n&amp;#x200B;\n\nThe structure:\n\n \n\nWhat is the structure of the text? What does the structure of the text reveal?\n\nIs the framework clear (e.g. different theories compared with a preferred theory)?\n\nHow is the content developed? Is the material developed historically, in order of importance, in terms of a debate?\n\nHow does the conclusion relate to the rest of the material? Does the conclusion work logically, and is it representative of the findings?\n\n&amp;#x200B;\n\nThe style:\n\n \n\nIn what style has the material been written? Eg. Is it formal, informal, analytical, narrative, persuasive, argumentative, or didactic?\n\nHow do the style and format influence your reaction to the material?\n\n&amp;#x200B;\n\n \n\nReading difficult material\n\n In the course of your studies, it is inevitable that you will come across dense and difficult reading material.\xa0\xa0It is easy to feel overwhelmed. But do not give up!\xa0\xa0The ability to unpack complex and \'dry\' material is an essential skill for academic work. \n\n&amp;#x200B;\n\nBreak your reading into portions:  Donâ€™t feel overwhelmed on the size or complexity of the reading material. Set yourself a goal of reading a section and work hard at just understanding that section. \n\n&amp;#x200B;\n\nSkim the text and get an overview:  Run your eyes over the titles, headings and abstract. Examine any graphs, diagrams, charts. Highlight and read the topic sentence (usually first sentence) of each paragraph. Look for any key words/phrases that are relevant to your topic or what you are wanting to find out. \n\n&amp;#x200B;\n\nFlag what you do not understand to re-read later.  Donâ€™t worry about the parts that you do not understand. A difficult text will always require more than one reading, and a partial understanding will make it easier when you revisit the material a second or third time. \n\n&amp;#x200B;\n\nFind some resources to help you understand.  You might need to find some other material to help you with the background to the reading. If you are having trouble with the vocabulary, find a subject-specific dictionary so you can understand the key words. If the difficult text youâ€™re reading is a seminal one for your topic, you might be able to find some reviews or critical articles analysing your text. \n\n&amp;#x200B;\n\nMake notes while you read.  It often helps to write while you are reading. Writing ideas down in your own words and organising information in a structure, summary or diagram that works for you will help you grasp the material. \n\n&amp;#x200B;\n\nTalk to others.  Talk through difficult material with your fellow students, lecturer or tutor. \n\n&amp;#x200B;\n\nDo not panic!  Put the book or article aside, and read it again the next day. This gives your brain a chance to process. You will be surprised by how much you can pick up a second time around! \n\n&amp;#x200B;\n\nEfficient note-taking strategies.  [https://www.youtube.com/watch?v=XELOxGx\\_ZZg](https://www.youtube.com/watch?v=XELOxGx_ZZg)', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#Studying and #Learning Effectively - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 110, 'hide_score': False, 'media_metadata': {'lm9io3ptz8x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 85, 'x': 108, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a690ccf5a5f542b42df4179730935de73b5e186'}, {'y': 170, 'x': 216, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82d24e253426c7d434c77cede485f5f1d2e54341'}, {'y': 252, 'x': 320, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a0849ab09d8b68b80148c39d6b4e463837ac14e'}, {'y': 505, 'x': 640, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=876b545faf12a7079e3a6cb8fbf771c0b0bf6516'}], 's': {'y': 607, 'x': 768, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;format=png&amp;auto=webp&amp;s=bd823975e2c04aa78391017c3ab7eff261046b70'}, 'id': 'lm9io3ptz8x41'}}, 'name': 't3_geyx3w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': True, 'thumbnail': 'https://b.thumbs.redditmedia.com/pgwfxutWY-G_dXN34KAKVpuEzp1Y6ibnE32S8enW7Tg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588850577.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University, Studying Effectively, 2020  &lt;a href=""https://www.monash.edu/rlo/study-skills/studying-effectively""&gt;https://www.monash.edu/rlo/study-skills/studying-effectively&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;New study patterns&lt;/p&gt;\n\n&lt;p&gt;Lectures and tutorials take up part of the day. You must plan your own long and short term timetables.&lt;/p&gt;\n\n&lt;p&gt;For every one-hour lecture or tutorial, about two hours of private study will be necessary.&lt;/p&gt;\n\n&lt;p&gt;Assignments tend to be long and not frequent. They are usually set many weeks ahead.&lt;/p&gt;\n\n&lt;p&gt;Wide reading is essential. You may be given a reading list, but you should read other sources as well.&lt;/p&gt;\n\n&lt;p&gt;Lecture groups may be large. It is up to you to approach your lecturer or tutor if you are having difficulties.&lt;/p&gt;\n\n&lt;p&gt;You will have to identify and make notes on the main points in lectures and texts.&lt;/p&gt;\n\n&lt;p&gt;You must acknowledge all your sources. To avoid plagiarism, you will need to learn referencing skills.&lt;/p&gt;\n\n&lt;p&gt;You are expected to develop independent thinking by:&lt;/p&gt;\n\n&lt;p&gt;memorising information,&lt;/p&gt;\n\n&lt;p&gt;asking questions,&lt;/p&gt;\n\n&lt;p&gt;examining evidence, and&lt;/p&gt;\n\n&lt;p&gt;thinking critically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Managing your time&lt;/p&gt;\n\n&lt;p&gt;Planning your semester&lt;/p&gt;\n\n&lt;p&gt;It is not uncommon to have several assignments due at the same time, and it is easy to run out of time without careful planning. All assignments due dates are listed in your unit guides, so you can make at least a draft version of this activity at the end of week 1.&lt;/p&gt;\n\n&lt;p&gt;It will help you plan ahead realistically so that you don&amp;#39;t miss any deadlines.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Write the name or code of each unit in the spaces across the top of the planner.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For each unit, fill in the due dates of all tests and assignments in the appropriate week.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For each assignment or test:  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Break down the preparation into a series of steps.&lt;br/&gt;\nb. Consider how much time will be needed for each step and record your time estimates next to the tasks.&lt;br/&gt;\nc. Working backwards from the due date, distribute the tasks in a logical order through the weeks available.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate the average number of hours you need to allocate each week to prepare for assignments and tests:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Add up the total number of hours needed in the first 6 weeks of the semester.&lt;br/&gt;\nb. Divide the sum by the number of weeks.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;p&gt;6 + 16 + 16 = 38 hours needed for assignments and test&lt;/p&gt;\n\n&lt;p&gt;38 hours / 6 weeks = 6 hours and 20 min per week&lt;/p&gt;\n\n&lt;p&gt;Planning your week&lt;/p&gt;\n\n&lt;p&gt;This weekly schedule should be used flexibly; every week will be different. However, this example will give you an idea of what your &amp;#39;average&amp;#39; week should look like. If you can&amp;#39;t fit everything in, you may need to consider reducing your extra-curricular activities during the semester.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Enter all your classes, recording the unit code or name and class type.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Enter any other commitments with set hours, e.g. club meetings, team sports practice, work.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in at least 30 minutes preparation time for each class, recording the unit code or name and class type. Some units will need longer. Classes should be reviewed on the same day as much as possible.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Allocate time to work on assignments or to study for tests. Consider your preferences: short bursts or long stretches?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in your travel time (to and from uni, sports practice, work, etc.) and meal times. Allow time for cooking and cleaning up if necessary. Give yourself a lunch break even on busy days.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in time for regular chores: shopping, washing, cleaning, etc. Be realistic: they take time.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Now add time to relax at the end of the day and socialise with family and friends.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finally, make sure you have some free &amp;#39;catch-up&amp;#39; hours to allow for the unexpected.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reference: Monash University, RLO Study Skills, 2020  &lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking""&gt;https://www.monash.edu/rlo/study-skills/reading-and-note-taking&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Reading and note-taking&lt;/p&gt;\n\n&lt;p&gt;Getting started&lt;/p&gt;\n\n&lt;p&gt;The amount of reading that you are expected to do at university can be daunting.&lt;/p&gt;\n\n&lt;p&gt;However, with some preparation and adjustment to your reading techniques, you will soon be able to read effectively and efficiently.&lt;/p&gt;\n\n&lt;p&gt;Top tip&lt;/p&gt;\n\n&lt;p&gt;The key to managing your reading load is to become an active reader - that is, you need to:&lt;/p&gt;\n\n&lt;p&gt;ask questions about what you need to know,&lt;/p&gt;\n\n&lt;p&gt;select readings that relate to your questions and tasks,&lt;/p&gt;\n\n&lt;p&gt;and read them efficiently.&lt;/p&gt;\n\n&lt;p&gt;Activity: Academic Reading&lt;/p&gt;\n\n&lt;p&gt;Take this quiz to recognise your current level of interpreting academic reading.&lt;/p&gt;\n\n&lt;p&gt;I have to read every book or article on my subject reading list. &lt;/p&gt;\n\n&lt;p&gt;Option 1: True&lt;/p&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;Option 2:  False&lt;/p&gt;\n\n&lt;p&gt;Correct answer.&lt;/p&gt;\n\n&lt;p&gt;Large reading lists can be very intimidating. You are not expected to read absolutely everything on your subject. University study requires a lot of reading within a limited time, so it is important to be selective about what you read. You need to make decisions about what is essential.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can spend many hours reading, and feel as if you are being productive, but actually not get very far with learning, or achieving your study goal. &lt;/p&gt;\n\n&lt;p&gt;Four essential pre-reading tips:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Know your purpose&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Always read with a purpose in mind.&lt;/p&gt;\n\n&lt;p&gt;For example, are you reading to:&lt;/p&gt;\n\n&lt;p&gt;gain an overview of the area of study?&lt;/p&gt;\n\n&lt;p&gt;locate specific information?&lt;/p&gt;\n\n&lt;p&gt;understand difficult concepts?&lt;/p&gt;\n\n&lt;p&gt;revise for exams?&lt;/p&gt;\n\n&lt;p&gt;complete a specific assignment?&lt;/p&gt;\n\n&lt;p&gt;When you know why you are reading, you will be better equipped to decide how you are going to read (ie. Which reading strategies you need to use â€“ more on that later!)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Be selective with what you read and focus on the question or task&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you know what you are looking for (i.e. have a purpose), you can start making some decisions about what is essential reading, and what can be saved for later.&lt;/p&gt;\n\n&lt;p&gt;Ask yourself what you need to find out. Identify:&lt;/p&gt;\n\n&lt;p&gt;Key topic words or phrases&lt;/p&gt;\n\n&lt;p&gt;Questions that you want to answer&lt;/p&gt;\n\n&lt;p&gt;Then actively look for answers and evidence to support these answers.&lt;/p&gt;\n\n&lt;p&gt;Tip:\xa0&lt;/p&gt;\n\n&lt;p&gt;have a copy of your assignment question with you and read with it nearby, so you donâ€™t waste time reading irrelevant material.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Break reading into segments&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Break things down to manageable chunks (e.g. number of pages, articles or chapters). Identify how much time you have and aim to read a certain number of pages, or for a set length of time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Keep track of what you have read&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Always note down where information and ideas have come from. This will also help when you need to reference the text. \xa0&lt;/p&gt;\n\n&lt;p&gt;Keep track of details such as: \xa0\xa0&lt;/p&gt;\n\n&lt;p&gt;Author&lt;/p&gt;\n\n&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;Place of Publication&lt;/p&gt;\n\n&lt;p&gt;Date&lt;/p&gt;\n\n&lt;p&gt;Page numbers&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reference: Monash University, RLO Study Skill, 2020,  &lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text""&gt;https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Approaching and evaluating a text&lt;/p&gt;\n\n&lt;p&gt;As you become more familiar with the text of your specific discipline area (economics, history, engineering, etc.), you will become better at predicting the kinds of questions you might find answers to in the text.&lt;/p&gt;\n\n&lt;p&gt;Being able to make predictions is important. Creating certain expectations will increase your alertness to the text, and you will be better at noticing whether or not your expectations are met.&lt;/p&gt;\n\n&lt;p&gt;The accuracy of your prediction is not usually the most important thing. What is important is recognising what the text does or does not deal with.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Predicting&lt;/p&gt;\n\n&lt;p&gt;Select any of the subject areas below by clicking on the ðŸ“·.&lt;/p&gt;\n\n&lt;p&gt;Look at the title of the book. Then consider what you already know about the topic. Think of some questions to which the book might supply answers.&lt;/p&gt;\n\n&lt;p&gt;Agriculture in semi-arid environments&lt;/p&gt;\n\n&lt;p&gt;... I don&amp;#39;t have any specialised knowledge about agriculture, and especially agriculture in semi-arid environments, but I do have everyday knowledge. I know agriculture needs water, and very often lots of it. Knowing that leads me to ask the following questions:&lt;/p&gt;\n\n&lt;p&gt;How can agriculture survive in such dry places?&lt;/p&gt;\n\n&lt;p&gt;Is it possible to develop agricultural practices that use less water, or is it possible to recycle water, or do farmers in such environments have to pipe in large amounts of water?&lt;/p&gt;\n\n&lt;p&gt;This leads to a further question:&lt;/p&gt;\n\n&lt;p&gt;If farmers have to pipe in water, and perhaps have to use lots of fertilisers to make the semi-arid soils nutritious and agriculturally productive, how can such agriculture be economically viable?&lt;/p&gt;\n\n&lt;p&gt;That makes me wonder:&lt;/p&gt;\n\n&lt;p&gt;Are there other ways of naturally enriching soils through changing farming practices?&lt;/p&gt;\n\n&lt;p&gt;NB. It is possible that none of these questions helps to focus the reader&amp;#39;s attention and increase his/her ability to make sense of the text. As the reader looks for answers, more precise ideas will come to his/her attention, and as a result, more specific questions can then be asked...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Civil Engineering - Soil plasticity&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I know nothing about soil plasticity, and therefore my questions are very basic. I need to know:&lt;/p&gt;\n\n&lt;p&gt;In what ways are soils &amp;#39;plastic&amp;#39;? What does plasticity mean here?&lt;/p&gt;\n\n&lt;p&gt;What is the significance of soil plasticity?&lt;/p&gt;\n\n&lt;p&gt;How does soil plasticity affect engineering design or practices?&lt;/p&gt;\n\n&lt;p&gt;If I were knowledgeable about the subject, I could ask more specific questions based on that knowledge.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Literature -  Madness and sexual politics in the feminist novel&lt;/p&gt;\n\n&lt;p&gt;... I have some background knowledge on this topic. I know for instance that sexual politics is about power relations between the two sexes. But I also know there are different theories about these power relations. So my first question might be:&lt;/p&gt;\n\n&lt;p&gt;Which theorists and theoretical approaches do this text draw on?&lt;/p&gt;\n\n&lt;p&gt;I also know that there is no clear definition of what we mean by a feminist novel. So I might ask:&lt;/p&gt;\n\n&lt;p&gt;How does this writer define a feminist novel? What novels does s/he discuss?&lt;/p&gt;\n\n&lt;p&gt;I know madness is usually the concern of medicine, but I also know that madness is of interest in humanities subjects. Some people argue that those who are mad do not necessarily have anything wrong with them, but that they are categorised as mad by society because they do not conform in certain ways. I suspect the writer of this article takes up this view of madness. I predict therefore that s/he will look at how women in novels become mad, and that s/he will link this to their resistance or nonconformity to struggle with male-dominated practices. Therefore I might ask:&lt;/p&gt;\n\n&lt;p&gt;What is meant by madness in this text?&lt;/p&gt;\n\n&lt;p&gt;Is female madness the result of male-oriented social relations? ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Education -  Gender issues in physics education&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I know there has been a lot of concern about girls taking an interest in physics at school. I also know that while some people think this is because girls are naturally not interested, others argue it is because physics is seen as a masculine subject, and so girls do not become interested, even though they could actually do as well as boys. I suspect this article adopts this last view and therefore sees the way we create masculinity and femininity in our society (gender issues) as a central problem in physics education. Therefore I might ask:&lt;/p&gt;\n\n&lt;p&gt;Is the gender issue discussed here the one I have predicted - that girls do not take up physics because it is not feminine? Or is it another issue?&lt;/p&gt;\n\n&lt;p&gt;What exactly are the issues to be discussed? (Is there a list of them?)&lt;/p&gt;\n\n&lt;p&gt;Does the author only describe the issues, or does s/he also suggest ways of overcoming the gender problems?&lt;/p&gt;\n\n&lt;p&gt;If s/he offers solutions, do these lie in changing teaching methodologies, or changing the physics curriculum, or something else?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Philosophy -  Is it good to make people happy?&lt;/p&gt;\n\n&lt;p&gt;... This looks like a strange topic. Everybody knows it is good to make people happy! But I know that philosophy wants to understand precisely why it might be good, and ethics wants to distinguish between good and bad actions. So a definition of good seems essential. Therefore, my first questions might be:&lt;/p&gt;\n\n&lt;p&gt;What is meant by good in this context? Can I quickly find a definition?&lt;/p&gt;\n\n&lt;p&gt;Why is it good to make people happy, and not merely nice?&lt;/p&gt;\n\n&lt;p&gt;I might also want to know about the link between goodness and happiness. If happiness is good, then presumably it should always be an aim of ours. But what happens if happiness is in conflict with justice? Pursuing justice can make some people very unhappy. So I might also ask:&lt;/p&gt;\n\n&lt;p&gt;What happens if the principle of happiness conflicts with another principle, such as justice? If happiness gives way to justice, does this mean it is not good, but merely desirable?&lt;/p&gt;\n\n&lt;p&gt;The more we think about this topic, the more questions will come to mind, and so we can get deeper and deeper into the text ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Psychology -  A theory of cultural values and some implications for work&lt;/p&gt;\n\n&lt;p&gt;What is the theory of cultural values?&lt;/p&gt;\n\n&lt;p&gt;How can theory have implications for work?&lt;/p&gt;\n\n&lt;p&gt;These questions immediately come to my mind. A further question follows:&lt;/p&gt;\n\n&lt;p&gt;What kind of theory does this article present?&lt;/p&gt;\n\n&lt;p&gt;It seems to me that cultural values include many things. Another question comes to mind:&lt;/p&gt;\n\n&lt;p&gt;What cultural values exactly will this text discuss?&lt;/p&gt;\n\n&lt;p&gt;This leads to further questions:&lt;/p&gt;\n\n&lt;p&gt;Is this text talking about cultural values that have a direct bearing on work?&lt;/p&gt;\n\n&lt;p&gt;If so, what aspects of work? Does it discuss the ways cultural values affect attitudes to work, or work efficiency, or the kinds of works that people will want to do?&lt;/p&gt;\n\n&lt;p&gt;I find very quickly that I have a lot of questions. This title seems so general it is hard to predict what it is likely to be about. Therefore my questions are more to do with trying to define the scope of this text. Once I have some sense of its scope, then I might be able to make predictions and ask questions about the content ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Strategies for effective reading&lt;/p&gt;\n\n&lt;p&gt;There are different strategies you can apply to your reading, depending on your goal. You might need to choose one or a few of these strategies for each text that you read.&lt;/p&gt;\n\n&lt;p&gt;In this section:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#previewing""&gt;Previewing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#skimming""&gt;Skimming&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#scanning""&gt;Scanning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#detailed_reading""&gt;Detailed reading&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Previewing&lt;/p&gt;\n\n&lt;p&gt;What is previewing?&lt;/p&gt;\n\n&lt;p&gt;Previewing is getting a sense of what&amp;#39;s in a given piece of work without reading the body of the text&lt;/p&gt;\n\n&lt;p&gt;When should you preview?&lt;/p&gt;\n\n&lt;p&gt;Previewing helps you decide whether a book or article is useful for your purpose. \xa0It gives you a general sense of the content so you can see if you want to read in more detail, and it helps you locate sections that you need to read, and sections you don&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;How to preview:&lt;/p&gt;\n\n&lt;p&gt;Read the title and author details&lt;/p&gt;\n\n&lt;p&gt;Read the abstract (if available)&lt;/p&gt;\n\n&lt;p&gt;Read main headings, chapter summaries, and anything that &amp;#39;jumps out&amp;#39; at you&lt;/p&gt;\n\n&lt;p&gt;Look at any diagrams, graphs, tables. \xa0These usually summarise the content of large written paragraphs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is skimming?&lt;/p&gt;\n\n&lt;p&gt;Skimming is reading small amounts from throughout the text.\xa0It is different from previewing because you&amp;#39;re reading the body of the text. The chief benefit of skimming is in being able to pick up the key ideas quickly. \xa0&lt;/p&gt;\n\n&lt;p&gt;How to skim&lt;/p&gt;\n\n&lt;p&gt;If the introduction is short read it in full. If long, read the first sentence of each paragraph. Then read the first sentence of each subsequent paragraph, or until you find the topic sentence (usually the first or second sentence).\xa0This will give you an overview of the content of the passage.\xa0It can also be useful to read the concluding paragraph in full.&lt;/p&gt;\n\n&lt;p&gt;For a report or research paper,\xa0first of all, read the Abstract. Then look over the section headings and subheadings and any figures or tables before skimming the text. It may also be useful to read the Conclusion.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t get bogged down. \xa0This is a fast process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you skim?&lt;/p&gt;\n\n&lt;p&gt;When you want to get an overview or the gist of a text. This can help you decide whether or not to read the full text.&lt;/p&gt;\n\n&lt;p&gt;Skimming adds to the information that you picked up in previewing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Test your skimming skills&lt;/p&gt;\n\n&lt;p&gt;In the excerpt from a book chapter below, the first paragraph is presented in full. In the following paragraphs (2-6), only the topic sentences appear. You should still be able to get the gist of the full passage.&lt;/p&gt;\n\n&lt;p&gt;Read the first paragraph and the topic sentence of each subsequent paragraph.&lt;/p&gt;\n\n&lt;p&gt;You should find you can easily answer the three questions which follow.&lt;/p&gt;\n\n&lt;p&gt;Use the blue arrows to move through the quiz.&lt;/p&gt;\n\n&lt;p&gt;Chapter Twelve: Trade routes and rituals&lt;br/&gt;\n1. Trade between distant people is often seen as a mark of a more advanced economic life. If this insight is valid, many groups of aboriginals must have been far from backward because their raw materials and manufactures were traded to people hundreds of miles away. It is probable that every tribe in Australia traded with its neighbours, and a few commodities were involved in such a sequence of transactions that they crossed from the tropical coast almost to the Southern Ocean.&lt;br/&gt;\n2. Pearl shell travelled further perhaps than any other Item. (13 lines deleted)&lt;br/&gt;\n3. In eastern Australia, the axe-stone also moved over a wide area. (6 lines deleted)&lt;br/&gt;\n4. A quarry which provided stone fit for stronger, sharper axes was likely to supply trade routes stretching in every direction. (6 lines deleted)&lt;br/&gt;\n5. As the written records were thin in tracing the trade-in stone axes from the Tamworth district; other ways of reconstructing the extent of the trade were needed. The petrological analysis was one promising technique.&lt;br/&gt;\n6. This kind of archaeological jigsaw - the exact matching of axe and quarry - can be solved only when every likely source of stone has been discovered and described. (3 lines deleted) â€¦ axes had gone overland through a chain of tribal territories to Cobar, Bourke, Wilcannia, and other points on the plains as remote as 500 miles from the home quarries. (2 lines deleted)&lt;br/&gt;\n(Blainey, G. (1975). Triumph of the nomads: A History of Ancient Australia. South Melbourne, Vic.: Macmillan. p. 203-204.)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the topic of the passage? &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The distribution of axe stone in eastern Australia.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;This topic was used to illustrate the topic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Aboriginal trade routes.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;The focus is not so much on the routes themselves as their far-reaching range.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; Trade among indigenous populations.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;This is true, but the scope in the passage is limited to Australia.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; The extent of trade among Australian aborigines &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What trade items were discussed?  Axe Stone&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is scanning?&lt;/p&gt;\n\n&lt;p&gt;You skim-read material to get the general picture. \xa0&lt;/p&gt;\n\n&lt;p&gt;You scan when looking for specific information.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you scan?&lt;/p&gt;\n\n&lt;p&gt;You may need to find specific details on a topic for an assignment or a task that your lecturer has set. \xa0There is little point in skimming a whole book for this purpose. \xa0You should scan the text for words related to the topic. \xa0You can run your eyes down the page looking for these expressions - in chapter headings or sub-headings, or in the text itself.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Test your scanning skills&lt;/p&gt;\n\n&lt;p&gt;You need to find the definition of â€˜postmodernityâ€™ for an assignment.&lt;/p&gt;\n\n&lt;p&gt;Your first step might be to look in the index for the word, and see if it can direct you to some definitions. However, if the whole book is about postmodernism, then you might have too many references to check. Another obvious place to look is in the Introduction.&lt;/p&gt;\n\n&lt;p&gt;Below is a section from the Introduction to Intimations of postmodernity.&lt;/p&gt;\n\n&lt;p&gt;Scan through to see if you can locate where in the text the author defines what he means by the term.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd823975e2c04aa78391017c3ab7eff261046b70""&gt; Adapted from Bauman, Z. (1992). Intimations of postmodernity. London: Routledge &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Which sentence in the text does the author define postmodernity?&lt;/p&gt;\n\n&lt;p&gt;7 Wrong answer.Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(&amp;#39;perhaps more than anything else.&amp;#39;)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;11 Correct answer. RIGHT ANSWER! In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(&amp;#39;perhaps more than anything else.&amp;#39;)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;3 Wrong answer. Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(perhaps more than anything else)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is a detailed reading?&lt;/p&gt;\n\n&lt;p&gt;Detailed reading is intensive reading that focuses on the important parts - key chapters, books, poems, pages. This is when you are likely to take detailed notes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you read in detail?&lt;/p&gt;\n\n&lt;p&gt;Intensive reading is usually the final step - after you&amp;#39;ve previewed, scanned and skimmed when you know that it is worth reading the full text.\xa0&lt;/p&gt;\n\n&lt;p&gt;In every unit of study, there will be key sources or texts that you will need to read carefully. \xa0&lt;/p&gt;\n\n&lt;p&gt;Expect to read these more than once and to take notes on important material.&lt;/p&gt;\n\n&lt;p&gt;You will often need to take notes while reading. This video on &lt;a href=""https://www.youtube.com/watch?v=XELOxGx_ZZg""&gt;Efficient note taking strategies&lt;/a&gt; will help you take better notes. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Developing your critical reading skills&lt;/p&gt;\n\n&lt;p&gt;In university study you will frequently hear your lecturer or tutor encouraging you to take a critical approach to reading and researching. A critical approach to reading does not mean criticising or &amp;#39;finding fault&amp;#39;. It means asking yourself why this particular author has written this particular text, and evaluating their strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;You can ask yourself many questions about the text - the more the better. These are shaped by what you already understand about the text, and what you need to get from it.&lt;/p&gt;\n\n&lt;p&gt;ðŸ“·&lt;/p&gt;\n\n&lt;p&gt;Consider the following&lt;/p&gt;\n\n&lt;p&gt;There are several questions to consider when approaching your reading. The following examples will give you a few ideas on the types of questions you should be asking when reading:&lt;/p&gt;\n\n&lt;p&gt;The author:&lt;/p&gt;\n\n&lt;p&gt;What is the author&amp;#39;s main theme or point?&lt;/p&gt;\n\n&lt;p&gt;Is the author making any particular assumptions? On what authority?&lt;/p&gt;\n\n&lt;p&gt;Which aspects does the author focus on and why? Does he/she omit any important points?&lt;/p&gt;\n\n&lt;p&gt;Are there additional clues about the author&amp;#39;s attitude or stance (e.g. from his/her position/qualifications, country of origin,\xa0the text&amp;#39;s date of publication or publisher,\xa0the type of text)?&lt;/p&gt;\n\n&lt;p&gt;What theoretical perspective has the author taken (e.g. which writers does she/he cite most often or most approvingly)?&lt;/p&gt;\n\n&lt;p&gt;What basis or criteria is the author using to make judgements?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The content:&lt;/p&gt;\n\n&lt;p&gt;What is the main point, thesis or argument?&lt;/p&gt;\n\n&lt;p&gt;What is the text really about (i.e. special agenda, underlying themes)?&lt;/p&gt;\n\n&lt;p&gt;What explanations or supporting evidence are drawn on? Do they seem adequate, completely relevant?&lt;/p&gt;\n\n&lt;p&gt;Is all the factual information correct as far as you know?&lt;/p&gt;\n\n&lt;p&gt;What aspect of the topic has the author chosen to focus on? What has s/he omitted?&lt;/p&gt;\n\n&lt;p&gt;What are the authorâ€™s assumptions? Are they explicitly or implicitly stated?&lt;/p&gt;\n\n&lt;p&gt;Is there any evidence of deliberate bias?&lt;/p&gt;\n\n&lt;p&gt;Is there any particular philosophy that influence the authorâ€™s view?&lt;/p&gt;\n\n&lt;p&gt;Does any idea/information interest, confuse or intrigue you?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The structure:&lt;/p&gt;\n\n&lt;p&gt;What is the structure of the text? What does the structure of the text reveal?&lt;/p&gt;\n\n&lt;p&gt;Is the framework clear (e.g. different theories compared with a preferred theory)?&lt;/p&gt;\n\n&lt;p&gt;How is the content developed? Is the material developed historically, in order of importance, in terms of a debate?&lt;/p&gt;\n\n&lt;p&gt;How does the conclusion relate to the rest of the material? Does the conclusion work logically, and is it representative of the findings?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The style:&lt;/p&gt;\n\n&lt;p&gt;In what style has the material been written? Eg. Is it formal, informal, analytical, narrative, persuasive, argumentative, or didactic?&lt;/p&gt;\n\n&lt;p&gt;How do the style and format influence your reaction to the material?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reading difficult material&lt;/p&gt;\n\n&lt;p&gt;In the course of your studies, it is inevitable that you will come across dense and difficult reading material.\xa0\xa0It is easy to feel overwhelmed. But do not give up!\xa0\xa0The ability to unpack complex and &amp;#39;dry&amp;#39; material is an essential skill for academic work. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Break your reading into portions:  Donâ€™t feel overwhelmed on the size or complexity of the reading material. Set yourself a goal of reading a section and work hard at just understanding that section. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Skim the text and get an overview:  Run your eyes over the titles, headings and abstract. Examine any graphs, diagrams, charts. Highlight and read the topic sentence (usually first sentence) of each paragraph. Look for any key words/phrases that are relevant to your topic or what you are wanting to find out. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Flag what you do not understand to re-read later.  Donâ€™t worry about the parts that you do not understand. A difficult text will always require more than one reading, and a partial understanding will make it easier when you revisit the material a second or third time. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Find some resources to help you understand.  You might need to find some other material to help you with the background to the reading. If you are having trouble with the vocabulary, find a subject-specific dictionary so you can understand the key words. If the difficult text youâ€™re reading is a seminal one for your topic, you might be able to find some reviews or critical articles analysing your text. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Make notes while you read.  It often helps to write while you are reading. Writing ideas down in your own words and organising information in a structure, summary or diagram that works for you will help you grasp the material. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Talk to others.  Talk through difficult material with your fellow students, lecturer or tutor. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do not panic!  Put the book or article aside, and read it again the next day. This gives your brain a chance to process. You will be surprised by how much you can pick up a second time around! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Efficient note-taking strategies.  &lt;a href=""https://www.youtube.com/watch?v=XELOxGx_ZZg""&gt;https://www.youtube.com/watch?v=XELOxGx_ZZg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'geyx3w', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/', 'subreddit_subscribers': 33, 'created_utc': 1588821777.0, 'num_crossposts': 43, 'media': None, 'is_video': False}]",t3_geyx3w,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying #learning - How to Learn Effectively at University,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfh2kh,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,True,default,False,,[],{},,,False,,1588920520.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfh2kh,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh2kh/studying_learning_how_to_learn_effectively_at/,all_ads,False,/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/,155203,1588891720.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University RLO Study Skills, 2020,  [https://www.monash.edu/rlo/study-skills/learning-at-university](https://www.monash.edu/rlo/study-skills/learning-at-university)\n\nMaking useful study notes\n\nWhy do you need to make notes?\n\nStudy notes are a way for you to summarise and synthesise the material that you are learning or collecting for a written assignment. A key point for making notes is that you need to re-write the material\xa0in your own words.\n\nTypes of notes: Flowcharts or process notes\n\nFlowcharts and similar process notes can be useful when making notes for certain processes or methods.\n\nExample 1: Historical timelines\n\nExample 2: Steps in a laboratory method\n\nExample 3: Mind Maps - For more information, visit [Brainstorming: Mind mapping](https://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/brainstorming-and-mind-mapping)\n\nTypes of notes: Cornell notes\n\nTo make Cornell notes, divide the page up into three areas:\n\nA larger notes column (on the right of the page)\n\nA smaller recall column (on the left of the page)\n\nA summary section (at the bottom of the page)\n\nNotes column: In the right-hand column, write down the bulk of the notes from your lectures or study materials as you are reading your textbook or online materials.\n\nRecall column: After you finish a chapter, section or study session, note down any questions, keywords and main ideas in the recall column.\n\nSummary section: At the bottom of the page, the summary section is used to summarise all the notes from the page, to make sense of the material and make revision easier.\n\nUsing your notes\n\nFor the study, revisit your notes:\n\nthe day after you wrote them,\n\nthe following week,\n\nat least once the following month.\n\nThis helps with your memory, meaning you will be better prepared for your exams!\n\nTip: If you are making research notes or using multiple different sources, also note down the reference details!\n\n&amp;#x200B;\n\nParticipating in tutorials\n\nTutorials are useful for:\n\nconsolidating your understanding of a topic/reading/lecture.\n\nExamining a topic critically by\n\nasking questions\n\nreflecting on the material\n\nmaking links to related topics/ideas.\n\nAttendance\n\nRegular attendance is advisable. Note that for some classes attendance is compulsory, while for others there can be a minimum number of tutorials that a student must attend. In some subjects, marks are awarded for student participation. Check with your tutor or in your unit guide.\n\nExpectations\n\nYou will be expected to:\n\nanswer questions from the lecturer, tutor and/or other students\n\ncontribute to the discussion by sharing and comparing ideas\n\ntake ownership of your learning by seeking clarification of any material you do not fully understanding.\n\nPreparation\n\nIt is essential to complete all set tasks, such as the required reading or set questions, before your tutorial so that you can participate fully in the discussion. Reflect on unit learning objectives and think of questions arising from the reading or the lecture that you can ask during the tutorial.\n\nTo contribute:\n\nThink of several questions/examples/comments you would like to make.\n\nLook for pauses during the tutorial session to enable you to enter the discussion.\n\nIndicate that you want to speak by making eye contact with the tutor or by taking a more alert body posture.\n\nYou can enter the discussion by:\n\nagreeing with what someone has said and adding your own thoughts\n\nE.g. ""I agree with what \\[NAME\\] said about \\_\\_\\_. In my viewâ€¦""\n\ndisagreeing with what someone has said and providing reasons for your position\n\nE.g. ""I think \\[NAME\\] made an interesting point; however, in my opinionâ€¦""\n\nraising new points/questions.\n\nE.g. ""I think that one aspect we have not considered isâ€¦\'\n\nlinking the discussion back to the weekly readings\n\nE.g. ""According to \\[AUTHOR\\]â€¦"" or ""In her article, \\[AUTHOR\\] argued thatâ€¦""\n\nAfter the tutorial:\n\nContinue the discussion with your classmates.\n\nContact the tutor to clarify any important points you didn\'t understand.\n\nFinish any unfinished work.\n\nRevise. Sit down and consolidate the concepts that you have learned.\n\nEnter questions in your notebook; write answers.\n\nCheck terms/ jargon.\n\n&amp;#x200B;\n\nLearning in practical environments\n\nThroughout your studies, you will perform exercises in practical environments such as laboratories, site visits or fieldwork.\n\nPractical exercises are designed to help you:\n\napply your theoretical understanding in a tactile way\n\nexpand your theoretical understanding\n\nuse techniques and equipment safely and correctly\n\ndevelop skills such as time management.\n\nMost practical exercises consist of three major parts.\n\nPreliminary work or â€˜pre-labsâ€™, which involve preparing for the exercise.\n\nThe exercise itself, where you will perform the activity under the guidance of an educator.\n\nA post-exercise submission to be completed within the session or at a later date. These can take the form of work such as reports or presentations.\n\nYou will be provided with detailed guidelines for each exercise you undertake. Below are some general tips on how to prepare for your exercise, minimise stress during your session, and complete any submissions.\n\nBefore the exercise:\n\nRead the materials and procedures for your exercise 2 to 3 days prior to your session (highlight key concepts, procedures and measurements).\n\nAsk yourself: what am I trying to determine in this exercise?\n\nIf a method is provided summarise the key steps in a dedicated notebook.\n\nComplete and pass any pre-exercise activities such as quizzes, risk assessments or additional reading.\n\nDuring the exercise:\n\nArrive on time! Educators generally provide context, useful tips and safety warnings at the start of the session.\n\nAsk your educator for clarification of the material and procedures.\n\nDiscuss your understanding and predictions with other students and the educator.\n\nTake detailed notes of procedures, observations and measurements in a dedicated notebook.\n\nAfter the exercise:\n\nConsult the assessment guide and/or marking rubric for any required submissions.\n\nClarify points of confusion with your educators and/or other students (unit forums are an ideal place).\n\nComplete any required submissions within the time allocated.\n\n&amp;#x200B;\n\nA guide to group work\n\nWhy work in a group or a team?\n\nDevelops excellent graduate attributes:\n\ndecision making and problem-solving skills\n\nproject management and organisational skills\n\ncommunication and conflict resolution skills.\n\nThe process\n\n1. Build a strong foundation\n\nGet to know your team.\n\nDiscuss strengths and weaknesses.\n\nMeet early and set rules.\n\nAgree on the aims, scope, and quality of the project.\n\n2. Get organised\n\nDecide on a method of group communication.\n\nAgree on who will do what.\n\nSet early deadlines.\n\nPlan on how to present the project as a unified work.\n\n3. Hold productive meetings\n\nMake sure everyone knows the goal of the meeting, and what to bring.\n\nHave a leader to keep the meeting on track.\n\nHave a note-taker to record decisions.\n\n4. Maintain relationships\n\nResolve problems as a group. Don\'t exclude others.\n\nAddress concerns as soon as they appear.\n\nRenegotiate as needed.\n\nUnderstand the other person\'s point of view.\n\nLeadership\n\nMeans\n\nhelping the group work efficiently,\n\nmonitoring progress,\n\nknowing when a team member needs help, and\n\nkeeping the group motivated.\n\nDoes NOT mean\n\ndoing all the work,\n\nmaking decisions and telling others what to do, or\n\nbeing solely responsible for the success or failure of the project.\n\nEveryone is responsible for the success or failure of the project, not individual team members.\n\nDifficulties\n\nYou can\'t work as quickly in a group as you can by yourself.\n\nGroup/team members may have conflicting ideas or viewpoints.\n\nGroup/team members may not contribute equally.\n\nFor successful group and teamwork\n\nBe patient, demonstrate good communication skills and be committed.\n\nFocus on the process rather than just the end product.\n\nCollaborating and communicating online\n\nIntroduction\n\nDuring your university studies:\n\nYou may need to complete group assessment tasks.\xa0Using\xa0online meeting and collaboration tools can make this process easier, particularly if you are not able to meet face-to-face.\n\nYou may be asked to comment on online discussion boards (including Moodle forums) as part of your unit assessment tasks.\n\nOnline communication and collaboration tools are useful during your studies as they can help you to work on group projects, share ideas and information, and provide encouragement and feedback on each otherâ€™s work.\n\nCommunicating and collaborating online are also important skills in the workplace, as many workplaces require staff to work in groups on shared tasks, often across multiple locations. The ability to effectively use online meeting and collaboration tools is a key employability skill.\n\nYou may already be using social networking tools such as Facebook to work on group projects.\xa0However, there are some key differences between using social networking tools for informal activities and using these tools informal study (e.g. assessment) or work activities.\xa0These differences include the language used and the structure of the interactions in an online meeting.\xa0For example, an online meeting for a group assessment task or a workplace shared task will often use an agenda and have a person designated as the meeting chair.\n\nThis module includes tips for effectively communicating and collaborating when:\n\n[participating in online meetings](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#meetings)\n\n[using online discussion boards](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#discussion), for example, Moodle forums\n\n[using online collaboration tools](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#tools).\n\nCommunicating online - key terms\n\n[Synchronous and asynchronous communication](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Synchronous_and_asynchronous_communication-1)\n\n[Verbal and non-verbal communication](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Verbal_and_non-verbal_communication-2)\n\nWithout non-verbal communication cues, it can be very difficult for your readers to detect sarcasm and other hidden meanings, and your words may be interpreted as being more unfriendly or bossy than intended. Use the following activity to explore some ways of communicating online effectively.\n\n You disagree with the statements made in a discussion forum post from another student in your class. \xa0Which of the following responses might be better? \n\n1.  One point that you might like to consider is â€¦, which was included in this weekâ€™s reading.\n2. No, you are wrong. \xa0You need to read this weekâ€™s required reading to see why you are wrong..\n\nDiscussion on 1:  Good choice! By using this type of response, you are indicating that you do not fully agree with the post, but in a way that is not direct and bossy. \xa0You are also giving the other student some useful information by referring directly to the point raised in the reading. \n\nDiscussion on 2:  This is probably not the best choice. \xa0This response uses direct language and may be interpreted as unfriendly and bossy. \xa0The student who you are responding to may ignore your feedback as it doesnâ€™t give them much useful information (other than that they need to do the reading).\xa0 Your feedback is more likely to be heeded when you use cautious language, such as â€œOne point that you might like to consider is â€¦, which was included in this weekâ€™s readingâ€. \xa0 \n\nOnline symbols can be used to express emotion and can be particularly useful when working in less formal situations such as collaborating on online projects with other students. \xa0 \n\nEmoji icons can be useful in expressing emotions when communicating online. However, emoji may not be appropriate in more formal settings.\n\n&amp;#x200B;\n\nParticipating in online meetings\n\nOnline meetings and virtual team projects are a common industry practice.\xa0They are used to communicate information, collect data, generate ideas, build teams, solve problems and make decisions.\n\nIn a university setting, group projects are a common form of assessment in many course units.\xa0They are collaborative learning opportunities that require students to evaluate ideas, analyse and derive meaning from information, and produce work cooperatively.\xa0Effective online meetings can help you to share information, discuss ideas, solve problems and produce work during your studies, as well as in the workplace.\n\n&amp;#x200B;\n\nUsing online discussion boards\n\nA discussion board, such as a Moodle forum, is different to an online meeting in that it is a communication tool that enables participants to post messages and to reply to others\' messages asynchronously (i.e. not at the same time).\n\nThe discussions on online discussion boards typically last for a longer period of time (days, weeks or months) and allow participants time to think about what they are going to contribute.\xa0Some units will include discussion board contributions as assessment tasks.\xa0In these assessment tasks, your lecturer will expect you to provide reflective, detailed responses to the topic and to other students\' posts. Often, to obtain high marks for these assessment types, you need to demonstrate your engagement with and understanding of the unit materials such as readings and lecture content.\n\nThe purpose of this section is to explore some ways of effectively using online discussion boards:\n\n[https://youtu.be/gfS4xgGmzbE](https://youtu.be/gfS4xgGmzbE)\n\nUsing online collaboration tools\n\nDigital tools\n\nThere are a large number of digital tools that can be used during your time at university. There is software available to be purchased or downloaded, including:\n\npackages in the Microsoft suite for written communication\n\nSkype and Zoom for online face-to-face communication and collaboration\n\nplanning tools such as Catme, Dapulse.com and Microsoft Project\n\nsmartphone applications.\n\nSome tools are freely available while others must be purchased.\n\nMonash\xa0University provides digital resources using the Google platform to create, store, share, collaborate and present different kinds of documents. This section will discuss how you can use these tools for group work, planning and documenting your projects.\n\nUsing digital tools for online communication\n\nThe digital tools shown below enable various types of online communication and collaboration including the sharing of files, the creation of websites for group work, and online meetings.\n\nHow to access these Google digital tools at Monash University\n\nMonash students and staff can access these via [my.monash](https://my.monash/) website.\n\n1. Choose an internet browser.\n2. Type my.monash in the browser window or search for my.monash\n\n3.\xa0Type your Monash username and password to access content from my.monash website.\n\n4.\xa0Click to open either the Email or Calendar tiles.\n\n5.\xa0To access other Google products, click on the small boxes on the top right-hand side of the screen\n\n6.\xa0Other Google products will be displayed. Select the digital tool you wish to use. Click More to see more of the digital tools.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying #learning - How to Learn Effectively at University', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gevwrs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': 1588813442.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588839123.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University RLO Study Skills, 2020,  &lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university""&gt;https://www.monash.edu/rlo/study-skills/learning-at-university&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Making useful study notes&lt;/p&gt;\n\n&lt;p&gt;Why do you need to make notes?&lt;/p&gt;\n\n&lt;p&gt;Study notes are a way for you to summarise and synthesise the material that you are learning or collecting for a written assignment. A key point for making notes is that you need to re-write the material\xa0in your own words.&lt;/p&gt;\n\n&lt;p&gt;Types of notes: Flowcharts or process notes&lt;/p&gt;\n\n&lt;p&gt;Flowcharts and similar process notes can be useful when making notes for certain processes or methods.&lt;/p&gt;\n\n&lt;p&gt;Example 1: Historical timelines&lt;/p&gt;\n\n&lt;p&gt;Example 2: Steps in a laboratory method&lt;/p&gt;\n\n&lt;p&gt;Example 3: Mind Maps - For more information, visit &lt;a href=""https://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/brainstorming-and-mind-mapping""&gt;Brainstorming: Mind mapping&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Types of notes: Cornell notes&lt;/p&gt;\n\n&lt;p&gt;To make Cornell notes, divide the page up into three areas:&lt;/p&gt;\n\n&lt;p&gt;A larger notes column (on the right of the page)&lt;/p&gt;\n\n&lt;p&gt;A smaller recall column (on the left of the page)&lt;/p&gt;\n\n&lt;p&gt;A summary section (at the bottom of the page)&lt;/p&gt;\n\n&lt;p&gt;Notes column: In the right-hand column, write down the bulk of the notes from your lectures or study materials as you are reading your textbook or online materials.&lt;/p&gt;\n\n&lt;p&gt;Recall column: After you finish a chapter, section or study session, note down any questions, keywords and main ideas in the recall column.&lt;/p&gt;\n\n&lt;p&gt;Summary section: At the bottom of the page, the summary section is used to summarise all the notes from the page, to make sense of the material and make revision easier.&lt;/p&gt;\n\n&lt;p&gt;Using your notes&lt;/p&gt;\n\n&lt;p&gt;For the study, revisit your notes:&lt;/p&gt;\n\n&lt;p&gt;the day after you wrote them,&lt;/p&gt;\n\n&lt;p&gt;the following week,&lt;/p&gt;\n\n&lt;p&gt;at least once the following month.&lt;/p&gt;\n\n&lt;p&gt;This helps with your memory, meaning you will be better prepared for your exams!&lt;/p&gt;\n\n&lt;p&gt;Tip: If you are making research notes or using multiple different sources, also note down the reference details!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Participating in tutorials&lt;/p&gt;\n\n&lt;p&gt;Tutorials are useful for:&lt;/p&gt;\n\n&lt;p&gt;consolidating your understanding of a topic/reading/lecture.&lt;/p&gt;\n\n&lt;p&gt;Examining a topic critically by&lt;/p&gt;\n\n&lt;p&gt;asking questions&lt;/p&gt;\n\n&lt;p&gt;reflecting on the material&lt;/p&gt;\n\n&lt;p&gt;making links to related topics/ideas.&lt;/p&gt;\n\n&lt;p&gt;Attendance&lt;/p&gt;\n\n&lt;p&gt;Regular attendance is advisable. Note that for some classes attendance is compulsory, while for others there can be a minimum number of tutorials that a student must attend. In some subjects, marks are awarded for student participation. Check with your tutor or in your unit guide.&lt;/p&gt;\n\n&lt;p&gt;Expectations&lt;/p&gt;\n\n&lt;p&gt;You will be expected to:&lt;/p&gt;\n\n&lt;p&gt;answer questions from the lecturer, tutor and/or other students&lt;/p&gt;\n\n&lt;p&gt;contribute to the discussion by sharing and comparing ideas&lt;/p&gt;\n\n&lt;p&gt;take ownership of your learning by seeking clarification of any material you do not fully understanding.&lt;/p&gt;\n\n&lt;p&gt;Preparation&lt;/p&gt;\n\n&lt;p&gt;It is essential to complete all set tasks, such as the required reading or set questions, before your tutorial so that you can participate fully in the discussion. Reflect on unit learning objectives and think of questions arising from the reading or the lecture that you can ask during the tutorial.&lt;/p&gt;\n\n&lt;p&gt;To contribute:&lt;/p&gt;\n\n&lt;p&gt;Think of several questions/examples/comments you would like to make.&lt;/p&gt;\n\n&lt;p&gt;Look for pauses during the tutorial session to enable you to enter the discussion.&lt;/p&gt;\n\n&lt;p&gt;Indicate that you want to speak by making eye contact with the tutor or by taking a more alert body posture.&lt;/p&gt;\n\n&lt;p&gt;You can enter the discussion by:&lt;/p&gt;\n\n&lt;p&gt;agreeing with what someone has said and adding your own thoughts&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I agree with what [NAME] said about ___. In my viewâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;disagreeing with what someone has said and providing reasons for your position&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I think [NAME] made an interesting point; however, in my opinionâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;raising new points/questions.&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I think that one aspect we have not considered isâ€¦&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;linking the discussion back to the weekly readings&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;According to [AUTHOR]â€¦&amp;quot; or &amp;quot;In her article, [AUTHOR] argued thatâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;After the tutorial:&lt;/p&gt;\n\n&lt;p&gt;Continue the discussion with your classmates.&lt;/p&gt;\n\n&lt;p&gt;Contact the tutor to clarify any important points you didn&amp;#39;t understand.&lt;/p&gt;\n\n&lt;p&gt;Finish any unfinished work.&lt;/p&gt;\n\n&lt;p&gt;Revise. Sit down and consolidate the concepts that you have learned.&lt;/p&gt;\n\n&lt;p&gt;Enter questions in your notebook; write answers.&lt;/p&gt;\n\n&lt;p&gt;Check terms/ jargon.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Learning in practical environments&lt;/p&gt;\n\n&lt;p&gt;Throughout your studies, you will perform exercises in practical environments such as laboratories, site visits or fieldwork.&lt;/p&gt;\n\n&lt;p&gt;Practical exercises are designed to help you:&lt;/p&gt;\n\n&lt;p&gt;apply your theoretical understanding in a tactile way&lt;/p&gt;\n\n&lt;p&gt;expand your theoretical understanding&lt;/p&gt;\n\n&lt;p&gt;use techniques and equipment safely and correctly&lt;/p&gt;\n\n&lt;p&gt;develop skills such as time management.&lt;/p&gt;\n\n&lt;p&gt;Most practical exercises consist of three major parts.&lt;/p&gt;\n\n&lt;p&gt;Preliminary work or â€˜pre-labsâ€™, which involve preparing for the exercise.&lt;/p&gt;\n\n&lt;p&gt;The exercise itself, where you will perform the activity under the guidance of an educator.&lt;/p&gt;\n\n&lt;p&gt;A post-exercise submission to be completed within the session or at a later date. These can take the form of work such as reports or presentations.&lt;/p&gt;\n\n&lt;p&gt;You will be provided with detailed guidelines for each exercise you undertake. Below are some general tips on how to prepare for your exercise, minimise stress during your session, and complete any submissions.&lt;/p&gt;\n\n&lt;p&gt;Before the exercise:&lt;/p&gt;\n\n&lt;p&gt;Read the materials and procedures for your exercise 2 to 3 days prior to your session (highlight key concepts, procedures and measurements).&lt;/p&gt;\n\n&lt;p&gt;Ask yourself: what am I trying to determine in this exercise?&lt;/p&gt;\n\n&lt;p&gt;If a method is provided summarise the key steps in a dedicated notebook.&lt;/p&gt;\n\n&lt;p&gt;Complete and pass any pre-exercise activities such as quizzes, risk assessments or additional reading.&lt;/p&gt;\n\n&lt;p&gt;During the exercise:&lt;/p&gt;\n\n&lt;p&gt;Arrive on time! Educators generally provide context, useful tips and safety warnings at the start of the session.&lt;/p&gt;\n\n&lt;p&gt;Ask your educator for clarification of the material and procedures.&lt;/p&gt;\n\n&lt;p&gt;Discuss your understanding and predictions with other students and the educator.&lt;/p&gt;\n\n&lt;p&gt;Take detailed notes of procedures, observations and measurements in a dedicated notebook.&lt;/p&gt;\n\n&lt;p&gt;After the exercise:&lt;/p&gt;\n\n&lt;p&gt;Consult the assessment guide and/or marking rubric for any required submissions.&lt;/p&gt;\n\n&lt;p&gt;Clarify points of confusion with your educators and/or other students (unit forums are an ideal place).&lt;/p&gt;\n\n&lt;p&gt;Complete any required submissions within the time allocated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A guide to group work&lt;/p&gt;\n\n&lt;p&gt;Why work in a group or a team?&lt;/p&gt;\n\n&lt;p&gt;Develops excellent graduate attributes:&lt;/p&gt;\n\n&lt;p&gt;decision making and problem-solving skills&lt;/p&gt;\n\n&lt;p&gt;project management and organisational skills&lt;/p&gt;\n\n&lt;p&gt;communication and conflict resolution skills.&lt;/p&gt;\n\n&lt;p&gt;The process&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Build a strong foundation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Get to know your team.&lt;/p&gt;\n\n&lt;p&gt;Discuss strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;Meet early and set rules.&lt;/p&gt;\n\n&lt;p&gt;Agree on the aims, scope, and quality of the project.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get organised&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Decide on a method of group communication.&lt;/p&gt;\n\n&lt;p&gt;Agree on who will do what.&lt;/p&gt;\n\n&lt;p&gt;Set early deadlines.&lt;/p&gt;\n\n&lt;p&gt;Plan on how to present the project as a unified work.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hold productive meetings&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Make sure everyone knows the goal of the meeting, and what to bring.&lt;/p&gt;\n\n&lt;p&gt;Have a leader to keep the meeting on track.&lt;/p&gt;\n\n&lt;p&gt;Have a note-taker to record decisions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Maintain relationships&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Resolve problems as a group. Don&amp;#39;t exclude others.&lt;/p&gt;\n\n&lt;p&gt;Address concerns as soon as they appear.&lt;/p&gt;\n\n&lt;p&gt;Renegotiate as needed.&lt;/p&gt;\n\n&lt;p&gt;Understand the other person&amp;#39;s point of view.&lt;/p&gt;\n\n&lt;p&gt;Leadership&lt;/p&gt;\n\n&lt;p&gt;Means&lt;/p&gt;\n\n&lt;p&gt;helping the group work efficiently,&lt;/p&gt;\n\n&lt;p&gt;monitoring progress,&lt;/p&gt;\n\n&lt;p&gt;knowing when a team member needs help, and&lt;/p&gt;\n\n&lt;p&gt;keeping the group motivated.&lt;/p&gt;\n\n&lt;p&gt;Does NOT mean&lt;/p&gt;\n\n&lt;p&gt;doing all the work,&lt;/p&gt;\n\n&lt;p&gt;making decisions and telling others what to do, or&lt;/p&gt;\n\n&lt;p&gt;being solely responsible for the success or failure of the project.&lt;/p&gt;\n\n&lt;p&gt;Everyone is responsible for the success or failure of the project, not individual team members.&lt;/p&gt;\n\n&lt;p&gt;Difficulties&lt;/p&gt;\n\n&lt;p&gt;You can&amp;#39;t work as quickly in a group as you can by yourself.&lt;/p&gt;\n\n&lt;p&gt;Group/team members may have conflicting ideas or viewpoints.&lt;/p&gt;\n\n&lt;p&gt;Group/team members may not contribute equally.&lt;/p&gt;\n\n&lt;p&gt;For successful group and teamwork&lt;/p&gt;\n\n&lt;p&gt;Be patient, demonstrate good communication skills and be committed.&lt;/p&gt;\n\n&lt;p&gt;Focus on the process rather than just the end product.&lt;/p&gt;\n\n&lt;p&gt;Collaborating and communicating online&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;During your university studies:&lt;/p&gt;\n\n&lt;p&gt;You may need to complete group assessment tasks.\xa0Using\xa0online meeting and collaboration tools can make this process easier, particularly if you are not able to meet face-to-face.&lt;/p&gt;\n\n&lt;p&gt;You may be asked to comment on online discussion boards (including Moodle forums) as part of your unit assessment tasks.&lt;/p&gt;\n\n&lt;p&gt;Online communication and collaboration tools are useful during your studies as they can help you to work on group projects, share ideas and information, and provide encouragement and feedback on each otherâ€™s work.&lt;/p&gt;\n\n&lt;p&gt;Communicating and collaborating online are also important skills in the workplace, as many workplaces require staff to work in groups on shared tasks, often across multiple locations. The ability to effectively use online meeting and collaboration tools is a key employability skill.&lt;/p&gt;\n\n&lt;p&gt;You may already be using social networking tools such as Facebook to work on group projects.\xa0However, there are some key differences between using social networking tools for informal activities and using these tools informal study (e.g. assessment) or work activities.\xa0These differences include the language used and the structure of the interactions in an online meeting.\xa0For example, an online meeting for a group assessment task or a workplace shared task will often use an agenda and have a person designated as the meeting chair.&lt;/p&gt;\n\n&lt;p&gt;This module includes tips for effectively communicating and collaborating when:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#meetings""&gt;participating in online meetings&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#discussion""&gt;using online discussion boards&lt;/a&gt;, for example, Moodle forums&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#tools""&gt;using online collaboration tools&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Communicating online - key terms&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Synchronous_and_asynchronous_communication-1""&gt;Synchronous and asynchronous communication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Verbal_and_non-verbal_communication-2""&gt;Verbal and non-verbal communication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Without non-verbal communication cues, it can be very difficult for your readers to detect sarcasm and other hidden meanings, and your words may be interpreted as being more unfriendly or bossy than intended. Use the following activity to explore some ways of communicating online effectively.&lt;/p&gt;\n\n&lt;p&gt;You disagree with the statements made in a discussion forum post from another student in your class. \xa0Which of the following responses might be better? &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; One point that you might like to consider is â€¦, which was included in this weekâ€™s reading.&lt;/li&gt;\n&lt;li&gt;No, you are wrong. \xa0You need to read this weekâ€™s required reading to see why you are wrong..&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Discussion on 1:  Good choice! By using this type of response, you are indicating that you do not fully agree with the post, but in a way that is not direct and bossy. \xa0You are also giving the other student some useful information by referring directly to the point raised in the reading. &lt;/p&gt;\n\n&lt;p&gt;Discussion on 2:  This is probably not the best choice. \xa0This response uses direct language and may be interpreted as unfriendly and bossy. \xa0The student who you are responding to may ignore your feedback as it doesnâ€™t give them much useful information (other than that they need to do the reading).\xa0 Your feedback is more likely to be heeded when you use cautious language, such as â€œOne point that you might like to consider is â€¦, which was included in this weekâ€™s readingâ€. \xa0 &lt;/p&gt;\n\n&lt;p&gt;Online symbols can be used to express emotion and can be particularly useful when working in less formal situations such as collaborating on online projects with other students. \xa0 &lt;/p&gt;\n\n&lt;p&gt;Emoji icons can be useful in expressing emotions when communicating online. However, emoji may not be appropriate in more formal settings.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Participating in online meetings&lt;/p&gt;\n\n&lt;p&gt;Online meetings and virtual team projects are a common industry practice.\xa0They are used to communicate information, collect data, generate ideas, build teams, solve problems and make decisions.&lt;/p&gt;\n\n&lt;p&gt;In a university setting, group projects are a common form of assessment in many course units.\xa0They are collaborative learning opportunities that require students to evaluate ideas, analyse and derive meaning from information, and produce work cooperatively.\xa0Effective online meetings can help you to share information, discuss ideas, solve problems and produce work during your studies, as well as in the workplace.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Using online discussion boards&lt;/p&gt;\n\n&lt;p&gt;A discussion board, such as a Moodle forum, is different to an online meeting in that it is a communication tool that enables participants to post messages and to reply to others&amp;#39; messages asynchronously (i.e. not at the same time).&lt;/p&gt;\n\n&lt;p&gt;The discussions on online discussion boards typically last for a longer period of time (days, weeks or months) and allow participants time to think about what they are going to contribute.\xa0Some units will include discussion board contributions as assessment tasks.\xa0In these assessment tasks, your lecturer will expect you to provide reflective, detailed responses to the topic and to other students&amp;#39; posts. Often, to obtain high marks for these assessment types, you need to demonstrate your engagement with and understanding of the unit materials such as readings and lecture content.&lt;/p&gt;\n\n&lt;p&gt;The purpose of this section is to explore some ways of effectively using online discussion boards:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://youtu.be/gfS4xgGmzbE""&gt;https://youtu.be/gfS4xgGmzbE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Using online collaboration tools&lt;/p&gt;\n\n&lt;p&gt;Digital tools&lt;/p&gt;\n\n&lt;p&gt;There are a large number of digital tools that can be used during your time at university. There is software available to be purchased or downloaded, including:&lt;/p&gt;\n\n&lt;p&gt;packages in the Microsoft suite for written communication&lt;/p&gt;\n\n&lt;p&gt;Skype and Zoom for online face-to-face communication and collaboration&lt;/p&gt;\n\n&lt;p&gt;planning tools such as Catme, Dapulse.com and Microsoft Project&lt;/p&gt;\n\n&lt;p&gt;smartphone applications.&lt;/p&gt;\n\n&lt;p&gt;Some tools are freely available while others must be purchased.&lt;/p&gt;\n\n&lt;p&gt;Monash\xa0University provides digital resources using the Google platform to create, store, share, collaborate and present different kinds of documents. This section will discuss how you can use these tools for group work, planning and documenting your projects.&lt;/p&gt;\n\n&lt;p&gt;Using digital tools for online communication&lt;/p&gt;\n\n&lt;p&gt;The digital tools shown below enable various types of online communication and collaboration including the sharing of files, the creation of websites for group work, and online meetings.&lt;/p&gt;\n\n&lt;p&gt;How to access these Google digital tools at Monash University&lt;/p&gt;\n\n&lt;p&gt;Monash students and staff can access these via &lt;a href=""https://my.monash/""&gt;my.monash&lt;/a&gt; website.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Choose an internet browser.&lt;/li&gt;\n&lt;li&gt;Type my.monash in the browser window or search for my.monash&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;3.\xa0Type your Monash username and password to access content from my.monash website.&lt;/p&gt;\n\n&lt;p&gt;4.\xa0Click to open either the Email or Calendar tiles.&lt;/p&gt;\n\n&lt;p&gt;5.\xa0To access other Google products, click on the small boxes on the top right-hand side of the screen&lt;/p&gt;\n\n&lt;p&gt;6.\xa0Other Google products will be displayed. Select the digital tool you wish to use. Click More to see more of the digital tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gevwrs', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/', 'subreddit_subscribers': 33, 'created_utc': 1588810323.0, 'num_crossposts': 68, 'media': None, 'is_video': False}]",t3_gevwrs,,
,learnmachinelearning,"Anyone with any experience taking Stanfordâ€™s cohort-based XCS229I?

Link: https://online.stanford.edu/courses/xcs229i-machine-learning

This is supposedly part of their AI Professional Certificate and a more rigorous treatment of the topics than the Coursera course. The 10 week course is $1,595 so not cheap by any means. I would not personally have to pay out of my own pocket to take the course but don it want to waste my employers money (or my time) if I can just watch the lectures for free in YouTube.",t2_5n5gtkbw,False,,0,False,Stanford XCS229I,[],r/learnmachinelearning,False,6,,0,,False,t3_gfkhrh,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588933120.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anyone with any experience taking Stanfordâ€™s cohort-based XCS229I?&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=""https://online.stanford.edu/courses/xcs229i-machine-learning""&gt;https://online.stanford.edu/courses/xcs229i-machine-learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is supposedly part of their AI Professional Certificate and a more rigorous treatment of the topics than the Coursera course. The 10 week course is $1,595 so not cheap by any means. I would not personally have to pay out of my own pocket to take the course but don it want to waste my employers money (or my time) if I can just watch the lectures for free in YouTube.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfkhrh,True,,throwawaymlquestion,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkhrh/stanford_xcs229i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfkhrh/stanford_xcs229i/,155203,1588904320.0,0,,False,,,,
,learnmachinelearning,,t2_4h2fow53,False,,0,False,Graph Neural Network model calibration for trusted predictions,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_gfivdg,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/ReI7VjHLtOcuYnxcPZbxp0tt2knTa2GINsIZgMpQepA.jpg,False,,[],{},link,,False,,1588926876.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?auto=webp&amp;s=4ec6e4346f82a426a74a947a5cddafd8282a0df5', 'width': 1024, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4237449b2889082b54e82a800fbd28619b6b5ee', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f71f677009d6092a0d56eb0cd3c415b82147ebb', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=608f23c0415ac964547f187b023df817ceeeda99', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e88d396b8faeaed1d4b0f0c3a177c4be70438deb', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dec2a7fb0d7b09e62496f958fef547109be57ea7', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'WsF9q0qWaDPohum0lvCnsKqgN9QPpiwsKa9hLCHalYY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfivdg,True,,StellarGraphLibrary,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfivdg/graph_neural_network_model_calibration_for/,all_ads,False,https://medium.com/stellargraph/graph-neural-network-model-calibration-for-trusted-predictions-e49628487e7b?source=friends_link&amp;sk=1364a91c903fbb47e78830300e6b591e,155203,1588898076.0,0,,False,,,,
,learnmachinelearning,,t2_rlnpqcu,False,,0,False,System Requirements for *Learning* ML,[],r/learnmachinelearning,False,6,,0,,False,t3_gfip17,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588926228.0,text,6,,,text,self.MLQuestions,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfip17,True,,cosmictypist,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfip17/system_requirements_for_learning_ml/,all_ads,False,/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/,155203,1588897428.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MLQuestions', 'selftext': ""Hi. This question may have been asked before, but I couldn't find it and hence posting here. In case it has been answered before, I'll appreciate being pointed in the right direction.\n\nBasically, I wanted to know how fast/advanced my system needs to be if I want to start learning ML from scratch. I did some search on Google and it seems that a fairly fast CPU (Intel Core i7) along with a GPU are generally recommended - but are these necessary? Are there ways to make use of some sort of a remoteGPU server while having fairly low-average specs for your system?\n\nI have a fairly old Intel core i5 2.4 GHz CPU with 2GB RAM. Yeah I'll probably need to get a new one but don't want to splurge if it is not required. Thanks."", 'author_fullname': 't2_rlnpqcu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'System Requirements for *Learning* ML', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MLQuestions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gfin87', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 8, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588926055.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MLQuestions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. This question may have been asked before, but I couldn&amp;#39;t find it and hence posting here. In case it has been answered before, I&amp;#39;ll appreciate being pointed in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Basically, I wanted to know how fast/advanced my system needs to be if I want to start learning ML from scratch. I did some search on Google and it seems that a fairly fast CPU (Intel Core i7) along with a GPU are generally recommended - but are these necessary? Are there ways to make use of some sort of a remoteGPU server while having fairly low-average specs for your system?&lt;/p&gt;\n\n&lt;p&gt;I have a fairly old Intel core i5 2.4 GHz CPU with 2GB RAM. Yeah I&amp;#39;ll probably need to get a new one but don&amp;#39;t want to splurge if it is not required. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_30rel', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfin87', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cosmictypist', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/', 'subreddit_subscribers': 27178, 'created_utc': 1588897255.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_gfin87,,
,learnmachinelearning,"I'm an absolute beginner to the field , I think my idea is supposed to be very simple and easy yet I'm feeling lost..

I want to train a model to localize only hand written brackets in pictures of a scanned textbook pages (written by me). 

So can I consider this as an object detection problem and proceed with that?  hand written Brackets are alot more simple than a real life object, however They are different too, there is a different type of noise in my case which is the tge text printed words which can be easily detected falsly as brackets as this noise is similar to the geometry of my brackets ...

And if that's the case, how am I supposed to label my hand written bracket with a rectangle?! , there is no way I can label it without including some of the surrounding text , will this affect the accuracy of my results?

I'm sorry to over explain my likely very simple problem, however this is my first project and I need to get it done, please help me ðŸ™

Thanks!",t2_15ee6s,False,,0,False,Lost with my first ML Project ðŸ˜­. I need HELP!,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfin5h,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,1588897550.0,,[],{},,,True,,1588926046.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m an absolute beginner to the field , I think my idea is supposed to be very simple and easy yet I&amp;#39;m feeling lost..&lt;/p&gt;

&lt;p&gt;I want to train a model to localize only hand written brackets in pictures of a scanned textbook pages (written by me). &lt;/p&gt;

&lt;p&gt;So can I consider this as an object detection problem and proceed with that?  hand written Brackets are alot more simple than a real life object, however They are different too, there is a different type of noise in my case which is the tge text printed words which can be easily detected falsly as brackets as this noise is similar to the geometry of my brackets ...&lt;/p&gt;

&lt;p&gt;And if that&amp;#39;s the case, how am I supposed to label my hand written bracket with a rectangle?! , there is no way I can label it without including some of the surrounding text , will this affect the accuracy of my results?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m sorry to over explain my likely very simple problem, however this is my first project and I need to get it done, please help me ðŸ™&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfin5h,True,,homamoooo1234,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfin5h/lost_with_my_first_ml_project_i_need_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfin5h/lost_with_my_first_ml_project_i_need_help/,155203,1588897246.0,0,,False,,,,
,learnmachinelearning," 

Machine Learning today is one of the most sought-after skills in the market. A lot of Software Engineers are picking up ML, simply because it is a highly paid skill.

So, how do you learn Machine Learning?

* First things first â€“ the prerequisites:
   * *Basic calculus*. In Machine Learning, youâ€™d be working on a lot of optimizations that require knowledge of Calculus. It would be highly recommended that you are aware of functions, limits, differentiation, maxima, minima, etc.
   * *Linear Algebra*. When you talk about ML, you will be dealing with matrices and vectors every day. So, knowledge of Linear Algebra is a must. However, youâ€™d also be required to know about other important topics like Eigenvalues and Eigenvectors.
   * *Probability*. Most ML algorithms try to â€œmodelâ€ the underlying phenomena that generated the observed data. All of this modelling is probabilistic. It is therefore highly recommended that you are comfortable with the theory of Probability.
* Getting into actual ML:
   * Take a great online course on ML. The most well-known course is the one offered byÂ Andrew Ng (Coursera). It is a great course and it teaches you the basics of Machine Learning â€“ Regression, classification, various ML algorithms, etc. The course also requires you to build a digit recognition system.
   * Once you have the basics in place, it would be a great idea to practice some problems onÂ Kaggle. Kaggle is a well-known Machine Learning contest platform where you can compete with others in training ML models on various datasets.
   * Take up ML projects. This is the most important point. Ideally, youâ€™d want to have not only ML experience but also some great projects on your resume that you can showcase. These projects will help you distinguish yourself from other candidates. After searching a lot for courses that teach ML through projects, this [Eduonix](https://inr.deals/track?id=100613576719&amp;src=backend&amp;url=https%3A%2F%2Fwww.eduonix.com%2Flearn-machine-learning-by-building-projects%3F)Â quite relevant.

The best way to learn Machine Learning is to actually apply it to real datasets and solve real problems. Machine Learning is as much of an art as it is a science. You will learn it from experience. Your focus should be on attempting multiple ML projects so as to gain experience and build a strong profile",t2_2om0uq0e,False,,0,False,Should I follow This Guide?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5xzq,False,dark,0.9,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,1588861265.0,,[],{},self,,True,,1588884660.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Machine Learning today is one of the most sought-after skills in the market. A lot of Software Engineers are picking up ML, simply because it is a highly paid skill.&lt;/p&gt;

&lt;p&gt;So, how do you learn Machine Learning?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First things first â€“ the prerequisites:

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Basic calculus&lt;/em&gt;. In Machine Learning, youâ€™d be working on a lot of optimizations that require knowledge of Calculus. It would be highly recommended that you are aware of functions, limits, differentiation, maxima, minima, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Linear Algebra&lt;/em&gt;. When you talk about ML, you will be dealing with matrices and vectors every day. So, knowledge of Linear Algebra is a must. However, youâ€™d also be required to know about other important topics like Eigenvalues and Eigenvectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Probability&lt;/em&gt;. Most ML algorithms try to â€œmodelâ€ the underlying phenomena that generated the observed data. All of this modelling is probabilistic. It is therefore highly recommended that you are comfortable with the theory of Probability.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Getting into actual ML:

&lt;ul&gt;
&lt;li&gt;Take a great online course on ML. The most well-known course is the one offered byÂ Andrew Ng (Coursera). It is a great course and it teaches you the basics of Machine Learning â€“ Regression, classification, various ML algorithms, etc. The course also requires you to build a digit recognition system.&lt;/li&gt;
&lt;li&gt;Once you have the basics in place, it would be a great idea to practice some problems onÂ Kaggle. Kaggle is a well-known Machine Learning contest platform where you can compete with others in training ML models on various datasets.&lt;/li&gt;
&lt;li&gt;Take up ML projects. This is the most important point. Ideally, youâ€™d want to have not only ML experience but also some great projects on your resume that you can showcase. These projects will help you distinguish yourself from other candidates. After searching a lot for courses that teach ML through projects, this &lt;a href=""https://inr.deals/track?id=100613576719&amp;amp;src=backend&amp;amp;url=https%3A%2F%2Fwww.eduonix.com%2Flearn-machine-learning-by-building-projects%3F""&gt;Eduonix&lt;/a&gt;Â quite relevant.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The best way to learn Machine Learning is to actually apply it to real datasets and solve real problems. Machine Learning is as much of an art as it is a science. You will learn it from experience. Your focus should be on attempting multiple ML projects so as to gain experience and build a strong profile&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?auto=webp&amp;s=0f335926f0d3960f9882daf4a67f417a10df4d66', 'width': 647, 'height': 422}, 'resolutions': [{'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0c3c1af91133e4e1b526f4e838db4eb741e8ce6', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12fce343264e852d8f714c9ec9e96f1813a8681e', 'width': 216, 'height': 140}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8443b6d856dcf75ecc6d025d467997c9046e5805', 'width': 320, 'height': 208}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e5bd15b6f7995a4b278556ea23bb689c6a1609f', 'width': 640, 'height': 417}], 'variants': {}, 'id': 'jST3NfM-cC_ASNZRu30Adudq5cD6mRpGIhGvenQ514g'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5xzq,True,,Amitagarwal7021,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5xzq/should_i_follow_this_guide/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5xzq/should_i_follow_this_guide/,155203,1588855860.0,0,,False,,,,
,learnmachinelearning,"Hey all,

  
So I've just started my MSc research project on Graph Neural  Networks and ngl I'm finding this whole research thing super daunting   and not sure exactly what I should be doing most of the time...

  
Does  anyone know of any good resources that outline the basics of how to conduct/write up research for a thesis, preferably specific to  computer science or machine learning?

  
I'd love it if there was a book that I could read that would help put me on the right path.",t2_fblqx,False,,0,False,Any resources for writing better academic research?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf2vg3,False,light,0.9,,public,13,0,{},,,False,[],,False,False,,{},HELP,False,13,,False,self,False,,[],{},,,True,,1588869930.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all,&lt;/p&gt;

&lt;p&gt;So I&amp;#39;ve just started my MSc research project on Graph Neural  Networks and ngl I&amp;#39;m finding this whole research thing super daunting   and not sure exactly what I should be doing most of the time...&lt;/p&gt;

&lt;p&gt;Does  anyone know of any good resources that outline the basics of how to conduct/write up research for a thesis, preferably specific to  computer science or machine learning?&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love it if there was a book that I could read that would help put me on the right path.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gf2vg3,True,,ghoumrassi,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2vg3/any_resources_for_writing_better_academic_research/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf2vg3/any_resources_for_writing_better_academic_research/,155203,1588841130.0,0,,False,,,,
,learnmachinelearning,"I was learning the basics of data preprocessing and came across two ways to encode categorical data. I tried to find the difference between the two encoders, when to use which encoder and why one hot encoder is used but couldn't find any good resource.",t2_5t9rtf1h,False,,0,False,One Hot Encoder vs Label Encoder,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfhbg1,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588921351.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was learning the basics of data preprocessing and came across two ways to encode categorical data. I tried to find the difference between the two encoders, when to use which encoder and why one hot encoder is used but couldn&amp;#39;t find any good resource.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfhbg1,True,,Sid200026,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfhbg1/one_hot_encoder_vs_label_encoder/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfhbg1/one_hot_encoder_vs_label_encoder/,155203,1588892551.0,0,,False,,,,
,learnmachinelearning,"Hey everyone. I'm a researcher who's currently doing a lot of work with very large spatial files and deep learning models, I've reached the point where I simply can't load the data needed for training into memory. What recommendations would you have for cloud storage and GPU training, and what sort of cost should I be expecting?  

I've been looking into using AWS with S3 for storing the data and SageMaker for training the models but I've only come out more confused by the variety Amazon has on offer. I've previously done a fair bit of web dev and have them running live so I'm comfortable working in predominantly CLI environments.

Thanks for all help!",t2_4g6qbb8j,False,,0,False,Best Cloud Storage &amp; GPU Training,[],r/learnmachinelearning,False,6,,0,,False,t3_gfh8dw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588921065.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone. I&amp;#39;m a researcher who&amp;#39;s currently doing a lot of work with very large spatial files and deep learning models, I&amp;#39;ve reached the point where I simply can&amp;#39;t load the data needed for training into memory. What recommendations would you have for cloud storage and GPU training, and what sort of cost should I be expecting?  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been looking into using AWS with S3 for storing the data and SageMaker for training the models but I&amp;#39;ve only come out more confused by the variety Amazon has on offer. I&amp;#39;ve previously done a fair bit of web dev and have them running live so I&amp;#39;m comfortable working in predominantly CLI environments.&lt;/p&gt;

&lt;p&gt;Thanks for all help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfh8dw,True,,EnergyVis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh8dw/best_cloud_storage_gpu_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfh8dw/best_cloud_storage_gpu_training/,155203,1588892265.0,0,,False,,,,
,learnmachinelearning,"For understanding which parts of the images to look into, we usually use GradCam or Cam. In CAM, we multiply the outermost conv features with the weights of the matrix pertaining to the class. Correct me if I am wrong though.
However, does anyone know which paper introduced this?",t2_1myrhoji,False,,0,False,Does anybody know the name of the paper where CAM was applied?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfh56o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588920758.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For understanding which parts of the images to look into, we usually use GradCam or Cam. In CAM, we multiply the outermost conv features with the weights of the matrix pertaining to the class. Correct me if I am wrong though.
However, does anyone know which paper introduced this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfh56o,True,,thearkamitra,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh56o/does_anybody_know_the_name_of_the_paper_where_cam/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfh56o/does_anybody_know_the_name_of_the_paper_where_cam/,155203,1588891958.0,0,,False,,,,
,learnmachinelearning,"I'm struggling to find a scalable solution that will allow me to make predictions for several hundred response variables using a common set of predictors using Keras in R. Since I'm pretty new to NN and DL, I simulated a small toy dataset that consists of five dependent response variables and a set of 60K predictors, and fit some very basic multi-output NN with Keras in R. The plan is to build on these NN and to apply them to a real dataset consisting of 60K predictors and \~200 response variables. The code for my very simple NN is:

    input &lt;- layer_input(shape = dim(trnX.RR)[2], name=""t_in"")
    
    layer_t &lt;- input %&gt;%
          layer_dense(units = units_M, activation='linear', 
                      kernel_regularizer = regularizer_l2(k))
        
    ## Output
    yhat_t1 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_t1"")
    yhat_t2 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_t2"")
    yhat_m1 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_m1"")
    yhat_m2 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_m2"")
    yhat_y &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_y"")
        
    # build model
    model &lt;- keras_model(inputs = input,
                  outputs = c(yhat_t1,yhat_t2,yhat_m1,yhat_m2,yhat_y)) %&gt;%
          compile(optimizer = ""rmsprop"",
                  loss=""mse"",
                  metrics=""mae"")
    
    model_fit &lt;- model %&gt;%
          fit(x = trnX.RR,
              y = list(trnYs[,1], trnYs[,2], trnYs[,3], trnYs[,4], trnYs[,5]),
              epochs = epochs_M,
              batch_size = 50,
              verbose = 0, validation_split = 0.2)

Coding the output layer for each response variable is not great when you have 100+ response variables. Is there a better way to do this? I've seen multi-output code in Python that is much more efficient (See the first chunk of code under model building here [https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6](https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6)).",t2_ztlov,False,,0,False,Scalable multi-output NN with keras in R,[],r/learnmachinelearning,False,6,,0,,False,t3_gfgrjx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588919439.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m struggling to find a scalable solution that will allow me to make predictions for several hundred response variables using a common set of predictors using Keras in R. Since I&amp;#39;m pretty new to NN and DL, I simulated a small toy dataset that consists of five dependent response variables and a set of 60K predictors, and fit some very basic multi-output NN with Keras in R. The plan is to build on these NN and to apply them to a real dataset consisting of 60K predictors and ~200 response variables. The code for my very simple NN is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input &amp;lt;- layer_input(shape = dim(trnX.RR)[2], name=&amp;quot;t_in&amp;quot;)

layer_t &amp;lt;- input %&amp;gt;%
      layer_dense(units = units_M, activation=&amp;#39;linear&amp;#39;, 
                  kernel_regularizer = regularizer_l2(k))

## Output
yhat_t1 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_t1&amp;quot;)
yhat_t2 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_t2&amp;quot;)
yhat_m1 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_m1&amp;quot;)
yhat_m2 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_m2&amp;quot;)
yhat_y &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_y&amp;quot;)

# build model
model &amp;lt;- keras_model(inputs = input,
              outputs = c(yhat_t1,yhat_t2,yhat_m1,yhat_m2,yhat_y)) %&amp;gt;%
      compile(optimizer = &amp;quot;rmsprop&amp;quot;,
              loss=&amp;quot;mse&amp;quot;,
              metrics=&amp;quot;mae&amp;quot;)

model_fit &amp;lt;- model %&amp;gt;%
      fit(x = trnX.RR,
          y = list(trnYs[,1], trnYs[,2], trnYs[,3], trnYs[,4], trnYs[,5]),
          epochs = epochs_M,
          batch_size = 50,
          verbose = 0, validation_split = 0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Coding the output layer for each response variable is not great when you have 100+ response variables. Is there a better way to do this? I&amp;#39;ve seen multi-output code in Python that is much more efficient (See the first chunk of code under model building here &lt;a href=""https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6""&gt;https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?auto=webp&amp;s=96e3097a6229b44ddb351653d2042328c5c1d77f', 'width': 1200, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efc7ce2ac39ae790c39989f75df706e8d7cd6d3f', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ced8175b8541647e09709fe336284a9ea9911fb2', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa7983c250103827ec6672c13e7857da7c68c14b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=33ba343cd1e9885b783734b2c520b89af5cb59d2', 'width': 640, 'height': 337}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79901ec86563628848db82d59c469e21bc020f12', 'width': 960, 'height': 505}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86ae05e59c88f25df2053ab664daec88923afc29', 'width': 1080, 'height': 568}], 'variants': {}, 'id': 'TlWGZM09jhy8Xf2FKW_9uq_xRvOB6q5mKjx4IfxteZI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfgrjx,True,,stinkyEyesMcGee,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfgrjx/scalable_multioutput_nn_with_keras_in_r/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfgrjx/scalable_multioutput_nn_with_keras_in_r/,155203,1588890639.0,0,,False,,,,
,learnmachinelearning,"if you have any starting projects recommendations that would be cool too. Assume that I have good enough base programing knowledge outside of deep learning.

Edit: Am instead of a in the title",t2_4dukeyr5,False,,0,False,"I want to get into deep learning (a currently playing around with TensorFlow), do you have any big recommendations? For a book or a lecture / tutorial that would help me get started and advance my through the subject?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfgjz6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,True,self,False,,[],{},,,True,,1588918724.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;if you have any starting projects recommendations that would be cool too. Assume that I have good enough base programing knowledge outside of deep learning.&lt;/p&gt;

&lt;p&gt;Edit: Am instead of a in the title&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfgjz6,True,,_AguruAguru,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfgjz6/i_want_to_get_into_deep_learning_a_currently/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfgjz6/i_want_to_get_into_deep_learning_a_currently/,155203,1588889924.0,0,,False,,,,
,learnmachinelearning," Tensorflow version - 1.14.0 Python version - 3.7.5 

This is the code I'm using. But when I try to save my model it says 

'Model object has no attribute '\_is\_graph\_network''

    from tensorflow 
    import keras 
    import tensorflow.python.keras.backend as K 
    from tensorflow.python.keras import callbacks 
    from tensorflow.python.keras 
    import Sequential from tensorflow.python.keras.models 
    import Model from tensorflow.python.keras.layers 
    import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D 
    from tensorflow.python.keras.preprocessing.image 
    import ImageDataGenerator 
    from tensorflow.python.keras.callbacks 
    import ModelCheckpoint, EarlyStopping 
    
    
    def create_model_v1():      
        model = keras.Sequential()       
        model.add(Conv2D(filters = 64, kernel_size = 3, padding='same', activation = 'relu',                                  input_shape=(img_rows, img_cols, color_type)))      
        model.add(MaxPooling2D(pool_size = 2))      
        model.add(Conv2D(filters = 128, padding='same', kernel_size = 3, activation = 'relu'))                                               model.add(MaxPooling2D(pool_size = 2))
    
    def create_model_v1():  
        model_v1 = create_model_v1()  
        history_v1 = model_v1.fit(x_train, y_train,validation_data=(x_test, y_test),callbacks=callbacks,epochs=nb_epoch, batch_size=batch_size, verbose=1)
    
    
    keras_file = 'saved_models/history1.h5'
    keras.models.save_model(history_v1, keras_file)
    
    converter = tf.lite.TocoConverter.from_keras_model_file(keras_file)
    tflite_model = converter.convert()
    open('linear.tflite', 'wb').write(tflite_model)",t2_2bqrjjyu,False,,0,False,"Model object has no attribute '_is_graph_network', when I try to save my model to tflite",[],r/learnmachinelearning,False,6,,0,,False,t3_gfga8y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588917839.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Tensorflow version - 1.14.0 Python version - 3.7.5 &lt;/p&gt;

&lt;p&gt;This is the code I&amp;#39;m using. But when I try to save my model it says &lt;/p&gt;

&lt;p&gt;&amp;#39;Model object has no attribute &amp;#39;_is_graph_network&amp;#39;&amp;#39;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from tensorflow 
import keras 
import tensorflow.python.keras.backend as K 
from tensorflow.python.keras import callbacks 
from tensorflow.python.keras 
import Sequential from tensorflow.python.keras.models 
import Model from tensorflow.python.keras.layers 
import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D 
from tensorflow.python.keras.preprocessing.image 
import ImageDataGenerator 
from tensorflow.python.keras.callbacks 
import ModelCheckpoint, EarlyStopping 


def create_model_v1():      
    model = keras.Sequential()       
    model.add(Conv2D(filters = 64, kernel_size = 3, padding=&amp;#39;same&amp;#39;, activation = &amp;#39;relu&amp;#39;,                                  input_shape=(img_rows, img_cols, color_type)))      
    model.add(MaxPooling2D(pool_size = 2))      
    model.add(Conv2D(filters = 128, padding=&amp;#39;same&amp;#39;, kernel_size = 3, activation = &amp;#39;relu&amp;#39;))                                               model.add(MaxPooling2D(pool_size = 2))

def create_model_v1():  
    model_v1 = create_model_v1()  
    history_v1 = model_v1.fit(x_train, y_train,validation_data=(x_test, y_test),callbacks=callbacks,epochs=nb_epoch, batch_size=batch_size, verbose=1)


keras_file = &amp;#39;saved_models/history1.h5&amp;#39;
keras.models.save_model(history_v1, keras_file)

converter = tf.lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(&amp;#39;linear.tflite&amp;#39;, &amp;#39;wb&amp;#39;).write(tflite_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfga8y,True,,Revanthmk23200,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfga8y/model_object_has_no_attribute_is_graph_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfga8y/model_object_has_no_attribute_is_graph_network/,155203,1588889039.0,0,,False,,,,
,learnmachinelearning,"Hello,
Im really new to programming snd have a bg interest in AI and machine learning and their applications. Problem is I dont have much programming knowledge- I only just finished week 1 in Harvardâ€™s CS50 course.
So, whatâ€™s the best to start learning? 
Many thanks in advance!",t2_3femx8ef,False,,0,False,Best way to start learning?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfbg5o,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588902647.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
Im really new to programming snd have a bg interest in AI and machine learning and their applications. Problem is I dont have much programming knowledge- I only just finished week 1 in Harvardâ€™s CS50 course.
So, whatâ€™s the best to start learning? 
Many thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfbg5o,True,,octopussssssssy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfbg5o/best_way_to_start_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfbg5o/best_way_to_start_learning/,155203,1588873847.0,0,,False,,,,
,learnmachinelearning,"Most books rely on closed form functions or differential equations.  I'm not really looking for that.  Are there books that look at mathematical modeling from a different perspective, like stochastic modeling, or a more probabilistic view?  Thanks.",t2_r9kac,False,,0,False,Any recommendations for mathematical modeling?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfdtzc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588910018.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most books rely on closed form functions or differential equations.  I&amp;#39;m not really looking for that.  Are there books that look at mathematical modeling from a different perspective, like stochastic modeling, or a more probabilistic view?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfdtzc,True,,babbab55,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfdtzc/any_recommendations_for_mathematical_modeling/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfdtzc/any_recommendations_for_mathematical_modeling/,155203,1588881218.0,0,,False,,,,
,learnmachinelearning,,t2_1084g6,False,,0,False,Body Movement Correction Using Pose Estimation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gf5h3w,False,light,0.75,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Body Movement Correction Using Pose Estimation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EpySUFqSO4c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gf5h3w', 'height': 338}",Project,False,4,,False,https://a.thumbs.redditmedia.com/qsm7JCT9udT66DaxswVTMbUJ-Tz_mnB8H4xP64ZYYP0.jpg,False,,[],{},rich:video,,False,,1588882778.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?auto=webp&amp;s=fa44800f3c70638e873929dbf9d4b4e0e663aca1', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23c042d7d7217224f5bfd0cb9f6da1687d539ea3', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5a38af28775399847f62fb7489d9b65346fcfa9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7923b6c33b8c7f4df7ff394df265944c235367dd', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fUivSBSCzTFa4-2HVW8LiKsDk3es9ZGWbcvpjIZkTIw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf5h3w,True,,rednivrug,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5h3w/body_movement_correction_using_pose_estimation/,all_ads,False,https://www.youtube.com/watch?v=EpySUFqSO4c,155203,1588853978.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Body Movement Correction Using Pose Estimation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EpySUFqSO4c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,,,,
,learnmachinelearning,"Imagine you have a picture of a dress and a picture of a person. You want to combine them into the picture of that person wearing that dress.

I want to ask if it is possible with deep learning? If it is possible, where can I start researching it? Thanks in advance.",t2_4bfdiwq7,False,,0,False,How to use machine learning to change clothes for people?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfdi9x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588909011.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Imagine you have a picture of a dress and a picture of a person. You want to combine them into the picture of that person wearing that dress.&lt;/p&gt;

&lt;p&gt;I want to ask if it is possible with deep learning? If it is possible, where can I start researching it? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfdi9x,True,,ConVit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfdi9x/how_to_use_machine_learning_to_change_clothes_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfdi9x/how_to_use_machine_learning_to_change_clothes_for/,155203,1588880211.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is Self-Supervised Learning ? Will machines be able to learn like humans ?,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfd4xf,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Self-Supervised Learning ? | Will machines be able to learn like humans ? 29', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lgVwtTof1ew/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfd4xf', 'height': 338}",,False,0,,False,https://a.thumbs.redditmedia.com/tVkKXl4MkOJQAI2Hefrms3-xYCXb-t4kvWLvRyhMnQ4.jpg,False,,[],{},rich:video,,False,,1588907866.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?auto=webp&amp;s=d7c90919d5674edbf28dd99035850188f54c60b5', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efa96672a7b85989c503d0b81c154d865d794b92', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7d2c0e61b4b4378090aa6dbbaa9133c5ae24224', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c12936ff6ce9472a6c7885a5d8fc4436de3d7e7', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ApmWhh_MYhLpQOTFLpJyvz6HrDHCk6PTEXWmBi2G55I'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfd4xf,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfd4xf/what_is_selfsupervised_learning_will_machines_be/,all_ads,False,https://youtu.be/lgVwtTof1ew,155203,1588879066.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Self-Supervised Learning ? | Will machines be able to learn like humans ? 29', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lgVwtTof1ew/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_icu1syl,False,,0,False,What Is Machine Learning? - Visual Explanations | Data Revenue,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_gfc9sv,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/zqyy8kZFfqXZKYBO09CzIqX6kE1_QFjE4Byx1uqwnqc.jpg,False,,[],{},link,,False,,1588905177.0,richtext,6,,,text,datarevenue.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?auto=webp&amp;s=a63181128d592b63016103297f71e24edcd97767', 'width': 1200, 'height': 627}, 'resolutions': [{'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c2135477b6c729dc1fb2061b79893156f521d0a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=654d179674876650e27f1eb36bf5b9148502e02e', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b1ca145da0f020c5f7d46a6863f30e3f741f493', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=95ec6b6d6ec8a67bb5289f7ab1964cdc8dcc7ff8', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fea3d43b6c19b4f07514726c100af54ea7dfea15', 'width': 960, 'height': 501}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a180e8c2fb4548f46b1c56b3a68ec256cffba152', 'width': 1080, 'height': 564}], 'variants': {}, 'id': 'sOH89vvM6svgLL71vuQxsI9aIXz73Y6ha0hHxiNg8xY'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfc9sv,True,,milosmudric,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfc9sv/what_is_machine_learning_visual_explanations_data/,all_ads,False,https://datarevenue.com/en-blog/what-is-machine-learning-a-visual-explanation?utm_source=Reddit&amp;utm_medium=learnMachineLearning,155203,1588876377.0,0,,False,,,,
,learnmachinelearning,"I've found AWS SageMaker very handy to jump on and run some ML code, but it's very annoying to have to upload and download files from UI, or copying the private GitHub key over.

I've set up SSH for myself months ago and finally decided to write a guide on it. Hope it's going to be helpful [https://biasandvariance.com/sagemaker-ssh-setup/](https://biasandvariance.com/sagemaker-ssh-setup/)",t2_2zijr2cb,False,,0,False,"SSHing to Sagemaker instance, step by step guide",[],r/learnmachinelearning,False,6,,0,,False,t3_gfc41a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588904676.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve found AWS SageMaker very handy to jump on and run some ML code, but it&amp;#39;s very annoying to have to upload and download files from UI, or copying the private GitHub key over.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve set up SSH for myself months ago and finally decided to write a guide on it. Hope it&amp;#39;s going to be helpful &lt;a href=""https://biasandvariance.com/sagemaker-ssh-setup/""&gt;https://biasandvariance.com/sagemaker-ssh-setup/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfc41a,True,,derivablefunc,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfc41a/sshing_to_sagemaker_instance_step_by_step_guide/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfc41a/sshing_to_sagemaker_instance_step_by_step_guide/,155203,1588875876.0,0,,False,,,,
,learnmachinelearning,,t2_5iu2csdl,False,,0,False,Making David Dobrik Video Titles with Recurring Neural Network,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gf732i,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TEACHING A COMPUTER TO CREATE DAVID DOBRIK VIDEOS!?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Rishi Chillara', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BG2xaWl1wnw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCKS74Dpy07o4uroFinQH1_g'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gf732i', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/oyudRI3ceEuMkWEJ4Hpu79rODdf-lc5_i4qk26sR2Bc.jpg,False,,[],{},rich:video,,False,,1588888863.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?auto=webp&amp;s=d59253b6b90b6ba2a717bf2fcb8c6e6ce3da9910', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47d86be9b21e04a7613c945621d9739e0c38dddd', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b645c936c7a671dd3130bda53014fbeb57e1b060', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a5aa2d694647e1f93ef33d76cd3afb1017b5b61', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ddJAQimHi_EkCTL1yACzhhg5MQNG-B8tVqPO6fXfxro'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf732i,True,,netflixandchillara,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf732i/making_david_dobrik_video_titles_with_recurring/,all_ads,False,https://youtu.be/BG2xaWl1wnw,155203,1588860063.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TEACHING A COMPUTER TO CREATE DAVID DOBRIK VIDEOS!?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Rishi Chillara', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BG2xaWl1wnw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCKS74Dpy07o4uroFinQH1_g'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hey guys
How do I start off with implementing papers?
Many sota ones have multiple modules and I honestly don't feel up for the task atm ;-;",t2_3mwi2rxe,False,,0,False,Paper implementation,[],r/learnmachinelearning,False,6,,0,,False,t3_gf4ys2,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588880590.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys
How do I start off with implementing papers?
Many sota ones have multiple modules and I honestly don&amp;#39;t feel up for the task atm ;-;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4ys2,True,,GrImPeAper23032000,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4ys2/paper_implementation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4ys2/paper_implementation/,155203,1588851790.0,0,,False,,,,
,learnmachinelearning,"At the early stages of planning to set up a model to do some prediction. The issue is lack of training data (a couple thousand rows). Have been thinking about different ways to generate more data (for training, validation, texting). 
  
There might be a little that can be done with text augmentation, creating fake data from the real data, pseudo labelling, resampling/oversampling/undersampling but I am curious how onerous it would be to set up a Generative Adversarial Network to create realistic but fake data. 
  
Keep in mind that all I'm doing is classifying based on a number of attributes based on a statistical model. Not trying to do visual detection in any way. But I do need statistically accurate data. 
  
So many of the models I can find that use GANs are visual detection....
  
Even if someone can point me to examples or papers where people are using a GAN to generate accurate ""text"" data would help.",t2_ffyi2,False,,0,False,"Generating training data, how practical is a GAN?",[],r/learnmachinelearning,False,6,,0,,False,t3_gf6q69,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588887592.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;At the early stages of planning to set up a model to do some prediction. The issue is lack of training data (a couple thousand rows). Have been thinking about different ways to generate more data (for training, validation, texting). &lt;/p&gt;

&lt;p&gt;There might be a little that can be done with text augmentation, creating fake data from the real data, pseudo labelling, resampling/oversampling/undersampling but I am curious how onerous it would be to set up a Generative Adversarial Network to create realistic but fake data. &lt;/p&gt;

&lt;p&gt;Keep in mind that all I&amp;#39;m doing is classifying based on a number of attributes based on a statistical model. Not trying to do visual detection in any way. But I do need statistically accurate data. &lt;/p&gt;

&lt;p&gt;So many of the models I can find that use GANs are visual detection....&lt;/p&gt;

&lt;p&gt;Even if someone can point me to examples or papers where people are using a GAN to generate accurate &amp;quot;text&amp;quot; data would help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6q69,True,,apercu_consulting,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6q69/generating_training_data_how_practical_is_a_gan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6q69/generating_training_data_how_practical_is_a_gan/,155203,1588858792.0,0,,False,,,,
,learnmachinelearning,"Hello all,

Recently I've been reading Bishop's excellent book. I already have a fair understanding of ML &amp; DL but I want to read it to acquire a deeper understanding of the core concepts described there.

However, I find the book quite challenging, even in the first chapter. For example, I am having a lot of trouble grasping equations 1.68 to 1.72 or the ""straightforward"" result in 1.58 (you can easily find the pdf online).

So, I would like to ask for two things:

1. Where should I go to refresh my math background needed for this? I wouldn't want to take a huge detour because I feel that this will distract me from my final goal and I already have some background in calculus, linear algebra and probabilities. Essentially, I would like to quickly go through some short theory and practical exercises that combine integrals, expected values and vectors/matrices.
2. Is there any in detail discussion group or video that deals with the book? For example, something that expands Bishop's ""straightforward"" to ""here is a detailed step-to-step calculation that leads us to this result""

Thank you very much for your time",t2_6eozcnvb,False,,0,False,"Regarding Bishop's ""Pattern Recognition and Machine Learning""",[],r/learnmachinelearning,False,6,,0,,False,t3_gf4qcu,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1588851068.0,,[],{},,,True,,1588879473.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;ve been reading Bishop&amp;#39;s excellent book. I already have a fair understanding of ML &amp;amp; DL but I want to read it to acquire a deeper understanding of the core concepts described there.&lt;/p&gt;

&lt;p&gt;However, I find the book quite challenging, even in the first chapter. For example, I am having a lot of trouble grasping equations 1.68 to 1.72 or the &amp;quot;straightforward&amp;quot; result in 1.58 (you can easily find the pdf online).&lt;/p&gt;

&lt;p&gt;So, I would like to ask for two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Where should I go to refresh my math background needed for this? I wouldn&amp;#39;t want to take a huge detour because I feel that this will distract me from my final goal and I already have some background in calculus, linear algebra and probabilities. Essentially, I would like to quickly go through some short theory and practical exercises that combine integrals, expected values and vectors/matrices.&lt;/li&gt;
&lt;li&gt;Is there any in detail discussion group or video that deals with the book? For example, something that expands Bishop&amp;#39;s &amp;quot;straightforward&amp;quot; to &amp;quot;here is a detailed step-to-step calculation that leads us to this result&amp;quot;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you very much for your time&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4qcu,True,,P52-328,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4qcu/regarding_bishops_pattern_recognition_and_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4qcu/regarding_bishops_pattern_recognition_and_machine/,155203,1588850673.0,0,,False,,,,
,learnmachinelearning," Coursera is free for university students now and soon I'll be having an interview for Data Science Intern position. I have experience mostly with machine learning with Scikit-learn, also Numpy, Scipy and Pandas, but not very much. I want to pick up some courses and prepare, I have quite a bit of time. For my case Python is preferable to R. Which ones would you suggest?",t2_wounm,False,,0,False,What Coursera courses would you suggest to prepare for Data Science Intern interview?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfaguq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Request,False,0,,False,self,False,,[],{},,,True,,1588899755.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Coursera is free for university students now and soon I&amp;#39;ll be having an interview for Data Science Intern position. I have experience mostly with machine learning with Scikit-learn, also Numpy, Scipy and Pandas, but not very much. I want to pick up some courses and prepare, I have quite a bit of time. For my case Python is preferable to R. Which ones would you suggest?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gfaguq,True,,qalis,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfaguq/what_coursera_courses_would_you_suggest_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfaguq/what_coursera_courses_would_you_suggest_to/,155203,1588870955.0,0,,False,,,,
,learnmachinelearning,,t2_63zbto72,False,,0,False,Implementation of 3D Photo inpainting,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_geg4zw,False,light,0.97,,public,390,0,{},140.0,,False,[],,True,False,,{},Project,False,390,,False,https://b.thumbs.redditmedia.com/EIp6H9fFk4O35-wQaMr_sLiRbMWNeCgCWjPO9dKZiro.jpg,False,,[],{},image,,False,,1588782832.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?format=png8&amp;s=a9ed8c98cb65e8bc0c748861c7210335ca6d2c6d', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0a0a6d8d67284d81d2e7768a2d0971296faaf77e', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=9635eb5d622349a9f1e3e5b7db99e3851a116972', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=77fa81dea801f2b4b1afd5923c173a9f10321c1f', 'width': 320, 'height': 426}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?s=3815b66431aa59597ecb25093cc7764dcadef142', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;crop=smart&amp;s=14ea85c8b61377d04b9696e61349d2874628794b', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;crop=smart&amp;s=f32d2c592959cbb6ad4c847faee8e002dfe8dfb6', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;crop=smart&amp;s=a5402ace5d65646084af38ba8ba99782b2830f71', 'width': 320, 'height': 426}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?format=mp4&amp;s=a9dc40864c4a37431eb2681075c75e9cc00eadff', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;format=mp4&amp;s=f259a806e3d4f3b18cbd4f3d7a541f0545d58237', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;format=mp4&amp;s=0fa7553043bea1f02bc6ae55c4f5161a56e7748a', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;format=mp4&amp;s=d11efe4f201e8dd2c57cc25001392e2e4c4a2f03', 'width': 320, 'height': 426}]}}, 'id': 'PxEWqC7qZfOJSIho3t9QpH-26WNojRnqWCQaq702UA8'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geg4zw,True,,wordflowai,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geg4zw/implementation_of_3d_photo_inpainting/,all_ads,False,https://i.redd.it/apdpu72qv3x41.gif,155203,1588754032.0,0,,False,,,,
,learnmachinelearning,,t2_24j8nwr,False,,0,False,"Hello, Iâ€™d like to make a neural network for getting points clouds to know what has been scanned or 3d scanners to know what has been scanned in order to automaticly process it in a drawn out plan. Is this possible?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf5w3k,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,default,False,,[],{},,,False,,1588884451.0,richtext,6,,,text,self.ArtificialInteligence,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf5w3k,True,,AVEdrums,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5w3k/hello_id_like_to_make_a_neural_network_for/,all_ads,False,/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/,155203,1588855651.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ArtificialInteligence', 'selftext': 'So Iâ€™m a student land sureveying and Iâ€™m doing research on how ML and AI and DL interferes in our profession and Iâ€™d like to make a neural network on: \n\nSo when we scan the environment, all these points are measured and Iâ€™d like to get the scanner to know what it has measured so it directly puts the information in the CAD program. So when it scans a wall, when I upload the data it automaticly draws a wall out of the points. The same for other aspects of objects of the buidling or environment. Is this possible? I really would like some help,thanks!!', 'author_fullname': 't2_24j8nwr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Hello, Iâ€™d like to make a neural network for getting points clouds to know what has been scanned or 3d scanners to know what has been scanned in order to automaticly process it in a drawn out plan. Is this possible?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ArtificialInteligence', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf5vll', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588884395.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ArtificialInteligence', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So Iâ€™m a student land sureveying and Iâ€™m doing research on how ML and AI and DL interferes in our profession and Iâ€™d like to make a neural network on: &lt;/p&gt;\n\n&lt;p&gt;So when we scan the environment, all these points are measured and Iâ€™d like to get the scanner to know what it has measured so it directly puts the information in the CAD program. So when it scans a wall, when I upload the data it automaticly draws a wall out of the points. The same for other aspects of objects of the buidling or environment. Is this possible? I really would like some help,thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3crzr', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gf5vll', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AVEdrums', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/', 'subreddit_subscribers': 34102, 'created_utc': 1588855595.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_gf5vll,,
,learnmachinelearning,"Please give me a link or the name of the site. 

I think it can help noobs like us become a little better in ML",t2_5fi9eeym,False,,0,False,Is there any place where I can practice mathematical problems or solve theoretical problems on ML?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf9p4z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588897404.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Please give me a link or the name of the site. &lt;/p&gt;

&lt;p&gt;I think it can help noobs like us become a little better in ML&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf9p4z,True,,dhokna,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf9p4z/is_there_any_place_where_i_can_practice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf9p4z/is_there_any_place_where_i_can_practice/,155203,1588868604.0,0,,False,,,,
,learnmachinelearning,"I'm processing 200k texts into embeddings of dimension 512, and Jupyter Notebook stops responding when it finishes.",t2_b6hjxvn,False,,0,False,"In Python, what can I use to process feature vectors and store them to clean away from memory?",[],r/learnmachinelearning,False,6,,0,,False,t3_gf5gol,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588882730.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m processing 200k texts into embeddings of dimension 512, and Jupyter Notebook stops responding when it finishes.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5gol,True,,R717159631668645,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5gol/in_python_what_can_i_use_to_process_feature/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5gol/in_python_what_can_i_use_to_process_feature/,155203,1588853930.0,0,,False,,,,
,learnmachinelearning,"In this tutorial, youâ€™ll learn how to encode melodies effectively to train a neural network with the aim to generate music. In the process, youâ€™ll also learn fundamental music theory concepts (e.g., key, time signature) that are important to understand the melody generation problem better.

This video is part of the series â€œGenerating melodies with LSTM netsâ€, a course thatâ€™ll teach you how to build RNN-LSTMs to generate effective melodies using TensorFlow.

Hereâ€™s the video:

[https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2](https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2)",t2_12ahau,False,,0,False,Music representation for melody generation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf8qtc,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1588894455.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this tutorial, youâ€™ll learn how to encode melodies effectively to train a neural network with the aim to generate music. In the process, youâ€™ll also learn fundamental music theory concepts (e.g., key, time signature) that are important to understand the melody generation problem better.&lt;/p&gt;

&lt;p&gt;This video is part of the series â€œGenerating melodies with LSTM netsâ€, a course thatâ€™ll teach you how to build RNN-LSTMs to generate effective melodies using TensorFlow.&lt;/p&gt;

&lt;p&gt;Hereâ€™s the video:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=LFDovU96EdY&amp;amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;amp;index=2""&gt;https://www.youtube.com/watch?v=LFDovU96EdY&amp;amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;amp;index=2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?auto=webp&amp;s=73d6e77c8501a3243a64934d63e734949dce6744', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d4e09f088ad5c8f245449dfbbfad938054fc5d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4372641e203f86eec7e32236b904dd4ead63fa6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13f227e3e50ff569cdf927461ba607f1531e232c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'iaCrpzcwgXUcic4WH0ZKOYi64AWzv1iJSFC4xa5F0r4'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf8qtc,True,,diabulusInMusica,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf8qtc/music_representation_for_melody_generation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf8qtc/music_representation_for_melody_generation/,155203,1588865655.0,0,,False,,,,
,learnmachinelearning,"in coursera,udemy or edx.

Thanks",t2_14sxpc49,False,,0,False,Any suggestions on Advanced Machine Learning MOOC's.,[],r/learnmachinelearning,False,6,,0,,False,t3_gewja9,False,dark,1.0,,public,16,0,{},,,False,[],,False,False,,{},,False,16,,False,self,False,,[],{},,,True,,1588841429.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;in coursera,udemy or edx.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gewja9,True,,esenthil,,23,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gewja9/any_suggestions_on_advanced_machine_learning_moocs/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gewja9/any_suggestions_on_advanced_machine_learning_moocs/,155203,1588812629.0,0,,False,,,,
,learnmachinelearning,,t2_329unddh,False,,0,False,Top 7 Frameworks That Have Enhanced Machine Learning,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,72.0,False,t3_gf3fcu,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},Discussion,False,3,,False,https://b.thumbs.redditmedia.com/Mg1gO0cutXWgjvPPRV2rPLK45VkKWnXMqlKdx-TV6gk.jpg,False,,[],{},link,,False,,1588872788.0,richtext,6,,,text,technostacks.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?auto=webp&amp;s=b20ad349d0d2a4e82e3b3e1bf6d5355f4f78e894', 'width': 1021, 'height': 529}, 'resolutions': [{'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f1b284895a959d0291fe835c7a88dfe044c07f6', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=396a3cc7afd29b7cc243c786a2bd8413f2ba0ebd', 'width': 216, 'height': 111}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bfb13357ce09a7812760beeef9816b2cc422ae7', 'width': 320, 'height': 165}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21843c0a2e83053cb7d6666a0eae7c14ecfcdf81', 'width': 640, 'height': 331}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=45c607ee7c925eda9196c1f4d9cef17f7fc8cb36', 'width': 960, 'height': 497}], 'variants': {}, 'id': 'L1eORhldnrXeb_c7TOriyOripKWWZdHgDDB6rDxb-e8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf3fcu,True,,MichaelOconnor1,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3fcu/top_7_frameworks_that_have_enhanced_machine/,all_ads,False,https://technostacks.com/blog/machine-learning-frameworks/,155203,1588843988.0,0,,False,,,,
,learnmachinelearning,"Hi !

I'm looking for learning buddies interested in learning reinforcement learning. I've played around with RL before, but I want to get more in depth. 

&amp;#x200B;

I'm planning on going though the book Reinforcement Learning An Introduction Second Edition, but I don't think we need to go through the same material to help each other out, although I would be fun to !

&amp;#x200B;

Anyways, if you're interested in learning RL, hit me up !",t2_2gaeqekv,False,,0,False,Looking for learning buddies to learn reinforcement learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gf87zh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588892747.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi !&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking for learning buddies interested in learning reinforcement learning. I&amp;#39;ve played around with RL before, but I want to get more in depth. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m planning on going though the book Reinforcement Learning An Introduction Second Edition, but I don&amp;#39;t think we need to go through the same material to help each other out, although I would be fun to !&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyways, if you&amp;#39;re interested in learning RL, hit me up !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf87zh,True,,simetin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf87zh/looking_for_learning_buddies_to_learn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf87zh/looking_for_learning_buddies_to_learn/,155203,1588863947.0,0,,False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,The Extent of AI and ML Technologyâ€™s Impact on the Banking Sector: The Blueprint for Future,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,64.0,False,t3_gf4nfq,False,light,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},Discussion,False,2,,False,https://b.thumbs.redditmedia.com/3Sv2Lh4sPhSBInnFgz2A0g7SaxJRThSEzPCWos7vIZQ.jpg,False,,[],{},link,,False,,1588879063.0,richtext,6,,,text,artiba.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?auto=webp&amp;s=7e73463528b3f1ec00ddad4dfdcb7cb1aa4f20b5', 'width': 800, 'height': 370}, 'resolutions': [{'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14a963b77d7ecc2cf7cfcaeebfe366b018b0eafb', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0dd6760440080dbb52c54adf6f0f8464380560e4', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb7647d16dc22a65332159736bde6e33451b150d', 'width': 320, 'height': 148}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31f74e8279935c76a869952b43eff761b26e51ab', 'width': 640, 'height': 296}], 'variants': {}, 'id': 'GbV_qWNexnb-NbFFtwMTFXkR2MgYvIQLnafehtQpeCY'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf4nfq,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4nfq/the_extent_of_ai_and_ml_technologys_impact_on_the/,all_ads,False,https://www.artiba.org/blog/ai-and-machine-learning-are-reshaping-the-global-banking-industry,155203,1588850263.0,0,,False,,,,
,learnmachinelearning,,t2_dpzgk,False,,0,False,Free Webinar on Introduction to Natural Language Processing,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_gf6wfk,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Un0eHYHmvVKy6ipnaOlVam6B-I6roaiEYrauR-jetlo.jpg,False,,[],{},link,,False,,1588888229.0,text,6,,,text,eventbrite.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?auto=webp&amp;s=76e72ca41b90717601e2641b902060778a0bc558', 'width': 1000, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=390d729b171af5dce45ff794338eed85ff77c32b', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e1b0ac00c1cc2618da260ae45982b34afb288b5', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9b0062658fb95b645cd09ad18bfcbab0867e76b', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ad44f249a984cba029eecad260e0931f576b3fa', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57c21d840eef27e3227d920511429e4e16631f82', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'zaiQsmrwpYwZxxNwh8-xb_WVPNqohHVWF54s80Or-Zw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6wfk,True,,Reginald_Martin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6wfk/free_webinar_on_introduction_to_natural_language/,all_ads,False,https://www.eventbrite.com/e/free-webinar-on-introduction-to-natural-language-processing-tickets-104126148112,155203,1588859429.0,0,,False,,,,
,learnmachinelearning,"I am working with mapped cylindrical images - so the image wraps around 360 degrees, with the left side ""joined"" to the right side. As such, my objects are always the same width as the image. The height is variable.

Is it possible to inform a region proposal network of this fact? I looked through detectron2's modules, and found [this line](https://github.com/facebookresearch/detectron2/blob/c526a492e4eb51cf8ad08b8af8f076dab72697e7/detectron2/structures/boxes.py#L210) regarding height and width threshold values for dropping/keeping a proposal.

Is it as simple as editing that to reference my image width? Or will this have some unintended side effects / not improve accuracy/false positives?

I suspect tuning the RPN network proposal size has been done before - but I couldn't find any specific references to it. If anyone has any resources/papers, please let me know!",t2_5hswm,False,,0,False,"I want to improve bounding box proposals in detectron2's RPN, as I know my objects always have a specific width. Is this possible?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf6r8t,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588887701.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working with mapped cylindrical images - so the image wraps around 360 degrees, with the left side &amp;quot;joined&amp;quot; to the right side. As such, my objects are always the same width as the image. The height is variable.&lt;/p&gt;

&lt;p&gt;Is it possible to inform a region proposal network of this fact? I looked through detectron2&amp;#39;s modules, and found &lt;a href=""https://github.com/facebookresearch/detectron2/blob/c526a492e4eb51cf8ad08b8af8f076dab72697e7/detectron2/structures/boxes.py#L210""&gt;this line&lt;/a&gt; regarding height and width threshold values for dropping/keeping a proposal.&lt;/p&gt;

&lt;p&gt;Is it as simple as editing that to reference my image width? Or will this have some unintended side effects / not improve accuracy/false positives?&lt;/p&gt;

&lt;p&gt;I suspect tuning the RPN network proposal size has been done before - but I couldn&amp;#39;t find any specific references to it. If anyone has any resources/papers, please let me know!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?auto=webp&amp;s=f680398e7963974ca0243b664917ac9952be3d5a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37c2c0153cd32a88568382d114cb65cee015284b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d00b5d2870902fc94ba53b0d4fd13f417386d957', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdf76d779dfc79c842c98f44da755e0c5860751d', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'TSNP042Xulx8o04vL-EtljVjrl9Il3rg6hZjwvpza3E'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf6r8t,True,,Fenr-i-r,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6r8t/i_want_to_improve_bounding_box_proposals_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6r8t/i_want_to_improve_bounding_box_proposals_in/,155203,1588858901.0,0,,False,,,,
,learnmachinelearning,"Hi, I am a software engineer looking for a solution by machine learning to compare my graphs.

My graphs are directed and each node has multiple dimension weights. I want to generate whole graph embeddings to compare. (not node nor edge embeddings)
Do you know any good software or ML algorithm to geranate them?

I have tried benedekrozemberczki/karateclub but it does not support directed graphs and nodes with weight.
Please give me your idea.
TIA.",t2_xmoea,False,,0,False,Directed graph embedding with node weight,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf6qfh,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588887620.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am a software engineer looking for a solution by machine learning to compare my graphs.&lt;/p&gt;

&lt;p&gt;My graphs are directed and each node has multiple dimension weights. I want to generate whole graph embeddings to compare. (not node nor edge embeddings)
Do you know any good software or ML algorithm to geranate them?&lt;/p&gt;

&lt;p&gt;I have tried benedekrozemberczki/karateclub but it does not support directed graphs and nodes with weight.
Please give me your idea.
TIA.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gf6qfh,True,,metaphoricwords,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6qfh/directed_graph_embedding_with_node_weight/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6qfh/directed_graph_embedding_with_node_weight/,155203,1588858820.0,0,,False,,,,
,learnmachinelearning,,t2_1myz87vv,False,,0,False,Cohen's kappa coefficient in Python,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_gf6lha,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/nnrqMOV9NWrNuZaeZjdCiSZWkJDoEXofEY9uQRQXmuc.jpg,False,,[],{},link,,False,,1588887116.0,text,6,,,text,bush-dev.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?auto=webp&amp;s=c5601aefd19481d4fea7093158128ee8bc7779ee', 'width': 1000, 'height': 667}, 'resolutions': [{'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=833a0bc74755bf7b46adcafd7086c178b75d1138', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88242a3100f1d216d5bb1d8413e8dae72f91b03c', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3a0d9463b348f5e188de85888d91c549dea6bbc', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d78fb7a33829fa598ec0176b8edb1948b60f14a3', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84901931ba1cf1e6e285ee05e608d41d388afb32', 'width': 960, 'height': 640}], 'variants': {}, 'id': 'cZgTK8hmv5pC-zhmJ_Ih3wjEo1GyXpxQYLV3ccidUQE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6lha,True,,bush_dev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6lha/cohens_kappa_coefficient_in_python/,all_ads,False,http://bush-dev.com/cohens-kappa-coefficient-in-python,155203,1588858316.0,0,,False,,,,
,learnmachinelearning,"Hi,
I used Random Forest to predict what measures a government must take to achieve a Covid-19 growth rate of less than 5%. I got 99% accuracy but 61% specificity. Is it a good model?

https://www.kaggle.com/gianlab/government-measures-against-covid-2",t2_4dpo2m4y,False,,0,False,Government measures against Covid,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5dwq,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588882416.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,
I used Random Forest to predict what measures a government must take to achieve a Covid-19 growth rate of less than 5%. I got 99% accuracy but 61% specificity. Is it a good model?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/gianlab/government-measures-against-covid-2""&gt;https://www.kaggle.com/gianlab/government-measures-against-covid-2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/x4RZ-_GIkVwaUtmgleV05BCjOdsqUGfFaLyBlP72NUg.jpg?auto=webp&amp;s=6d3f76b84a633a2a41c9c2d39039f059c1f09974', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'eE0bEqjeT_FoRR2Lo7jX20-FVqPnvpBwCX-_MlcZFX8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5dwq,True,,lovepeacejoy4,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5dwq/government_measures_against_covid/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5dwq/government_measures_against_covid/,155203,1588853616.0,0,,False,,,,
,learnmachinelearning,"I am unable to understand the relationship between training and validation performance metrics for my NLP experiment (BERT-LSTM-Linear model). Are there any papers out there exploring what causes 

1. High training accuracy, low validation accuracy
2. Oscillating loss during training
3. A decrease in F1 scores (weight, micro, macro) during training

I understand that it must change from data to data, but reading through some papers describing relationships between the metrics or even trying to explore might help put things into some perspective.",t2_9ptho1m,False,,0,False,NLP papers exploring training and validation performance metrics?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf550p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588881397.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am unable to understand the relationship between training and validation performance metrics for my NLP experiment (BERT-LSTM-Linear model). Are there any papers out there exploring what causes &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;High training accuracy, low validation accuracy&lt;/li&gt;
&lt;li&gt;Oscillating loss during training&lt;/li&gt;
&lt;li&gt;A decrease in F1 scores (weight, micro, macro) during training&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I understand that it must change from data to data, but reading through some papers describing relationships between the metrics or even trying to explore might help put things into some perspective.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf550p,True,,freaky_eater,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf550p/nlp_papers_exploring_training_and_validation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf550p/nlp_papers_exploring_training_and_validation/,155203,1588852597.0,0,,False,,,,
,learnmachinelearning,"Isn't there a term for a problem with no hidden information, e.g. the problem state is fully visible at each iteration?",t2_4we1a,False,,0,False,[Question] Name for the term of problems with complete information,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5453,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588881289.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Isn&amp;#39;t there a term for a problem with no hidden information, e.g. the problem state is fully visible at each iteration?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5453,True,,Stewie977,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5453/question_name_for_the_term_of_problems_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5453/question_name_for_the_term_of_problems_with/,155203,1588852489.0,0,,False,,,,
,learnmachinelearning,"Here is a [TF example](https://github.com/tensorflow/examples/blob/master/community/en/autoencoder.ipynb) for autoencoder and it passes a single vector of MINST dataset with fixed size  (784,) . 

If my input data is not fixed-size then what should I do? how can I change it to be flexible in input data?",t2_1k4nqnsa,False,,0,False,Autoencoder with not fix size,[],r/learnmachinelearning,False,6,,0,,False,t3_gf1qhl,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588863911.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is a &lt;a href=""https://github.com/tensorflow/examples/blob/master/community/en/autoencoder.ipynb""&gt;TF example&lt;/a&gt; for autoencoder and it passes a single vector of MINST dataset with fixed size  (784,) . &lt;/p&gt;

&lt;p&gt;If my input data is not fixed-size then what should I do? how can I change it to be flexible in input data?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1qhl,True,,kiasari,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1qhl/autoencoder_with_not_fix_size/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf1qhl/autoencoder_with_not_fix_size/,155203,1588835111.0,0,,False,,,,
,learnmachinelearning,"Hello!  


I'm writing this post because I need help with my college project - I am saying this because I have some strict requirements due to that. One of them is necessity of using model that will help with predicting certain values. I hope that you will be able to give me some advice.

My project in few words is tool for predicting your internet speed- based on measurements. The more measurements there are gathered, the better result.

The most important part of my project is the predictor. I have automated process of gathering data, now its time to use that data. And here comes my problem. I decided to use ARIMA model for time series forecasting. But it turned out to be complete disaster with my data. I don't know if arima in python is just not that good as it should be or it's just wrong model for my task. 

Should I maybe use R language for that? will it perform better? Which model should I use with my data? 

Here's sample of my data - it's not complete for now. It will be spectrum of full 24hours.

[https://i.imgur.com/xuyFiSx.png](https://i.imgur.com/xuyFiSx.png)

&amp;#x200B;

Thanks for any advice and have a nice day!",t2_nst5o,False,,0,False,Which model should I use for my predictor? (little data)(timeseriesforecasting),[],r/learnmachinelearning,False,6,,0,,False,t3_gf4b9d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588877373.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!  &lt;/p&gt;

&lt;p&gt;I&amp;#39;m writing this post because I need help with my college project - I am saying this because I have some strict requirements due to that. One of them is necessity of using model that will help with predicting certain values. I hope that you will be able to give me some advice.&lt;/p&gt;

&lt;p&gt;My project in few words is tool for predicting your internet speed- based on measurements. The more measurements there are gathered, the better result.&lt;/p&gt;

&lt;p&gt;The most important part of my project is the predictor. I have automated process of gathering data, now its time to use that data. And here comes my problem. I decided to use ARIMA model for time series forecasting. But it turned out to be complete disaster with my data. I don&amp;#39;t know if arima in python is just not that good as it should be or it&amp;#39;s just wrong model for my task. &lt;/p&gt;

&lt;p&gt;Should I maybe use R language for that? will it perform better? Which model should I use with my data? &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s sample of my data - it&amp;#39;s not complete for now. It will be spectrum of full 24hours.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.imgur.com/xuyFiSx.png""&gt;https://i.imgur.com/xuyFiSx.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any advice and have a nice day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vuPSR-xlP0WJKC0qV4QDwV1q557vzZGwUsXTZZ5Ec1k.png?auto=webp&amp;s=b198bda107895ddd36a4e9f9dd4dde9f3d6cf6e3', 'width': 182, 'height': 865}, 'resolutions': [{'url': 'https://external-preview.redd.it/vuPSR-xlP0WJKC0qV4QDwV1q557vzZGwUsXTZZ5Ec1k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=16715678af7abbfa80d9a1562d16f49dc1981e6b', 'width': 108, 'height': 216}], 'variants': {}, 'id': 'DHgshvdwrgf92TAKD16PfKn4h_sIv5tujj0g5VZmnus'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4b9d,True,,cl_m4ster,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4b9d/which_model_should_i_use_for_my_predictor_little/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4b9d/which_model_should_i_use_for_my_predictor_little/,155203,1588848573.0,0,,False,,,,
,learnmachinelearning,"Any good books for Tensorflow 2.0 and also any online course for all levels. Thanks for the help,",t2_14sxpc49,False,,0,False,Tensorflow 2.0 Books or courses,[],r/learnmachinelearning,False,6,,0,,False,t3_gewl58,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1588841608.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any good books for Tensorflow 2.0 and also any online course for all levels. Thanks for the help,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gewl58,True,,esenthil,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gewl58/tensorflow_20_books_or_courses/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gewl58/tensorflow_20_books_or_courses/,155203,1588812808.0,0,,False,,,,
,learnmachinelearning,"I have total 3000 900x900 matrix( same as 1 band image) include SAR scatter values. (SAR scatter pixels give us geometry information where signals comes like ""34.515, 29.1618, 15.5489""). I want to use this matrix feature. My labels is total 3000 900x900 matrix also there are pixels only two values 0 and 1. 1 is building 0 is non-building. I'd like train my scatter values for labels 1 or 0 . After that i will use created model on my test SAR images.

1- Which losses, activations, optimazer etc. may i use ?

2- I tried several times in Python (Tensorflow) but there is a lot of features (3000x900x900) and labels same dimension as well. There is an error OOM. For this error can i use genaretor ? Is this make sense?

3- I am new to machine learning if there is a nonsense in my sentences sorry about it.
Thanks for your helps",t2_3dwu4yfm,False,,0,False,Suggestion machine learning model and tips multi dimensional input and output,[],r/learnmachinelearning,False,6,,0,,False,t3_gf3nv8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588874033.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have total 3000 900x900 matrix( same as 1 band image) include SAR scatter values. (SAR scatter pixels give us geometry information where signals comes like &amp;quot;34.515, 29.1618, 15.5489&amp;quot;). I want to use this matrix feature. My labels is total 3000 900x900 matrix also there are pixels only two values 0 and 1. 1 is building 0 is non-building. I&amp;#39;d like train my scatter values for labels 1 or 0 . After that i will use created model on my test SAR images.&lt;/p&gt;

&lt;p&gt;1- Which losses, activations, optimazer etc. may i use ?&lt;/p&gt;

&lt;p&gt;2- I tried several times in Python (Tensorflow) but there is a lot of features (3000x900x900) and labels same dimension as well. There is an error OOM. For this error can i use genaretor ? Is this make sense?&lt;/p&gt;

&lt;p&gt;3- I am new to machine learning if there is a nonsense in my sentences sorry about it.
Thanks for your helps&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3nv8,True,,cartwhell07,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3nv8/suggestion_machine_learning_model_and_tips_multi/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf3nv8/suggestion_machine_learning_model_and_tips_multi/,155203,1588845233.0,0,,False,,,,
,learnmachinelearning,,t2_5pglpm9p,False,,0,False,5 Booming AI and ML Trends 2020,[],r/learnmachinelearning,False,6,,0,73.0,False,t3_gf3ngt,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/u1YhSofEcxxN8XbWeaejEGeQJSHaTmaJsJJw5CQzhbk.jpg,False,,[],{},link,,False,,1588873978.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?auto=webp&amp;s=125a2fc27267efe35cace13d9fa3170570be6ba8', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d70e961e7631d50c96c6ce6c92b46344c6f80e5', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e18ef5af193df2dc849587e6cebc07a4ac4c8eb3', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3226865fe5f3698229c923638f94f46fe7016c2', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d705dd59e553805311b3c8670705dbe8ba599f6c', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f7136325cbd84265c9e7c8f687aecbb9f6cdf8b', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72634008d56f3f3a588db6ecadd5f2d7da3bffae', 'width': 1080, 'height': 565}], 'variants': {}, 'id': '3Cwo_31cs1xzlXXKDTbqT1iw_-TqVOL1iu_gR5cQWzw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3ngt,True,,Ramesh_Sethi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3ngt/5_booming_ai_and_ml_trends_2020/,all_ads,False,https://medium.com/@Oodles_ai/5-booming-ai-and-ml-trends-2020-5f22589eea3e,155203,1588845178.0,0,,False,,,,
,learnmachinelearning,,t2_4hnnxddz,False,,0,False,Log transformation increased my data Skewness. Need help,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_geuwia,False,light,0.9,,public,7,0,{},140.0,,False,[],,True,False,,{},HELP,False,7,,False,https://b.thumbs.redditmedia.com/PsoxyfOIStJG2jHxu71wCL0vQkux6alytnQycK7NFOs.jpg,False,,[],{},image,,False,,1588835590.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?auto=webp&amp;s=2dbeac26cffa40bb02b1a9ee275839223ab9bbe0', 'width': 1920, 'height': 1920}, 'resolutions': [{'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4177ba99515adea53b2925703c4b6a914e6dfaf8', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b94f4336d487c8a4fe5219baa287bf5604aba3e', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cf0f6bedf127a0b68b18f1ab8bbff7e8367ece1', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d361256d81323d4d0819c0ccc43d15c396a97fa5', 'width': 640, 'height': 640}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=739d19b23fba63d57e025216d9bf927df73b1eeb', 'width': 960, 'height': 960}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd5efe99410cdd52bd7bb244e7877e9ec4ef403c', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'LkI25zMKYOb9GHdTNUFB9vXJtks65Oaiow6OBPeyF30'}], 'enabled': True}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geuwia,True,,hyper482,,18,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuwia/log_transformation_increased_my_data_skewness/,all_ads,False,https://i.redd.it/87hd4e5s88x41.jpg,155203,1588806790.0,0,,False,,,,
,learnmachinelearning,"The profile has a major shift in behavior (e.g. Didn't post frequently and suddenly posts frequently).

What are the possible ways to approach this problem???",t2_tabm4,False,,0,False,How to detect sudden user behavior change on twitter over time?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf3j8o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588873355.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The profile has a major shift in behavior (e.g. Didn&amp;#39;t post frequently and suddenly posts frequently).&lt;/p&gt;

&lt;p&gt;What are the possible ways to approach this problem???&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3j8o,True,,waheed0332,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3j8o/how_to_detect_sudden_user_behavior_change_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf3j8o/how_to_detect_sudden_user_behavior_change_on/,155203,1588844555.0,0,,False,,,,
,learnmachinelearning,,t2_47jpmh5m,False,,0,False,Your complete learning path to start a career in machine learning.,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geq3ln,False,dark,0.75,,public,11,0,{},140.0,,False,[],,False,False,,{},,False,11,,False,https://b.thumbs.redditmedia.com/A0e0SeE-wdwF4-aKyyKQyR6MjETnTl0Ro_7PNy2QWMw.jpg,False,,[],{},link,,False,,1588820231.0,text,6,,,text,kdnuggets.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?auto=webp&amp;s=524afcdaf62407becb958f6dec41743d5cdbd5a9', 'width': 1400, 'height': 1866}, 'resolutions': [{'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3faf100be8c1920cd5b68579054b4d66cfa24bb5', 'width': 108, 'height': 143}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3ee3a09706729af760d552dfeac1c6c0375ead6', 'width': 216, 'height': 287}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84720d18e8b849e0a9a02247636a8b010ccda1f4', 'width': 320, 'height': 426}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa7b12162f81e806ad9828ca0b8bbded6ca4002', 'width': 640, 'height': 853}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7963756ad494dbcb0cd249e8ea0652a261077d06', 'width': 960, 'height': 1279}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d349e383c3a5a881869a6f4b811cca114fe9cd4a', 'width': 1080, 'height': 1439}], 'variants': {}, 'id': 'eneKBvxPey6iAK0yJy6VELxOxNxBg5TW9S_sE2SNcT4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geq3ln,True,,ItisAhmad,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geq3ln/your_complete_learning_path_to_start_a_career_in/,all_ads,False,https://www.kdnuggets.com/2020/05/beginners-learning-path-machine-learning.html,155203,1588791431.0,0,,False,,,,
,learnmachinelearning,https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787,t2_45tb49o2,False,,0,False,Understanding the mathematics behind linear regression- part 1,[],r/learnmachinelearning,False,6,,0,,False,t3_gf26j6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588866241.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787""&gt;https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf26j6,True,,codegeass30,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf26j6/understanding_the_mathematics_behind_linear/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf26j6/understanding_the_mathematics_behind_linear/,155203,1588837441.0,0,,False,,,,
,learnmachinelearning,"I'm doing multiclass classification in python. There are over 300 classes and 5 instances for each class. I'm currently using random forest classifier. After extensive hyperparameter tuning, the best accuracy performance is around 10% only. I've done other classification problems pretty well so I'm thinking what is causing such bad performance. 

I'd like to know if 5 instances for each class for training is too little and I'd like any suggestions on the problem. Thank you ðŸ˜Š",t2_5oiyst74,False,,0,False,5 samples per class for classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gf22mf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588865672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing multiclass classification in python. There are over 300 classes and 5 instances for each class. I&amp;#39;m currently using random forest classifier. After extensive hyperparameter tuning, the best accuracy performance is around 10% only. I&amp;#39;ve done other classification problems pretty well so I&amp;#39;m thinking what is causing such bad performance. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to know if 5 instances for each class for training is too little and I&amp;#39;d like any suggestions on the problem. Thank you ðŸ˜Š&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf22mf,True,,xxare,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf22mf/5_samples_per_class_for_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf22mf/5_samples_per_class_for_classification/,155203,1588836872.0,0,,False,,,,
,learnmachinelearning,,t2_1jddy6x5,False,,0,False,Holy smoke! Differentiation is now upstreamed to Swift. It appropriate to say ð›Swift. Yay! ðŸŽ‰ ðŸ¥³,[],r/learnmachinelearning,False,6,,0,138.0,False,t3_gf1upz,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Iohh_96XHituEFUpTbZD5UY8Z_ygTTb4bZ30cugxoqY.jpg,False,,[],{},link,,False,,1588864514.0,text,6,,,text,forums.swift.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?auto=webp&amp;s=f629c0a49e4aaa92449ba32794cf3e737717b518', 'width': 590, 'height': 585}, 'resolutions': [{'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d3658b06e9263859a81ad28fd0b4e39fa6ae4da', 'width': 108, 'height': 107}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6abf1209e759dbbd85985f6a6039c6c82d4403a', 'width': 216, 'height': 214}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8dceba83e7b210c9e71853c9314981e05e36e5d', 'width': 320, 'height': 317}], 'variants': {}, 'id': 'pEdzDuPhjQNkUbvjZWcOHMphCOfFW9cWD3aLjM-IrtU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1upz,True,,rahulbhalley,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1upz/holy_smoke_differentiation_is_now_upstreamed_to/,all_ads,False,https://forums.swift.org/t/trajectory-for-evaluating-adding-automatic-differentiation-to-swift/30048/7?u=dan-zheng,155203,1588835714.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'swift', 'selftext': '', 'author_fullname': 't2_1jddy6x5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Holy smoke! Differentiation is now upstreamed to Swift. It appropriate to say ð›Swift. Yay! ðŸŽ‰ ðŸ¥³', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/swift', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 138, 'hide_score': False, 'name': 't3_gf1tie', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 147, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 147, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Iohh_96XHituEFUpTbZD5UY8Z_ygTTb4bZ30cugxoqY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588864341.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'forums.swift.org', 'allow_live_comments': True, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?auto=webp&amp;s=f629c0a49e4aaa92449ba32794cf3e737717b518', 'width': 590, 'height': 585}, 'resolutions': [{'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d3658b06e9263859a81ad28fd0b4e39fa6ae4da', 'width': 108, 'height': 107}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6abf1209e759dbbd85985f6a6039c6c82d4403a', 'width': 216, 'height': 214}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8dceba83e7b210c9e71853c9314981e05e36e5d', 'width': 320, 'height': 317}], 'variants': {}, 'id': 'pEdzDuPhjQNkUbvjZWcOHMphCOfFW9cWD3aLjM-IrtU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2z6zi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gf1tie', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'rahulbhalley', 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/swift/comments/gf1tie/holy_smoke_differentiation_is_now_upstreamed_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://forums.swift.org/t/trajectory-for-evaluating-adding-automatic-differentiation-to-swift/30048/7?u=dan-zheng', 'subreddit_subscribers': 62949, 'created_utc': 1588835541.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",t3_gf1tie,,
,learnmachinelearning,"I wrote a technical article on how to structure Juptyer notebooks for machine learning projects. Basically my workflow and tips on using Jupyter notebook for productive DS tasks. Let me know what you think, thanks!

https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb",t2_j9tvg,False,,0,False,I wrote a technical article on how to structure Juptyer notebooks for Machine Learning projects,[],r/learnmachinelearning,False,6,,0,,False,t3_gf1pdw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588863764.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a technical article on how to structure Juptyer notebooks for machine learning projects. Basically my workflow and tips on using Jupyter notebook for productive DS tasks. Let me know what you think, thanks!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb""&gt;https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?auto=webp&amp;s=8a738956afb851262167a873d329931f52ee42b7', 'width': 1200, 'height': 752}, 'resolutions': [{'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d61980aea9b9080ee7fa192883f3b9fcf385464', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d252fbdeae13283cefdf7b0a217567948888216', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1626610262de91c49473f71013df141f024fb5f', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5bfc54e9754b2e9d4fcf1bf59a13acf21d3840f', 'width': 640, 'height': 401}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfec06e571892c7851e78daf83e4482170893563', 'width': 960, 'height': 601}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7f15a3dd15ade69da3102188184df1d91d10581', 'width': 1080, 'height': 676}], 'variants': {}, 'id': '_BddIxP4XWrPlelML-pKNBjdx_-mOdsZuo7G7NjrzhU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1pdw,True,,desmondyeoh,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1pdw/i_wrote_a_technical_article_on_how_to_structure/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf1pdw/i_wrote_a_technical_article_on_how_to_structure/,155203,1588834964.0,0,,False,,,,
,learnmachinelearning,"So I know how to solve probability questions in combinatorics and about discrete/continuous CDF/PDFs / common distributions etc.

But everytime I read a book (e.g., ""Statistical Inference"" or ""All of Statistics: A Concise Course in Statistical Inference""), there's a moment where they introduce a marginal distribution or something (actually I feel like I understand the idea for a discrete case) and continuous priors and I'm kinda lost without seeing any simple examples.

I wanna learn the idea behind the methods of maximum likelihood and random buzzwords like Fisher information.",t2_l109wse,False,,0,False,Could you recommend me a book about statistics (the details inside)?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gerz9k,False,light,0.84,,public,4,0,{},,,False,[],,False,False,,{},HELP,False,4,,False,self,False,,[],{},,,True,,1588826080.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I know how to solve probability questions in combinatorics and about discrete/continuous CDF/PDFs / common distributions etc.&lt;/p&gt;

&lt;p&gt;But everytime I read a book (e.g., &amp;quot;Statistical Inference&amp;quot; or &amp;quot;All of Statistics: A Concise Course in Statistical Inference&amp;quot;), there&amp;#39;s a moment where they introduce a marginal distribution or something (actually I feel like I understand the idea for a discrete case) and continuous priors and I&amp;#39;m kinda lost without seeing any simple examples.&lt;/p&gt;

&lt;p&gt;I wanna learn the idea behind the methods of maximum likelihood and random buzzwords like Fisher information.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gerz9k,True,,giannis_34,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gerz9k/could_you_recommend_me_a_book_about_statistics/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gerz9k/could_you_recommend_me_a_book_about_statistics/,155203,1588797280.0,0,,False,,,,
,learnmachinelearning,"How can I encourage a network's output to be sparse? 

I'm in an RL setting, creating trajectories by giving an input state, then sampling a continuous vector action. Then eventually I train on good trajectories. I know that sparse outputs will be better than denser ones by nature of the task. Should I just randomly mask some idxs of the output?",t2_12zuf2,False,,0,False,How to encourage sparse output?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf09yz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588856671.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How can I encourage a network&amp;#39;s output to be sparse? &lt;/p&gt;

&lt;p&gt;I&amp;#39;m in an RL setting, creating trajectories by giving an input state, then sampling a continuous vector action. Then eventually I train on good trajectories. I know that sparse outputs will be better than denser ones by nature of the task. Should I just randomly mask some idxs of the output?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf09yz,True,,throwaway775849,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf09yz/how_to_encourage_sparse_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf09yz/how_to_encourage_sparse_output/,155203,1588827871.0,0,,False,,,,
,learnmachinelearning,,t2_60jokg20,False,,0,False,Guys please suggest me from where should I learn statistics for machine learning .,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf06fg,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588856231.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf06fg,True,,omkar_00,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf06fg/guys_please_suggest_me_from_where_should_i_learn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf06fg/guys_please_suggest_me_from_where_should_i_learn/,155203,1588827431.0,0,,False,,,,
,learnmachinelearning,"If articles like this are not allowed please remove.

All of us here would be familiar with jupyter notebooks, they are a great tool for learning and coding for data science and machine learning.

One constant annoyance for me is the fact that most of these notebooks are so un-intuitive and it takes forever to trace variables through the code to figure out what's going on. I'm writing a series of posts on how to improve user experience in jupyter notebooks; I've published my first one on handling user input. I welcome any feedback.

[https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d](https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d)",t2_faofu,False,,0,False,User experience in jypyter notebooks,[],r/learnmachinelearning,False,6,,0,,False,t3_gf05bu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1588856086.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If articles like this are not allowed please remove.&lt;/p&gt;

&lt;p&gt;All of us here would be familiar with jupyter notebooks, they are a great tool for learning and coding for data science and machine learning.&lt;/p&gt;

&lt;p&gt;One constant annoyance for me is the fact that most of these notebooks are so un-intuitive and it takes forever to trace variables through the code to figure out what&amp;#39;s going on. I&amp;#39;m writing a series of posts on how to improve user experience in jupyter notebooks; I&amp;#39;ve published my first one on handling user input. I welcome any feedback.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d""&gt;https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?auto=webp&amp;s=db10c1c33dd1d24a61ed23c5df2cf024e4efce41', 'width': 1200, 'height': 794}, 'resolutions': [{'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4438df21957bb5013befe7b16bfec7b62c86f080', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=717928c91a188d140568d5c7c45ebe635e591b26', 'width': 216, 'height': 142}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c837096ca122a82ea01d6af89865f11561e4322d', 'width': 320, 'height': 211}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8383635d07da8ba5485b3224d89e021ab7878a4a', 'width': 640, 'height': 423}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f256c71068288d66a35ad0ceb167b126bda2182', 'width': 960, 'height': 635}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15724ec04be6b189a3f673b123c34f3862db710', 'width': 1080, 'height': 714}], 'variants': {}, 'id': 'mqEwOWA7Q6KbXlsNkxzPMqAvcLEiKXFlt3pmCbGsevs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf05bu,True,,thezaza101,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf05bu/user_experience_in_jypyter_notebooks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf05bu/user_experience_in_jypyter_notebooks/,155203,1588827286.0,0,,False,,,,
,learnmachinelearning,"Recently,Iâ€˜m trying to make my App more intelligent and close to the needs of users.Because most of my users use Huawei phones,I used HUAWEI ML Kit on my app.

Currently the experience is very good,Or do you have a better experience?

&amp;#x200B;

https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;format=png&amp;auto=webp&amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1",t2_67j6c1h9,False,,0,False,"In machine learning, I have made new discoveries","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,55.0,False,t3_gf2h70,False,light,0.43,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/3UtCSHUB7kiLhGriCG_aJ2rMpICKeXqL-LaxXoawdgU.jpg,False,,[],{},,,True,,1588867805.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recently,Iâ€˜m trying to make my App more intelligent and close to the needs of users.Because most of my users use Huawei phones,I used HUAWEI ML Kit on my app.&lt;/p&gt;

&lt;p&gt;Currently the experience is very good,Or do you have a better experience?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1""&gt;https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf2h70,True,,Ivy_zhao,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2h70/in_machine_learning_i_have_made_new_discoveries/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf2h70/in_machine_learning_i_have_made_new_discoveries/,155203,1588839005.0,0,,False,,,"{'d6oquz2kwax41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=14e043b7fefdc0d88d394ef28dc9dfc9e8a02322'}, {'y': 86, 'x': 216, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=665c9edb85886b9b06878f670450dfb99a355c25'}, {'y': 127, 'x': 320, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0a7a3b8ed43d10443aee49b39ea9a9ee4e43c96'}], 's': {'y': 240, 'x': 602, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;format=png&amp;auto=webp&amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1'}, 'id': 'd6oquz2kwax41'}}",
,learnmachinelearning,,t2_695y5lr4,False,,0,False,Understanding the shape of large scale data,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_getrpw,False,light,0.67,,public,2,0,{},140.0,,False,[],,False,False,,{},Discussion,False,2,,False,https://a.thumbs.redditmedia.com/ODx-Gi0xkgxOEKqbBtbJ-CUWtjyIiblb219uJo6dAC0.jpg,False,,[],{},link,,False,,1588831789.0,richtext,6,,,text,ai.googleblog.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?auto=webp&amp;s=d45552298a94c0bc0e771853afe179cbb0e3f951', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=467975d187bd3f0e5cf8f0880665db7e4eca4fcb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=121a848b6e9c40ce8f4c995b663108493b3b069d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e62560ba614db17109e1924a187a6575b9f7399', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46bf2a0eba7661a832cec202bf2cec4660a37b4e', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73be8d146553ec5d17c995a6cc1c0f963532ae61', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af590ed730005b197bb9c10c9bc2c173078730ef', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'q7V8BDv9opANwPukjjPK-R82fNE4Qq4vwXKRxiy1DHo'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,getrpw,True,,jsamwrites,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getrpw/understanding_the_shape_of_large_scale_data/,all_ads,False,https://ai.googleblog.com/2020/05/understanding-shape-of-large-scale-data.html,155203,1588802989.0,0,,False,,,,
,learnmachinelearning,"Hello all, I'm a 3rd year undergraduate. I love to contribute to AI research community. Bit how to start doing research in AI? 
In my opinion the order of doing research is as follows:
1. Find the problem to solve
2. Read many research papers that are already piblished in the related problem area
3. After knowing about the problem through papers, blogs, etc.. 'You will probably find an idea towards the solution  of your problem'

But, after reading those papers what if I can't find the solution? What if I don't get any idea to solve the problem? 

Should I change my area of research? Will research take a lot of time? To publishing a paper and to contribute to the community, will it take a lot of time?

I'm struck, How to get started in research? I'm still a 3rd year undergraduate. 

Thankyou",t2_6dn8cxrd,False,,0,False,Advice for research!,[],r/learnmachinelearning,False,6,,0,,False,t3_gezbji,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588852298.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I&amp;#39;m a 3rd year undergraduate. I love to contribute to AI research community. Bit how to start doing research in AI? 
In my opinion the order of doing research is as follows:
1. Find the problem to solve
2. Read many research papers that are already piblished in the related problem area
3. After knowing about the problem through papers, blogs, etc.. &amp;#39;You will probably find an idea towards the solution  of your problem&amp;#39;&lt;/p&gt;

&lt;p&gt;But, after reading those papers what if I can&amp;#39;t find the solution? What if I don&amp;#39;t get any idea to solve the problem? &lt;/p&gt;

&lt;p&gt;Should I change my area of research? Will research take a lot of time? To publishing a paper and to contribute to the community, will it take a lot of time?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m struck, How to get started in research? I&amp;#39;m still a 3rd year undergraduate. &lt;/p&gt;

&lt;p&gt;Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gezbji,True,,saiyan6174,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gezbji/advice_for_research/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gezbji/advice_for_research/,155203,1588823498.0,0,,False,,,,
,learnmachinelearning,"I have a medium-sized project that's outgrown my local GPU and figured I'd give it it a try on google colab. I've looked at a few tutorials and I'm a bit confused about the general workflow, and session persistence. Namely, there's a fair number of steps involved in setting up the environment, pointing it to my large-ish file database, and of course, doing this all in jupyter notebook is all kind of tedious. Does the colab hosting ensure my setup efforts all ""persist"" or am I going to have to do this each and every time? Ideally, I'd like to just upload my local python script and run it but it seems it doesn't quite work that way.",t2_3hprhzvw,False,,0,False,Some beginner questions about google Colab,[],r/learnmachinelearning,False,6,,0,,False,t3_geytzr,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588850234.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a medium-sized project that&amp;#39;s outgrown my local GPU and figured I&amp;#39;d give it it a try on google colab. I&amp;#39;ve looked at a few tutorials and I&amp;#39;m a bit confused about the general workflow, and session persistence. Namely, there&amp;#39;s a fair number of steps involved in setting up the environment, pointing it to my large-ish file database, and of course, doing this all in jupyter notebook is all kind of tedious. Does the colab hosting ensure my setup efforts all &amp;quot;persist&amp;quot; or am I going to have to do this each and every time? Ideally, I&amp;#39;d like to just upload my local python script and run it but it seems it doesn&amp;#39;t quite work that way.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geytzr,True,,Theweekendstate,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geytzr/some_beginner_questions_about_google_colab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geytzr/some_beginner_questions_about_google_colab/,155203,1588821434.0,0,,False,,,,
,learnmachinelearning,"I am trying to create a generative/discriminator network system. My goal is for a generator to create a pattern of pixels (28x28), overlay it on a background image, then have a discriminator try to locate the pattern in the background image. However, I am struggling to write the custom training loop. It has something to do with going back and forth between tensors and nd\_arrays, but I can't figure out how to solve my problem without doing so.

Here's a link to a notebook that has an example:

[https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV\_?usp=sharing](https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing)

Here is the training loop:

`#@tf.function`  
`#the example code had this annotation, but my code breaks with it`  
`def train_step():`  
 `#Generate noisy seeds`  
`noise = tf.random.normal([BATCH_SIZE, noise_dim])`  
 `with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:`  
`pattern = generator(noise, training=True)`  
`gen_tape.watch(pattern)`  
`imageDataset, lblDataset = getDatasetFromPattern(np.reshape(pattern, (28,28)), 1)`  
`discriminator_output = discriminator(imageDataset, training=True)`  
`gen_loss = generator_loss(lblDataset, discriminator_output)`  
`disc_loss = discriminator_loss(lblDataset, discriminator_output)`  
`gradients_of_generator = gen_tape.gradient(disc_loss, generator.trainable_variables, gen_tape)`  
`gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)`  
`generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))`  
`discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))`

Here s the getDatasetFromPattern pattern:

`def getDatasetFromPattern(nd_array, num_samples):`  
`height = nd_array.shape[0]`  
`width = nd_array.shape[1]`  
`backgrounds = [Image.fromarray(np.zeros((640, 480, CHANNELS), dtype=np.uint8))]`  
`patternImg = Image.fromarray(nd_array, 'L')`  
`imgArray = np.ndarray((num_samples * len(backgrounds),`  
`BACKGROUND_HEIGHT, BACKGROUND_WIDTH, 3))`  
   
`lblArray = np.ndarray((num_samples * len(backgrounds), 2))`  
 `for i in range(num_samples):`  
`x = random.randint(0, BACKGROUND_WIDTH - width)`  
`y = random.randint(0, BACKGROUND_HEIGHT - height)`  
 `for bg in backgrounds:`  
`bg.paste(patternImg, (x, y), patternImg.convert(""RGBA""))`  
`imgArray[i, :, :, :] = bg`  
`lblArray[i] = [x, y]`  
 `return tf.stack(imgArray), tf.stack(lblArray)`

Any help is greatly appreciated!",t2_48qz3zo4,False,,0,False,How to manipulate image tensors TF/custom training loops in TF,[],r/learnmachinelearning,False,6,,0,,False,t3_geyiwk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588848969.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create a generative/discriminator network system. My goal is for a generator to create a pattern of pixels (28x28), overlay it on a background image, then have a discriminator try to locate the pattern in the background image. However, I am struggling to write the custom training loop. It has something to do with going back and forth between tensors and nd_arrays, but I can&amp;#39;t figure out how to solve my problem without doing so.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a link to a notebook that has an example:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing""&gt;https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is the training loop:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#@tf.function&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;#the example code had this annotation, but my code breaks with it&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;def train_step():&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;#Generate noisy seeds&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;noise = tf.random.normal([BATCH_SIZE, noise_dim])&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;pattern = generator(noise, training=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gen_tape.watch(pattern)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imageDataset, lblDataset = getDatasetFromPattern(np.reshape(pattern, (28,28)), 1)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;discriminator_output = discriminator(imageDataset, training=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gen_loss = generator_loss(lblDataset, discriminator_output)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;disc_loss = discriminator_loss(lblDataset, discriminator_output)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gradients_of_generator = gen_tape.gradient(disc_loss, generator.trainable_variables, gen_tape)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here s the getDatasetFromPattern pattern:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def getDatasetFromPattern(nd_array, num_samples):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;height = nd_array.shape[0]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;width = nd_array.shape[1]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;backgrounds = [Image.fromarray(np.zeros((640, 480, CHANNELS), dtype=np.uint8))]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;patternImg = Image.fromarray(nd_array, &amp;#39;L&amp;#39;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imgArray = np.ndarray((num_samples * len(backgrounds),&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;BACKGROUND_HEIGHT, BACKGROUND_WIDTH, 3))&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;lblArray = np.ndarray((num_samples * len(backgrounds), 2))&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for i in range(num_samples):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;x = random.randint(0, BACKGROUND_WIDTH - width)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;y = random.randint(0, BACKGROUND_HEIGHT - height)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for bg in backgrounds:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;bg.paste(patternImg, (x, y), patternImg.convert(&amp;quot;RGBA&amp;quot;))&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imgArray[i, :, :, :] = bg&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;lblArray[i] = [x, y]&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;return tf.stack(imgArray), tf.stack(lblArray)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Any help is greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?auto=webp&amp;s=ffc13b63083ce9180bdf2cf28705ee407b659975', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f53340195fae0f84ec3871625087bf02fd428ab', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6722a649351a89204dede7d65ad009eea43566c', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'mAwt5FN14h93icEvsjkWqFiukRT9FqTVVq54ZHDgaos'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geyiwk,True,,fullyLethal,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geyiwk/how_to_manipulate_image_tensors_tfcustom_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geyiwk/how_to_manipulate_image_tensors_tfcustom_training/,155203,1588820169.0,0,,False,,,,
,learnmachinelearning,,t2_uh8z4m5,False,,0,False,Faster machine learning on larger graphs: how NumPy and Pandas slashed memory and time in StellarGraph,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_geyanz,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/mcl9tRjRqyffpXRnTzyhnKLIoQhcG3QnqMZBDsNmuIg.jpg,False,,[],{},link,,False,,1588848024.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?auto=webp&amp;s=24c955dd2e72ed7a7f80de843ed052388bab4529', 'width': 1024, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d39df137a29f0ac8fd3197bb961832570482c23f', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=958ad8f7e2da6602fb3407ae3ab9434c2027654f', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1ce7fabc376a384446bd92655d3e9d78c18dd54', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=159c76d1403f22c3fc5da08f5eb92e25433b788a', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a16246ab3539e638686c7f3febf240c605d2a45', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'RYjGRysZcCqaR-JRAX0qqwNsOehdIU5Oo2daJzePuiU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geyanz,True,,huonw,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geyanz/faster_machine_learning_on_larger_graphs_how/,all_ads,False,https://medium.com/stellargraph/faster-machine-learning-on-larger-graphs-how-numpy-and-pandas-slashed-memory-and-time-in-79b6c63870ef,155203,1588819224.0,0,,False,,,,
,learnmachinelearning,"Hello everyone, it's my first time posting here so apologies if this is the wrong sub for this question...

I work for an advertising company that is trying to aggregate consumer commentary about a client's product. Scraping and extracting data on social media platforms is well documented BUT I was wondering if anyone had experience with mining/scraping/crawling (not sure the right word here) the internet as a whole to find more consumer commentary?

What I'm envisioning is a system where you can upload 30-40 relevant website URLs, some example text/commentary from consumers that we're looking for (we can give the system thousands of examples if it needs it), and let it loose to find more websites/text from OTHER sources than the 30-40 initial websites we gave it.

Does something like this exist? I've spoken to a few developer friends and they seem to think something like that is difficult since you have to somehow code the website layout for the scraper to understand where text is located on a page, let alone WHAT to scrape. But does anyone know of a company that can do this (maybe even self service?). It'd be great if we could get commentary from thousands of websites. Thank you ahead of time!",t2_mpw09,False,,0,False,Mining Public Text Data,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_geubsy,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},,,True,,1588833642.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, it&amp;#39;s my first time posting here so apologies if this is the wrong sub for this question...&lt;/p&gt;

&lt;p&gt;I work for an advertising company that is trying to aggregate consumer commentary about a client&amp;#39;s product. Scraping and extracting data on social media platforms is well documented BUT I was wondering if anyone had experience with mining/scraping/crawling (not sure the right word here) the internet as a whole to find more consumer commentary?&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m envisioning is a system where you can upload 30-40 relevant website URLs, some example text/commentary from consumers that we&amp;#39;re looking for (we can give the system thousands of examples if it needs it), and let it loose to find more websites/text from OTHER sources than the 30-40 initial websites we gave it.&lt;/p&gt;

&lt;p&gt;Does something like this exist? I&amp;#39;ve spoken to a few developer friends and they seem to think something like that is difficult since you have to somehow code the website layout for the scraper to understand where text is located on a page, let alone WHAT to scrape. But does anyone know of a company that can do this (maybe even self service?). It&amp;#39;d be great if we could get commentary from thousands of websites. Thank you ahead of time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geubsy,True,,whorehey19,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geubsy/mining_public_text_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geubsy/mining_public_text_data/,155203,1588804842.0,0,,False,,,,
,learnmachinelearning,"This is a Discord server where everyone wanting to learn machine learning is welcomed!

Share you projects, interesting research papers, courses, kaggle competitions, learn together ask any questions related to the field!

The more we are, the more we learn! Spread your knowledge and Learn ML Together!

Join us: [https://discord.gg/SVse4Sr](https://discord.gg/SVse4Sr)",t2_c14wpji,False,,0,False,"A discord server for everyone working / learning Al, ML &amp; DL. Share your project, papers, ask questions, learn together, create Kaggle competition teams and more!",[],r/learnmachinelearning,False,6,,0,,False,t3_getr3k,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588831732.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a Discord server where everyone wanting to learn machine learning is welcomed!&lt;/p&gt;

&lt;p&gt;Share you projects, interesting research papers, courses, kaggle competitions, learn together ask any questions related to the field!&lt;/p&gt;

&lt;p&gt;The more we are, the more we learn! Spread your knowledge and Learn ML Together!&lt;/p&gt;

&lt;p&gt;Join us: &lt;a href=""https://discord.gg/SVse4Sr""&gt;https://discord.gg/SVse4Sr&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?auto=webp&amp;s=bbf5a5f8cdd32a365c20b807d6e558010bf21ad3', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7e52d627b24baf552b6a16dd42fad4b83bfdd50', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8796fb46cfe3d0305f26fc605b5c6505c2cd4ffe', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'MId8qCjBpXwiZK4_j8Xrn-37sQy_bUZ9YanGSEY9-S8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,getr3k,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getr3k/a_discord_server_for_everyone_working_learning_al/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/getr3k/a_discord_server_for_everyone_working_learning_al/,155203,1588802932.0,0,,False,,,,
,learnmachinelearning,I recently just finished Python Crash Course and did numerous projects of my own. Iâ€™m very curious about Machine Learning and would like more insight. Any recommendations?,t2_115nlpd7,False,,0,False,What books should I read to get started with Machine Learning using Python ?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gexf4z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588844658.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently just finished Python Crash Course and did numerous projects of my own. Iâ€™m very curious about Machine Learning and would like more insight. Any recommendations?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gexf4z,True,,FlySeddy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gexf4z/what_books_should_i_read_to_get_started_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gexf4z/what_books_should_i_read_to_get_started_with/,155203,1588815858.0,0,,False,,,,
,learnmachinelearning,,t2_4xto167r,False,,0,False,"MIT-OCW: A 2020 Vision of Linear Algebra, Spring 2020 | Gilbert Strang | Brand new, intuitive, short videos on Linear Algebra",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gdy9ve,False,dark,0.99,,public,718,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Intro: A New Way to Start Linear Algebra', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'MIT OpenCourseWare', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YrHlHbtiSM0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/MIT'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdy9ve', 'height': 338}",,False,718,,False,https://b.thumbs.redditmedia.com/jV7_Z5kH2VFt_ROohoDrxjPv4U2lzDq7qov6DYd4OgE.jpg,False,,[],{},rich:video,,False,,1588716139.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?auto=webp&amp;s=00f256c18ac7eb362188fd81a37d990320424387', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3473d4ae632260c746e99e9c641b02f7db40497', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=48cc4db6a21a4c37f67e16d57fea4acea1a2e275', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de56a18d15adacb01d4015b213dfef562ef3307a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'jazTf8nlqJzP_Ur50WSVB0-b5bQ77d0DGUwyO4tBEZw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdy9ve,True,,samketa,,33,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdy9ve/mitocw_a_2020_vision_of_linear_algebra_spring/,all_ads,False,https://www.youtube.com/watch?v=YrHlHbtiSM0&amp;list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek,155203,1588687339.0,3,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Intro: A New Way to Start Linear Algebra', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'MIT OpenCourseWare', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YrHlHbtiSM0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/MIT'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_633vt2ak,False,,0,False,New algorithms help scientists connect data points from multiple sources to solve high-risk problems,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,103.0,False,t3_getdqk,False,light,1.0,,public,2,0,{},140.0,,False,[],,True,False,,{},Discussion,False,2,,False,https://b.thumbs.redditmedia.com/c2Odg0jY5B-hdWkBaB42GGgd5FEPPu8QnkVZgZ6J3-w.jpg,False,,[],{},image,,False,,1588830520.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?auto=webp&amp;s=7128b0a1f24aed9c8116a5e6f08e7c7155c798b5', 'width': 1280, 'height': 949}, 'resolutions': [{'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c5696868bd2a52cfc8dcac22d86fda4373b71d0', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deaed160345f2554fe75660117e6ba156f138da7', 'width': 216, 'height': 160}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ddc6f86f51707a440ffd86ee177bced326cc789', 'width': 320, 'height': 237}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb7e2304a2457f5986e19bf6445a0379f376ea2c', 'width': 640, 'height': 474}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a5f6abfacd1a0dcb5bdab68af7fc7d09ac6a7ee', 'width': 960, 'height': 711}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f35c323653f41054361d8122bdf892259d8c6bf', 'width': 1080, 'height': 800}], 'variants': {}, 'id': 'zrRxrlDYLKhTNj8clke9XLj95pXFRdbdgEr6ExfYXTk'}], 'enabled': True}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,getdqk,True,,GeaninaKera,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getdqk/new_algorithms_help_scientists_connect_data/,all_ads,False,https://i.redd.it/kxi0qjqht7x41.jpg,155203,1588801720.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'VisualCoding', 'selftext': '', 'author_fullname': 't2_633vt2ak', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'New algorithms help scientists connect data points from multiple sources to solve high-risk problems', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/VisualCoding', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 103, 'hide_score': False, 'name': 't3_getd4a', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 9, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/c2Odg0jY5B-hdWkBaB42GGgd5FEPPu8QnkVZgZ6J3-w.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588830462.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?auto=webp&amp;s=7128b0a1f24aed9c8116a5e6f08e7c7155c798b5', 'width': 1280, 'height': 949}, 'resolutions': [{'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c5696868bd2a52cfc8dcac22d86fda4373b71d0', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deaed160345f2554fe75660117e6ba156f138da7', 'width': 216, 'height': 160}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ddc6f86f51707a440ffd86ee177bced326cc789', 'width': 320, 'height': 237}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb7e2304a2457f5986e19bf6445a0379f376ea2c', 'width': 640, 'height': 474}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a5f6abfacd1a0dcb5bdab68af7fc7d09ac6a7ee', 'width': 960, 'height': 711}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f35c323653f41054361d8122bdf892259d8c6bf', 'width': 1080, 'height': 800}], 'variants': {}, 'id': 'zrRxrlDYLKhTNj8clke9XLj95pXFRdbdgEr6ExfYXTk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2jgq2d', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'getd4a', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'GeaninaKera', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/VisualCoding/comments/getd4a/new_algorithms_help_scientists_connect_data/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://i.redd.it/kxi0qjqht7x41.jpg', 'subreddit_subscribers': 585, 'created_utc': 1588801662.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_getd4a,,
,learnmachinelearning,,t2_3d8dg3uh,False,,0,False,AI Generates SharinGAN - Part 2 (Tried To Get Better Results),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gepy8f,False,light,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates SharinGAN - Part 2 (Tried To Get Better Results)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Mtb6MhGXrU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gepy8f', 'height': 338}",Project,False,3,,False,https://b.thumbs.redditmedia.com/D4UterRi_csn-u1rXh6Le4YtLmOYgblXexDYt_SUCdk.jpg,False,,[],{},rich:video,,False,,1588819781.0,richtext,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?auto=webp&amp;s=e7062f8c1bd7721d58050c90600a023ff4764d7c', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=796002cd86bff68f1f8ecaf15d29008512068121', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58ea39e444d5927020c111a01ca4a4944b2dfb51', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f0720ed55d89ec8be9a1887f965b47034970a33', 'width': 320, 'height': 240}], 'variants': {}, 'id': '4tBptj2vi4L8qdtI4TPnYnWDcd1Plwg4_QfbCgh2GUI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gepy8f,True,,oFlamingo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gepy8f/ai_generates_sharingan_part_2_tried_to_get_better/,all_ads,False,https://www.youtube.com/watch?v=Mtb6MhGXrU4,155203,1588790981.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates SharinGAN - Part 2 (Tried To Get Better Results)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Mtb6MhGXrU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,,,,
,learnmachinelearning,,t2_4eskgjow,False,,0,False,Great article about LSTM Networks,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geh67p,False,dark,0.94,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://a.thumbs.redditmedia.com/8Jl84Nbj4TDv6yYpzKjp2GVMBgOQbrKWW4TDdS04zy0.jpg,False,,[],{},link,,False,,1588788239.0,text,6,,,text,colah.github.io,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?auto=webp&amp;s=f6b09874635bb0653fd4d03a806ee1fb2ff66f89', 'width': 600, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2996612ff7a9fc61114d2c281f2ff7dcb7d732cf', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1a774718f27448361578b84d4d76fc7b649a710', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19cc02439bd6e94150f6efa421a142b6cae1c94a', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SFKPM5-FCzRAy7BEoJqEnwOql0eWz7NWthvpDeCK8qA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geh67p,True,,jsanrom,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geh67p/great_article_about_lstm_networks/,all_ads,False,https://colah.github.io/posts/2015-08-Understanding-LSTMs/,155203,1588759439.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: Photo-Realistic Virtual Try-On,[],r/learnmachinelearning,False,6,,0,76.0,False,t3_geuvbf,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/kWlFZ245BAgE_fiEWUeXV5bEvd-MLEO2_9G_jmHMFTI.jpg,False,,[],{},link,,False,,1588835476.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?auto=webp&amp;s=44b2e5dec112285d7768028b1dce3792624fe8bf', 'width': 638, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d871a6546c4df9a28a6339e3f7d6d58c5cf777b', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13800f679f77a4253734b2371cff4545c07363dc', 'width': 216, 'height': 118}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349af67a18b5cbe022530a5957a92eab8ca47164', 'width': 320, 'height': 175}], 'variants': {}, 'id': 'B7sUwDQ3vFL22HPV8SBYSE_lUB3uQur9P24m0QIM_uM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geuvbf,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuvbf/from_cvpr_20_photorealistic_virtual_tryon/,all_ads,False,/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/,155203,1588806676.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""From CVPR '20: Photo-Realistic Virtual Try-On\n\nFor project, code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05863)\n\nhttps://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;format=png&amp;auto=webp&amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48\n\nThey propose a novel visual try-on network which In comparison to the state-of-the-art methods can generate photo-realistic images with much better perceptual quality and richer fine-details."", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: Photo-Realistic Virtual Try-On"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 76, 'hide_score': False, 'media_metadata': {'1gwv97ax58x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 59, 'x': 108, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44881ccdb4abcb06bacaedc285001ea727b65322'}, {'y': 118, 'x': 216, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e595b0a3f833bfd5cb70974e448221fad0673fd8'}, {'y': 174, 'x': 320, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dce1aa730c16ac443f499002aaa993d0575a6ecd'}, {'y': 349, 'x': 640, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9ccb324a50d5f1f432e9066280e0ce0ec7f5e2d2'}, {'y': 524, 'x': 960, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=410c51c8b921b46854a3d22333954ada8c31aeaa'}, {'y': 590, 'x': 1080, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05d2cc24e24c45d72aabb7100ce1499a6c911585'}], 's': {'y': 1074, 'x': 1964, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;format=png&amp;auto=webp&amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48'}, 'id': '1gwv97ax58x41'}}, 'name': 't3_geumem', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 19, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 19, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/kWlFZ245BAgE_fiEWUeXV5bEvd-MLEO2_9G_jmHMFTI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588834640.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From CVPR &amp;#39;20: Photo-Realistic Virtual Try-On&lt;/p&gt;\n\n&lt;p&gt;For project, code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05863""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48""&gt;https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They propose a novel visual try-on network which In comparison to the state-of-the-art methods can generate photo-realistic images with much better perceptual quality and richer fine-details.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?auto=webp&amp;s=44b2e5dec112285d7768028b1dce3792624fe8bf', 'width': 638, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d871a6546c4df9a28a6339e3f7d6d58c5cf777b', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13800f679f77a4253734b2371cff4545c07363dc', 'width': 216, 'height': 118}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349af67a18b5cbe022530a5957a92eab8ca47164', 'width': 320, 'height': 175}], 'variants': {}, 'id': 'B7sUwDQ3vFL22HPV8SBYSE_lUB3uQur9P24m0QIM_uM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'geumem', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/', 'subreddit_subscribers': 3386, 'created_utc': 1588805840.0, 'num_crossposts': 9, 'media': None, 'is_video': False}]",t3_geumem,,
,learnmachinelearning,"I was thinking of a program that could discern between quality of instrument tone (Or sound quality). Particularly for beginners, it would be helpful to have a tool that could rate your tone say from 1-10. As a trombone player I'd want to start with trombone tone, and if it works expand to many different instruments.

I'm not a programmer and I only know basic stuff about machine learning. How doable is this?",t2_ge5os,False,,0,False,I have an idea that uses machine learning but I don't know ho doable it is.,[],r/learnmachinelearning,False,6,,0,,False,t3_geq1av,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588820038.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was thinking of a program that could discern between quality of instrument tone (Or sound quality). Particularly for beginners, it would be helpful to have a tool that could rate your tone say from 1-10. As a trombone player I&amp;#39;d want to start with trombone tone, and if it works expand to many different instruments.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not a programmer and I only know basic stuff about machine learning. How doable is this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geq1av,True,,Floppy_Trombone,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geq1av/i_have_an_idea_that_uses_machine_learning_but_i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geq1av/i_have_an_idea_that_uses_machine_learning_but_i/,155203,1588791238.0,0,,False,,,,
,learnmachinelearning," I've downloaded it from github. I also have python 3.8, but I can't seem to get it up and running. I instructions I have found online assumes I have prior knowledge, which I do not. Please EILI5.",t2_aual3,False,,0,False,How do I get Tacotron 2 up and running?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gepx0v,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588819672.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve downloaded it from github. I also have python 3.8, but I can&amp;#39;t seem to get it up and running. I instructions I have found online assumes I have prior knowledge, which I do not. Please EILI5.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gepx0v,True,,BobLordOfTheCows,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gepx0v/how_do_i_get_tacotron_2_up_and_running/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gepx0v/how_do_i_get_tacotron_2_up_and_running/,155203,1588790872.0,0,,False,,,,
,learnmachinelearning,"Hi all, so I come from a pretty heavy classical stats/probability background and I've been reading about machine learning, and I'm confused as to what machine learners are talking about when they talk about bias, especially with respect to the bias-variance tradeoff. To give you an example of what I'm confused about, suppose you have some data from a true model which is quadratic with an additive error:

y = x^2 + epsilon, epsilon ~ N(0, sigma^2)

The way I would describe bias and the bias-variance tradeoff from my probability background is: if you were to fit a quadratic regresion using ordinary least squares, your estimates would be unbiased. Alternatively, you could fit the same quadratic regression using a method like ridge regression to get estimates that add a little bit of bias but have a big reduction in variance, so you've traded some bias for some variance to reduce total MSE. 

But, I google ""bias-variance tradeoff"" and I get articles that basically explain it like this: if you fit a simple linear regression, it has high bias. If you fit a high-order polynomial regression, it has high variance. The goal is to find the best in-between model which would balance bias and variance to get the optimum MSE on a test set. This is usually indicated by something that looks like the OLS quadratic regression (though often the method isn't described so I can't be entirely sure, correct me if I'm wrong here). But since both the quadratic regression and high-order polynomial regression give unbiased estimates in this scenario, going from one to the other isn't trading off bias for variance at all, and so this would make it seem as if the bias-variance tradeoff is just about model selection. The only way I can think to make sense of bias in this context is that it basically refers to the mean squared error on the training set.

Can somebody clarify what bias and the bias-variance tradeoff mean in this context? I just want to make sure I understand what the terminology means in machine learning so I'm not confused when I get to heavier stuff. Thank you!",t2_gf0jz,False,,0,False,What do machine learners mean by bias and the bias-variance tradeoff?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_get7by,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588829931.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, so I come from a pretty heavy classical stats/probability background and I&amp;#39;ve been reading about machine learning, and I&amp;#39;m confused as to what machine learners are talking about when they talk about bias, especially with respect to the bias-variance tradeoff. To give you an example of what I&amp;#39;m confused about, suppose you have some data from a true model which is quadratic with an additive error:&lt;/p&gt;

&lt;p&gt;y = x&lt;sup&gt;2&lt;/sup&gt; + epsilon, epsilon ~ N(0, sigma&lt;sup&gt;2)&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The way I would describe bias and the bias-variance tradeoff from my probability background is: if you were to fit a quadratic regresion using ordinary least squares, your estimates would be unbiased. Alternatively, you could fit the same quadratic regression using a method like ridge regression to get estimates that add a little bit of bias but have a big reduction in variance, so you&amp;#39;ve traded some bias for some variance to reduce total MSE. &lt;/p&gt;

&lt;p&gt;But, I google &amp;quot;bias-variance tradeoff&amp;quot; and I get articles that basically explain it like this: if you fit a simple linear regression, it has high bias. If you fit a high-order polynomial regression, it has high variance. The goal is to find the best in-between model which would balance bias and variance to get the optimum MSE on a test set. This is usually indicated by something that looks like the OLS quadratic regression (though often the method isn&amp;#39;t described so I can&amp;#39;t be entirely sure, correct me if I&amp;#39;m wrong here). But since both the quadratic regression and high-order polynomial regression give unbiased estimates in this scenario, going from one to the other isn&amp;#39;t trading off bias for variance at all, and so this would make it seem as if the bias-variance tradeoff is just about model selection. The only way I can think to make sense of bias in this context is that it basically refers to the mean squared error on the training set.&lt;/p&gt;

&lt;p&gt;Can somebody clarify what bias and the bias-variance tradeoff mean in this context? I just want to make sure I understand what the terminology means in machine learning so I&amp;#39;m not confused when I get to heavier stuff. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,get7by,True,,rcf1105,,3,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/get7by/what_do_machine_learners_mean_by_bias_and_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/get7by/what_do_machine_learners_mean_by_bias_and_the/,155203,1588801131.0,0,,False,,,,
,learnmachinelearning,,t2_zhiq2,False,,0,False,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_get3bg,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/8FUnpU0mtnLea7v2vxcctVYrm28Z_ZOpPErF5P9e5ig.jpg,False,,[],{},link,,False,,1588829572.0,text,6,,,text,twilio.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?auto=webp&amp;s=d443172d1e1fd3fff035f4ddcae2ae2f32718ff9', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=704d5ff8a5527dbe8a9c8040b9e2421b8832b282', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55b6443b4372db0876ab18da3f86f80ff5d8954b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e0cb43ee97763a49f422e8fdada935e79f590e0', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ae4c533749018a680e8d6dedd7d6d750f17d849', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ed0a4921e93afa900f24a08d7d4486a9d3a8ac1', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=561ba27d816b234faec21065a354e8fb590c3977', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'GgaU-qidR2wgONCcrVwW5mnAb0Wk1z6XxjWFLlUqPvw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,get3bg,True,,lizziepika,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/get3bg/predict_wins_and_losses_with_scikit_learn/,all_ads,False,https://www.twilio.com/blog/predict-wins-losses-scikitlearn-sms,155203,1588800772.0,0,,False,,,,
,learnmachinelearning,"I wrote a chatbot using a DQN as a dialog manager. As an example, let's say it is e-commerce. It accepts encoded information from users, so let's say the user says 'I want shoes'. All available shoes would be queried from a database and a list of available products would be returned. The DQN then will get a list of available actions as well as the state of the conversation as an input and then make the next best action. In our case, let's say the action is ""ask for size"".

I have 2 questions based on this system that I can explain more if needed

1: The available actions are input as a binary mask to the DQN where if an action is available, it is a 1 and if not, 0. Is this a proper approach and if not, what would be a better way to do it?

2: If we wanted to add additional actions without having to retrain every time, what would be the best way to do so? This is assuming the requested product is a sub-category of the first, so without adding 'high heels', how could we still query for shoes? My first thought was to do some kind of clustering like k-means where we could add high heels to a 'shoe' category, in the database have all high heels as a subset of shoes, so when we see 'I want high heels' we could classify high heels into shoes so the agent knows it is a shoe we are asking about.",t2_qjb55,False,,0,False,What is the best way to add actions to a chatbot?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_geopio,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588815865.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a chatbot using a DQN as a dialog manager. As an example, let&amp;#39;s say it is e-commerce. It accepts encoded information from users, so let&amp;#39;s say the user says &amp;#39;I want shoes&amp;#39;. All available shoes would be queried from a database and a list of available products would be returned. The DQN then will get a list of available actions as well as the state of the conversation as an input and then make the next best action. In our case, let&amp;#39;s say the action is &amp;quot;ask for size&amp;quot;.&lt;/p&gt;

&lt;p&gt;I have 2 questions based on this system that I can explain more if needed&lt;/p&gt;

&lt;p&gt;1: The available actions are input as a binary mask to the DQN where if an action is available, it is a 1 and if not, 0. Is this a proper approach and if not, what would be a better way to do it?&lt;/p&gt;

&lt;p&gt;2: If we wanted to add additional actions without having to retrain every time, what would be the best way to do so? This is assuming the requested product is a sub-category of the first, so without adding &amp;#39;high heels&amp;#39;, how could we still query for shoes? My first thought was to do some kind of clustering like k-means where we could add high heels to a &amp;#39;shoe&amp;#39; category, in the database have all high heels as a subset of shoes, so when we see &amp;#39;I want high heels&amp;#39; we could classify high heels into shoes so the agent knows it is a shoe we are asking about.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,geopio,True,,Awill1aB,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geopio/what_is_the_best_way_to_add_actions_to_a_chatbot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geopio/what_is_the_best_way_to_add_actions_to_a_chatbot/,155203,1588787065.0,0,,False,,,,
,learnmachinelearning,"So i have this project research and i have few questions. I looked up some research papers and some web articles but still having some basic questions.

1)Does this mean that it should be able to answer the question that what is happening in the video and audio (like user asks question and machine should be able to give answer) or is it sort of video/audio action describer?

2) No  idea how to implement this. I only know ML and basics of AI. Would like some guidance or any useful links for reference.",t2_6dqc4uv8,False,,0,False,Audio- video scene aware dialogue,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_geofbt,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588814961.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So i have this project research and i have few questions. I looked up some research papers and some web articles but still having some basic questions.&lt;/p&gt;

&lt;p&gt;1)Does this mean that it should be able to answer the question that what is happening in the video and audio (like user asks question and machine should be able to give answer) or is it sort of video/audio action describer?&lt;/p&gt;

&lt;p&gt;2) No  idea how to implement this. I only know ML and basics of AI. Would like some guidance or any useful links for reference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geofbt,True,,RayS0l0,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geofbt/audio_video_scene_aware_dialogue/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geofbt/audio_video_scene_aware_dialogue/,155203,1588786161.0,0,,False,,,,
,learnmachinelearning,,t2_uh03y,False,,0,False,My implementations of Deep Generative Models!,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geo7zf,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/CPyu9UNNkEpH_BM9zftQ4il2ud3fX1RTtyynNcuYNes.jpg,False,,[],{},link,,False,,1588814318.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?auto=webp&amp;s=480ba39f8f4bdaa75c5b86fdcb150e7a2fbd1f3d', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a992a8eb38f6335fe4627e8be40fb62927349f74', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de262d0e2cef7ec7638907aad51e26f8e8401496', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'GwTAT4o_dTpoQkFOoJhKxJv91Xty0MbAbemkdXNZbuA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geo7zf,True,,wellfriedbeans,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geo7zf/my_implementations_of_deep_generative_models/,all_ads,False,https://github.com/ameya98/DeepGenerativeModels,155203,1588785518.0,0,,False,,,,
,learnmachinelearning,"I have an MS in computer science, where I took 2 courses in AI and ML.  Also, I took the big ML course offered on Coursera.



Is there a well known online resource where I can learn more advanced concepts in ML, deep learning, and NLP?",t2_2k1plfwa,False,,0,False,Best place to learn more intermediate and advanced machine learning?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gek1fy,False,light,1.0,,public,5,0,{},,,False,[],,False,False,,{},Discussion,False,5,,False,self,False,,[],{},,,True,,1588800641.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an MS in computer science, where I took 2 courses in AI and ML.  Also, I took the big ML course offered on Coursera.&lt;/p&gt;

&lt;p&gt;Is there a well known online resource where I can learn more advanced concepts in ML, deep learning, and NLP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gek1fy,True,,memcpy94,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gek1fy/best_place_to_learn_more_intermediate_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gek1fy/best_place_to_learn_more_intermediate_and/,155203,1588771841.0,0,,False,,,,
,learnmachinelearning,"Sorry if this post is a little off-topic, I was not sure if this question is more suitable to r/learnpython or here. This question is very scikit-learn specific.

&amp;#x200B;

I am a student working on a project where we try to test how well various classifiers help with an asset allocation decision but we are having an issue with the label we will try to predict. Our label is a dummy variable, with various integers corresponding to different asset allocations. We have daily data but would like to incorporate monthly rebalancing such that this label must have the same integer value for a whole given month and can only (potentially) change at the turn of a month. 

&amp;#x200B;

How do reflect this constraint when using scikit-learn? Aggregating our daily data into montly time-series is not really an option due to the nature of our features. 

&amp;#x200B;

Thanks for any inputs!",t2_a08sq,False,,0,False,Tactical Asset Allocation with ML Classifiers,[],r/learnmachinelearning,False,6,,0,,False,t3_genm1a,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588812433.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this post is a little off-topic, I was not sure if this question is more suitable to &lt;a href=""/r/learnpython""&gt;r/learnpython&lt;/a&gt; or here. This question is very scikit-learn specific.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am a student working on a project where we try to test how well various classifiers help with an asset allocation decision but we are having an issue with the label we will try to predict. Our label is a dummy variable, with various integers corresponding to different asset allocations. We have daily data but would like to incorporate monthly rebalancing such that this label must have the same integer value for a whole given month and can only (potentially) change at the turn of a month. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;How do reflect this constraint when using scikit-learn? Aggregating our daily data into montly time-series is not really an option due to the nature of our features. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any inputs!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,genm1a,True,,blacksiddis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/genm1a/tactical_asset_allocation_with_ml_classifiers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/genm1a/tactical_asset_allocation_with_ml_classifiers/,155203,1588783633.0,0,,False,,,,
,learnmachinelearning,,t2_3f35usxe,False,,0,False,[DeepFake] How to make deep fake video,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_geriz9,False,light,0.67,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""[DeepFake] How to make deep fake video 'We will meet again' - Deep Lazy Guy"", 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Deep Lazy Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wnwf9j-6-eU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCroUoJv7loN07RSaGpmGfrw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/geriz9', 'height': 338}",Project,False,1,,False,https://b.thumbs.redditmedia.com/sH5DkQl7GkAaidT97_VIn77ASeQsSkTz787sGQ0mizc.jpg,False,,[],{},rich:video,,False,,1588824677.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?auto=webp&amp;s=0329b426928729a12a7ca84e9e332eebd5d67ca2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b864a0a39be62dbdb88691c96edb98ec635858e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e130f0b3183a7e8f195dcbd7fc3434a9b1cfbf58', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bf7bb4d2157ecd4979802565853f6dc56e3882c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'qMastc7r39UnAIROICUISMW_t6FGAWmqF1F4dfMcyrw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geriz9,True,,catisfying,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geriz9/deepfake_how_to_make_deep_fake_video/,all_ads,False,https://www.youtube.com/watch?v=Wnwf9j-6-eU,155203,1588795877.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""[DeepFake] How to make deep fake video 'We will meet again' - Deep Lazy Guy"", 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Deep Lazy Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wnwf9j-6-eU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCroUoJv7loN07RSaGpmGfrw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hey everybody ðŸ‘‹ðŸ½

Are there ML engineers who can tell me about their experience working in the field?? Currently, I am confused between ML and Blockchain. Can anybody tell me how to tackle this confused â€œpathâ€ problem??

Thanks ðŸ™",t2_2ufovsg1,False,,0,False,Machine learning engineers where you at??,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gevapf,False,light,0.25,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588836955.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everybody ðŸ‘‹ðŸ½&lt;/p&gt;

&lt;p&gt;Are there ML engineers who can tell me about their experience working in the field?? Currently, I am confused between ML and Blockchain. Can anybody tell me how to tackle this confused â€œpathâ€ problem??&lt;/p&gt;

&lt;p&gt;Thanks ðŸ™&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gevapf,True,,codefreak-123,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gevapf/machine_learning_engineers_where_you_at/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gevapf/machine_learning_engineers_where_you_at/,155203,1588808155.0,0,,False,,,,
,learnmachinelearning," Sorry if my question is too generic, but I'm new to deep learning. I'm creating a GAN network to generate CT images.

The train dataset is of 848x848 image, and the generated ones should have to be the same size, but I get CUDA out of memory error.

I tried putting the batch size to 1 and still get the same error.

Is there any way for me to solve this issue since I can't decrease the batch size more?",t2_8iq8g3v,False,,0,False,Deep Convolutional GAN for CT images,[],r/learnmachinelearning,False,6,,0,,False,t3_geqy18,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588822873.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if my question is too generic, but I&amp;#39;m new to deep learning. I&amp;#39;m creating a GAN network to generate CT images.&lt;/p&gt;

&lt;p&gt;The train dataset is of 848x848 image, and the generated ones should have to be the same size, but I get CUDA out of memory error.&lt;/p&gt;

&lt;p&gt;I tried putting the batch size to 1 and still get the same error.&lt;/p&gt;

&lt;p&gt;Is there any way for me to solve this issue since I can&amp;#39;t decrease the batch size more?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geqy18,True,,brgreen25,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geqy18/deep_convolutional_gan_for_ct_images/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geqy18/deep_convolutional_gan_for_ct_images/,155203,1588794073.0,0,,False,,,,
,learnmachinelearning,"Currently I want a Data Science internship. I have also thought of doing the IBM AI and Engineering certification. My long run goal is to be a practitioner of  Machine Learning, I would just like help taking those first steps.",t2_2x9bm678,False,,0,False,"Hey, guys, I was a Data Analytics Intern at a company and would like to take the next step toward certification to be used toward another internship I will apply for. I want to listen to what you guys think.",[],r/learnmachinelearning,False,6,,0,,False,t3_geqy03,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588822870.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I want a Data Science internship. I have also thought of doing the IBM AI and Engineering certification. My long run goal is to be a practitioner of  Machine Learning, I would just like help taking those first steps.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geqy03,True,,T-ROY_T-REDDIT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geqy03/hey_guys_i_was_a_data_analytics_intern_at_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geqy03/hey_guys_i_was_a_data_analytics_intern_at_a/,155203,1588794070.0,0,,False,,,,
,learnmachinelearning,,t2_51mclnu7,False,,0,False,Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gegp0y,False,dark,1.0,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XoMwSeL8y3E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gegp0y', 'height': 338}",,False,4,,False,https://a.thumbs.redditmedia.com/iy2xWbMFepAV98HfNkqDZBQ2Ii6cwe3zt-_JzZ4XrI4.jpg,False,,[],{},rich:video,,False,,1588785798.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?auto=webp&amp;s=2195d1a4b0df679e92c7c7f3950487e562901d90', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cce0ccffa6e615dd514f0ec7e557e9177d16828', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41215ac3276e6e72b35fa2ded761d88c9453d44d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349698b3d029d655f1ad3d227ea4e237aea6cd70', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cVViJ9CbyRzhTg-gvLngavnp3QdbaBewhRcSu6vQou0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gegp0y,True,,TheNerdyDevYT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gegp0y/machine_learning_tutorials_from_novice_to_pro_15/,all_ads,False,https://www.youtube.com/watch?v=XoMwSeL8y3E,155203,1588756998.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XoMwSeL8y3E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,,,,
,learnmachinelearning,"I've got some background in ML working through some sample supervised classification tutorials using Python and TensorFlow.

I have data in a CSV file, with each row having input features and the last column being its classification label. I'm looking for options (IBM? Google?) that would let me upload the CSV, choose some type of model, train and test it, then be able to feed it new samples to classify. 

Any recommendations on what platforms could  can do this pretty straight forward?",t2_81ir9,False,,0,False,Best online platform for supervised classification?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_genkta,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Request,False,1,,False,self,False,,[],{},,,True,,1588812325.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got some background in ML working through some sample supervised classification tutorials using Python and TensorFlow.&lt;/p&gt;

&lt;p&gt;I have data in a CSV file, with each row having input features and the last column being its classification label. I&amp;#39;m looking for options (IBM? Google?) that would let me upload the CSV, choose some type of model, train and test it, then be able to feed it new samples to classify. &lt;/p&gt;

&lt;p&gt;Any recommendations on what platforms could  can do this pretty straight forward?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,genkta,True,,timex40,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/genkta/best_online_platform_for_supervised_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/genkta/best_online_platform_for_supervised_classification/,155203,1588783525.0,0,,False,,,,
,learnmachinelearning,,t2_4q5pfd01,False,,0,False,"Amazon Alexa supports development of custom and smart home skills that get invoked after specific voice commands and general phrases. Check below the video weâ€™ve made to show how this skill works, including what youâ€™ll see on your phone at this moment.",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gefe1a,False,dark,0.87,,public,6,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_96', 'dash_url': 'https://v.redd.it/r2dv0x4pj3x41/DASHPlaylist.mpd', 'duration': 130, 'hls_url': 'https://v.redd.it/r2dv0x4pj3x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/kPhNAhx2raYBqcSAAWHKWE0jzcnqoP9-8oYi6rlzaMw.jpg,False,,[],{},hosted:video,,False,,1588778770.0,text,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?format=pjpg&amp;auto=webp&amp;s=10571f3ba1ab4970484f9acfa2c35ac15edcdc62', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be88fa0219f44991bca0877fae4d05d42648b5b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0b600f38cfa175af7e592020bfb9a9a317ef6b6b', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dab893fde773ebc5c99689f37cfb3c8e7768aa0c', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b93e3e17439d843b3d6efd2968c8a2a917c51dc0', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79889e9f539f4935f10192a427d1efff3c5f3772', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=168ba31f13be6786fd524333a8f739aff43169ca', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '_9XJytoPJ1pQ3G_cMOmkmvh2L9sWf4i6sfg3M14tI3A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gefe1a,True,,alexandra_moroz,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gefe1a/amazon_alexa_supports_development_of_custom_and/,all_ads,False,https://v.redd.it/r2dv0x4pj3x41,155203,1588749970.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_96', 'dash_url': 'https://v.redd.it/r2dv0x4pj3x41/DASHPlaylist.mpd', 'duration': 130, 'hls_url': 'https://v.redd.it/r2dv0x4pj3x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"I have a dataset which looks something like this 

[DataSet](https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;format=png&amp;auto=webp&amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10)

I want to build a recommendation system for research papers, based on content based recommendation system techniques.   
So if a user chooses his relevant keywords  my model should be able recommend papers similar to it.   


Can somebody point out how i can achieve this and create a model?   
Thanks",t2_y2rn6,False,,0,False,What model should I use in this scenario?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,30.0,False,t3_geguyw,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},HELP,False,3,,False,https://b.thumbs.redditmedia.com/UYFLZASfDzu8Nqa94i33lFDwdOZG4tzM2FAunQ04RJo.jpg,False,,[],{},,,True,,1588786675.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset which looks something like this &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10""&gt;DataSet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I want to build a recommendation system for research papers, based on content based recommendation system techniques.&lt;br/&gt;
So if a user chooses his relevant keywords  my model should be able recommend papers similar to it.   &lt;/p&gt;

&lt;p&gt;Can somebody point out how i can achieve this and create a model?&lt;br/&gt;
Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geguyw,True,,megatronus8010,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geguyw/what_model_should_i_use_in_this_scenario/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geguyw/what_model_should_i_use_in_this_scenario/,155203,1588757875.0,0,,False,,,"{'b2dr72iv64x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 23, 'x': 108, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4dd23a36c017ac971afb2858c268de2957be390d'}, {'y': 47, 'x': 216, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b036bc634388caa6fc471d74f0536ef84f40b42e'}, {'y': 70, 'x': 320, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac8006ee963c26b8f42f7149c8231f753001a3bf'}, {'y': 141, 'x': 640, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e198df5e3fb0676ee65d51ac3757cf4e3a7a4fa'}, {'y': 212, 'x': 960, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=11a74c00120275f6fcfb1c2e3eddd21b90f9277a'}, {'y': 239, 'x': 1080, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a6b6ddb71f090b8dd35e18f33b2b83f0b42488a'}], 's': {'y': 268, 'x': 1211, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;format=png&amp;auto=webp&amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10'}, 'id': 'b2dr72iv64x41'}}",
,learnmachinelearning,,t2_rpxuc,False,,0,False,Telegram channel - Data Science Digest - Join us today!,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_gem571,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/p_OSeLJEt8j9bh0b4KstI_GJnPd7KbqMWvnBtZAFOQ8.jpg,False,,[],{},link,,False,,1588807682.0,text,6,,,text,t.me,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?auto=webp&amp;s=54c350f8b8e19f2b72605d5d263d846e82b9efdb', 'width': 320, 'height': 320}, 'resolutions': [{'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f912c632fb75a309352590e64aa4ee88ac06e6d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2147e82a4176e76d99df38a96f0f91114055c813', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c3607d628e552bd0a4eb266b0ac2b4ed74594f7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'M29R14E1g8-GPffRiVdjtQYI9XGFJhWIY9RBOIEZOg4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gem571,True,,flyelephant,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gem571/telegram_channel_data_science_digest_join_us_today/,all_ads,False,https://t.me/DataScienceDigest,155203,1588778882.0,0,,False,,,,
,learnmachinelearning,"I work in neuroscience but I learned to program as a hobby when I was 12 and took a computational neuroscience course during my undergrad which was half machine learning. I've been interested in solidifying my practical skills in this domain so I recently tried out Andrew Ng's Deep Learning specialization on Coursera because it seemed like a decent review and you get a free certificate out of it. Unfortunately I didn't learn many new things, but it was a good refresher. Basically the first half of the undergrad course I mentioned but with more detail on sequence models and some tips for working in production vs academic research environments. Aside from working on projects and/or competitions, what resources would recommend going forward? I generally understand the mathematical formalisms and the intuition behind what I've seen so far. Are there any more advanced courses or textbooks I should read? The AI for Medicine specialization seemed relevant to me but not necessarily much more advanced.",t2_6e9cdhk4,False,,0,False,Intermediate Machine Learning Resources,[],r/learnmachinelearning,False,6,,0,,False,t3_geceem,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1588764434.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work in neuroscience but I learned to program as a hobby when I was 12 and took a computational neuroscience course during my undergrad which was half machine learning. I&amp;#39;ve been interested in solidifying my practical skills in this domain so I recently tried out Andrew Ng&amp;#39;s Deep Learning specialization on Coursera because it seemed like a decent review and you get a free certificate out of it. Unfortunately I didn&amp;#39;t learn many new things, but it was a good refresher. Basically the first half of the undergrad course I mentioned but with more detail on sequence models and some tips for working in production vs academic research environments. Aside from working on projects and/or competitions, what resources would recommend going forward? I generally understand the mathematical formalisms and the intuition behind what I&amp;#39;ve seen so far. Are there any more advanced courses or textbooks I should read? The AI for Medicine specialization seemed relevant to me but not necessarily much more advanced.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geceem,True,,bekpey235,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geceem/intermediate_machine_learning_resources/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geceem/intermediate_machine_learning_resources/,155203,1588735634.0,1,,False,,,,
,learnmachinelearning,"I just publish an article on Face recognition in Javascript, with a focus on the main program for others to build on

[Post](https://heartbeat.fritz.ai/facial-recognition-system-with-javascript-f9659c381434)",t2_m7ce4vv,False,,0,False,Face recognition in Javascript,[],r/learnmachinelearning,False,6,,0,,False,t3_gelajo,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588804996.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just publish an article on Face recognition in Javascript, with a focus on the main program for others to build on&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/facial-recognition-system-with-javascript-f9659c381434""&gt;Post&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?auto=webp&amp;s=e81629334aba99b67a64b2c79d30a0c41311e5da', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=270b0a93a536e6b1b208a09cdf095ea6d7cd8490', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b19a9cd59188a6cf9b81bd7d85a128b2c83e4400', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=341d6ed80c204f8a65a08e6b616572833e71cd67', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a9f823de95cfa5e75e57589a34931d05829f022', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8611893a0dac4b3595ab62e51876ebe9ec3b8590', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e15c2a7376a4b150ca1496321f1a65538f983dd1', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'CDYBcLs3ktXru7AvVyO17tpccldZUEgkozr2JQ6fC4o'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gelajo,True,,steveoni,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gelajo/face_recognition_in_javascript/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gelajo/face_recognition_in_javascript/,155203,1588776196.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Learning Convolutional Neural Networks with Interactive Visualization,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gel01t,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Learning Convolutional Neural Networks with Interactive Visualization', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IGOn-82OZ_8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gel01t', 'height': 338}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/xgS_VF2RuIeCGXnQYOxgfanhqosKIcA_lhO7KRuaHlM.jpg,False,,[],{},rich:video,,False,,1588804031.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?auto=webp&amp;s=a74e4edc2a8140a10d2c4523821fd91e6308b5bd', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1bf3f2e066393978750ad66d280663c28923ea21', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9ec9ccf4a4cff8a0aad4434979cad6ffce9fb7f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b105273682eff2a67dc8a45d1279b54f2819697a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ELZ_Z_iJXnNbzU3SFQaDBbYmeoZb-D3NKqAr3JSg7dc'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gel01t,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gel01t/learning_convolutional_neural_networks_with/,all_ads,False,https://youtu.be/IGOn-82OZ_8,155203,1588775231.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Learning Convolutional Neural Networks with Interactive Visualization', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IGOn-82OZ_8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_l8c0m,False,,0,False,Remembering long parameters lists for bash/python for reproducibility,[],r/learnmachinelearning,False,6,,0,90.0,False,t3_gekztl,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/WE4V0sqyDOdZ67M0cR-HkHAoXI1B_Cfy8-YjxnjXHBg.jpg,False,,[],{},link,,False,,1588804009.0,text,6,,,text,angelov.ai,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?auto=webp&amp;s=2930dd6e5ffad1c594a311d13ab062ad2cc78e76', 'width': 1024, 'height': 661}, 'resolutions': [{'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d2f7de80ae4e4ae90486fb735595305d7bb6c25', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64caf9917f3fdb13f1a1749f5363e976fad04eb8', 'width': 216, 'height': 139}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=56072aea5c75e0ee956eaf12816035501a336a47', 'width': 320, 'height': 206}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a0116003b0721f96bec410c9cbe21e2403c014b', 'width': 640, 'height': 413}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b3307fe7dd8ac55ed9882869c9b346e4b3fa984', 'width': 960, 'height': 619}], 'variants': {}, 'id': 'hVsKygQuiJoRrEhYLXWvxjETmOnohY7EkBvSygg-6Dk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gekztl,True,,23pointsNorth,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekztl/remembering_long_parameters_lists_for_bashpython/,all_ads,False,https://angelov.ai/post/2020/remember-your-bash-spells/,155203,1588775209.0,0,,False,,,,
,learnmachinelearning,,t2_5lfdx43m,False,,0,False,Formula to predict values in logistic regression,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,47.0,False,t3_gekqo2,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/RPoJjdIO8ehF1dhs3lEbsIGRQkg4mTVtioL_kta59NI.jpg,False,,[],{},image,,False,,1588803121.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?auto=webp&amp;s=072ee71e908776aa2002c3f2741e9d0a300de2a4', 'width': 1536, 'height': 526}, 'resolutions': [{'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3777d24a39d49a77ee7c3579d4262f4316f2d1e2', 'width': 108, 'height': 36}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b5d584a6ba699daf7ca788e4054143e5fba5256', 'width': 216, 'height': 73}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4381db716b768b8318081cbb4f2e2f3dcaef6e63', 'width': 320, 'height': 109}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e318f8da562fb52147840c2f5f83584c38f25992', 'width': 640, 'height': 219}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6623153e1a72dd8045dba6ae5c583af68e22edec', 'width': 960, 'height': 328}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d51f837cd14b3b0c4978f62b754be2acccc11956', 'width': 1080, 'height': 369}], 'variants': {}, 'id': 'eBNTbd2c-VShmbf8jYgh2bM4riCZxvq7iB88z033HsA'}], 'enabled': True}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gekqo2,True,,invidae,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekqo2/formula_to_predict_values_in_logistic_regression/,all_ads,False,https://i.redd.it/mxz1vgr8k5x41.jpg,155203,1588774321.0,0,,False,,,,
,learnmachinelearning,"Hello, I'm writing a report about the mathematics behind feedforward neural networks. When I was presenting my proposal to the instructor, he said that I need to be careful using the superscript notation to refer to vectors/matrices, since it is not considered a ""standard"" math notation.

Here is an example of the superscript notation in question. The x^((i)) and y^((i)) refers to the i^(th) training example and the i^(th) ground-truth label respectively.

&amp;#x200B;

https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;format=png&amp;auto=webp&amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361

Is this superscript notation considered ""standard"" in maths, or is it just limited to the field of machine learning? I tried to search for sources but couldn't find the origin of the notation.

[This](https://stats.stackexchange.com/questions/193908/in-machine-learning-why-are-superscripts-used-instead-of-subscripts) stackoverflow thread also discusses this issue, but none of the answers there referenced any reputable sources. I probably need to find a reputable source in order to convince my instructor.

&amp;#x200B;

Thanks for the help!",t2_1u44tp6m,False,,0,False,"Is the superscript notation used in machine learning considered ""standard"" math notation?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,32.0,False,t3_gekfgp,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/m_fflNj6_SA_UOLY__YCCoDNT0MI_fDaKklzAjdGzwY.jpg,False,,[],{},self,,True,,1588802060.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m writing a report about the mathematics behind feedforward neural networks. When I was presenting my proposal to the instructor, he said that I need to be careful using the superscript notation to refer to vectors/matrices, since it is not considered a &amp;quot;standard&amp;quot; math notation.&lt;/p&gt;

&lt;p&gt;Here is an example of the superscript notation in question. The x&lt;sup&gt;(i&lt;/sup&gt;) and y&lt;sup&gt;(i&lt;/sup&gt;) refers to the i&lt;sup&gt;th&lt;/sup&gt; training example and the i&lt;sup&gt;th&lt;/sup&gt; ground-truth label respectively.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361""&gt;https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is this superscript notation considered &amp;quot;standard&amp;quot; in maths, or is it just limited to the field of machine learning? I tried to search for sources but couldn&amp;#39;t find the origin of the notation.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://stats.stackexchange.com/questions/193908/in-machine-learning-why-are-superscripts-used-instead-of-subscripts""&gt;This&lt;/a&gt; stackoverflow thread also discusses this issue, but none of the answers there referenced any reputable sources. I probably need to find a reputable source in order to convince my instructor.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?auto=webp&amp;s=e25be8cb5cf2448d39a7e5ffc877e4d466b776d3', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9645cb9b1f6437724f04c3b63b841cf7766f27d6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7ee570d3611e1ef9314d658423c42908808820', 'width': 216, 'height': 216}], 'variants': {}, 'id': '63C1GbYQbI4tHZMkw99e-qlyoYaGnG58yfnmnFUJ34s'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gekfgp,True,,Unturned3,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekfgp/is_the_superscript_notation_used_in_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gekfgp/is_the_superscript_notation_used_in_machine/,155203,1588773260.0,0,,False,,,"{'3gfr7tryf5x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 25, 'x': 108, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b59fc419d312e3e67b76afa296f7488df7bd0c53'}, {'y': 50, 'x': 216, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=535304b63d7d7820e4f9eb92ea27f9bc74abfcb3'}, {'y': 74, 'x': 320, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=566a47be4466462f39d564b1ab427cc3ee96e47d'}], 's': {'y': 77, 'x': 329, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;format=png&amp;auto=webp&amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361'}, 'id': '3gfr7tryf5x41'}}",
,learnmachinelearning," I had the opportunity to get lost in a world of algorithm evaluation on my latest blog article. It's a niche topic but anyone look at ML who finds this interesting such as me, might find my article a good read.

[https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/](https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/)",t2_5u3zdmy8,False,,0,False,Precision and Recall - Evaluating Classification Algorithms like secrets detection,[],r/learnmachinelearning,False,6,,0,,False,t3_gek4d3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588800933.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had the opportunity to get lost in a world of algorithm evaluation on my latest blog article. It&amp;#39;s a niche topic but anyone look at ML who finds this interesting such as me, might find my article a good read.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/""&gt;https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?auto=webp&amp;s=ae6357c9da049d40bed8d5d0353fc1fbf677478a', 'width': 1180, 'height': 690}, 'resolutions': [{'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01100f06dfcebd81841314d0bfec3faacf1d234e', 'width': 108, 'height': 63}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59adb74871885c40894148326907fb341598ef9a', 'width': 216, 'height': 126}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e16a06d7f5ac9d057ccf232a9cd758148842c5c', 'width': 320, 'height': 187}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82cce0c7fce3ed30588c03483ee8f8d8cbbeface', 'width': 640, 'height': 374}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd98fff1c20505e4a81dae48712e03eff4e4a7d0', 'width': 960, 'height': 561}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27d9314ef79c15fb956a356bf532689ccfd7ca36', 'width': 1080, 'height': 631}], 'variants': {}, 'id': 'DnxQVypYnehZ7JfD2358CJ7yTgUFxpd7o6UDsjXnnk8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gek4d3,True,,Mackenzie-GG,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gek4d3/precision_and_recall_evaluating_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gek4d3/precision_and_recall_evaluating_classification/,155203,1588772133.0,0,,False,,,,
,learnmachinelearning," Suppose I have timeseries (X) and I also have labels (binary) corresponding to each timestep. The label at timestep ""t"" is based on the value of X at timestep ""t"", i.e., for example if x\_t &gt; 0.6 then y\_t = 1 else 0. My objective here is to predict the label at (t+1) given historic data till time ""t"". Its upto us to decide how much of history we should use.

We can achieve the objective in 2 ways:

1. Train a LSTM using X and predict the value of X at time ""t+1"" and then classify X at time ""t+1"" to one of the classes (using another model or one additional layer of softmax). The training input here being X\[t-k: t\] (can use a window of size k to generate inputs from sequence) and the its corresponding output being X\[t+1\].
2. Train a LSTM using X and predict the label at time ""t+1"" directly. The training input here being X\[t-k: t\] (can use a window of size k to generate inputs from sequence) and the its corresponding output being the label of x at time ""t+1"". Since training data is historic data, we know the labels for each x.

I'm confused whether approach 2 make sense logically. I understand that LSTM will try to build a model and find a mapping between the inputs and outputs. But how different it will be from LSTM which is trained on the same inputs but its output was the label at time ""t"" instead of time ""t+1"".

Approach 1 make sense logically, but will approach 2 do the same, i.e., will the LSTM internally first try to predict the value of x at time t+1 and then classify x at time t+1.

Any comments or suggestions?",t2_83ul0,False,,0,False,Time series classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gejgxl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588798507.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose I have timeseries (X) and I also have labels (binary) corresponding to each timestep. The label at timestep &amp;quot;t&amp;quot; is based on the value of X at timestep &amp;quot;t&amp;quot;, i.e., for example if x_t &amp;gt; 0.6 then y_t = 1 else 0. My objective here is to predict the label at (t+1) given historic data till time &amp;quot;t&amp;quot;. Its upto us to decide how much of history we should use.&lt;/p&gt;

&lt;p&gt;We can achieve the objective in 2 ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train a LSTM using X and predict the value of X at time &amp;quot;t+1&amp;quot; and then classify X at time &amp;quot;t+1&amp;quot; to one of the classes (using another model or one additional layer of softmax). The training input here being X[t-k: t] (can use a window of size k to generate inputs from sequence) and the its corresponding output being X[t+1].&lt;/li&gt;
&lt;li&gt;Train a LSTM using X and predict the label at time &amp;quot;t+1&amp;quot; directly. The training input here being X[t-k: t] (can use a window of size k to generate inputs from sequence) and the its corresponding output being the label of x at time &amp;quot;t+1&amp;quot;. Since training data is historic data, we know the labels for each x.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m confused whether approach 2 make sense logically. I understand that LSTM will try to build a model and find a mapping between the inputs and outputs. But how different it will be from LSTM which is trained on the same inputs but its output was the label at time &amp;quot;t&amp;quot; instead of time &amp;quot;t+1&amp;quot;.&lt;/p&gt;

&lt;p&gt;Approach 1 make sense logically, but will approach 2 do the same, i.e., will the LSTM internally first try to predict the value of x at time t+1 and then classify x at time t+1.&lt;/p&gt;

&lt;p&gt;Any comments or suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gejgxl,True,,Laboulaye,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gejgxl/time_series_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gejgxl/time_series_classification/,155203,1588769707.0,0,,False,,,,
,learnmachinelearning,"So, I just finished testing and evaluating my CNN DL Model to classify images into one of ten classes.

I'd like to convert this work into a research paper and hence should do tests on other standardized datasets to show the results.

The problem is, that this a new model that works on a self made data-set and all the other datasets have only 2-3 classes that are a subset of my data-set.

Is there a way to adapt my model that predicts 10 classes so that it will work for these other datasets?

I'd like to hear your opinions on how you guys generally handle this problem.",t2_107f70iq,False,,0,False,Testing your shiny new model on standardized datasets,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gefgzu,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,self,False,,[],{},,,True,,1588779217.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I just finished testing and evaluating my CNN DL Model to classify images into one of ten classes.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to convert this work into a research paper and hence should do tests on other standardized datasets to show the results.&lt;/p&gt;

&lt;p&gt;The problem is, that this a new model that works on a self made data-set and all the other datasets have only 2-3 classes that are a subset of my data-set.&lt;/p&gt;

&lt;p&gt;Is there a way to adapt my model that predicts 10 classes so that it will work for these other datasets?&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to hear your opinions on how you guys generally handle this problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gefgzu,True,,PsydeliX_,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gefgzu/testing_your_shiny_new_model_on_standardized/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gefgzu/testing_your_shiny_new_model_on_standardized/,155203,1588750417.0,0,,False,,,,
,learnmachinelearning,"It doesn't matter if it's something trivial. As long as it's new information about machine learning you didn't know until this week, feel free to share!",t2_6l4z3,False,,0,False,TWIL (This Week I Learned) - Share something new that you have learned this week!,[],r/learnmachinelearning,False,6,,0,,False,t3_gef568,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,True,self,False,,[],{},,,True,,1588777475.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It doesn&amp;#39;t matter if it&amp;#39;s something trivial. As long as it&amp;#39;s new information about machine learning you didn&amp;#39;t know until this week, feel free to share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gef568,True,,AutoModerator,,1,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gef568/twil_this_week_i_learned_share_something_new_that/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gef568/twil_this_week_i_learned_share_something_new_that/,155203,1588748675.0,0,,False,,,,
,learnmachinelearning,,t2_205ygpnb,False,,0,False,"Machine Learning with Python : Part 1: Model Persistent :: How to do and make : Independent and Dependent data , import Logistics Regression and Using Logistic Regression, fit and predict data, import pickle and dump data .",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gedazi,False,dark,1.0,,public,3,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_96', 'dash_url': 'https://v.redd.it/v9ilgjfgo2x41/DASHPlaylist.mpd', 'duration': 49, 'hls_url': 'https://v.redd.it/v9ilgjfgo2x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/I7Vmc0BkFoLDKcArg8KHnyfcFccUaAERQsw1uZdxAIU.jpg,False,,[],{},hosted:video,,False,,1588768277.0,text,6,,,text,v.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?format=pjpg&amp;auto=webp&amp;s=4e8822c805190883939674eb03500991e4ab3254', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=85f06fad76fd1680af23e60059420f8d115ec137', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81b1199e8daa30be7c1f50bb9bb9ceb28e3f269d', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=51b434689d61d54b1d47e937a86d02dd4b72b1bc', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a7600f71f90ccaa87a812013d4cdccb2dd76f109', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e21102012a2cef3f3a8d9581ecbc5ec2766b2fa5', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=546cd91b1a1198af1f93feac7925aef7c69249b1', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'aX0JJlKw7sHMUW3xNiIJdBP1VuGoKlocQ_-L57lVc3c'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gedazi,True,,iamrealadvait,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gedazi/machine_learning_with_python_part_1_model/,all_ads,False,https://v.redd.it/v9ilgjfgo2x41,155203,1588739477.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_96', 'dash_url': 'https://v.redd.it/v9ilgjfgo2x41/DASHPlaylist.mpd', 'duration': 49, 'hls_url': 'https://v.redd.it/v9ilgjfgo2x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,,t2_5a1j5sce,False,,0,False,These Are The Essential Mathematical Skills To Get A Job As A Data Scientist/Machine Learning Engineer,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gehhjh,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/Frlg4kTfODkFzEWnYMBdxfd7FFPyY08UUIFclZriEw4.jpg,False,,[],{},link,,False,,1588789733.0,text,6,,,text,laconicml.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?auto=webp&amp;s=c4df69d1e4c581de679a5e78f9a51ad1c946553d', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2c677fab77dae842f03e1bf39fcd1361ba738c2', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41290e26c3152f1f5218429063e92a1a6066827d', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11e3ab612170ec802057e1e102524a65eb2a1d98', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6513dab7451d29d6bbfc37792f6504733c649936', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8335af811d9179ab5f298a0f079ee398cd732fd0', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/JXTKUUGwvAtszh6-951xf2SlaXNJ4MLD-RG0lu9LvJU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=034d707f5d3afcf59c45ea9258d5f08c8aa6892a', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'qDwmZS34l84z1od-N5hTkoni9QEraaNps8LzunCHkoo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gehhjh,True,,yung_quan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gehhjh/these_are_the_essential_mathematical_skills_to/,all_ads,False,https://laconicml.com/mathematical-skills-data-scientist/,155203,1588760933.0,0,,False,,,,
,learnmachinelearning,"I have a classification problem where I use a GridSearch strategy to find a classifier to classify cells (the hard part is to get the signals from a new optical system I designed, the ML part I'm working on is for sure trivial for ML people). The GridSearch converges towards the same classifier each time (GradientBoostingCLassifier) with very good performances.

I then create overlay images (volumes to be more specific) with synthetic colors for each class of cells with predicted and true classes on the validation set. The problem is only a few cells are colored because most of the cells are in the train and test sets. So my idea is to do a sort of cross training/testing/validating where the validation set is switched so that each cell appear once in the validation set so I can create an overlay result with all the cells.

Before attempting such monstrosity I would like to control that the Decision Trees are quite similar. Although the performances are stable when shuffling the train, test and validation datasets I don't know if the classifier converges towards the **same** tree. The problem for me is to define the similarity between GradientBoostingClassifier, I tried to look at the literature but it's more about theoretical work rather than actual implementations.

Thank you for your help!",t2_2dm0k26o,False,,0,False,Similarity metric for trees,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_geh2k2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588787744.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a classification problem where I use a GridSearch strategy to find a classifier to classify cells (the hard part is to get the signals from a new optical system I designed, the ML part I&amp;#39;m working on is for sure trivial for ML people). The GridSearch converges towards the same classifier each time (GradientBoostingCLassifier) with very good performances.&lt;/p&gt;

&lt;p&gt;I then create overlay images (volumes to be more specific) with synthetic colors for each class of cells with predicted and true classes on the validation set. The problem is only a few cells are colored because most of the cells are in the train and test sets. So my idea is to do a sort of cross training/testing/validating where the validation set is switched so that each cell appear once in the validation set so I can create an overlay result with all the cells.&lt;/p&gt;

&lt;p&gt;Before attempting such monstrosity I would like to control that the Decision Trees are quite similar. Although the performances are stable when shuffling the train, test and validation datasets I don&amp;#39;t know if the classifier converges towards the &lt;strong&gt;same&lt;/strong&gt; tree. The problem for me is to define the similarity between GradientBoostingClassifier, I tried to look at the literature but it&amp;#39;s more about theoretical work rather than actual implementations.&lt;/p&gt;

&lt;p&gt;Thank you for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,geh2k2,True,,wkns,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geh2k2/similarity_metric_for_trees/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geh2k2/similarity_metric_for_trees/,155203,1588758944.0,0,,False,,,,
,learnmachinelearning,"Hi, so I'm wondering whether I should use the Tensorflow data API. I tried it and it seems quite complicated and annoying (but obviously this is something to learn). Do you recommend using it, or should I stick to the classical methods?

Thanks ;)",t2_axav08j,False,,0,False,Should I use the Tensorflow data API,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_geh1y4,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588787656.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, so I&amp;#39;m wondering whether I should use the Tensorflow data API. I tried it and it seems quite complicated and annoying (but obviously this is something to learn). Do you recommend using it, or should I stick to the classical methods?&lt;/p&gt;

&lt;p&gt;Thanks ;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geh1y4,True,,everek123,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geh1y4/should_i_use_the_tensorflow_data_api/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geh1y4/should_i_use_the_tensorflow_data_api/,155203,1588758856.0,0,,False,,,,
,learnmachinelearning,"I know layer normalization and I am having difficulty in understanding the difference between layer and instance normalization. 
The definition of instance normalization is given as ""instance normalization normalizes across each channel in each training example instead of normalizing across input features in an training example"". 
Here what is meant by channel? How does this make it different and better normalization method than others in applications like style transfer?",t2_4w7oamkl,False,,0,False,What is instance normalization?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gegdxb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588784180.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know layer normalization and I am having difficulty in understanding the difference between layer and instance normalization. 
The definition of instance normalization is given as &amp;quot;instance normalization normalizes across each channel in each training example instead of normalizing across input features in an training example&amp;quot;. 
Here what is meant by channel? How does this make it different and better normalization method than others in applications like style transfer?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gegdxb,True,,HTKasd,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gegdxb/what_is_instance_normalization/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gegdxb/what_is_instance_normalization/,155203,1588755380.0,0,,False,,,,
,learnmachinelearning,"Open-source project DVC was released 3 years ago. Their new article shares what learned from this journey and how the lessons are reflected in DVC 1.0 pre-release: [DVC 3 Years and 1.0 Pre-release](https://dvc.org/blog/dvc-3-years-and-1-0-release)

tl;dr;

1. Forcing users to make Git commits for each machine learning (ML) experiment creates a bit too much overhead.
2. ML pipelines evolve much faster than data engineering pipelines.
3. Versioning metrics and plots are no less important than data versioning.
4. In ML projects, data transfer optimization is still really important part.
5. Open-source community helps tremendously to define the requirements and build the product.",t2_m8sjl,False,,0,False,Data Version Control (DVC) - Git for ML data - 3 years &amp; 1.0 pre-release,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gegcn6,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,1588755709.0,,[],{},self,,True,,1588783988.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Open-source project DVC was released 3 years ago. Their new article shares what learned from this journey and how the lessons are reflected in DVC 1.0 pre-release: &lt;a href=""https://dvc.org/blog/dvc-3-years-and-1-0-release""&gt;DVC 3 Years and 1.0 Pre-release&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;tl;dr;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Forcing users to make Git commits for each machine learning (ML) experiment creates a bit too much overhead.&lt;/li&gt;
&lt;li&gt;ML pipelines evolve much faster than data engineering pipelines.&lt;/li&gt;
&lt;li&gt;Versioning metrics and plots are no less important than data versioning.&lt;/li&gt;
&lt;li&gt;In ML projects, data transfer optimization is still really important part.&lt;/li&gt;
&lt;li&gt;Open-source community helps tremendously to define the requirements and build the product.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/tqZl7cPZcMMVPA5_pylDXjL-1Ef4xg34gfOpBsSrZYk.jpg?auto=webp&amp;s=dab7e0c3a999b3d700d6346deb623a0824c674bb', 'width': 850, 'height': 478}, 'resolutions': [{'url': 'https://external-preview.redd.it/tqZl7cPZcMMVPA5_pylDXjL-1Ef4xg34gfOpBsSrZYk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14d27ddedda0316468d87a803854b186057b79fb', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/tqZl7cPZcMMVPA5_pylDXjL-1Ef4xg34gfOpBsSrZYk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15ea53020b48884618fe8d92648a10dff5e86b30', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/tqZl7cPZcMMVPA5_pylDXjL-1Ef4xg34gfOpBsSrZYk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7f16a9067aff9c726053898bc206ffa6885186f9', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/tqZl7cPZcMMVPA5_pylDXjL-1Ef4xg34gfOpBsSrZYk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ef545968e1190596889e63dd0a1941e46c860e2', 'width': 640, 'height': 359}], 'variants': {}, 'id': '7dlZOPslY1CxSjhX9IB1X_Yz8drATXjcMbb27rq9z0M'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gegcn6,True,,thumbsdrivesmecrazy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gegcn6/data_version_control_dvc_git_for_ml_data_3_years/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gegcn6/data_version_control_dvc_git_for_ml_data_3_years/,155203,1588755188.0,0,,False,,,,
,learnmachinelearning,"How is that even possible? Training on the square of a single number n is obvious, you can make a hidden layer with n neurons. But if there are two numbers it seems impossible to me.",t2_2xz6764h,False,,0,False,How is it possible for a neural network to accurately approximate the square of a number?,[],r/learnmachinelearning,False,6,,0,,False,t3_geg157,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588782275.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How is that even possible? Training on the square of a single number n is obvious, you can make a hidden layer with n neurons. But if there are two numbers it seems impossible to me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geg157,True,,User1377420,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geg157/how_is_it_possible_for_a_neural_network_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geg157/how_is_it_possible_for_a_neural_network_to/,155203,1588753475.0,0,,False,,,,
,learnmachinelearning,I think a one-step forecasting model is easier to design and maybe not so complex like a multi-step model. What is difference between using one-step model recursively vs. a model that outputs a sequence (multi-step model)?,t2_6e1nuxcx,False,,0,False,Recursive one-step vs. multi-step time series forecasting?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gefor6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588780424.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I think a one-step forecasting model is easier to design and maybe not so complex like a multi-step model. What is difference between using one-step model recursively vs. a model that outputs a sequence (multi-step model)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gefor6,True,,alex-trbznk,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gefor6/recursive_onestep_vs_multistep_time_series/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gefor6/recursive_onestep_vs_multistep_time_series/,155203,1588751624.0,0,,False,,,,
,learnmachinelearning,,t2_5rs2yt94,False,,0,False,Train your own lyric generator using distill-gpt2,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gef6by,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/WcH1shrrVN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to: AI Learns to Write Modern Talking Lyrics', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/WcH1shrrVN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'adam0ling', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/WcH1shrrVN4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCu6JhqCuDiAjhSI6YdHYu_Q'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/WcH1shrrVN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gef6by', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/VMuS_C80FdITfnNeoxjotieBSDuZLlzxix3KJoMnDzY.jpg,False,,[],{},rich:video,,False,,1588777640.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_gkgCY4MM7H3qROHxQJPxlhukf9NuW8Pzo1goGa-BNI.jpg?auto=webp&amp;s=dcdcad512f79a3d15d2059911037eca6809448af', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/_gkgCY4MM7H3qROHxQJPxlhukf9NuW8Pzo1goGa-BNI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4a6c534942297644bd3342f858ccf03ac784ef2', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/_gkgCY4MM7H3qROHxQJPxlhukf9NuW8Pzo1goGa-BNI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4261ae0b1b7e8d94503527df19b28748128b0222', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/_gkgCY4MM7H3qROHxQJPxlhukf9NuW8Pzo1goGa-BNI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf6105b1913f79eb4e2f8e47586ec871dabf7095', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fTacmpv6KbAQeEPHKUaE6TbXd1ZL19LYa9P-m2E3otQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gef6by,True,,adam0ling,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gef6by/train_your_own_lyric_generator_using_distillgpt2/,all_ads,False,https://youtu.be/WcH1shrrVN4,155203,1588748840.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to: AI Learns to Write Modern Talking Lyrics', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/WcH1shrrVN4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'adam0ling', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/WcH1shrrVN4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCu6JhqCuDiAjhSI6YdHYu_Q'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Can artificial intelligence already help composing songs that would be successful at winning the Eurovision contest? In the AI Song Contest teams from all over Europe and Australia compete attempting to create the next Eurovision hit with the help of artificial intelligence.

I am a member of the Beatroots team and our song was composed by an end-to-end algorithm. We put out our auto generated music on Spotify ([https://open.spotify.com/artist/0bEJEKrBzaYr6SJsdliQIU?si=z8I0oBIXTcSkUYNO-ATqkQ](https://open.spotify.com/artist/0bEJEKrBzaYr6SJsdliQIU?si=z8I0oBIXTcSkUYNO-ATqkQ)) You can create your own AI generated song with this algorithm in this Google Colab (you have to copy it locally before using, or run in playground mode): [https://colab.research.google.com/drive/1lNdSSTGQswsRO0fRiK4E82Eozmu1mOvI#scrollTo=Zah\_Sx1gHtE4](https://colab.research.google.com/drive/1lNdSSTGQswsRO0fRiK4E82Eozmu1mOvI#scrollTo=Zah_Sx1gHtE4)

We also gave a webinar on our approach, which you can find right here on youtube: [https://www.youtube.com/watch?v=pQCsZhVwdi8&amp;t=8s](https://www.youtube.com/watch?v=pQCsZhVwdi8&amp;t=8s)

We used 200 old Eurovision songs normalised in midi files split by section as training data. The encodings from Magenta's MusicVAE are the input of our custom built Variational Auto-Encoder. We built several models, each generating either an intro, verse, chorus... This is actually a very similar approach to the recent OpenAI Jukebox, but with symbolic music as input data instead of raw waveforms. Our model also runs on your desktop while you sit in your sofa next to your gf binging Gossip Girl :)

We combine all our section models to create the final song by implementing a shortest path algorithm between all generated harmonies in the MusicVAE encoding space.

Just as in the real competition, there is a jury as well as well as a public vote. Please vote for your favourite song on [https://www.vprobroadcast.com/titles/ai-songcontest/about.html](https://www.vprobroadcast.com/titles/ai-songcontest/about.html). You can vote until the 10th of May. There is also more information on each team's creation processes. SPOlLER: other teams have more professional sounding songs because they added a human touch, we went a bit too far and geeky with the staying inside and let everything be generated by our beloved laptops.

enjoy! :)

tldr; vote for Beatroots [https://www.vprobroadcast.com/titles/ai-songcontest/about.html](https://www.vprobroadcast.com/titles/ai-songcontest/about.html).",t2_1t6fdpf3,False,,0,False,AI song contest: Beatroots submission,[],r/learnmachinelearning,False,6,,0,,False,t3_ge3bi5,False,dark,0.92,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},self,,True,,1588732456.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can artificial intelligence already help composing songs that would be successful at winning the Eurovision contest? In the AI Song Contest teams from all over Europe and Australia compete attempting to create the next Eurovision hit with the help of artificial intelligence.&lt;/p&gt;

&lt;p&gt;I am a member of the Beatroots team and our song was composed by an end-to-end algorithm. We put out our auto generated music on Spotify (&lt;a href=""https://open.spotify.com/artist/0bEJEKrBzaYr6SJsdliQIU?si=z8I0oBIXTcSkUYNO-ATqkQ""&gt;https://open.spotify.com/artist/0bEJEKrBzaYr6SJsdliQIU?si=z8I0oBIXTcSkUYNO-ATqkQ&lt;/a&gt;) You can create your own AI generated song with this algorithm in this Google Colab (you have to copy it locally before using, or run in playground mode): &lt;a href=""https://colab.research.google.com/drive/1lNdSSTGQswsRO0fRiK4E82Eozmu1mOvI#scrollTo=Zah_Sx1gHtE4""&gt;https://colab.research.google.com/drive/1lNdSSTGQswsRO0fRiK4E82Eozmu1mOvI#scrollTo=Zah_Sx1gHtE4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We also gave a webinar on our approach, which you can find right here on youtube: &lt;a href=""https://www.youtube.com/watch?v=pQCsZhVwdi8&amp;amp;t=8s""&gt;https://www.youtube.com/watch?v=pQCsZhVwdi8&amp;amp;t=8s&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We used 200 old Eurovision songs normalised in midi files split by section as training data. The encodings from Magenta&amp;#39;s MusicVAE are the input of our custom built Variational Auto-Encoder. We built several models, each generating either an intro, verse, chorus... This is actually a very similar approach to the recent OpenAI Jukebox, but with symbolic music as input data instead of raw waveforms. Our model also runs on your desktop while you sit in your sofa next to your gf binging Gossip Girl :)&lt;/p&gt;

&lt;p&gt;We combine all our section models to create the final song by implementing a shortest path algorithm between all generated harmonies in the MusicVAE encoding space.&lt;/p&gt;

&lt;p&gt;Just as in the real competition, there is a jury as well as well as a public vote. Please vote for your favourite song on &lt;a href=""https://www.vprobroadcast.com/titles/ai-songcontest/about.html""&gt;https://www.vprobroadcast.com/titles/ai-songcontest/about.html&lt;/a&gt;. You can vote until the 10th of May. There is also more information on each team&amp;#39;s creation processes. SPOlLER: other teams have more professional sounding songs because they added a human touch, we went a bit too far and geeky with the staying inside and let everything be generated by our beloved laptops.&lt;/p&gt;

&lt;p&gt;enjoy! :)&lt;/p&gt;

&lt;p&gt;tldr; vote for Beatroots &lt;a href=""https://www.vprobroadcast.com/titles/ai-songcontest/about.html""&gt;https://www.vprobroadcast.com/titles/ai-songcontest/about.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zhLkpghsr-rpHVNUZz0K5czDkgMDvlvsLDy6q30F2nw.jpg?auto=webp&amp;s=ca2a8b6921cfe7d68d892a0d11778558fecf0ae9', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/zhLkpghsr-rpHVNUZz0K5czDkgMDvlvsLDy6q30F2nw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc27909c8b12eb41bfc546620eb27f093726a386', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/zhLkpghsr-rpHVNUZz0K5czDkgMDvlvsLDy6q30F2nw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8a250121273de09dcc4f2513b3d9a416a83073f', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'ibzL4988zAGhjit7ZvUkOdBAN938v_fjXN4UED2DfO0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge3bi5,True,,AndroidNeedHeaven,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge3bi5/ai_song_contest_beatroots_submission/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge3bi5/ai_song_contest_beatroots_submission/,155203,1588703656.0,0,,False,,,,
,learnmachinelearning,,t2_4csu169w,False,,0,False,Online courses are great but I think beginners should be aware of this downside!,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_geex8z,False,light,0.6,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YhOz5G_H704?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'The Course Loop and How to Break It', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YhOz5G_H704?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YhOz5G_H704/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YhOz5G_H704?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/geex8z', 'height': 338}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/YItDbSJMKOGKfsFXmoPqQbniZbgXRbSraz--2OCX_BM.jpg,False,,[],{},rich:video,,False,,1588776289.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qZ9vh1zRS7fboQgraVHOT0VEx1v9BwuAwuMTTnieREs.jpg?auto=webp&amp;s=f480fd98a7121e2176720497a9316b58f84dfb65', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/qZ9vh1zRS7fboQgraVHOT0VEx1v9BwuAwuMTTnieREs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f15300bb5e0c1baab093b0fbd73ed7d4ded25dfb', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/qZ9vh1zRS7fboQgraVHOT0VEx1v9BwuAwuMTTnieREs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=507dd163b138c0ee205e35c7ebc3f594d29c4af1', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/qZ9vh1zRS7fboQgraVHOT0VEx1v9BwuAwuMTTnieREs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e9b291cee4e7ec6b9a5f8eb60fef5aa132c5dde', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'GxKV6x9Fjh8FzeaMIppxEi0XsVi7f-9m2IzDyKs4Quk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,geex8z,True,,nerdy_wits,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geex8z/online_courses_are_great_but_i_think_beginners/,all_ads,False,https://youtu.be/YhOz5G_H704,155203,1588747489.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'The Course Loop and How to Break It', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YhOz5G_H704?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YhOz5G_H704/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Has anyone here used Hidden Markov Models (HMM)  before? What are their advantages compared to other ML techniques? Can they be used for unsupervised outlier detection?

Thanks!",t2_o4xj9,False,,0,False,Hidden Markov Models,[],r/learnmachinelearning,False,6,,0,,False,t3_geeptf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588775158.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone here used Hidden Markov Models (HMM)  before? What are their advantages compared to other ML techniques? Can they be used for unsupervised outlier detection?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geeptf,True,,blueest,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geeptf/hidden_markov_models/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geeptf/hidden_markov_models/,155203,1588746358.0,0,,False,,,,
,learnmachinelearning,,t2_3m3wzbiw,False,,0,False,Machine Learning Tutorial | Face Recognition in 10 min,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_geefyc,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/taIlzb0OUTU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning Tutorial  | Face Recognition in 10 min', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/taIlzb0OUTU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/taIlzb0OUTU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/taIlzb0OUTU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/geefyc', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/OST9NU_9Pt6YITtSFGKfj5aXoYixpp0NtQEb8179nsA.jpg,False,,[],{},rich:video,,False,,1588773716.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rOEdKhaVHL6HJ1jE9_XsK6JUt0-GWAn2H_Us0kq6Lw4.jpg?auto=webp&amp;s=5cd102e6558767a9866a6ee15fab5c30a82e0ea4', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/rOEdKhaVHL6HJ1jE9_XsK6JUt0-GWAn2H_Us0kq6Lw4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a1dac7b3187e91881b2a6d2e0915af3e8646f28', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/rOEdKhaVHL6HJ1jE9_XsK6JUt0-GWAn2H_Us0kq6Lw4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=417902a7b5eab9c22c94cd6455940910f4f6f92a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/rOEdKhaVHL6HJ1jE9_XsK6JUt0-GWAn2H_Us0kq6Lw4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=811706c70eeeab796b29cee26327b00e1081bca7', 'width': 320, 'height': 240}], 'variants': {}, 'id': '2oX6pJI2iWVbXKTEU3E0jFWXF9U4m368yYzX-J5Ms_M'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geefyc,True,,lukescriptwalker,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geefyc/machine_learning_tutorial_face_recognition_in_10/,all_ads,False,https://youtu.be/taIlzb0OUTU,155203,1588744916.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning Tutorial  | Face Recognition in 10 min', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/taIlzb0OUTU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/taIlzb0OUTU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"&amp;#x200B;

https://preview.redd.it/xozu2obh33x41.png?width=1366&amp;format=png&amp;auto=webp&amp;s=cf30787ef849cbf1dd4140fc9ede7642428118de

I was reading this presentation on Slideshare. Since the slides were uploaded in 2016,  I was wondering if it is still true. I need to know as  I am doing some research on deploying several ML models trained using Keras into production. I haven't used Docker before and I am working my way through it. 

Currently, we are looking at multiple ways to deploy our pipeline and I needed to figure out a way asap but most preferably I would want to use Heroku ( as I have prior experience with it).",t2_2opctu9w,False,,0,False,Is it true that Docker and Deep Learning are a bad match?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_geef7o,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/gc0M9m9lnmECYB2sWsCrIi_7GYDE_MTqdgdsFpBsbvk.jpg,False,,[],{},,,True,,1588773610.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/xozu2obh33x41.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf30787ef849cbf1dd4140fc9ede7642428118de""&gt;https://preview.redd.it/xozu2obh33x41.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf30787ef849cbf1dd4140fc9ede7642428118de&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I was reading this presentation on Slideshare. Since the slides were uploaded in 2016,  I was wondering if it is still true. I need to know as  I am doing some research on deploying several ML models trained using Keras into production. I haven&amp;#39;t used Docker before and I am working my way through it. &lt;/p&gt;

&lt;p&gt;Currently, we are looking at multiple ways to deploy our pipeline and I needed to figure out a way asap but most preferably I would want to use Heroku ( as I have prior experience with it).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,geef7o,True,,bananaskywalker,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geef7o/is_it_true_that_docker_and_deep_learning_are_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geef7o/is_it_true_that_docker_and_deep_learning_are_a/,155203,1588744810.0,0,,False,,,"{'xozu2obh33x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b7e8aec63656df9e7ac9b2a4113bd64dfc11c6b'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c5872e551b6dbace6cb7b5b94dab56576b9efdd'}, {'y': 179, 'x': 320, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a9978e45ddb0cb482ed901e5151642e75fcb917'}, {'y': 359, 'x': 640, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8a6cf3e48f34b54e934c53a57f6038e2bb836b1'}, {'y': 539, 'x': 960, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=851b107a607377ddaa0529f027c137a75ddb0aa8'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb8b4e364e0de4d20cb48f4166d1140c050f32c7'}], 's': {'y': 768, 'x': 1366, 'u': 'https://preview.redd.it/xozu2obh33x41.png?width=1366&amp;format=png&amp;auto=webp&amp;s=cf30787ef849cbf1dd4140fc9ede7642428118de'}, 'id': 'xozu2obh33x41'}}",
,learnmachinelearning,"Besides the assumptions related to being a non-parametric test, doesn't KNN also assume that each predictor variable is an equal influencer of the response variable?  Doesn't this sort of making it extremely prone to over-fitting?",t2_4y4ei,False,,0,False,Regarding the assumptions of a KNN classifier?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gear4g,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588757677.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Besides the assumptions related to being a non-parametric test, doesn&amp;#39;t KNN also assume that each predictor variable is an equal influencer of the response variable?  Doesn&amp;#39;t this sort of making it extremely prone to over-fitting?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gear4g,True,,helloreddits456464,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gear4g/regarding_the_assumptions_of_a_knn_classifier/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gear4g/regarding_the_assumptions_of_a_knn_classifier/,155203,1588728877.0,0,,False,,,,
,learnmachinelearning,"I've been lately reading the mathematics behind ML algorithms and also learning the statistics and maths that goes with it and coding them along the way in my [github](https://github.com/ZER-0-NE/ML_problems/tree/master/ML-practice) repo.  


I'm doing my best to understand these things but I want to get started on the side implementing some research papers. This can really help me understand (or at least get a start) on how to approach them.  
How did you get started with implementing research papers? What papers did you start with? Do you have some papers I could get started with as a beginner?",t2_s529ats,False,,0,False,Start implementing reasearch papers with maths,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gds0tx,False,light,0.97,,public,74,0,{},,,False,[],,False,False,,{},Discussion,False,74,,False,self,False,,[],{},self,,True,,1588686121.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been lately reading the mathematics behind ML algorithms and also learning the statistics and maths that goes with it and coding them along the way in my &lt;a href=""https://github.com/ZER-0-NE/ML_problems/tree/master/ML-practice""&gt;github&lt;/a&gt; repo.  &lt;/p&gt;

&lt;p&gt;I&amp;#39;m doing my best to understand these things but I want to get started on the side implementing some research papers. This can really help me understand (or at least get a start) on how to approach them.&lt;br/&gt;
How did you get started with implementing research papers? What papers did you start with? Do you have some papers I could get started with as a beginner?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FLZYvBC6TiBt0HU0o3A5vNF4833xgHte5VTSl3NM5EE.jpg?auto=webp&amp;s=f6df7206dc5f317322105136577706b535724fd3', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/FLZYvBC6TiBt0HU0o3A5vNF4833xgHte5VTSl3NM5EE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfd67360ff406edd34faa699c43295def12e755e', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/FLZYvBC6TiBt0HU0o3A5vNF4833xgHte5VTSl3NM5EE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=adb0792fa114aeb0958193bcc773001dc13598bf', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/FLZYvBC6TiBt0HU0o3A5vNF4833xgHte5VTSl3NM5EE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26f29cb7e21462e8475135c4cfcbc3197e92c2b4', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'hTllbtTLpX5d_pWevQRIcFJwpQT0KVcKLKIsE5YRYiY'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gds0tx,True,,ZER_0_NE,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gds0tx/start_implementing_reasearch_papers_with_maths/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gds0tx/start_implementing_reasearch_papers_with_maths/,155203,1588657321.0,0,,False,,,,
,learnmachinelearning,"To become a data scientist/machine learning engineer, it's unfortunately just *not enough to just be able to write code*! I hope this helps everyone understand the basics Git, the terminal, cloud computing, Linux, design patterns/antipatterns and all the other stuff we need to know:  
[https://www.kamwithk.com/the-complete-coding-practitioners-handbook-ck9u1vmgv03kg7bs1e5zwit2z](https://www.kamwithk.com/the-complete-coding-practitioners-handbook-ck9u1vmgv03kg7bs1e5zwit2z)",t2_4h41hw55,False,,0,False,The other stuff you NEED to know,[],r/learnmachinelearning,False,6,,0,,False,t3_gedi1r,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1588769157.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To become a data scientist/machine learning engineer, it&amp;#39;s unfortunately just &lt;em&gt;not enough to just be able to write code&lt;/em&gt;! I hope this helps everyone understand the basics Git, the terminal, cloud computing, Linux, design patterns/antipatterns and all the other stuff we need to know:&lt;br/&gt;
&lt;a href=""https://www.kamwithk.com/the-complete-coding-practitioners-handbook-ck9u1vmgv03kg7bs1e5zwit2z""&gt;https://www.kamwithk.com/the-complete-coding-practitioners-handbook-ck9u1vmgv03kg7bs1e5zwit2z&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UfODER3ZiQyZyMNBgiE1oWgZhOpP-HTQRRi7mEOF6bY.jpg?auto=webp&amp;s=ad97f85b22a4ee63e2d9cb233283c401ff1da396', 'width': 800, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/UfODER3ZiQyZyMNBgiE1oWgZhOpP-HTQRRi7mEOF6bY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e47b8216f9fa4d1a0bac2a0affb9812eefb97e69', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/UfODER3ZiQyZyMNBgiE1oWgZhOpP-HTQRRi7mEOF6bY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7243648648b03dbb70e770fcee287e78b00147aa', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/UfODER3ZiQyZyMNBgiE1oWgZhOpP-HTQRRi7mEOF6bY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cab8ff5f0bdc007cec08c08c06b2dbaabe7f8b6', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/UfODER3ZiQyZyMNBgiE1oWgZhOpP-HTQRRi7mEOF6bY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4968fb4c8d21ef9fd5620eacf663668e4850725c', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'Fck3kJPdTa_wsinrjUCprHNfii7CVx4tjyzq7RctPWA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gedi1r,True,,KamWithK,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gedi1r/the_other_stuff_you_need_to_know/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gedi1r/the_other_stuff_you_need_to_know/,155203,1588740357.0,0,,False,,,,
,learnmachinelearning,"Hey everyone. I am a student who wants to get into the field of RL. RL now uses deep learning methods to make the algos more efficient but I donot know any ML or DL. When I searched for ways to get started in the field, the sea of information overwhelmed me and I started panicking about howmuch I donot know. I will be grateful if someone gives me a guide on how to get started in the field of ML/DL. Also if you are in the same place as me and want to start in the field, I would love to study together with you.",t2_52s0yg2m,False,,0,False,Overwhelmed with the sea of information but will be starting today,[],r/learnmachinelearning,False,6,,0,,False,t3_ge97en,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588751891.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone. I am a student who wants to get into the field of RL. RL now uses deep learning methods to make the algos more efficient but I donot know any ML or DL. When I searched for ways to get started in the field, the sea of information overwhelmed me and I started panicking about howmuch I donot know. I will be grateful if someone gives me a guide on how to get started in the field of ML/DL. Also if you are in the same place as me and want to start in the field, I would love to study together with you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge97en,True,,retro-rabbit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge97en/overwhelmed_with_the_sea_of_information_but_will/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge97en/overwhelmed_with_the_sea_of_information_but_will/,155203,1588723091.0,0,,False,,,,
,learnmachinelearning,"StellarGraph is an open-source library implementing a variety of state-of-the-art graph machine learning algorithms. The project is delivered as part of CSIROâ€™s Data61.

We are thrilled to announce the **major milestone of a** [**full 1.0 release**](https://github.com/stellargraph/stellargraph/releases/) **of the library;** the culmination of three years of active research and engineering. 

V1.0 extends StellarGraph performance and capability with new algorithms for spatio-temporal data and graph classification, an updated StellarGraph class, and better demo notebooks and documentation.

**New algorithms include:**

* GCNSupervisedGraphClassification: supervised graph classification model based on Graph Convolutional layers (GCN).
* DeepGraphCNN: supervised graph classification based on GCN, a new SortPooling layer and asymmetric adjacency normalisation.
* GraphConvolutionLSTM: time series prediction on spatio-temporal data, combining GCN with a LSTM model to augment the conventional time-series model with information from nearby data points.

**Enhanced algorithms:**

* DeepGraphInfomax: can be used to train almost any model in an unsupervised way, for example HinSAGE for unsupervised heterogeneous graphs with node features.
* UnsupervisedSampler: supports a walker parameter to use other random walking algorithms such as BiasedRandomWalk, in addition to the default UniformRandomWalk.

The new release incorporates extensive **performance enhancements**, some of which include: 

* StellarGraph class now faster, easier to construct and smaller, with reduced memory usage to support larger graphs. 
* Better [demonstration notebooks](https://stellargraph.readthedocs.io/en/stable/demos/index.html) and documentation to make the library more accessible to new and existing users.
* Better Neo4j connectivity, including GraphSAGE neighborhood sampling from Neo4j and a demo notebook for loading and storing Neo4j graphs.
* Node feature sampling now \~4Ã— faster via better data layout, speeding up configurations of GraphSAGE (and HinSAGE) 
* Addition of PROTEINS dataset for graph classification demo[ ](https://github.com/stellargraph/stellargraph/pull/1282)
* Creating a RelationalFullBatchNodeGenerator now 18x faster and requires much less memory (560x smaller) 

Jump into the new release on [GitHub](https://github.com/stellargraph/stellargraph). StellarGraph is a Python 3 library. See full v1.0 release notes [here](https://github.com/stellargraph/stellargraph/releases/tag/v1.0.0).

We always welcome feedback and contributions. 

With thanks and celebration, the StellarGraph team.",t2_4h2fow53,False,,0,False,[R] Announcing the release of StellarGraph version 1.0 open-source Python Machine Learning Library for graphs,[],r/learnmachinelearning,False,6,,0,,False,t3_ge5wgf,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588740715.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;StellarGraph is an open-source library implementing a variety of state-of-the-art graph machine learning algorithms. The project is delivered as part of CSIROâ€™s Data61.&lt;/p&gt;

&lt;p&gt;We are thrilled to announce the &lt;strong&gt;major milestone of a&lt;/strong&gt; &lt;a href=""https://github.com/stellargraph/stellargraph/releases/""&gt;&lt;strong&gt;full 1.0 release&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;of the library;&lt;/strong&gt; the culmination of three years of active research and engineering. &lt;/p&gt;

&lt;p&gt;V1.0 extends StellarGraph performance and capability with new algorithms for spatio-temporal data and graph classification, an updated StellarGraph class, and better demo notebooks and documentation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New algorithms include:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GCNSupervisedGraphClassification: supervised graph classification model based on Graph Convolutional layers (GCN).&lt;/li&gt;
&lt;li&gt;DeepGraphCNN: supervised graph classification based on GCN, a new SortPooling layer and asymmetric adjacency normalisation.&lt;/li&gt;
&lt;li&gt;GraphConvolutionLSTM: time series prediction on spatio-temporal data, combining GCN with a LSTM model to augment the conventional time-series model with information from nearby data points.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Enhanced algorithms:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DeepGraphInfomax: can be used to train almost any model in an unsupervised way, for example HinSAGE for unsupervised heterogeneous graphs with node features.&lt;/li&gt;
&lt;li&gt;UnsupervisedSampler: supports a walker parameter to use other random walking algorithms such as BiasedRandomWalk, in addition to the default UniformRandomWalk.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The new release incorporates extensive &lt;strong&gt;performance enhancements&lt;/strong&gt;, some of which include: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;StellarGraph class now faster, easier to construct and smaller, with reduced memory usage to support larger graphs. &lt;/li&gt;
&lt;li&gt;Better &lt;a href=""https://stellargraph.readthedocs.io/en/stable/demos/index.html""&gt;demonstration notebooks&lt;/a&gt; and documentation to make the library more accessible to new and existing users.&lt;/li&gt;
&lt;li&gt;Better Neo4j connectivity, including GraphSAGE neighborhood sampling from Neo4j and a demo notebook for loading and storing Neo4j graphs.&lt;/li&gt;
&lt;li&gt;Node feature sampling now ~4Ã— faster via better data layout, speeding up configurations of GraphSAGE (and HinSAGE) &lt;/li&gt;
&lt;li&gt;Addition of PROTEINS dataset for graph classification demo&lt;a href=""https://github.com/stellargraph/stellargraph/pull/1282""&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Creating a RelationalFullBatchNodeGenerator now 18x faster and requires much less memory (560x smaller) &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jump into the new release on &lt;a href=""https://github.com/stellargraph/stellargraph""&gt;GitHub&lt;/a&gt;. StellarGraph is a Python 3 library. See full v1.0 release notes &lt;a href=""https://github.com/stellargraph/stellargraph/releases/tag/v1.0.0""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We always welcome feedback and contributions. &lt;/p&gt;

&lt;p&gt;With thanks and celebration, the StellarGraph team.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uucF7Lp1gxAmg8Hrxu3jyF2ETzNOXbA7wP1xoDVgfJ4.jpg?auto=webp&amp;s=fbf4396b0de020462b1d3130f1f63c0b2bb26786', 'width': 182, 'height': 182}, 'resolutions': [{'url': 'https://external-preview.redd.it/uucF7Lp1gxAmg8Hrxu3jyF2ETzNOXbA7wP1xoDVgfJ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6cfb0e9646bdb8ef9125c3dbfc85869f8530960', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'Ii0T5CFG684Kpt8uqkoEozOPeND6q_X_eAraLK5QJWo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge5wgf,True,,StellarGraphLibrary,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge5wgf/r_announcing_the_release_of_stellargraph_version/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge5wgf/r_announcing_the_release_of_stellargraph_version/,155203,1588711915.0,0,,False,,,,
,learnmachinelearning,,t2_3ps7w65l,False,,0,False,Confusion Matrix | How to evaluate your classification model,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gdom9y,False,dark,0.96,,public,96,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/jr_BcU4QlNE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Confusion Matrix | How to evaluate classification model | Machine Learning Basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/jr_BcU4QlNE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Intuitive Machine Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jr_BcU4QlNE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiuhuf2Xq0d05_4sHG0xmQA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/jr_BcU4QlNE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdom9y', 'height': 338}",,False,96,,True,https://a.thumbs.redditmedia.com/pZGrP_Q1ziwc1oc4K3K96eSQrriRhhSr4Ixex44EjE8.jpg,False,,[],{},rich:video,,False,,1588671184.0,text,6,,,text,youtu.be,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iZV03DTpHdg_rfSuwMuo6uR1KXxC-H_tA0RwegbsF40.jpg?auto=webp&amp;s=7fd17048c4acc984c361552ed492f66f01ae2649', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/iZV03DTpHdg_rfSuwMuo6uR1KXxC-H_tA0RwegbsF40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1131f7659c562d7ddcc5d667bfb4a0678d472dbf', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/iZV03DTpHdg_rfSuwMuo6uR1KXxC-H_tA0RwegbsF40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59c6251d87cba7cefd036a53fe8be72753b8517b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/iZV03DTpHdg_rfSuwMuo6uR1KXxC-H_tA0RwegbsF40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fe14bb541accd44dd297787457d5af89b9f755be', 'width': 320, 'height': 240}], 'variants': {}, 'id': '0-34T9tYzxrZwjYMrmxfx1gcNECVPTIXF5oHCi-oKIs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdom9y,True,,wstcpyt1988,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdom9y/confusion_matrix_how_to_evaluate_your/,all_ads,False,https://youtu.be/jr_BcU4QlNE,155203,1588642384.0,2,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Confusion Matrix | How to evaluate classification model | Machine Learning Basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/jr_BcU4QlNE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Intuitive Machine Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jr_BcU4QlNE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiuhuf2Xq0d05_4sHG0xmQA'}}",False,,,,
,learnmachinelearning,"I am new to the whole XGBoost stuff and also ML overall. As I understand, it uses extreme gradient boosting for random forests.

What exactly is the difference between XGBoost and RF in the sklearn package? 


And how many estimators (n_estimators in skl) does XGB use if it is not specified?

Then finally, how do you select all the eta/gamma/alpha hyperparameters for XGB? It seems infeasible to do a cross validation grid search. Is it just select things arbitrarily? 

I know for regularized logistic regression you can use LogisticRegressionCV() in sklearn or cv.glmnet in R to help you find that. But in this model there is only 1 hyperparameter (or 2 if you use elastic net)",t2_wmwkc,False,,0,False,How to select XGBoost hyperparameters? And other qs,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ge9uwt,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588754287.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to the whole XGBoost stuff and also ML overall. As I understand, it uses extreme gradient boosting for random forests.&lt;/p&gt;

&lt;p&gt;What exactly is the difference between XGBoost and RF in the sklearn package? &lt;/p&gt;

&lt;p&gt;And how many estimators (n_estimators in skl) does XGB use if it is not specified?&lt;/p&gt;

&lt;p&gt;Then finally, how do you select all the eta/gamma/alpha hyperparameters for XGB? It seems infeasible to do a cross validation grid search. Is it just select things arbitrarily? &lt;/p&gt;

&lt;p&gt;I know for regularized logistic regression you can use LogisticRegressionCV() in sklearn or cv.glmnet in R to help you find that. But in this model there is only 1 hyperparameter (or 2 if you use elastic net)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ge9uwt,True,,ice_shadow,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge9uwt/how_to_select_xgboost_hyperparameters_and_other_qs/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge9uwt/how_to_select_xgboost_hyperparameters_and_other_qs/,155203,1588725487.0,0,,False,,,,
,learnmachinelearning,,t2_3qfhbsn9,False,,0,False,Meet AI Gahaku: The AI Artist Created by a Programming Hobbyist,[],r/learnmachinelearning,False,6,,0,64.0,False,t3_gdtkk8,False,dark,0.89,,public,20,0,{},140.0,,False,[],,False,False,,{},,False,20,,False,https://b.thumbs.redditmedia.com/Pu2eIk2oCDKxo99tOzwo7nwlFuO6ph4ZjDQkMOGc8vo.jpg,False,,[],{},link,,False,,1588694387.0,text,6,,,text,lionbridge.ai,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?auto=webp&amp;s=0d71b49119c04bde09ff62a3bf94a7cfa700947d', 'width': 1368, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d13b196e21793883424b3e1d3d8271d8a8c48a8a', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=36d93faf02301ab303ebc1c3b2cae3c9eec51873', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b2310f06e39368290e7f4d73d576284c7ea9209', 'width': 320, 'height': 147}, {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7aaee1c4bc16b54c780d65c754e5c2dc64cc0be5', 'width': 640, 'height': 295}, {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b0e655dc855496b03a89ac132ac85bbcf4d7c574', 'width': 960, 'height': 443}, {'url': 'https://external-preview.redd.it/f7ZxN3jnf01hH_se_U4TI8EGC_rSznh0_r2xjuUa5SE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4ac749e9c0a916be5fa71424bfdf4ddde331517', 'width': 1080, 'height': 498}], 'variants': {}, 'id': 'YbXyZPtY5Uz6cAqyruif2cG5FS1Vh6kGR4NpHwpfTCE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdtkk8,True,,LimarcAmbalina,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdtkk8/meet_ai_gahaku_the_ai_artist_created_by_a/,all_ads,False,https://lionbridge.ai/articles/meet-the-maker-of-the-ai-artist-ai-gahaku/,155203,1588665587.0,0,,False,,,,
,learnmachinelearning,"I am a 45+ hobbyist who really like automation, I would like to learn machinelearning so I could make code that controls a simple game or such by learning. I found a course on google about tensor flow... I figure I will have to study some math to remember stuff from when i was young and some new (new for me) stuff. Is this list a correct list for learning machine learning in general or would it be wise to pick another course and need fewer math courses?

https://developers.google.com/machine-learning/crash-course/prereqs-and-prework

&gt; Algebra

&gt; variables, coefficients, and functions

&gt; linear equations such as 
&gt; logarithms, and logarithmic equations such as 
&gt; sigmoid function

&gt; Linear algebra

&gt; tensor and tensor rank

&gt; matrix multiplication

&gt; Trigonometry

&gt; tanh (discussed as an activation function; no prior knowledge needed)

&gt; Statistics

&gt; mean, median, outliers, and standard deviation

&gt; ability to read a histogram

&gt; Calculus (optional, for advanced topics)

&gt; concept of a derivative (you won't have to actually calculate derivatives)

&gt; gradient or slope

&gt; partial derivatives (which are closely related to gradients)

&gt; chain rule (for a full understanding of the backpropagation algorithm for training neural networks)",t2_ojrdc,False,,0,False,Are these math requirements right?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdz4wm,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},Question,False,5,,False,self,False,,[],{},self,,True,,1588719095.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a 45+ hobbyist who really like automation, I would like to learn machinelearning so I could make code that controls a simple game or such by learning. I found a course on google about tensor flow... I figure I will have to study some math to remember stuff from when i was young and some new (new for me) stuff. Is this list a correct list for learning machine learning in general or would it be wise to pick another course and need fewer math courses?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://developers.google.com/machine-learning/crash-course/prereqs-and-prework""&gt;https://developers.google.com/machine-learning/crash-course/prereqs-and-prework&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Algebra&lt;/p&gt;

&lt;p&gt;variables, coefficients, and functions&lt;/p&gt;

&lt;p&gt;linear equations such as 
logarithms, and logarithmic equations such as 
sigmoid function&lt;/p&gt;

&lt;p&gt;Linear algebra&lt;/p&gt;

&lt;p&gt;tensor and tensor rank&lt;/p&gt;

&lt;p&gt;matrix multiplication&lt;/p&gt;

&lt;p&gt;Trigonometry&lt;/p&gt;

&lt;p&gt;tanh (discussed as an activation function; no prior knowledge needed)&lt;/p&gt;

&lt;p&gt;Statistics&lt;/p&gt;

&lt;p&gt;mean, median, outliers, and standard deviation&lt;/p&gt;

&lt;p&gt;ability to read a histogram&lt;/p&gt;

&lt;p&gt;Calculus (optional, for advanced topics)&lt;/p&gt;

&lt;p&gt;concept of a derivative (you won&amp;#39;t have to actually calculate derivatives)&lt;/p&gt;

&lt;p&gt;gradient or slope&lt;/p&gt;

&lt;p&gt;partial derivatives (which are closely related to gradients)&lt;/p&gt;

&lt;p&gt;chain rule (for a full understanding of the backpropagation algorithm for training neural networks)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?auto=webp&amp;s=42269cbd8c73c5b3ded1a67e6f9f64f6f5c8d015', 'width': 3288, 'height': 1818}, 'resolutions': [{'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=556a7200cc224cd45bffc87a44e9cc7f015fdc01', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=252883b1b13ae16722b93228467cd6e07420b678', 'width': 216, 'height': 119}, {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b83a424fb51e9595c082ca0625a960e598c9e6', 'width': 320, 'height': 176}, {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f310c2b270c91e90502ed62b58bfe92922e8f8df', 'width': 640, 'height': 353}, {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d78624fcdfc6815dd195873fbbe2f554b287273d', 'width': 960, 'height': 530}, {'url': 'https://external-preview.redd.it/O4OEwoaylRV9LgVMHWTR1QUHByg6QD-7fBdj_4HkAzE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a7b8e42e5aaf2539071d22544c1cccb2e64a59c4', 'width': 1080, 'height': 597}], 'variants': {}, 'id': '6rteAEumL7mh6cOUXuBE9haV1icuLYX5s0wHnKFzzk4'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdz4wm,True,,hugthemachines,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdz4wm/are_these_math_requirements_right/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdz4wm/are_these_math_requirements_right/,155203,1588690295.0,0,,False,,,,
,learnmachinelearning,"I'm a sophomore currently taking Computer Science. To those people who are steps ahead of me, if you can become an undergrad again, what you would've done differently? What would you focus on? Any advice on getting into the AI field?

Whether you work in the industry, academe, or whatever, your thoughts are appreciated!",t2_4j4u18pu,False,,0,False,What you would've done differently if you're an undergrad student again?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ge39yh,False,light,0.76,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,self,False,,[],{},,,True,,1588732322.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a sophomore currently taking Computer Science. To those people who are steps ahead of me, if you can become an undergrad again, what you would&amp;#39;ve done differently? What would you focus on? Any advice on getting into the AI field?&lt;/p&gt;

&lt;p&gt;Whether you work in the industry, academe, or whatever, your thoughts are appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ge39yh,True,,melona277,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge39yh/what_you_wouldve_done_differently_if_youre_an/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge39yh/what_you_wouldve_done_differently_if_youre_an/,155203,1588703522.0,0,,False,,,,
,learnmachinelearning,"Hello, I'm 17 years old and have recently have become very interested in machine learning/AI. I have some prior experience with coding in Python, but I'm afraid that I am too young to start trying to learn. I am currently taking college level calculus and have a 2017 Macbook Pro. Will I need to upgrade to run these algorithms? Anyways, should I wait later in life to learn about AI or where can I start now? Thanks all!",t2_505xuxfz,False,,0,False,Too Young to Start?,[],r/learnmachinelearning,False,6,,0,,False,t3_ge7e0g,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588745559.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m 17 years old and have recently have become very interested in machine learning/AI. I have some prior experience with coding in Python, but I&amp;#39;m afraid that I am too young to start trying to learn. I am currently taking college level calculus and have a 2017 Macbook Pro. Will I need to upgrade to run these algorithms? Anyways, should I wait later in life to learn about AI or where can I start now? Thanks all!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge7e0g,True,,extrabubb,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge7e0g/too_young_to_start/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge7e0g/too_young_to_start/,155203,1588716759.0,0,,False,,,,
,learnmachinelearning,"As the title, says for someone who has experience with Deep Learning and Traditional ML what are some good resources for RL that are not too beginner-ish for someone at my level",t2_kfdra,False,,0,False,Good Courses/Resources to start Reinforced Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_ge7548,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588744740.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title, says for someone who has experience with Deep Learning and Traditional ML what are some good resources for RL that are not too beginner-ish for someone at my level&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge7548,True,,Unchart3disOP,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge7548/good_coursesresources_to_start_reinforced_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge7548/good_coursesresources_to_start_reinforced_learning/,155203,1588715940.0,0,,False,,,,
,learnmachinelearning,,t2_1q9vqmqd,False,,0,False,Ensemble classification based on generalized additive models,[],r/learnmachinelearning,False,6,,0,,False,t3_ge6x2y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588744005.0,text,6,,,text,sciencedirect.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge6x2y,True,,_quanttrader_,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge6x2y/ensemble_classification_based_on_generalized/,all_ads,False,https://www.sciencedirect.com/science/article/abs/pii/S0167947309004654,155203,1588715205.0,0,,False,,,,
,learnmachinelearning,"Hello guys, 

I am diving into NLP and deep learning for sequences to sequences and I want to know :

***if you could only buy one book about NLP and deep learning what would it be?***

When I put the question in google search I came across [this](https://www.manning.com/books/natural-language-processing-in-action), what are your thought on it?

Thanks in advance for your response!!

Regards",t2_4m2zb1fg,False,,0,False,One book recommendation on Natural Language Processing and Deep Learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_gdt917,False,dark,0.93,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},,,True,,1588692672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys, &lt;/p&gt;

&lt;p&gt;I am diving into NLP and deep learning for sequences to sequences and I want to know :&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;if you could only buy one book about NLP and deep learning what would it be?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When I put the question in google search I came across &lt;a href=""https://www.manning.com/books/natural-language-processing-in-action""&gt;this&lt;/a&gt;, what are your thought on it?&lt;/p&gt;

&lt;p&gt;Thanks in advance for your response!!&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdt917,True,,esp_py,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdt917/one_book_recommendation_on_natural_language/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdt917/one_book_recommendation_on_natural_language/,155203,1588663872.0,0,,False,,,,
,learnmachinelearning,"Can we measure distances between objects in a video feed?

Can someone point me to some good literature in this area? I would really appreciate it, thank you.",t2_10fpln,False,,0,False,A model that predicts distance between objects in a video,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdwncj,False,dark,1.0,,public,6,0,{},,,False,[],,False,False,,{},Question,False,6,,False,self,False,,[],{},,,True,,1588709699.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can we measure distances between objects in a video feed?&lt;/p&gt;

&lt;p&gt;Can someone point me to some good literature in this area? I would really appreciate it, thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdwncj,True,,thehellnokitty,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdwncj/a_model_that_predicts_distance_between_objects_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdwncj/a_model_that_predicts_distance_between_objects_in/,155203,1588680899.0,0,,False,,,,
,learnmachinelearning,"I have understood how the Neural Network works amd tains. But I have no idea how many nodes to put in ach layer, what actication functions to choose etc. I have completed a problem in classification with success but I am not able to achieve anything in a regression problem. Please will you show me a guide about these things. I mean how do professionals do such tasks?",t2_5fi9eeym,False,,0,False,Need a guide in constructing Neural Networks,[],r/learnmachinelearning,False,6,,0,,False,t3_ge5y0p,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588740856.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have understood how the Neural Network works amd tains. But I have no idea how many nodes to put in ach layer, what actication functions to choose etc. I have completed a problem in classification with success but I am not able to achieve anything in a regression problem. Please will you show me a guide about these things. I mean how do professionals do such tasks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge5y0p,True,,dhokna,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge5y0p/need_a_guide_in_constructing_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge5y0p/need_a_guide_in_constructing_neural_networks/,155203,1588712056.0,0,,False,,,,
,learnmachinelearning,"Hey fellow Machine Learning engineers!

I am a newbie in the field of machine learning. Currently, I am studying this course on edX:  [https://courses.edx.org/courses/course-v1:IBM+ML0101EN+1T2020/course/](https://courses.edx.org/courses/course-v1:IBM+ML0101EN+1T2020/course/) . Whatever machine learning course I chose to study has complex syntax which most of the courses don't explain. This course says ""Practical Introduction to Machine Learning in Python"", however, it doesn't give any introduction on what the code does. I have been jumping from courses to courses just to find the right course that could teach me ML from scratch.

I feel like giving up now. I am really passionate about machine learning but don't have a clear plan. I just finished the stats course from Khan Academy and currently studying linear algebra. Therefore, I have basic knowledge for stats and linear algebra.However, I also need something to get me started with ML in Python.

Sorry if I am all over the place, but I need help. Are there any fellow ML Engineers who can guide with what courses to take as a beginner who has basic knowledge of stats and linear algebra? Also, are there any ML Engineers whom I can contact if I need any help?

And please don't recommend me courses that only give the code without an explanation. 

Thanks a ton!",t2_2ufovsg1,False,,0,False,ML newbie needs HELP!,[],r/learnmachinelearning,False,6,,0,,False,t3_ge5rsh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588740300.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey fellow Machine Learning engineers!&lt;/p&gt;

&lt;p&gt;I am a newbie in the field of machine learning. Currently, I am studying this course on edX:  &lt;a href=""https://courses.edx.org/courses/course-v1:IBM+ML0101EN+1T2020/course/""&gt;https://courses.edx.org/courses/course-v1:IBM+ML0101EN+1T2020/course/&lt;/a&gt; . Whatever machine learning course I chose to study has complex syntax which most of the courses don&amp;#39;t explain. This course says &amp;quot;Practical Introduction to Machine Learning in Python&amp;quot;, however, it doesn&amp;#39;t give any introduction on what the code does. I have been jumping from courses to courses just to find the right course that could teach me ML from scratch.&lt;/p&gt;

&lt;p&gt;I feel like giving up now. I am really passionate about machine learning but don&amp;#39;t have a clear plan. I just finished the stats course from Khan Academy and currently studying linear algebra. Therefore, I have basic knowledge for stats and linear algebra.However, I also need something to get me started with ML in Python.&lt;/p&gt;

&lt;p&gt;Sorry if I am all over the place, but I need help. Are there any fellow ML Engineers who can guide with what courses to take as a beginner who has basic knowledge of stats and linear algebra? Also, are there any ML Engineers whom I can contact if I need any help?&lt;/p&gt;

&lt;p&gt;And please don&amp;#39;t recommend me courses that only give the code without an explanation. &lt;/p&gt;

&lt;p&gt;Thanks a ton!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xoJWyKGnyx17VVL8nMEs4U8LgzcNeadQu3LXaA6hj1E.jpg?auto=webp&amp;s=1a6d174c9473d84ff2a86dfad39726cd9c8393f5', 'width': 75, 'height': 75}, 'resolutions': [], 'variants': {}, 'id': 'gRkyVflXgFrjly9ILgYrLmMD4HBjUQh9qNFISu2ohJA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge5rsh,True,,codefreak-123,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge5rsh/ml_newbie_needs_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge5rsh/ml_newbie_needs_help/,155203,1588711500.0,0,,False,,,,
,learnmachinelearning,"1. Introductory blog about the series. \[[https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4](https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4)\]([https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4](https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4))

2. Overfeat paper: \[[https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f](https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f)\]([https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f](https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f))

3. RCNN paper:  \[[https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a](https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a)\]([https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a](https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a))

4. Sppnet:

â€œReview: Spatial Pyramid Pooling\[1406.4729\]â€ by Sanchit Tanwar [https://link.medium.com/zLcyA4RSN5](https://link.medium.com/zLcyA4RSN5)

5. Fast Rcnn:

[https://medium.com/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70](https://medium.com/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70)

6. Faster RCNN:

[https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3](https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3)",t2_1h6inaap,False,,0,False,I am writing a series of blogs about object detection where I will be reviewing object detection papers. I have written first blog of the series. Here is the link.,[],r/learnmachinelearning,False,6,,0,,False,t3_ge17uo,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588725770.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Introductory blog about the series. [&lt;a href=""https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4""&gt;https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4&lt;/a&gt;](&lt;a href=""https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4""&gt;https://medium.com/@sanchittanwar75/getting-started-with-object-detection-179677feb3f4&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Overfeat paper: [&lt;a href=""https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f""&gt;https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f&lt;/a&gt;](&lt;a href=""https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f""&gt;https://medium.com/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RCNN paper:  [&lt;a href=""https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a""&gt;https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a&lt;/a&gt;](&lt;a href=""https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a""&gt;https://medium.com/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sppnet:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;â€œReview: Spatial Pyramid Pooling[1406.4729]â€ by Sanchit Tanwar &lt;a href=""https://link.medium.com/zLcyA4RSN5""&gt;https://link.medium.com/zLcyA4RSN5&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fast Rcnn:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70""&gt;https://medium.com/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Faster RCNN:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3""&gt;https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/w335jzfIEb20ZC5n-N1FBUMC3YJCrEUVnSEPg62x1jI.jpg?auto=webp&amp;s=2f13a18c13757fd2ebd6a6d496f9ca439849483f', 'width': 640, 'height': 270}, 'resolutions': [{'url': 'https://external-preview.redd.it/w335jzfIEb20ZC5n-N1FBUMC3YJCrEUVnSEPg62x1jI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e04e9cf5353872357d5a831c9f41026fade324e', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/w335jzfIEb20ZC5n-N1FBUMC3YJCrEUVnSEPg62x1jI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=861e0cfd23fd0059c7a32881c0542b73ddff8dad', 'width': 216, 'height': 91}, {'url': 'https://external-preview.redd.it/w335jzfIEb20ZC5n-N1FBUMC3YJCrEUVnSEPg62x1jI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad9dc34e9702682a9752b0bac537ca8a261fe99a', 'width': 320, 'height': 135}, {'url': 'https://external-preview.redd.it/w335jzfIEb20ZC5n-N1FBUMC3YJCrEUVnSEPg62x1jI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf15fba11e433c10ea5bfedc067a15985c0a1ca6', 'width': 640, 'height': 270}], 'variants': {}, 'id': '6pcHmoGM69V-H9M-X5fGDvqG7NrtpkBCaoRokwkEiy4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge17uo,True,,sanchit2843,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge17uo/i_am_writing_a_series_of_blogs_about_object/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge17uo/i_am_writing_a_series_of_blogs_about_object/,155203,1588696970.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,An end-to-end automated method for producing animation rigs from input character models,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ge5877,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/kqHqN6o8amo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'An end-to-end automated method for producing animation rigs from input character models', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/kqHqN6o8amo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/kqHqN6o8amo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/kqHqN6o8amo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ge5877', 'height': 338}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/UdgXwI_u5Fa3PxgbgeaIOGf-3DMufs3IYvhH6yQhwzY.jpg,False,,[],{},rich:video,,False,,1588738747.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ScL_oPH6EN0P27rBHO8-kAMYnOC0jz-LZg0WFzfm77M.jpg?auto=webp&amp;s=5b2a9938d023636119c911572fa39d28d41de630', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ScL_oPH6EN0P27rBHO8-kAMYnOC0jz-LZg0WFzfm77M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ed71904fb929a3695ee8410e78efd134510dc13', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ScL_oPH6EN0P27rBHO8-kAMYnOC0jz-LZg0WFzfm77M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6d592fd241cfaa3a5cc21f8cf3789b2270fa7fb', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ScL_oPH6EN0P27rBHO8-kAMYnOC0jz-LZg0WFzfm77M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbab9c8b87d59be65a916c4b0b5c4f3dc40b38b0', 'width': 320, 'height': 240}], 'variants': {}, 'id': '9miSte112to3bvA7ZemV9sIbOVTlZBcuD0BwLjAbRtk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ge5877,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge5877/an_endtoend_automated_method_for_producing/,all_ads,False,https://youtu.be/kqHqN6o8amo,155203,1588709947.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'An end-to-end automated method for producing animation rigs from input character models', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/kqHqN6o8amo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/kqHqN6o8amo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_5a1j5sce,False,,0,False,Top 10 Machine Learning Books That You Should Read Before You Start With It,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gdy54d,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/Sn0H3_3tgduaoRidrCya8hOayt-_8bYkgmL6HVUy-gg.jpg,False,,[],{},link,,False,,1588715696.0,text,6,,,text,laconicml.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?auto=webp&amp;s=9bd55be6e90885b505c54ec97ad7214881d488c3', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a13c0b0bcd3622da0bd0128b63f5c5531e1cacb', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3907d09ca801950938747c9a94b736016e40888a', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=63f69378b1857b4564edc45515538a1caaaa61db', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f9b8b5e3fc2791df56b38d878cac05daa9342dc', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8f9267d6a466862b559e9d00c10aee095845a6f7', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/sIiBHPV7ew9RXa8qGamADny3RPwBZyoSdhC3WNukaNw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9890ad787eec8403dab20ece0091d1481347105c', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'JxrnTm19ePKjiDkeH4LY6IxeWfasnjRGgKcKt_QlezQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdy54d,True,,yung_quan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdy54d/top_10_machine_learning_books_that_you_should/,all_ads,False,https://laconicml.com/top-10-machine-learning-books/,155203,1588686896.0,0,,False,,,,
,learnmachinelearning,"## TL;DR

Hey readers!

I've open-sourced new [**ðŸ¤– Interactive Machine Learning Experiments**](https://github.com/trekhleb/machine-learning-experiments) project on GitHub. Each experiment consists of ðŸ‹ï¸ *Jupyter/Colab notebook* (to see how a model was trained) and ðŸŽ¨ *demo page* (to see a model in action right in your browser).

Although the models may be a little dumb (remember, these are just experiments, not a production ready code), they will try to do their best to:

* ðŸ–Œ Recognize digits or sketches you draw in your browser
* ðŸ“¸ Detect and recognize the objects you'll show to your camera
* ðŸŒ… Classify your uploaded image
* ðŸ“ Write a Shakespeare poem with you
* âœŠðŸ–âœŒï¸ Play with you in Rock-Paper-Scissors game
* etc.

I've trained the models on *Python* using *TensorFlow 2* with *Keras* support  and then consumed them for a demo in a browser using *React* and *JavaScript* version of *Tensorflow*.

https://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;format=png&amp;auto=webp&amp;s=946b73c9dc85f432abb025e285286f9209fb62ae

## Models performance

âš ï¸ First, let's set our expectations.ï¸ The repository contains machine learning **experiments** and **not** a production ready, reusable, optimised and fine-tuned code and models. This is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. Models might not perform well and there is a place for overfitting/underfitting.

Therefore, sometimes you might see things like this:

&amp;#x200B;

https://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;format=png&amp;auto=webp&amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317

But be patient, sometimes the model might get smarter ðŸ¤“ and give you this:

&amp;#x200B;

https://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;format=png&amp;auto=webp&amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a

## Background

I'm a [software engineer](https://www.linkedin.com/in/trekhleb/) and for the last several years now I've been doing mostly frontend and backend programming. In my spare time, as a hobby, I decided to dig into machine learning topics to make it less *like magic* and *more like math* to myself.

1. ðŸ—“ Since **Python** might be a good choice to start experimenting with Machine Learning I decided to learn its basic syntax first. As a result a [ðŸ Playground and Cheatsheet for Learning Python](https://github.com/trekhleb/learn-python) project came out. This was just to practice Python and at the same time to have a cheatsheet of basic syntax once I need it (for things like `dict_via_comprehension = {x: x**2 for x in (2, 4, 6)}` etc.).
2. ðŸ—“ After learning a bit of Python I wanted to dig into the basic **math** behind Machine Learning. So after passing an awesome [Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning) on Coursera the [ðŸ¤– Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) project came out. This time it was about creating a cheatsheet for basic machine learning math algorithms like linear regression, logistic regression, k-means, multilayer perceptron etc.
3. ðŸ—“ The next attempt to play around with basic Machine Learning math was [ðŸ¤– NanoNeuron](https://github.com/trekhleb/nano-neuron). It was about 7 simple JavaScript functions that supposed to give you a feeling of how machines can actually ""learn"".
4. ðŸ—“ After finishing yet another awesome [Deep Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/deep-learning) on Coursera I decided to practice a bit more with **multilayer perceptrons**, **convolutional** and **recurrent neural networks** (CNNs and RNNs). This time instead of implementing everything from scratch I decided to start using some machine learning framework. I ended up using [TensorFlow 2](https://www.tensorflow.org/) with [Keras](https://www.tensorflow.org/guide/keras/overview). I also didn't want to focus too much on math (letting the framework do it for me) and instead I wanted to come up with something more practical, applicable and something I could try to play with right in my browser. As a result new [ðŸ¤– Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) came out that I want to describe a bit more here.

## Tech-stack

## Models training

* ðŸ‹ðŸ»â€ I used [Keras](https://www.tensorflow.org/guide/keras/overview) inside [TensorFlow 2](https://www.tensorflow.org/) for modelling and training. Since I had zero experience with machine learning frameworks, I needed to start with something. One of the selling points in favor of TensorFlow was that it has both Python and [JavaScript flavor](https://www.tensorflow.org/js) of the library with similar API. So eventually I used Python version for training and JavaScript version for demos.
* ðŸ‹ðŸ»â€ I trained TensorFlow models on Python inside [Jupyter](https://jupyter.org/) notebooks locally and sometimes used [Colab](https://colab.research.google.com/) to make the training faster on GPU.
* ðŸ’» Most of the models were trained on good old MacBook's Pro CPU (2,9 GHz Dual-Core Intel Core i5).
* ðŸ”¢ Of course there is no way you could run away from [NumPy](https://numpy.org/) for matrix/tensors operations.

## Models demo

* ðŸ‹ðŸ»â€ I used [TensorFlow.js](https://www.tensorflow.org/js) to do predictions with previously trained models.
* â™»ï¸ To convert *Keras HDF5* models to *TensorFlow.js Layers* format I used [TensorFlow.js converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter). This might be inefficient to transfer the whole model (megabytes of data) to the browser instead of making predictions through HTTP requests, but again, remember that these are just experiments and not production-ready code and architecture. I wanted to avoid having a dedicated back-end service to make architecture simpler.
* ðŸ‘¨ðŸ»â€ðŸŽ¨ The [Demo application](http://trekhleb.github.io/machine-learning-experiments) was created on [React](https://reactjs.org/) using [create-react-app](https://github.com/facebook/create-react-app) starter with a default [Flow](https://flow.org/en/) flavour for type checking.
* ðŸ’…ðŸ» For styling, I used [Material UI](https://material-ui.com/). It was, as they say, ""to kill two birds"" at once and try out a new styling framework (sorry, [Bootstrap](https://getbootstrap.com/) ðŸ¤·ðŸ»â€).

## Experiments

So, in short, you may access Demo page and Jupyter notebooks by these links:

* ðŸŽ¨ [**Launch ML experiments demo**](http://trekhleb.github.io/machine-learning-experiments)
* ðŸ‹ï¸ [**Check ML experiments Jupyter notebooks**](https://github.com/trekhleb/machine-learning-experiments)

## Experiments with Multilayer Perceptron (MLP)

&gt;A [multilayer perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a class of feedforward artificial neural network (ANN). Multilayer perceptrons are sometimes referred to as ""vanilla"" neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer.

## Handwritten Digits Recognition

You draw a digit, and the model tries to recognize it.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionMLP)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)

&amp;#x200B;

https://i.redd.it/8rc55ws7uyw41.gif

## Handwritten Sketch Recognition

You draw a sketch, and the model tries to recognize it.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionMLP)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)

&amp;#x200B;

https://i.redd.it/uhavc6d9uyw41.gif

## Experiments with Convolutional Neural Networks (CNN)

&gt;A [convolutional neural network (CNN, or ConvNet)](https://en.wikipedia.org/wiki/Convolutional_neural_network) is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). They are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.

## Handwritten Digits Recognition (CNN)

You draw a digit, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionCNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)

&amp;#x200B;

https://i.redd.it/jakuk4lauyw41.gif

## Handwritten Sketch Recognition (CNN)

You draw a sketch, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionCNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)

&amp;#x200B;

https://i.redd.it/dzc4aghbuyw41.gif

## Rock Paper Scissors (CNN)

You play a Rock-Paper-Scissors game with the model. This experiment uses CNN that is trained from scratch.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsCNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)

&amp;#x200B;

https://i.redd.it/ewnsaudcuyw41.gif

## Rock Paper Scissors (MobilenetV2)

You play a Rock-Paper-Scissors game with the model. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsMobilenetV2)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)

&amp;#x200B;

https://i.redd.it/70ruj86euyw41.gif

## Objects Detection (MobileNetV2)

You show to the model your environment through your camera, and it will try to detect and recognize the objects. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ObjectsDetectionSSDLiteMobilenetV2)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)

&amp;#x200B;

https://i.redd.it/16xi3nthuyw41.gif

## Image Classification (MobileNetV2)

You upload a picture, and the model tries to classify it depending on what it ""sees"" on the picture. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ImageClassificationMobilenetV2)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)

&amp;#x200B;

https://i.redd.it/pv8zkkuiuyw41.gif

## Experiments with Recurrent Neural Networks (RNN)

&gt;A [recurrent neural network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network) is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. They are used for machine translation, speech recognition, voice synthesis etc.

## Numbers Summation

You type a summation expression (i.e. `17+38`), and the model predicts the result (i.e. `55`). The interesting part here is that the model treats the input as a *sequence*, meaning it learned that when you type a sequence `1` â†’ `17` â†’ `17+` â†’ `17+3` â†’ `17+38` it ""translates"" it to another sequence `55`. You may think about it as translating a Spanish `Hola` sequence to English `Hello`.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/NumbersSummationRNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)

&amp;#x200B;

https://i.redd.it/rix288wjuyw41.gif

## Shakespeare Text Generation

You start typing a poem like Shakespeare, and the model will continue it like Shakespeare. At least it will try to do so ðŸ˜€.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationShakespeareRNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)

&amp;#x200B;

https://i.redd.it/nbkm3askuyw41.gif

## Wikipedia Text Generation

You start typing a Wiki article, and the model tries to continue it.

* ðŸŽ¨ [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationWikipediaRNN)
* ðŸ‹ï¸ [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)
* ï¸ðŸ‹ï¸  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)

&amp;#x200B;

https://i.redd.it/a0l2g1pluyw41.gif

## Future plans

As I've mentioned above the main purpose of [the repository](https://github.com/trekhleb/machine-learning-experiments) is to be more like a playground for learning rather than for production-ready models. Therefore, the main plan is to **continue learning and experimenting** with deep-learning challenges and approaches. The next interesting challenges to play with might be:

* Emotions detection
* Style transfer
* Language translation
* Generating images (i.e. handwritten numbers)
* etc.

Another interesting opportunity would be to **tune existing models to make them more performant**. I believe it might give a better understanding of how to overcome overfitting and underfitting and what to do with the model if it just stuck on `60%` accuracy level for both training and validation sets and doesn't want to improve anymore ðŸ¤”.

Anyways, I hope you might find some useful insights for models training from [the repository](https://github.com/trekhleb/machine-learning-experiments) or at least to have some fun playing around with the demos!

Happy learning! ðŸ¤–",t2_2vlttls,False,,0,False,ðŸ¤– Interactive Machine Learning Experiments,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,70.0,False,t3_gdzv8h,False,light,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/HYPVaRjcD4YPdtkGgdN8wxOVd1OAUHS2XutH2fOMXts.jpg,1588693058.0,,[],{},self,,True,,1588721464.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h2&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;Hey readers!&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve open-sourced new &lt;a href=""https://github.com/trekhleb/machine-learning-experiments""&gt;&lt;strong&gt;ðŸ¤– Interactive Machine Learning Experiments&lt;/strong&gt;&lt;/a&gt; project on GitHub. Each experiment consists of ðŸ‹ï¸ &lt;em&gt;Jupyter/Colab notebook&lt;/em&gt; (to see how a model was trained) and ðŸŽ¨ &lt;em&gt;demo page&lt;/em&gt; (to see a model in action right in your browser).&lt;/p&gt;

&lt;p&gt;Although the models may be a little dumb (remember, these are just experiments, not a production ready code), they will try to do their best to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸ–Œ Recognize digits or sketches you draw in your browser&lt;/li&gt;
&lt;li&gt;ðŸ“¸ Detect and recognize the objects you&amp;#39;ll show to your camera&lt;/li&gt;
&lt;li&gt;ðŸŒ… Classify your uploaded image&lt;/li&gt;
&lt;li&gt;ðŸ“ Write a Shakespeare poem with you&lt;/li&gt;
&lt;li&gt;âœŠðŸ–âœŒï¸ Play with you in Rock-Paper-Scissors game&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#39;ve trained the models on &lt;em&gt;Python&lt;/em&gt; using &lt;em&gt;TensorFlow 2&lt;/em&gt; with &lt;em&gt;Keras&lt;/em&gt; support  and then consumed them for a demo in a browser using &lt;em&gt;React&lt;/em&gt; and &lt;em&gt;JavaScript&lt;/em&gt; version of &lt;em&gt;Tensorflow&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=946b73c9dc85f432abb025e285286f9209fb62ae""&gt;https://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=946b73c9dc85f432abb025e285286f9209fb62ae&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Models performance&lt;/h2&gt;

&lt;p&gt;âš ï¸ First, let&amp;#39;s set our expectations.ï¸ The repository contains machine learning &lt;strong&gt;experiments&lt;/strong&gt; and &lt;strong&gt;not&lt;/strong&gt; a production ready, reusable, optimised and fine-tuned code and models. This is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. Models might not perform well and there is a place for overfitting/underfitting.&lt;/p&gt;

&lt;p&gt;Therefore, sometimes you might see things like this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317""&gt;https://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But be patient, sometimes the model might get smarter ðŸ¤“ and give you this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a""&gt;https://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt;I&amp;#39;m a &lt;a href=""https://www.linkedin.com/in/trekhleb/""&gt;software engineer&lt;/a&gt; and for the last several years now I&amp;#39;ve been doing mostly frontend and backend programming. In my spare time, as a hobby, I decided to dig into machine learning topics to make it less &lt;em&gt;like magic&lt;/em&gt; and &lt;em&gt;more like math&lt;/em&gt; to myself.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ðŸ—“ Since &lt;strong&gt;Python&lt;/strong&gt; might be a good choice to start experimenting with Machine Learning I decided to learn its basic syntax first. As a result a &lt;a href=""https://github.com/trekhleb/learn-python""&gt;ðŸ Playground and Cheatsheet for Learning Python&lt;/a&gt; project came out. This was just to practice Python and at the same time to have a cheatsheet of basic syntax once I need it (for things like &lt;code&gt;dict_via_comprehension = {x: x**2 for x in (2, 4, 6)}&lt;/code&gt; etc.).&lt;/li&gt;
&lt;li&gt;ðŸ—“ After learning a bit of Python I wanted to dig into the basic &lt;strong&gt;math&lt;/strong&gt; behind Machine Learning. So after passing an awesome &lt;a href=""https://www.coursera.org/learn/machine-learning""&gt;Machine Learning course by Andrew Ng&lt;/a&gt; on Coursera the &lt;a href=""https://github.com/trekhleb/homemade-machine-learning""&gt;ðŸ¤– Homemade Machine Learning&lt;/a&gt; project came out. This time it was about creating a cheatsheet for basic machine learning math algorithms like linear regression, logistic regression, k-means, multilayer perceptron etc.&lt;/li&gt;
&lt;li&gt;ðŸ—“ The next attempt to play around with basic Machine Learning math was &lt;a href=""https://github.com/trekhleb/nano-neuron""&gt;ðŸ¤– NanoNeuron&lt;/a&gt;. It was about 7 simple JavaScript functions that supposed to give you a feeling of how machines can actually &amp;quot;learn&amp;quot;.&lt;/li&gt;
&lt;li&gt;ðŸ—“ After finishing yet another awesome &lt;a href=""https://www.coursera.org/specializations/deep-learning""&gt;Deep Learning Specialization by Andrew Ng&lt;/a&gt; on Coursera I decided to practice a bit more with &lt;strong&gt;multilayer perceptrons&lt;/strong&gt;, &lt;strong&gt;convolutional&lt;/strong&gt; and &lt;strong&gt;recurrent neural networks&lt;/strong&gt; (CNNs and RNNs). This time instead of implementing everything from scratch I decided to start using some machine learning framework. I ended up using &lt;a href=""https://www.tensorflow.org/""&gt;TensorFlow 2&lt;/a&gt; with &lt;a href=""https://www.tensorflow.org/guide/keras/overview""&gt;Keras&lt;/a&gt;. I also didn&amp;#39;t want to focus too much on math (letting the framework do it for me) and instead I wanted to come up with something more practical, applicable and something I could try to play with right in my browser. As a result new &lt;a href=""https://github.com/trekhleb/machine-learning-experiments""&gt;ðŸ¤– Interactive Machine Learning Experiments&lt;/a&gt; came out that I want to describe a bit more here.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Tech-stack&lt;/h2&gt;

&lt;h2&gt;Models training&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;ðŸ‹ðŸ»â€ I used &lt;a href=""https://www.tensorflow.org/guide/keras/overview""&gt;Keras&lt;/a&gt; inside &lt;a href=""https://www.tensorflow.org/""&gt;TensorFlow 2&lt;/a&gt; for modelling and training. Since I had zero experience with machine learning frameworks, I needed to start with something. One of the selling points in favor of TensorFlow was that it has both Python and &lt;a href=""https://www.tensorflow.org/js""&gt;JavaScript flavor&lt;/a&gt; of the library with similar API. So eventually I used Python version for training and JavaScript version for demos.&lt;/li&gt;
&lt;li&gt;ðŸ‹ðŸ»â€ I trained TensorFlow models on Python inside &lt;a href=""https://jupyter.org/""&gt;Jupyter&lt;/a&gt; notebooks locally and sometimes used &lt;a href=""https://colab.research.google.com/""&gt;Colab&lt;/a&gt; to make the training faster on GPU.&lt;/li&gt;
&lt;li&gt;ðŸ’» Most of the models were trained on good old MacBook&amp;#39;s Pro CPU (2,9 GHz Dual-Core Intel Core i5).&lt;/li&gt;
&lt;li&gt;ðŸ”¢ Of course there is no way you could run away from &lt;a href=""https://numpy.org/""&gt;NumPy&lt;/a&gt; for matrix/tensors operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Models demo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;ðŸ‹ðŸ»â€ I used &lt;a href=""https://www.tensorflow.org/js""&gt;TensorFlow.js&lt;/a&gt; to do predictions with previously trained models.&lt;/li&gt;
&lt;li&gt;â™»ï¸ To convert &lt;em&gt;Keras HDF5&lt;/em&gt; models to &lt;em&gt;TensorFlow.js Layers&lt;/em&gt; format I used &lt;a href=""https://github.com/tensorflow/tfjs/tree/master/tfjs-converter""&gt;TensorFlow.js converter&lt;/a&gt;. This might be inefficient to transfer the whole model (megabytes of data) to the browser instead of making predictions through HTTP requests, but again, remember that these are just experiments and not production-ready code and architecture. I wanted to avoid having a dedicated back-end service to make architecture simpler.&lt;/li&gt;
&lt;li&gt;ðŸ‘¨ðŸ»â€ðŸŽ¨ The &lt;a href=""http://trekhleb.github.io/machine-learning-experiments""&gt;Demo application&lt;/a&gt; was created on &lt;a href=""https://reactjs.org/""&gt;React&lt;/a&gt; using &lt;a href=""https://github.com/facebook/create-react-app""&gt;create-react-app&lt;/a&gt; starter with a default &lt;a href=""https://flow.org/en/""&gt;Flow&lt;/a&gt; flavour for type checking.&lt;/li&gt;
&lt;li&gt;ðŸ’…ðŸ» For styling, I used &lt;a href=""https://material-ui.com/""&gt;Material UI&lt;/a&gt;. It was, as they say, &amp;quot;to kill two birds&amp;quot; at once and try out a new styling framework (sorry, &lt;a href=""https://getbootstrap.com/""&gt;Bootstrap&lt;/a&gt; ðŸ¤·ðŸ»â€).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Experiments&lt;/h2&gt;

&lt;p&gt;So, in short, you may access Demo page and Jupyter notebooks by these links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""http://trekhleb.github.io/machine-learning-experiments""&gt;&lt;strong&gt;Launch ML experiments demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://github.com/trekhleb/machine-learning-experiments""&gt;&lt;strong&gt;Check ML experiments Jupyter notebooks&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Experiments with Multilayer Perceptron (MLP)&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A &lt;a href=""https://en.wikipedia.org/wiki/Multilayer_perceptron""&gt;multilayer perceptron (MLP)&lt;/a&gt; is a class of feedforward artificial neural network (ANN). Multilayer perceptrons are sometimes referred to as &amp;quot;vanilla&amp;quot; neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Handwritten Digits Recognition&lt;/h2&gt;

&lt;p&gt;You draw a digit, and the model tries to recognize it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionMLP""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/8rc55ws7uyw41.gif""&gt;https://i.redd.it/8rc55ws7uyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Handwritten Sketch Recognition&lt;/h2&gt;

&lt;p&gt;You draw a sketch, and the model tries to recognize it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionMLP""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/uhavc6d9uyw41.gif""&gt;https://i.redd.it/uhavc6d9uyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Experiments with Convolutional Neural Networks (CNN)&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A &lt;a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network""&gt;convolutional neural network (CNN, or ConvNet)&lt;/a&gt; is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). They are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Handwritten Digits Recognition (CNN)&lt;/h2&gt;

&lt;p&gt;You draw a digit, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionCNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/jakuk4lauyw41.gif""&gt;https://i.redd.it/jakuk4lauyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Handwritten Sketch Recognition (CNN)&lt;/h2&gt;

&lt;p&gt;You draw a sketch, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionCNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/dzc4aghbuyw41.gif""&gt;https://i.redd.it/dzc4aghbuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Rock Paper Scissors (CNN)&lt;/h2&gt;

&lt;p&gt;You play a Rock-Paper-Scissors game with the model. This experiment uses CNN that is trained from scratch.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsCNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/ewnsaudcuyw41.gif""&gt;https://i.redd.it/ewnsaudcuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Rock Paper Scissors (MobilenetV2)&lt;/h2&gt;

&lt;p&gt;You play a Rock-Paper-Scissors game with the model. This model uses transfer learning and is based on &lt;a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2""&gt;MobilenetV2&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsMobilenetV2""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/70ruj86euyw41.gif""&gt;https://i.redd.it/70ruj86euyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Objects Detection (MobileNetV2)&lt;/h2&gt;

&lt;p&gt;You show to the model your environment through your camera, and it will try to detect and recognize the objects. This model uses transfer learning and is based on &lt;a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2""&gt;MobilenetV2&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/ObjectsDetectionSSDLiteMobilenetV2""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/16xi3nthuyw41.gif""&gt;https://i.redd.it/16xi3nthuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Image Classification (MobileNetV2)&lt;/h2&gt;

&lt;p&gt;You upload a picture, and the model tries to classify it depending on what it &amp;quot;sees&amp;quot; on the picture. This model uses transfer learning and is based on &lt;a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2""&gt;MobilenetV2&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/ImageClassificationMobilenetV2""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/pv8zkkuiuyw41.gif""&gt;https://i.redd.it/pv8zkkuiuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Experiments with Recurrent Neural Networks (RNN)&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A &lt;a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network""&gt;recurrent neural network (RNN)&lt;/a&gt; is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. They are used for machine translation, speech recognition, voice synthesis etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Numbers Summation&lt;/h2&gt;

&lt;p&gt;You type a summation expression (i.e. &lt;code&gt;17+38&lt;/code&gt;), and the model predicts the result (i.e. &lt;code&gt;55&lt;/code&gt;). The interesting part here is that the model treats the input as a &lt;em&gt;sequence&lt;/em&gt;, meaning it learned that when you type a sequence &lt;code&gt;1&lt;/code&gt; â†’ &lt;code&gt;17&lt;/code&gt; â†’ &lt;code&gt;17+&lt;/code&gt; â†’ &lt;code&gt;17+3&lt;/code&gt; â†’ &lt;code&gt;17+38&lt;/code&gt; it &amp;quot;translates&amp;quot; it to another sequence &lt;code&gt;55&lt;/code&gt;. You may think about it as translating a Spanish &lt;code&gt;Hola&lt;/code&gt; sequence to English &lt;code&gt;Hello&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/NumbersSummationRNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/rix288wjuyw41.gif""&gt;https://i.redd.it/rix288wjuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Shakespeare Text Generation&lt;/h2&gt;

&lt;p&gt;You start typing a poem like Shakespeare, and the model will continue it like Shakespeare. At least it will try to do so ðŸ˜€.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationShakespeareRNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/nbkm3askuyw41.gif""&gt;https://i.redd.it/nbkm3askuyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Wikipedia Text Generation&lt;/h2&gt;

&lt;p&gt;You start typing a Wiki article, and the model tries to continue it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ðŸŽ¨ &lt;a href=""https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationWikipediaRNN""&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ðŸ‹ï¸ &lt;a href=""https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb""&gt;Training in Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ï¸ðŸ‹ï¸  &lt;a href=""https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb""&gt;Training in Colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.redd.it/a0l2g1pluyw41.gif""&gt;https://i.redd.it/a0l2g1pluyw41.gif&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Future plans&lt;/h2&gt;

&lt;p&gt;As I&amp;#39;ve mentioned above the main purpose of &lt;a href=""https://github.com/trekhleb/machine-learning-experiments""&gt;the repository&lt;/a&gt; is to be more like a playground for learning rather than for production-ready models. Therefore, the main plan is to &lt;strong&gt;continue learning and experimenting&lt;/strong&gt; with deep-learning challenges and approaches. The next interesting challenges to play with might be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emotions detection&lt;/li&gt;
&lt;li&gt;Style transfer&lt;/li&gt;
&lt;li&gt;Language translation&lt;/li&gt;
&lt;li&gt;Generating images (i.e. handwritten numbers)&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another interesting opportunity would be to &lt;strong&gt;tune existing models to make them more performant&lt;/strong&gt;. I believe it might give a better understanding of how to overcome overfitting and underfitting and what to do with the model if it just stuck on &lt;code&gt;60%&lt;/code&gt; accuracy level for both training and validation sets and doesn&amp;#39;t want to improve anymore ðŸ¤”.&lt;/p&gt;

&lt;p&gt;Anyways, I hope you might find some useful insights for models training from &lt;a href=""https://github.com/trekhleb/machine-learning-experiments""&gt;the repository&lt;/a&gt; or at least to have some fun playing around with the demos!&lt;/p&gt;

&lt;p&gt;Happy learning! ðŸ¤–&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?auto=webp&amp;s=ab7e49ab961f455f7bf217000a1fea8577f820c2', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9b171b0fee4d2a254fe800d6d09beed38b2fc0df', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7cde76b2ad1a7b435cc5b74800c4eb9c6d7d656', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1de851ae3e5d68f14381f434da90276f3f227228', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b98e1818860d6d8a95d8f7f9f629a6cf2b49622', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a008ba203a09ee8737773517f253d582e86a96e', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/lGCddR9646z9ZoPN7uyoiLEzljMgUQjp55B5gqLdKuQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e85c8d5edd3338291d05c8f390169ccbe142baa', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'TgXFsulyME3PcUKpOLTbwO0cxCoVoeNta2UxvmsvjOw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gdzv8h,True,,trekhleb,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdzv8h/interactive_machine_learning_experiments/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdzv8h/interactive_machine_learning_experiments/,155203,1588692664.0,0,,False,,,"{'70ruj86euyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 45, 'x': 108, 'u': 'https://preview.redd.it/70ruj86euyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1de2a89215196ebc7a486391ff7103403bde2479'}, {'y': 90, 'x': 216, 'u': 'https://preview.redd.it/70ruj86euyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=1b6fc779a56d116c9b78948edaaf5fd27304560a'}, {'y': 133, 'x': 320, 'u': 'https://preview.redd.it/70ruj86euyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=8da0d876c8e796173daae47881f03e779c851e73'}], 's': {'y': 250, 'gif': 'https://i.redd.it/70ruj86euyw41.gif', 'mp4': 'https://preview.redd.it/70ruj86euyw41.gif?format=mp4&amp;s=3e5fbbfd9fdd3b4f66ac432bd7394a8c6d0efcdb', 'x': 600}, 'id': '70ruj86euyw41'}, 'uhavc6d9uyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/uhavc6d9uyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1bee2d3f80b61ae9dd6fb8e755e02288c4f68518'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/uhavc6d9uyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=f420348a49d5b502db516fd845c01c76644f89c2'}, {'y': 164, 'x': 320, 'u': 'https://preview.redd.it/uhavc6d9uyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=98679b02f15f423e4a10501b1a900d6681c2d6c7'}], 's': {'y': 308, 'gif': 'https://i.redd.it/uhavc6d9uyw41.gif', 'mp4': 'https://preview.redd.it/uhavc6d9uyw41.gif?format=mp4&amp;s=bac856c5b19b1862fb4144d57f3d65a2794a0343', 'x': 600}, 'id': 'uhavc6d9uyw41'}, 'eyvhdtp3uyw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 45, 'x': 108, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92cfca6e19b13b95918e04c63e9f7b5420ebc72e'}, {'y': 90, 'x': 216, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=52879eda079362229356d3b58c5043ce0d520640'}, {'y': 134, 'x': 320, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2f0f6631d21a5a4e87498151e5f9b2d42207a10'}, {'y': 268, 'x': 640, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10d89434754445cf75bffd14e487333bfbaee579'}, {'y': 402, 'x': 960, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=acaa7abe88e235a752dcbe32502cd8b28e86bd2c'}, {'y': 452, 'x': 1080, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a5e66e435b83dcdaff85830c6735e6d75dc6d4e9'}], 's': {'y': 506, 'x': 1208, 'u': 'https://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;format=png&amp;auto=webp&amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a'}, 'id': 'eyvhdtp3uyw41'}, 'ewnsaudcuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 45, 'x': 108, 'u': 'https://preview.redd.it/ewnsaudcuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=afb4e1fdfb85a9ae5771a9b534c5bfeb0f941c40'}, {'y': 90, 'x': 216, 'u': 'https://preview.redd.it/ewnsaudcuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=8a849854592fe1f08d32587b0619ae1c61cce7c4'}, {'y': 133, 'x': 320, 'u': 'https://preview.redd.it/ewnsaudcuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=2ab83117f713f9e3acc71998f85cf65a0f50c5a8'}], 's': {'y': 250, 'gif': 'https://i.redd.it/ewnsaudcuyw41.gif', 'mp4': 'https://preview.redd.it/ewnsaudcuyw41.gif?format=mp4&amp;s=9a9ae1d12f9369e73514575c1d7d8fd6dd7da373', 'x': 600}, 'id': 'ewnsaudcuyw41'}, '3u5jdch1uyw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 54, 'x': 108, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b68eb8c3c6f98bdd260202bd7d49e9994105e80b'}, {'y': 108, 'x': 216, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=45de72ddb80dc7c3ec63bc807b2ad0851ebbbc18'}, {'y': 160, 'x': 320, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eea85b89c416e83b6de23700b726de5bd6dc900c'}, {'y': 320, 'x': 640, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34466f82bdcb5498b71cb59c246ab98c3a82c2e6'}, {'y': 480, 'x': 960, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e8895d6f50d73ac6720c3c1ac5906eb21583f44'}, {'y': 540, 'x': 1080, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=69c6289f06f640330072841b18e44aa55ba23ce8'}], 's': {'y': 900, 'x': 1800, 'u': 'https://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;format=png&amp;auto=webp&amp;s=946b73c9dc85f432abb025e285286f9209fb62ae'}, 'id': '3u5jdch1uyw41'}, 'jakuk4lauyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 32, 'x': 108, 'u': 'https://preview.redd.it/jakuk4lauyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=540c727f91c100cd59d5b3c9cc7338351f282426'}, {'y': 64, 'x': 216, 'u': 'https://preview.redd.it/jakuk4lauyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=79618cffa6e8e2a6069be25c2e3124e69149ccfc'}, {'y': 96, 'x': 320, 'u': 'https://preview.redd.it/jakuk4lauyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=3f08af8c398221ec6246031d59737809a57241d5'}], 's': {'y': 180, 'gif': 'https://i.redd.it/jakuk4lauyw41.gif', 'mp4': 'https://preview.redd.it/jakuk4lauyw41.gif?format=mp4&amp;s=831d62347b7de45b78c061bfad164c3176ae4936', 'x': 600}, 'id': 'jakuk4lauyw41'}, '8rc55ws7uyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 32, 'x': 108, 'u': 'https://preview.redd.it/8rc55ws7uyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=5401c43c763043f66dfde96db8d8ad3a7e5c89d9'}, {'y': 64, 'x': 216, 'u': 'https://preview.redd.it/8rc55ws7uyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=514c3c43c6b0295becc05a9b9d0a24a29050c719'}, {'y': 96, 'x': 320, 'u': 'https://preview.redd.it/8rc55ws7uyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=10bffcad869c74b1c3ded87cdedb5fe5e0010e51'}], 's': {'y': 180, 'gif': 'https://i.redd.it/8rc55ws7uyw41.gif', 'mp4': 'https://preview.redd.it/8rc55ws7uyw41.gif?format=mp4&amp;s=e97725b9aa0d260b1d186c1a369d75973f9d06fa', 'x': 600}, 'id': '8rc55ws7uyw41'}, 'nbkm3askuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/nbkm3askuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0fd9e9ed752e2f192dbac515b85c817eea12e28c'}, {'y': 112, 'x': 216, 'u': 'https://preview.redd.it/nbkm3askuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=433f0853bd7fec13e3e6924d9fc92836e2154796'}, {'y': 166, 'x': 320, 'u': 'https://preview.redd.it/nbkm3askuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=409ddf4e5b740186bf206d307afa2be7ea2b770d'}], 's': {'y': 313, 'gif': 'https://i.redd.it/nbkm3askuyw41.gif', 'mp4': 'https://preview.redd.it/nbkm3askuyw41.gif?format=mp4&amp;s=cdbd921ff732e941fccafe82bc76895d0583dd8f', 'x': 600}, 'id': 'nbkm3askuyw41'}, 'dzc4aghbuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/dzc4aghbuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0e99068ca659e53c00e9ee944bcdfd3b5b69c91a'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/dzc4aghbuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=b4cc6c3acfef3ac86b3d98952a20351f2b443f15'}, {'y': 164, 'x': 320, 'u': 'https://preview.redd.it/dzc4aghbuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=1ee03b2bef8c2dad30eeff893d4a8e54a4024c71'}], 's': {'y': 308, 'gif': 'https://i.redd.it/dzc4aghbuyw41.gif', 'mp4': 'https://preview.redd.it/dzc4aghbuyw41.gif?format=mp4&amp;s=595514284e875a6c0882f9155bd31aaa21a6a0ea', 'x': 600}, 'id': 'dzc4aghbuyw41'}, 'pv8zkkuiuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/pv8zkkuiuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1349896d2dccefb919896797605dc0570f7757e4'}, {'y': 83, 'x': 216, 'u': 'https://preview.redd.it/pv8zkkuiuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=2b269bb68bf3118170d0f633e513ef96ca096114'}, {'y': 123, 'x': 320, 'u': 'https://preview.redd.it/pv8zkkuiuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=afc24808fe3a72741b534e3f7f4cb9e941be8165'}], 's': {'y': 232, 'gif': 'https://i.redd.it/pv8zkkuiuyw41.gif', 'mp4': 'https://preview.redd.it/pv8zkkuiuyw41.gif?format=mp4&amp;s=a8f562789b4eeadfdc4a8f8eed582c241752e80b', 'x': 600}, 'id': 'pv8zkkuiuyw41'}, '16xi3nthuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 104, 'x': 108, 'u': 'https://preview.redd.it/16xi3nthuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=273757d4d3b7de138e600aeeebc203c899fff3eb'}, {'y': 209, 'x': 216, 'u': 'https://preview.redd.it/16xi3nthuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=f90ede561a9701097564159d53687d98f581c282'}, {'y': 310, 'x': 320, 'u': 'https://preview.redd.it/16xi3nthuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=6f26abd6770c4a3d2dac633895347eb4d8736016'}], 's': {'y': 388, 'gif': 'https://i.redd.it/16xi3nthuyw41.gif', 'mp4': 'https://preview.redd.it/16xi3nthuyw41.gif?format=mp4&amp;s=626184fe2b3788012ba8069cb9309a5788dfcc85', 'x': 400}, 'id': '16xi3nthuyw41'}, 'a0l2g1pluyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 37, 'x': 108, 'u': 'https://preview.redd.it/a0l2g1pluyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=c3a5dc721921f2a8943aca5ad7f24a96f80fa72b'}, {'y': 74, 'x': 216, 'u': 'https://preview.redd.it/a0l2g1pluyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=27ec7c9148863a02ee637f66863e147b4955f59a'}, {'y': 109, 'x': 320, 'u': 'https://preview.redd.it/a0l2g1pluyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=71fd2466fc24ffa1daa53bf4c41af1583dce2215'}], 's': {'y': 206, 'gif': 'https://i.redd.it/a0l2g1pluyw41.gif', 'mp4': 'https://preview.redd.it/a0l2g1pluyw41.gif?format=mp4&amp;s=926604e65addb7ea0aa1e06e2afda56efc5867a2', 'x': 600}, 'id': 'a0l2g1pluyw41'}, 'j2q8whk2uyw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 45, 'x': 108, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=234d5b8ae5ccf9d0d84c6cab91ffcb1982577243'}, {'y': 91, 'x': 216, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb1e95cb2953575ab13db312dd1c1e1c86ff3c66'}, {'y': 135, 'x': 320, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8175242529dcd7f3db812edb7afbbb9fe2a41ff'}, {'y': 271, 'x': 640, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c975ed75c2ccb9bc6f7388c3764b232cd08acd88'}, {'y': 407, 'x': 960, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4f58f24d6c11b3b0777e894dc3fba35b41011e4'}, {'y': 457, 'x': 1080, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfb91f4db8390aa619ddd045a7b04684b7795561'}], 's': {'y': 508, 'x': 1198, 'u': 'https://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;format=png&amp;auto=webp&amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317'}, 'id': 'j2q8whk2uyw41'}, 'rix288wjuyw41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/rix288wjuyw41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=fdd832390cdefa5875f187bac3214b0e173b331e'}, {'y': 83, 'x': 216, 'u': 'https://preview.redd.it/rix288wjuyw41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=fa179e48efa3f2c126216d65284a1b6698c65a6c'}, {'y': 123, 'x': 320, 'u': 'https://preview.redd.it/rix288wjuyw41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=9672f54c9a3796252ba9fc77aa2783ce8f1e4e56'}], 's': {'y': 232, 'gif': 'https://i.redd.it/rix288wjuyw41.gif', 'mp4': 'https://preview.redd.it/rix288wjuyw41.gif?format=mp4&amp;s=c09f7f775b9582dbda2332df697c285a1b694eb4', 'x': 600}, 'id': 'rix288wjuyw41'}}",
,learnmachinelearning,"[Link to repository with notebook](https://github.com/Asa-Nisi-Masa/backprop-numpy)

Every once in a while I try to re-derive backprop equations so I don't forget them. This time I decided to put them neatly in a notebook just so I have a reference. Just in case someone finds it useful I put it on github. It's a derivation/implementation of backprop for feed-forward neural net with two hidden layers and support for batch training.

Don't expect much readability (again, this is mainly for myself) - I skip most of the algebraic manipulations. LaTeX does not seem to render properly on github (renders correctly locally though).",t2_6e5ar317,False,,0,False,Sharing (mostly skipped) derivation and implementation of backpropagation in numpy,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ge42nt,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1588734938.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://github.com/Asa-Nisi-Masa/backprop-numpy""&gt;Link to repository with notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Every once in a while I try to re-derive backprop equations so I don&amp;#39;t forget them. This time I decided to put them neatly in a notebook just so I have a reference. Just in case someone finds it useful I put it on github. It&amp;#39;s a derivation/implementation of backprop for feed-forward neural net with two hidden layers and support for batch training.&lt;/p&gt;

&lt;p&gt;Don&amp;#39;t expect much readability (again, this is mainly for myself) - I skip most of the algebraic manipulations. LaTeX does not seem to render properly on github (renders correctly locally though).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/07LJ4FNfiZ9XXUYomsswRZ3xM4WHdMxC7tBpY2XGPig.jpg?auto=webp&amp;s=acfb9f19608172e98e294ac7d37d0da3e63b3c2c', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/07LJ4FNfiZ9XXUYomsswRZ3xM4WHdMxC7tBpY2XGPig.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a9156ffb7f21dd8f72f08d3ea135c5b6592039f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/07LJ4FNfiZ9XXUYomsswRZ3xM4WHdMxC7tBpY2XGPig.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65b1d34ad91b2769a26c88ea4e665145d30e7e86', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/07LJ4FNfiZ9XXUYomsswRZ3xM4WHdMxC7tBpY2XGPig.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=024a33229762edb8a5de39ce428eeeab8b0f8be3', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'm3jW_lo10Hi2INbJAkFxhnDFoXrX4OVEfWWRrNDPdsk'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ge42nt,True,,ASA--NISI--MASA,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge42nt/sharing_mostly_skipped_derivation_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge42nt/sharing_mostly_skipped_derivation_and/,155203,1588706138.0,0,,False,,,,
,learnmachinelearning,"I want to learn machine learning  as I have started the but I am confused how to start ...anybody who helps will be appreciated 

I REALLY WANTS TO LEARN",t2_643t0s95,False,,0,False,STARTED MACHINE LEARNING,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ge3wat,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588734357.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to learn machine learning  as I have started the but I am confused how to start ...anybody who helps will be appreciated &lt;/p&gt;

&lt;p&gt;I REALLY WANTS TO LEARN&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ge3wat,True,,TeacherAtAAnonymity,,3,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge3wat/started_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge3wat/started_machine_learning/,155203,1588705557.0,0,,False,,,,
,learnmachinelearning,,t2_51mclnu7,False,,0,False,Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multi...,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gduydk,False,dark,1.0,,public,6,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multiple Linear Regression', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uEe3PxQmHyM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gduydk', 'height': 338}",,False,6,,False,https://b.thumbs.redditmedia.com/W6y1G0uuxuk7k20MiPegyd2vJ61Eg3RBdeF83XrlD_E.jpg,False,,[],{},rich:video,,False,,1588701503.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?auto=webp&amp;s=e5477f527433b1e6e30ccf1a7e6d9a9949c6d909', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66609f7f9ba0202fc7da8265cdbcb8371f908a55', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27896b727bb6120ac04b7e10d7b3d648df442f35', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c78d2a9766046d6948d8d2bb4ced40525f369ce1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'chyDtVjkj-LsnfuAX65adm8NKwUrwSuSUJW8095gQRI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gduydk,True,,TheNerdyDevYT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gduydk/machine_learning_tutorials_from_novice_to_pro_13/,all_ads,False,https://www.youtube.com/watch?v=uEe3PxQmHyM&amp;feature=share,155203,1588672703.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multiple Linear Regression', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uEe3PxQmHyM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,,,,
,learnmachinelearning,"So I've built a lyric generation model using an rnn. My model is a LSTM into a Dense Layer. The input for the model is a numpy array of 20 words converted into one-hot format (31186 words in the vocab, so it's a 31186 long one hot). When I train the model and try to randomly sample I seem to only get a prediction of only 3 or 4 of very low indices in the one-hot ( I used a tokenizer to enumerate the words). When in reality the lines fed in are usually much longer and use far more words. Is there a better way to do this so I get better results?",t2_4wkzwqpa,False,,0,False,Text Generation Question,[],r/learnmachinelearning,False,6,,0,,False,t3_ge2931,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588729015.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;ve built a lyric generation model using an rnn. My model is a LSTM into a Dense Layer. The input for the model is a numpy array of 20 words converted into one-hot format (31186 words in the vocab, so it&amp;#39;s a 31186 long one hot). When I train the model and try to randomly sample I seem to only get a prediction of only 3 or 4 of very low indices in the one-hot ( I used a tokenizer to enumerate the words). When in reality the lines fed in are usually much longer and use far more words. Is there a better way to do this so I get better results?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge2931,True,,CollegeHelp4compe,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge2931/text_generation_question/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge2931/text_generation_question/,155203,1588700215.0,0,,False,,,,
,learnmachinelearning,"Iâ€™m confused on generating a final model in machine learning. Basically, my procedure is to first process the data, split the model to train and test and then fit a model to train data. Then use cross validation and other methods to find the model score. Then adjust parameters for best model. Finally, use model on all data. I was confused on what exactly it meant by all your data, I saw this on a website. Does that mean the original dataset? Isnâ€™t that overfitting? Or do I do final model on the test data? Basically, my original data set is around 40 samples but my test set is small only 8, and I want the model to include estimates for all samples. Is this possible?",t2_55qnkwk,False,,0,False,Final machine learning model,[],r/learnmachinelearning,False,6,,0,,False,t3_ge12y6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588725345.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m confused on generating a final model in machine learning. Basically, my procedure is to first process the data, split the model to train and test and then fit a model to train data. Then use cross validation and other methods to find the model score. Then adjust parameters for best model. Finally, use model on all data. I was confused on what exactly it meant by all your data, I saw this on a website. Does that mean the original dataset? Isnâ€™t that overfitting? Or do I do final model on the test data? Basically, my original data set is around 40 samples but my test set is small only 8, and I want the model to include estimates for all samples. Is this possible?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ge12y6,True,,kbabqiqja,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ge12y6/final_machine_learning_model/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ge12y6/final_machine_learning_model/,155203,1588696545.0,0,,False,,,,
,learnmachinelearning,"Assume that a bank does not have a credit scoring model yet. Detailed data from the past are available which can be used to create a classification model. This model will be used for the next six month.

Now, after 6 months, the model should be updated. What is the best way to do this? Because now only the data records of the customers are available that have not been rejected by the current model. Regarding the rejected customers, it is of course unclear whether they might actually have been creditworthy. The new sample is thus distorted and only contains the customers who have been marked as ""good"" by the current model. Should I still use this data?

Or should it from the beginning have been done in such a way that e.g. random 10% of customers are not rated by the model at all (and get a loan), so that an undistorted sample is available for retraining?",t2_5ex9k3qm,False,,0,False,Classification model retraining,[],r/learnmachinelearning,False,6,,0,,False,t3_gdvf2o,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588703853.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Assume that a bank does not have a credit scoring model yet. Detailed data from the past are available which can be used to create a classification model. This model will be used for the next six month.&lt;/p&gt;

&lt;p&gt;Now, after 6 months, the model should be updated. What is the best way to do this? Because now only the data records of the customers are available that have not been rejected by the current model. Regarding the rejected customers, it is of course unclear whether they might actually have been creditworthy. The new sample is thus distorted and only contains the customers who have been marked as &amp;quot;good&amp;quot; by the current model. Should I still use this data?&lt;/p&gt;

&lt;p&gt;Or should it from the beginning have been done in such a way that e.g. random 10% of customers are not rated by the model at all (and get a loan), so that an undistorted sample is available for retraining?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdvf2o,True,,DominikBallreich,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdvf2o/classification_model_retraining/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdvf2o/classification_model_retraining/,155203,1588675053.0,0,,False,,,,
,learnmachinelearning,"I need some project ideas in basic ml and NN!
to enhance my skill.
Could you link me to resources where i can find datasets and trained models like stuff!
Thanks",t2_6dcghpa8,False,,0,False,Any ideas for projects?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdz56c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588719119.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need some project ideas in basic ml and NN!
to enhance my skill.
Could you link me to resources where i can find datasets and trained models like stuff!
Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdz56c,True,,Pawan315,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdz56c/any_ideas_for_projects/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdz56c/any_ideas_for_projects/,155203,1588690319.0,0,,False,,,,
,learnmachinelearning,"I am trying to transition into the Data Science field and and very curious about getting as much practical and theoretical knowledge as possible. Is there any resources that can help with identifying the most appropraite models based on the underlying data. I would like to know if you can recommend a resource(s) that specifically speaks to selecting a classification model based on the underlying data and not just on trying different models and the checking their metrics. 

For example what would make you chose SVM over Logistic Regression, or Naive Bayes over Random Forests.

Thanks",t2_picbg,False,,0,False,Classification Models! How to chose?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdyspg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588717942.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to transition into the Data Science field and and very curious about getting as much practical and theoretical knowledge as possible. Is there any resources that can help with identifying the most appropraite models based on the underlying data. I would like to know if you can recommend a resource(s) that specifically speaks to selecting a classification model based on the underlying data and not just on trying different models and the checking their metrics. &lt;/p&gt;

&lt;p&gt;For example what would make you chose SVM over Logistic Regression, or Naive Bayes over Random Forests.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdyspg,True,,hiback,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdyspg/classification_models_how_to_chose/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdyspg/classification_models_how_to_chose/,155203,1588689142.0,0,,False,,,,
,learnmachinelearning,"Geometric deep learning (GDL) is one of many advances in computer vision sparked by performance improvements in DL. This article covers an introduction to Geometric Deep Learning, its interpretation in the context of ""relational inductive bias"" (a term coined by DeepMind's researchers in the field), and several of many interesting use-cases (including graph segmentation, graph classification, and KGCNs). The article is also loaded with relevant references for those interested in delving deeper.

Article link: [https://blog.paperspace.com/introduction-to-geometric-deep-learning/](https://blog.paperspace.com/introduction-to-geometric-deep-learning/)",t2_15en0l,False,,0,False,[Article] Introduction to Geometric Deep Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gdyik1,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1588716968.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Geometric deep learning (GDL) is one of many advances in computer vision sparked by performance improvements in DL. This article covers an introduction to Geometric Deep Learning, its interpretation in the context of &amp;quot;relational inductive bias&amp;quot; (a term coined by DeepMind&amp;#39;s researchers in the field), and several of many interesting use-cases (including graph segmentation, graph classification, and KGCNs). The article is also loaded with relevant references for those interested in delving deeper.&lt;/p&gt;

&lt;p&gt;Article link: &lt;a href=""https://blog.paperspace.com/introduction-to-geometric-deep-learning/""&gt;https://blog.paperspace.com/introduction-to-geometric-deep-learning/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?auto=webp&amp;s=36b905a60f53739cb9eba4b790ba50d2ab3b4e2e', 'width': 2000, 'height': 1734}, 'resolutions': [{'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f568c762ee9382e97287538e9ed568900de89ed0', 'width': 108, 'height': 93}, {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=24415da207b35fb387991f452bbd99204de5169d', 'width': 216, 'height': 187}, {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d7241094499267c13efb3df2951a3f020750e4a', 'width': 320, 'height': 277}, {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37b8f099b708f5559d71ff2753e0d360f5e8a677', 'width': 640, 'height': 554}, {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4c3623a26ecc78808ed74aa3aae09d5021d7063', 'width': 960, 'height': 832}, {'url': 'https://external-preview.redd.it/envHPq6Yuie3_Qpvsn6cyOCGHb-BSlVeLZG8dBTfocI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3d4aa81194035bfe7a7a9105788f8e5a8ad929a', 'width': 1080, 'height': 936}], 'variants': {}, 'id': 'p4SZ2yBnpbZr6--F4eHTBSRfk-RXzJpp6u3k_vhH6MM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdyik1,True,,hellopaperspace,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdyik1/article_introduction_to_geometric_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdyik1/article_introduction_to_geometric_deep_learning/,155203,1588688168.0,0,,False,,,,
,learnmachinelearning,,t2_cvc9f,False,,0,False,Multi-Label Image Classification with PyTorch: Image Tagging,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,93.0,False,t3_gdxnjh,False,light,0.67,,public,1,0,{},140.0,,False,[],,True,False,,{},Discussion,False,1,,False,https://a.thumbs.redditmedia.com/jWnrxBe7jJz7WD9YKexOXGz4r0LM0Fg_o9c6ZVdHrZ8.jpg,False,,[],{},image,,False,,1588713889.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/184m8x4g6yw41.jpg?auto=webp&amp;s=5b9f3ab6dac5330ac18504075360c0dc5308a74d', 'width': 600, 'height': 400}, 'resolutions': [{'url': 'https://preview.redd.it/184m8x4g6yw41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8ec9dbeb5b7ddd2f0fdc2c35200be6e3febff168', 'width': 108, 'height': 72}, {'url': 'https://preview.redd.it/184m8x4g6yw41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77f2027f7bafa45e4f8a5feb3276317cc6c5767a', 'width': 216, 'height': 144}, {'url': 'https://preview.redd.it/184m8x4g6yw41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa15026626318d0ca9c46d1c8bb4462222cc753e', 'width': 320, 'height': 213}], 'variants': {}, 'id': 'lUdr6lhYgfyDNfEQeH2Rn87X3nUMgZLXUEGAsEjo97A'}], 'enabled': True}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdxnjh,True,,spmallick,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdxnjh/multilabel_image_classification_with_pytorch/,all_ads,False,https://i.redd.it/184m8x4g6yw41.jpg,155203,1588685089.0,0,,False,,,,
,learnmachinelearning,,t2_1084g6,False,,0,False,How OpenPose Work. A deep Dive into Pose estimation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gdxh9p,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/bcPJeK7TrfY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How Pose Estimation works : OpenPose Part 2', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/bcPJeK7TrfY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bcPJeK7TrfY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/bcPJeK7TrfY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdxh9p', 'height': 344}",Project,False,1,,False,https://b.thumbs.redditmedia.com/qLHBwAPSZgOzcoDEUhOMxZQeKwC6sykZCEclZdsjIyI.jpg,False,,[],{},rich:video,,False,,1588713209.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XwkOS0E_HnXBLdEFxGRp7B3YraIY-tZBQGFYiu4j2g4.jpg?auto=webp&amp;s=278400fbc9eaa3f8627a3a7995c8c9286d598779', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/XwkOS0E_HnXBLdEFxGRp7B3YraIY-tZBQGFYiu4j2g4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=932f358b376decaf2593fc1d399a729de8960cb5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/XwkOS0E_HnXBLdEFxGRp7B3YraIY-tZBQGFYiu4j2g4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88c5ff901677f90671e285ab17b1d326fa52716a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/XwkOS0E_HnXBLdEFxGRp7B3YraIY-tZBQGFYiu4j2g4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab0f8fcd7e674dc595e488a9ad32ceefabe0b9c8', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zSrBH6Tv6u_kcI_OposzeflorTcxhCIAMBKNEDEvmvw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gdxh9p,True,,rednivrug,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdxh9p/how_openpose_work_a_deep_dive_into_pose_estimation/,all_ads,False,https://www.youtube.com/watch?v=bcPJeK7TrfY,155203,1588684409.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How Pose Estimation works : OpenPose Part 2', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/bcPJeK7TrfY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/bcPJeK7TrfY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I have used some basic data mining techniques in the past at work and I also took an elective data mining course in college. I have been re-learning data mining recently using the ISLR book.

So, when I was reading the multiple regression chapter I came across the example that uses the *advertising* dataset. Now, I was coding this alongside on R as I was reading the chapter just to see if I get the same results as in the book. I got the same summary for the multiple regression as in the book.

But when the book touches upon the confidence interval for sales *(in page 82)*, I did not get the same result. For a *TV budget* of 100,000 and *radio budget* of 20,000, the confidence interval of the *mean sales* in the book is *\[10985, 11528\]*. On my PC I get *\[7921, 8779\]*.

Since I got everything else including *F-stat, R-squared, RSE, etc* correct until that point, I guessed that my code for prediction is wrong. I tried to find out the correct syntax, but couldn't find any. So, I thought I'll seek redditors' advise on where I went wrong. My code is below:

*Importing data*

    ```{r}
    rm(list=ls()) 
    data1 &lt;- read.csv(""Advertising.csv"", header=TRUE) 
    names(data1)
    sum(is.na(data1)) 
    ```

*Fitting the model*

    ```{r  eval = T}
    set.seed(100)
    fit.1 &lt;- lm(sales ~ TV + radio + newspaper, data = data1)
    summary(fit.1)
    ```

*Predicting CI*

    ```{r  eval = T}
    new.dat &lt;- data.frame(TV=100000,radio=20000,newspaper=0)
    predict(fit.1, new.dat, interval = 'confidence')
    ```

Can any kind soul advise me on where I've gone wrong?

Edit: I have added the output I got in the comments.",t2_mltad,False,,0,False,Doubt with regards to Advertising data set from ISLR,[],r/learnmachinelearning,False,6,,0,,False,t3_gdxd87,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588701626.0,,[],{},,,True,,1588712734.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have used some basic data mining techniques in the past at work and I also took an elective data mining course in college. I have been re-learning data mining recently using the ISLR book.&lt;/p&gt;

&lt;p&gt;So, when I was reading the multiple regression chapter I came across the example that uses the &lt;em&gt;advertising&lt;/em&gt; dataset. Now, I was coding this alongside on R as I was reading the chapter just to see if I get the same results as in the book. I got the same summary for the multiple regression as in the book.&lt;/p&gt;

&lt;p&gt;But when the book touches upon the confidence interval for sales &lt;em&gt;(in page 82)&lt;/em&gt;, I did not get the same result. For a &lt;em&gt;TV budget&lt;/em&gt; of 100,000 and &lt;em&gt;radio budget&lt;/em&gt; of 20,000, the confidence interval of the &lt;em&gt;mean sales&lt;/em&gt; in the book is &lt;em&gt;[10985, 11528]&lt;/em&gt;. On my PC I get &lt;em&gt;[7921, 8779]&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since I got everything else including &lt;em&gt;F-stat, R-squared, RSE, etc&lt;/em&gt; correct until that point, I guessed that my code for prediction is wrong. I tried to find out the correct syntax, but couldn&amp;#39;t find any. So, I thought I&amp;#39;ll seek redditors&amp;#39; advise on where I went wrong. My code is below:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Importing data&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```{r}
rm(list=ls()) 
data1 &amp;lt;- read.csv(&amp;quot;Advertising.csv&amp;quot;, header=TRUE) 
names(data1)
sum(is.na(data1)) 
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Fitting the model&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```{r  eval = T}
set.seed(100)
fit.1 &amp;lt;- lm(sales ~ TV + radio + newspaper, data = data1)
summary(fit.1)
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Predicting CI&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```{r  eval = T}
new.dat &amp;lt;- data.frame(TV=100000,radio=20000,newspaper=0)
predict(fit.1, new.dat, interval = &amp;#39;confidence&amp;#39;)
```
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Can any kind soul advise me on where I&amp;#39;ve gone wrong?&lt;/p&gt;

&lt;p&gt;Edit: I have added the output I got in the comments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdxd87,True,,banana_1986,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdxd87/doubt_with_regards_to_advertising_data_set_from/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdxd87/doubt_with_regards_to_advertising_data_set_from/,155203,1588683934.0,0,,False,,,,
,learnmachinelearning,,t2_2i0qwoem,False,,0,False,[D] List of text classification tips and tricks (from kaggle competitions). What did we miss?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdufo9,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,default,False,,[],{},link,,False,,1588698923.0,richtext,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?auto=webp&amp;s=918331cfa2bc489619b8823e1427bcd43e9fde46', 'width': 1920, 'height': 1377}, 'resolutions': [{'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a842c8578f9865e1e1339cbf241ed2c8903ad81d', 'width': 108, 'height': 77}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e11c59771ea8eda413c97d2101d5c0082b93fb68', 'width': 216, 'height': 154}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4df4fceb35a4b232a05d1cef472c54d0ad6c8176', 'width': 320, 'height': 229}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f631378d4e0fcd9a2841183985d10bf9958fb7b2', 'width': 640, 'height': 459}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=02b1aec1dc7f1e7b4055607c058c30023268aa6a', 'width': 960, 'height': 688}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=331e90ee1fecb77841db34cd75d7b122226fb95a', 'width': 1080, 'height': 774}], 'variants': {}, 'id': 'gCQMz03Og6OEMLXuT4tDR_M0rr1MVbL10Trd_Wf-8-Q'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdufo9,True,,ai_yoda,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdufo9/d_list_of_text_classification_tips_and_tricks/,all_ads,False,/r/MachineLearning/comments/gaqm5z/d_list_of_text_classification_tips_and_tricks/,155203,1588670123.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi all,\n\nYou may remember that a couple of weeks ago we compiled a list of tricks for image segmentation problems.\n\nThis time we've gone through the latest 5 Kaggle competitions in text classification and extracted some great insights from the discussions and winning solutions and put them into [this article](https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-text-classification-tips-and-tricks-kaggle-competitions).\n\nIt took some work but we structured them into:\n\n* Dealing with large datasets\n* Small datasets and external data\n* Data exploration for NLP\n* Data cleaning\n* Text representation\n* Modeling\n* Evaluation and cross-validation\n* Runtime tricks\n* Model ensembling\n\nWhat do you think should be added to this?\n\nAny additional tips that come from your experience working with text classification problems (both research and industry) that you could share?"", 'author_fullname': 't2_2i0qwoem', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] List of text classification tips and tricks (from kaggle competitions). What did we miss?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gaqm5z', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 218, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 218, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588257728.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;You may remember that a couple of weeks ago we compiled a list of tricks for image segmentation problems.&lt;/p&gt;\n\n&lt;p&gt;This time we&amp;#39;ve gone through the latest 5 Kaggle competitions in text classification and extracted some great insights from the discussions and winning solutions and put them into &lt;a href=""https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-text-classification-tips-and-tricks-kaggle-competitions""&gt;this article&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It took some work but we structured them into:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dealing with large datasets&lt;/li&gt;\n&lt;li&gt;Small datasets and external data&lt;/li&gt;\n&lt;li&gt;Data exploration for NLP&lt;/li&gt;\n&lt;li&gt;Data cleaning&lt;/li&gt;\n&lt;li&gt;Text representation&lt;/li&gt;\n&lt;li&gt;Modeling&lt;/li&gt;\n&lt;li&gt;Evaluation and cross-validation&lt;/li&gt;\n&lt;li&gt;Runtime tricks&lt;/li&gt;\n&lt;li&gt;Model ensembling&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What do you think should be added to this?&lt;/p&gt;\n\n&lt;p&gt;Any additional tips that come from your experience working with text classification problems (both research and industry) that you could share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?auto=webp&amp;s=918331cfa2bc489619b8823e1427bcd43e9fde46', 'width': 1920, 'height': 1377}, 'resolutions': [{'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a842c8578f9865e1e1339cbf241ed2c8903ad81d', 'width': 108, 'height': 77}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e11c59771ea8eda413c97d2101d5c0082b93fb68', 'width': 216, 'height': 154}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4df4fceb35a4b232a05d1cef472c54d0ad6c8176', 'width': 320, 'height': 229}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f631378d4e0fcd9a2841183985d10bf9958fb7b2', 'width': 640, 'height': 459}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=02b1aec1dc7f1e7b4055607c058c30023268aa6a', 'width': 960, 'height': 688}, {'url': 'https://external-preview.redd.it/cGrAqBnotmfi5UKoz1r6IN47rGh4ApY0pY4m84WvTSc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=331e90ee1fecb77841db34cd75d7b122226fb95a', 'width': 1080, 'height': 774}], 'variants': {}, 'id': 'gCQMz03Og6OEMLXuT4tDR_M0rr1MVbL10Trd_Wf-8-Q'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gaqm5z', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ai_yoda', 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/gaqm5z/d_list_of_text_classification_tips_and_tricks/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/gaqm5z/d_list_of_text_classification_tips_and_tricks/', 'subreddit_subscribers': 1044414, 'created_utc': 1588228928.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_gaqm5z,,
,learnmachinelearning,,t2_42ti910c,False,,0,False,Used LSTM with some algorithmic twist and here is what came out of it,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gdvnkb,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/yroZs4EVLxw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Magical Sunshine - Neuro Chords | Piano Music Music generated by AI', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/yroZs4EVLxw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Neuro Chords', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yroZs4EVLxw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWqnekVEEXNgGNHMcrblqzg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/yroZs4EVLxw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdvnkb', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/E7NORGktFkfBVH37ORDINEH6V-naWxm3dY3Dw5oRlEU.jpg,False,,[],{},rich:video,,False,,1588705035.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GizUDDeLIT10cxFJz4vd418p0BugkiJ4rFnaxnaSVkU.jpg?auto=webp&amp;s=784bbc8978661bc853a492fe8495f1e3ebbd0ffe', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/GizUDDeLIT10cxFJz4vd418p0BugkiJ4rFnaxnaSVkU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0e548bee9d93e074f6037388d1dd5d1c774b1ff', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/GizUDDeLIT10cxFJz4vd418p0BugkiJ4rFnaxnaSVkU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5017075b4402428c254ff09221bf1043975903a4', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/GizUDDeLIT10cxFJz4vd418p0BugkiJ4rFnaxnaSVkU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97bffa1d2ca4c0de738bd723832a8b4c446aa345', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'XSzDLI5i2CLo5963bG0wWj69IAA_eUIKAUy_Clv8ytM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdvnkb,True,,vaiv101,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdvnkb/used_lstm_with_some_algorithmic_twist_and_here_is/,all_ads,False,https://youtu.be/yroZs4EVLxw,155203,1588676235.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Magical Sunshine - Neuro Chords | Piano Music Music generated by AI', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/yroZs4EVLxw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Neuro Chords', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/yroZs4EVLxw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWqnekVEEXNgGNHMcrblqzg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hello,

I am trying to segment hair and face from a picture. I am doing the dumb/simple way of doing this by extracting face and area till neck . I then tried subtracting the two arrays but its throwing error. Tried subtraction with opencv also but getting errors.

Any ideas on other simple methods to try?",t2_3mqf0d2e,False,,0,False,[Need help] Hair segmentation using python and machine learning,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdvf23,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588703849.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to segment hair and face from a picture. I am doing the dumb/simple way of doing this by extracting face and area till neck . I then tried subtracting the two arrays but its throwing error. Tried subtraction with opencv also but getting errors.&lt;/p&gt;

&lt;p&gt;Any ideas on other simple methods to try?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gdvf23,True,,shwetashri,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdvf23/need_help_hair_segmentation_using_python_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdvf23/need_help_hair_segmentation_using_python_and/,155203,1588675049.0,0,,False,,,,
,learnmachinelearning,"So, I've been working with TensorFlow/Keras recently, I've got the hang of training a NN to predict something based on existing data using Keras.Sequential.

But what if you want to train a network to take actions based on data? Like, for example, play chess? What should I look into?

Any help much appreciated.",t2_26lp84x0,False,,0,False,Training a network to take actions rather than predict outcomes,[],r/learnmachinelearning,False,6,,0,,False,t3_gdv9o9,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588703098.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I&amp;#39;ve been working with TensorFlow/Keras recently, I&amp;#39;ve got the hang of training a NN to predict something based on existing data using Keras.Sequential.&lt;/p&gt;

&lt;p&gt;But what if you want to train a network to take actions based on data? Like, for example, play chess? What should I look into?&lt;/p&gt;

&lt;p&gt;Any help much appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdv9o9,True,,RichKat666,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdv9o9/training_a_network_to_take_actions_rather_than/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdv9o9/training_a_network_to_take_actions_rather_than/,155203,1588674298.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,AI + table tennis.,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gdlbqo,False,light,0.78,,public,10,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HLZM5fsHeMM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI + table tennis.', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HLZM5fsHeMM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HLZM5fsHeMM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HLZM5fsHeMM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdlbqo', 'height': 338}",Discussion,False,10,,False,https://b.thumbs.redditmedia.com/svJjtY7nyHOSebpZmPIgA6ZgCUzpRpb7ZxfCbZ0u_5s.jpg,False,,[],{},rich:video,,False,,1588659033.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ruNlXz9zR4qDgRf41swwAQGBLi-1yumci9t8Jb5yG_k.jpg?auto=webp&amp;s=2873cbd26ea8dc7389ef07f2f75f074454681ac5', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ruNlXz9zR4qDgRf41swwAQGBLi-1yumci9t8Jb5yG_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=688528da647a96220f1350afda62b9e1c909ce7c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ruNlXz9zR4qDgRf41swwAQGBLi-1yumci9t8Jb5yG_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=902362498af9f2fd2297f80b16d6f4b22c5d79e9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ruNlXz9zR4qDgRf41swwAQGBLi-1yumci9t8Jb5yG_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9cdab98120a7a1cdf7a929f6babfc250fa0a3107', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'tM--GsROt4dFm4JVb6_W8d355c5vy02meZ4vD7fzdfk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdlbqo,True,,cmillionaire9,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdlbqo/ai_table_tennis/,all_ads,False,https://youtu.be/HLZM5fsHeMM,155203,1588630233.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI + table tennis.', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HLZM5fsHeMM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HLZM5fsHeMM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,,,,
,learnmachinelearning,,t2_1bodl8op,False,,0,False,There are a new free bounding boxes smartphone dataset available,[],r/learnmachinelearning,False,6,,0,,False,t3_gdv0mu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588701815.0,text,6,,,text,makeml.app,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdv0mu,True,,lekorotkov,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdv0mu/there_are_a_new_free_bounding_boxes_smartphone/,all_ads,False,https://makeml.app/datasets/smartphones,155203,1588673015.0,0,,False,,,,
,learnmachinelearning,"Hi all, I've been learning about the background of gradient boosted trees from the [awesome documentation of XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) but as with all learning, I feel I've answered one question but am now asking 10 more!

When training gradient boosted ensembles in an additive training approach, at each iteration are we 

1. Adding better and better estimators (i.e. gradient descent on each succesive estimator till we end up with a *perfect* estimator in an ensemble of lesser estimators)?
2. Adding estimators which optimise the overal ensemble w.r.t the objective function rather than optimising each added estimator?

My gut is telling me that it's 2 otherwise you would have an singularly optimised estimator in an ensmble containing poor estimators but just wanted to check incase my understanding was off!

Hope y'all are keeping healthy and safe!",t2_ki717,False,,0,False,Clarification about Gradient Boosted Trees,[],r/learnmachinelearning,False,6,,0,,False,t3_gduvt3,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588701162.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, I&amp;#39;ve been learning about the background of gradient boosted trees from the &lt;a href=""https://xgboost.readthedocs.io/en/latest/tutorials/model.html""&gt;awesome documentation of XGBoost&lt;/a&gt; but as with all learning, I feel I&amp;#39;ve answered one question but am now asking 10 more!&lt;/p&gt;

&lt;p&gt;When training gradient boosted ensembles in an additive training approach, at each iteration are we &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Adding better and better estimators (i.e. gradient descent on each succesive estimator till we end up with a &lt;em&gt;perfect&lt;/em&gt; estimator in an ensemble of lesser estimators)?&lt;/li&gt;
&lt;li&gt;Adding estimators which optimise the overal ensemble w.r.t the objective function rather than optimising each added estimator?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My gut is telling me that it&amp;#39;s 2 otherwise you would have an singularly optimised estimator in an ensmble containing poor estimators but just wanted to check incase my understanding was off!&lt;/p&gt;

&lt;p&gt;Hope y&amp;#39;all are keeping healthy and safe!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5Hp8Y74l0BjFvLFLZmMaHncEEGSuGR4c_SqIiwLBrow.jpg?auto=webp&amp;s=a483889c2c7653c9e6e2bf10a683326322e9ae8d', 'width': 910, 'height': 460}, 'resolutions': [{'url': 'https://external-preview.redd.it/5Hp8Y74l0BjFvLFLZmMaHncEEGSuGR4c_SqIiwLBrow.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f7024e7ba15d0afeb16164da5a72a18c8c874de', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/5Hp8Y74l0BjFvLFLZmMaHncEEGSuGR4c_SqIiwLBrow.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=890c4980cd1b53edff189b7fa3c6cd05249a363a', 'width': 216, 'height': 109}, {'url': 'https://external-preview.redd.it/5Hp8Y74l0BjFvLFLZmMaHncEEGSuGR4c_SqIiwLBrow.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2cb3fa51b1084fd7eb8db6c406c4b665542ecf47', 'width': 320, 'height': 161}, {'url': 'https://external-preview.redd.it/5Hp8Y74l0BjFvLFLZmMaHncEEGSuGR4c_SqIiwLBrow.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf5a386c8539561fdc087871c8093fd0da483698', 'width': 640, 'height': 323}], 'variants': {}, 'id': 'hi9sft-ALSOmkpNVwAaoBwoqtNiwQiba8GJdxpOhp9k'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gduvt3,True,,The_Bundaberg_Joey,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gduvt3/clarification_about_gradient_boosted_trees/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gduvt3/clarification_about_gradient_boosted_trees/,155203,1588672362.0,0,,False,,,,
,learnmachinelearning,"AIcrowd is excited to announce the launch of *AIcrowd Blitz* âš¡- our fortnight-long marathon of interesting AI puzzles ðŸŽ‰.

Whether you are an AI veteran or someone who is just finding feet in the world of ML and AI, there is something for each one of you. And did we mention there are some cash prizes up for grabs too !! ðŸ’°Â 

Our problems have always been intriguing and this time would be no exception. So put on that puzzle hat ðŸŽ© and join us in this marathon.Â 

***What*** **âš¡: AIcrowd Blitz**

***When*** **ðŸ—“ï¸: 2nd Mayâ€™20 17:00 CEST - 16th Mayâ€™20 17:00 CEST**

***Sneak Peek*** ðŸ§: We have taken some of the classic ML problems and given it a flavor of our own.

[https://www.aicrowd.com/challenges/aicrowd-blitz-may-2020/](https://www.aicrowd.com/challenges/aicrowd-blitz-may-2020/)

I will be happy to take back feedbacks from the community and improve wherever we can and I hope it will be a lot of fun and nice learning experience solving the problems.",t2_li8tq,False,,0,False,[Educational Competition] AIcrowd Blitzâš¡- May 2020,[],r/learnmachinelearning,False,6,,0,,False,t3_gdp6ed,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1588673400.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;AIcrowd is excited to announce the launch of &lt;em&gt;AIcrowd Blitz&lt;/em&gt; âš¡- our fortnight-long marathon of interesting AI puzzles ðŸŽ‰.&lt;/p&gt;

&lt;p&gt;Whether you are an AI veteran or someone who is just finding feet in the world of ML and AI, there is something for each one of you. And did we mention there are some cash prizes up for grabs too !! ðŸ’°Â &lt;/p&gt;

&lt;p&gt;Our problems have always been intriguing and this time would be no exception. So put on that puzzle hat ðŸŽ© and join us in this marathon.Â &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;âš¡: AIcrowd Blitz&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;When&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;ðŸ—“ï¸: 2nd Mayâ€™20 17:00 CEST - 16th Mayâ€™20 17:00 CEST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sneak Peek&lt;/em&gt;&lt;/strong&gt; ðŸ§: We have taken some of the classic ML problems and given it a flavor of our own.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.aicrowd.com/challenges/aicrowd-blitz-may-2020/""&gt;https://www.aicrowd.com/challenges/aicrowd-blitz-may-2020/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will be happy to take back feedbacks from the community and improve wherever we can and I hope it will be a lot of fun and nice learning experience solving the problems.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IDeU7x75Aw1gpihJU2z8eJV-ApTv53RLfk0mS9Soct8.jpg?auto=webp&amp;s=5c725749878709359851dd34752bddef4e13f6f6', 'width': 744, 'height': 1052}, 'resolutions': [{'url': 'https://external-preview.redd.it/IDeU7x75Aw1gpihJU2z8eJV-ApTv53RLfk0mS9Soct8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=848e0179ee90aec88fc897d30070ae3141562156', 'width': 108, 'height': 152}, {'url': 'https://external-preview.redd.it/IDeU7x75Aw1gpihJU2z8eJV-ApTv53RLfk0mS9Soct8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97943139c4ec663428c1958d71bb1133fcb51f64', 'width': 216, 'height': 305}, {'url': 'https://external-preview.redd.it/IDeU7x75Aw1gpihJU2z8eJV-ApTv53RLfk0mS9Soct8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=714c0c5b410dc5d21191d2112cbedefc35453d4b', 'width': 320, 'height': 452}, {'url': 'https://external-preview.redd.it/IDeU7x75Aw1gpihJU2z8eJV-ApTv53RLfk0mS9Soct8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aa119bb54b94ece700d612bdbf2fe11b6c262b2', 'width': 640, 'height': 904}], 'variants': {}, 'id': 'Gu-jUDxJVHsr5hvKWVu4m9g4sPOBT0UbJqE5pSlKnv4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdp6ed,True,,skbly7,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdp6ed/educational_competition_aicrowd_blitz_may_2020/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdp6ed/educational_competition_aicrowd_blitz_may_2020/,155203,1588644600.0,0,,False,,,,
,learnmachinelearning,"This is my first time working on real world data and I've been working on it for a whole week and I am stuck now. If this is not the type of question that I can ask here, please tell me and I will remove this.

I have 3 independent parameters where 2 of them are like coordinates. These are the variations of the dependent parameter with the variables :

2 plots are exactly like this : 

 

https://preview.redd.it/pd5eik8kvww41.png?width=542&amp;format=png&amp;auto=webp&amp;s=2b17fc78523ed655f880b3b12dafb68da23fc317

The 3rd one varies like this :

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/s7fg8ar9www41.png?width=522&amp;format=png&amp;auto=webp&amp;s=64f2dd5071aa209da46034fbfd6d1607a04f1673

I've used linear, polynomial, SVR, random forest, lasso, Ridge, elastic net regression techniques with some basic type of data preprocessing (standard scaling, normalisation) but I'm not getting good results. I used fastai library and used neural networks too but they weren't much better either.

I don't want direct answers. Please can anyone just point me in the right direction like what am I doing wrong and what can i do more? I will be using poisson regression and LSTM based regression after this.",,False,,0,False,Regression Problem on real world data,[],r/learnmachinelearning,False,6,,0,89.0,False,t3_gduc9m,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,,https://b.thumbs.redditmedia.com/sRtdnrPZSMsDD7-uhRiqrbHJkFtPZADCZCwwfNSnqYc.jpg,False,,,{},,,True,,1588698448.0,text,6,,,,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is my first time working on real world data and I&amp;#39;ve been working on it for a whole week and I am stuck now. If this is not the type of question that I can ask here, please tell me and I will remove this.&lt;/p&gt;

&lt;p&gt;I have 3 independent parameters where 2 of them are like coordinates. These are the variations of the dependent parameter with the variables :&lt;/p&gt;

&lt;p&gt;2 plots are exactly like this : &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/pd5eik8kvww41.png?width=542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b17fc78523ed655f880b3b12dafb68da23fc317""&gt;https://preview.redd.it/pd5eik8kvww41.png?width=542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b17fc78523ed655f880b3b12dafb68da23fc317&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The 3rd one varies like this :&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/s7fg8ar9www41.png?width=522&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=64f2dd5071aa209da46034fbfd6d1607a04f1673""&gt;https://preview.redd.it/s7fg8ar9www41.png?width=522&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=64f2dd5071aa209da46034fbfd6d1607a04f1673&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve used linear, polynomial, SVR, random forest, lasso, Ridge, elastic net regression techniques with some basic type of data preprocessing (standard scaling, normalisation) but I&amp;#39;m not getting good results. I used fastai library and used neural networks too but they weren&amp;#39;t much better either.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t want direct answers. Please can anyone just point me in the right direction like what am I doing wrong and what can i do more? I will be using poisson regression and LSTM based regression after this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gduc9m,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/learnmachinelearning/comments/gduc9m/regression_problem_on_real_world_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gduc9m/regression_problem_on_real_world_data/,155203,1588669648.0,0,,False,,,"{'s7fg8ar9www41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 62, 'x': 108, 'u': 'https://preview.redd.it/s7fg8ar9www41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fee51a9560e4c181a3128452abae7170b4a92de3'}, {'y': 125, 'x': 216, 'u': 'https://preview.redd.it/s7fg8ar9www41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e38e81ca4863831724afa415f3085b12f7a2ef4'}, {'y': 185, 'x': 320, 'u': 'https://preview.redd.it/s7fg8ar9www41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2f083c0a3cf4fbc3be309c1a1446caf65b9419b'}], 's': {'y': 303, 'x': 522, 'u': 'https://preview.redd.it/s7fg8ar9www41.png?width=522&amp;format=png&amp;auto=webp&amp;s=64f2dd5071aa209da46034fbfd6d1607a04f1673'}, 'id': 's7fg8ar9www41'}, 'pd5eik8kvww41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 68, 'x': 108, 'u': 'https://preview.redd.it/pd5eik8kvww41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=92000aa8218538421a2e642234d664be78eb8cc7'}, {'y': 137, 'x': 216, 'u': 'https://preview.redd.it/pd5eik8kvww41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1e5d11ad30b917d61ea0e067abecb68c3ed69a6'}, {'y': 203, 'x': 320, 'u': 'https://preview.redd.it/pd5eik8kvww41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=92794e7331b8e49f5bbdc955eab9658ce7f9f177'}], 's': {'y': 345, 'x': 542, 'u': 'https://preview.redd.it/pd5eik8kvww41.png?width=542&amp;format=png&amp;auto=webp&amp;s=2b17fc78523ed655f880b3b12dafb68da23fc317'}, 'id': 'pd5eik8kvww41'}}",
,learnmachinelearning,,t2_5u4gl9ds,False,,0,False,20 Best Machine Learning Courses to Learn,[],r/learnmachinelearning,False,6,,0,79.0,False,t3_gdxzwy,False,dark,0.2,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/0EaIAjn_PWJvdUFFH7ejQ8Ek9MWZpSdSl25qqtMP5TY.jpg,False,,[],{},link,,False,,1588715171.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?auto=webp&amp;s=f1ad193fcf8925fda7eecec2e2eddfc4978a50e5', 'width': 450, 'height': 255}, 'resolutions': [{'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=059991a004c0567d929560e64ef195b8250be27a', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c91ed9356ba77210f07c6883599fb9c9967e6c43', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a2dd55e68edd7ddc298bbd764964413874d0cb', 'width': 320, 'height': 181}], 'variants': {}, 'id': 'hNiHZGAMdlAXHG8i2zCj8xKlnCAJpcumaPYiw_vZHrE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdxzwy,True,,gamesbon,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdxzwy/20_best_machine_learning_courses_to_learn/,all_ads,False,https://medium.com/@devexpert/20-best-machine-learning-courses-online-4a7863c4326a,155203,1588686371.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,Top 15 AI Articles You Should Read This Month - April 2020,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gdt26j,False,dark,0.66,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/1_teSgzEAE5waskdxrtZPZ02ZA4wHpa_hByOUiiL61s.jpg,False,,[],{},link,,False,,1588691667.0,text,6,,,text,rubikscode.net,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?auto=webp&amp;s=f5997d281c56b90a5a72c5ad3dbf7034c6f4a6c0', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2742b4b7bfd5a8cd33349217fd6364e3abcbb607', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12b7f30c0e6fee5d83be55500f6f6dc7be461ed5', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bd8cdd7939b8ecfdb887523e98ed0ee6300d8ec', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=43243088e33eb87712c3d919f1c59c73c74da86a', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eba99bfa4ae447b10c8d6cc16595ffa8502cb63', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/bM0Vm_Ci6tVmJ1zpR-tssHgxdQ-jcs7glPsqaO2FApk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59038531ba91bfa2706079e1cc72019e508267f7', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'geLripYatV7zGGZmga9Dut91-FINjUsMdKiFB53T-oI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdt26j,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdt26j/top_15_ai_articles_you_should_read_this_month/,all_ads,False,https://rubikscode.net/2020/05/05/top-15-ai-articles-you-should-read-this-month-april-2020/,155203,1588662867.0,0,,False,,,,
,learnmachinelearning,"Just created My first NEAT model that plays flappy bird using [NEAT-Python](https://neat-python.readthedocs.io/en/latest/) and [Pygame](https://www.pygame.org/news).

Thought I'd share.

GitHub :  [https://github.com/Shell1500/NEAT-Python-Flappy-Bird](https://github.com/Shell1500/NEAT-Python-Flappy-Bird) 

https://reddit.com/link/gdmnny/video/fxnbffen1uw41/player",t2_1myifi4q,False,,0,False,NEAT model that plays Flappy Bird,[],r/learnmachinelearning,False,6,,0,,False,t3_gdmnny,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1588691400.0,,[],{},,,True,,1588663736.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just created My first NEAT model that plays flappy bird using &lt;a href=""https://neat-python.readthedocs.io/en/latest/""&gt;NEAT-Python&lt;/a&gt; and &lt;a href=""https://www.pygame.org/news""&gt;Pygame&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thought I&amp;#39;d share.&lt;/p&gt;

&lt;p&gt;GitHub :  &lt;a href=""https://github.com/Shell1500/NEAT-Python-Flappy-Bird""&gt;https://github.com/Shell1500/NEAT-Python-Flappy-Bird&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/gdmnny/video/fxnbffen1uw41/player""&gt;https://reddit.com/link/gdmnny/video/fxnbffen1uw41/player&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdmnny,True,,therealshell1500,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdmnny/neat_model_that_plays_flappy_bird/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdmnny/neat_model_that_plays_flappy_bird/,155203,1588634936.0,0,,False,,,"{'fxnbffen1uw41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gdmnny/asset/fxnbffen1uw41/DASHPlaylist.mpd', 'x': 1280, 'y': 720, 'hlsUrl': 'https://v.redd.it/link/gdmnny/asset/fxnbffen1uw41/HLSPlaylist.m3u8', 'id': 'fxnbffen1uw41', 'isGif': False}}",
,learnmachinelearning,"Hello r/learnmachinelearning!

In trying to better understand the math behind neural networks I decided to bring together some of my findings in a math blog. You can find my most recent post here: [https://willisk.github.io/2020/Multiclass-neural-network/](https://willisk.github.io/2020/Multiclass-neural-network/), where I start by building a neural network from scratch, while explaining all the formulas, derivatives, etc. 

In particular, a thing of interest might be, how one can deal with a matrix-by-matrix derivative or how the softmax layer and the cross-entropy loss work together.

I would really appreciate some feedback, positive or negative, on anything, design, mistakes, ideas, first impressions, questions.

I hope to be able to write more advanced articles, if time permits.",t2_67bfzkbv,False,,0,False,A closer look on the math behind neural networks,[],r/learnmachinelearning,False,6,,0,,False,t3_gdg7w2,False,dark,0.92,,public,9,0,{},,,False,[],,False,False,,{},,False,9,,False,self,False,,[],{},,,True,,1588642980.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello &lt;a href=""/r/learnmachinelearning""&gt;r/learnmachinelearning&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;In trying to better understand the math behind neural networks I decided to bring together some of my findings in a math blog. You can find my most recent post here: &lt;a href=""https://willisk.github.io/2020/Multiclass-neural-network/""&gt;https://willisk.github.io/2020/Multiclass-neural-network/&lt;/a&gt;, where I start by building a neural network from scratch, while explaining all the formulas, derivatives, etc. &lt;/p&gt;

&lt;p&gt;In particular, a thing of interest might be, how one can deal with a matrix-by-matrix derivative or how the softmax layer and the cross-entropy loss work together.&lt;/p&gt;

&lt;p&gt;I would really appreciate some feedback, positive or negative, on anything, design, mistakes, ideas, first impressions, questions.&lt;/p&gt;

&lt;p&gt;I hope to be able to write more advanced articles, if time permits.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdg7w2,True,,ReddViolet,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdg7w2/a_closer_look_on_the_math_behind_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdg7w2/a_closer_look_on_the_math_behind_neural_networks/,155203,1588614180.0,0,,False,,,,
,learnmachinelearning,,t2_1jz7x6p,False,,0,False,"20 Activation Functions in Python for Deep Neural Network | ELU, ReLU, Leaky-ReLU, Sigmoid, Cosine",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gdcnr9,False,dark,0.87,,public,17,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/D1twox0mcrM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '20 Activation Functions in Python for Deep Neural Network  | ELU, ReLU, Leaky-ReLU,  Sigmoid, Cosine', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/D1twox0mcrM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'CodeThisCodeThat', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/D1twox0mcrM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCts-XMcexTiPSR8QbyRGFxA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/D1twox0mcrM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdcnr9', 'height': 338}",,False,17,,False,https://b.thumbs.redditmedia.com/2MA62zKNHMkIz7mm3TsEO6LwiXKfT0T65ppPaxK8HKI.jpg,False,,[],{},rich:video,,False,,1588631576.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/stiiNwacWQbk5nlerks_83rDUTR0E3DiAk2nVIJ1VlI.jpg?auto=webp&amp;s=9fe1d5eb11de9efcf496d7ed3d25cb5bc3a6f695', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/stiiNwacWQbk5nlerks_83rDUTR0E3DiAk2nVIJ1VlI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0f0d8c8c4fa1a19ea7b3b48e2ae7493d023b3fb', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/stiiNwacWQbk5nlerks_83rDUTR0E3DiAk2nVIJ1VlI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7710dc83648384a67b9267759eae5bb3e86a821d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/stiiNwacWQbk5nlerks_83rDUTR0E3DiAk2nVIJ1VlI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d521fc74efabb02569d85a902e0e10d6fcdfb255', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'JFsVwvOxZgLhmR3umWw4jqa-_Kt3Uecy5UDXZFwVWxw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdcnr9,True,,research_pie,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdcnr9/20_activation_functions_in_python_for_deep_neural/,all_ads,False,https://www.youtube.com/watch?v=D1twox0mcrM&amp;feature=share,155203,1588602776.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '20 Activation Functions in Python for Deep Neural Network  | ELU, ReLU, Leaky-ReLU,  Sigmoid, Cosine', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/D1twox0mcrM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'CodeThisCodeThat', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/D1twox0mcrM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCts-XMcexTiPSR8QbyRGFxA'}}",False,,,,
,learnmachinelearning,,t2_36shq3rh,False,,0,False,The Definitive Data Scientist Setup,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,74.0,False,t3_gdmmdj,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},Discussion,False,3,,False,https://a.thumbs.redditmedia.com/Q0IRGoJxy1DLH8N9VJAQPqGqh_uhKfYu9dgmdKDQ468.jpg,False,,[],{},link,,False,,1588663605.0,richtext,6,,,text,davidadrian.cc,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?auto=webp&amp;s=a22bc4afe98b59f11f8fa387caf73189ef42f14d', 'width': 1280, 'height': 680}, 'resolutions': [{'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e6e82d5bbf0a3820d8d8eb17e3fd754b35e4ac56', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92e3fb7a5c5ff4ab7e274dbaf81e4b7823f18a63', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4a74253bec22aaa9e49a032cf48be9398c99380', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7169cf9cde1d5e0a5e9b1c4ae807fefe24837678', 'width': 640, 'height': 340}, {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cb6d0636be0a5e5873513268285af9b195a9372', 'width': 960, 'height': 510}, {'url': 'https://external-preview.redd.it/Zja4Fg26wv2zlOFG167soKNFxTWKYS6aQIXhdx7XNFg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=600e37f5162877d24f89d90c4a2a70aaf57c3b4e', 'width': 1080, 'height': 573}], 'variants': {}, 'id': '0T-6fPGz1XZZ8w6mECljWo6yKiymychaufv3xQ8IInQ'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdmmdj,True,,thegurus,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdmmdj/the_definitive_data_scientist_setup/,all_ads,False,https://davidadrian.cc/definitive-data-scientist-setup/,155203,1588634805.0,0,,False,,,,
,learnmachinelearning,"what type of regression would I do if I wanted to predict a ratio?  for example,  the ratio I want to predict is total sold/total inventory.  I have data for sold and I have data for inventory, and the number that I want is the ratio. I know I can predict sold, but is there a way to predict the ratio instead?",t2_i6go6an,False,,0,False,how do I predict ratios?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdrdsy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588682981.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;what type of regression would I do if I wanted to predict a ratio?  for example,  the ratio I want to predict is total sold/total inventory.  I have data for sold and I have data for inventory, and the number that I want is the ratio. I know I can predict sold, but is there a way to predict the ratio instead?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdrdsy,True,,mrdlau,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdrdsy/how_do_i_predict_ratios/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdrdsy/how_do_i_predict_ratios/,155203,1588654181.0,0,,False,,,,
,learnmachinelearning,"suppose, i'm trying to measure the impact of certain characteristics on sales.  my dependent variable is 0/1 - not buy/buy.  my predictors are a variety of things:  customer characteristics (categorical), cost(continuous), discounts(yes/no) and discount amount(continuous).

If I want to measure the impact that each predictor has on sales, I would use a logistic regression and use the coefficients as a measure.  Besides that, are their any other techniques I can use? I know that trees and such can be used for classification problem...but in terms of understanding the impacts of the predictor, what other options do I have?",t2_i6go6an,False,,0,False,"besides logistic regression, what other methods are there to measure the impact of predictor on my dependent (in a classification problem)",[],r/learnmachinelearning,False,6,,0,,False,t3_gdrc00,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588682736.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;suppose, i&amp;#39;m trying to measure the impact of certain characteristics on sales.  my dependent variable is 0/1 - not buy/buy.  my predictors are a variety of things:  customer characteristics (categorical), cost(continuous), discounts(yes/no) and discount amount(continuous).&lt;/p&gt;

&lt;p&gt;If I want to measure the impact that each predictor has on sales, I would use a logistic regression and use the coefficients as a measure.  Besides that, are their any other techniques I can use? I know that trees and such can be used for classification problem...but in terms of understanding the impacts of the predictor, what other options do I have?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdrc00,True,,mrdlau,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdrc00/besides_logistic_regression_what_other_methods/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdrc00/besides_logistic_regression_what_other_methods/,155203,1588653936.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,Latest from Facebook researchers: Consistent Video Depth Estimation!,[],r/learnmachinelearning,False,6,,0,35.0,False,t3_gdo1ee,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/49QOJRWxU93YvPvFD0zg363Ld2eAjXf794eXn3ySSQw.jpg,False,,[],{},link,,False,,1588668922.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?auto=webp&amp;s=0cf4e159b7ad1d8961630a9fbbc84c7c47e3ebfd', 'width': 1412, 'height': 362}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6f572be171fe528a06215d037185be5e8c297d4', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b3a18f52f2bd124b3499a3df6e92a1c005a65ba', 'width': 216, 'height': 55}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4306107b39812cb82b31955c393d4a102a2cc57', 'width': 320, 'height': 82}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70888f76cf6ddb0173d583c0dc9f2d4729ef0f19', 'width': 640, 'height': 164}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=180373670304a559a7f53e250e62ab0ad52d7099', 'width': 960, 'height': 246}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0838c25150012c5dcacca7fa6885729f2a17d10', 'width': 1080, 'height': 276}], 'variants': {}, 'id': 'QUzT--qm9SODfwvgEO5SXUyafA5Pf238gBPo31L9LPE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdo1ee,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdo1ee/latest_from_facebook_researchers_consistent_video/,all_ads,False,/r/LatestInML/comments/gdnfzn/latest_from_facebook_researchers_consistent_video/,155203,1588640122.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Latest from Facebook researchers: Consistent Video Depth Estimation!\n\nFor project and code: [click here](https://www.catalyzex.com/paper/arxiv:2004.15021)\n\nhttps://reddit.com/link/gdnfzn/video/jjdhe5ogauw41/player\n\nThe video depth estimation is fully dense, globally scale-consistent, and capable of handling dynamically moving objects', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Latest from Facebook researchers: Consistent Video Depth Estimation!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 35, 'hide_score': False, 'media_metadata': {'jjdhe5ogauw41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gdnfzn/asset/jjdhe5ogauw41/DASHPlaylist.mpd', 'x': 390, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/gdnfzn/asset/jjdhe5ogauw41/HLSPlaylist.m3u8', 'id': 'jjdhe5ogauw41', 'isGif': False}}, 'name': 't3_gdnfzn', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 25, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 25, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/49QOJRWxU93YvPvFD0zg363Ld2eAjXf794eXn3ySSQw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588666662.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Latest from Facebook researchers: Consistent Video Depth Estimation!&lt;/p&gt;\n\n&lt;p&gt;For project and code: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.15021""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/gdnfzn/video/jjdhe5ogauw41/player""&gt;https://reddit.com/link/gdnfzn/video/jjdhe5ogauw41/player&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The video depth estimation is fully dense, globally scale-consistent, and capable of handling dynamically moving objects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?auto=webp&amp;s=0cf4e159b7ad1d8961630a9fbbc84c7c47e3ebfd', 'width': 1412, 'height': 362}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6f572be171fe528a06215d037185be5e8c297d4', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b3a18f52f2bd124b3499a3df6e92a1c005a65ba', 'width': 216, 'height': 55}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4306107b39812cb82b31955c393d4a102a2cc57', 'width': 320, 'height': 82}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=70888f76cf6ddb0173d583c0dc9f2d4729ef0f19', 'width': 640, 'height': 164}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=180373670304a559a7f53e250e62ab0ad52d7099', 'width': 960, 'height': 246}, {'url': 'https://external-preview.redd.it/ZekGPrOZR70942ItuOvpwBJOnVCggSswZrke3H0HhWg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0838c25150012c5dcacca7fa6885729f2a17d10', 'width': 1080, 'height': 276}], 'variants': {}, 'id': 'QUzT--qm9SODfwvgEO5SXUyafA5Pf238gBPo31L9LPE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gdnfzn', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gdnfzn/latest_from_facebook_researchers_consistent_video/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gdnfzn/latest_from_facebook_researchers_consistent_video/', 'subreddit_subscribers': 3386, 'created_utc': 1588637862.0, 'num_crossposts': 8, 'media': None, 'is_video': False}]",t3_gdnfzn,,
,learnmachinelearning," 

Hello all!

I'm new in machine learning, I'm working on LSTM.

According to Tensorflow library, we have this:

    tf.keras.layers.LSTM( 
     units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, Â  Â  kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', Â  Â  bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, Â  Â  recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, Â  Â  kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, Â  Â  dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, Â  Â  return_state=False, go_backwards=False, stateful=False, time_major=False, Â  Â  unroll=False, **kwargs )

it says the activation function is 'tanh' while LSTM has many hidden layers that use 'sigmoid' and 'tanh' activation functions! How this can be ?

Another question: I know I can modify these parameters, but is there any tutorial or a guide to make a reasonable changes to the code?

Thank you

**1 Comment**",t2_ejuk030,False,,0,False,LSTM parameters,[],r/learnmachinelearning,False,6,,0,,False,t3_gdr6q7,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588682016.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new in machine learning, I&amp;#39;m working on LSTM.&lt;/p&gt;

&lt;p&gt;According to Tensorflow library, we have this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tf.keras.layers.LSTM( 
 units, activation=&amp;#39;tanh&amp;#39;, recurrent_activation=&amp;#39;sigmoid&amp;#39;, use_bias=True, Â  Â  kernel_initializer=&amp;#39;glorot_uniform&amp;#39;, recurrent_initializer=&amp;#39;orthogonal&amp;#39;, Â  Â  bias_initializer=&amp;#39;zeros&amp;#39;, unit_forget_bias=True, kernel_regularizer=None, Â  Â  recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, Â  Â  kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, Â  Â  dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, Â  Â  return_state=False, go_backwards=False, stateful=False, time_major=False, Â  Â  unroll=False, **kwargs )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it says the activation function is &amp;#39;tanh&amp;#39; while LSTM has many hidden layers that use &amp;#39;sigmoid&amp;#39; and &amp;#39;tanh&amp;#39; activation functions! How this can be ?&lt;/p&gt;

&lt;p&gt;Another question: I know I can modify these parameters, but is there any tutorial or a guide to make a reasonable changes to the code?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 Comment&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdr6q7,True,,PhD_student900,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdr6q7/lstm_parameters/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdr6q7/lstm_parameters/,155203,1588653216.0,0,,False,,,,
,learnmachinelearning,"I'm working on a project in where I need to use regression method on MRI images of patient. There is a csv file containing certain parameters related to each patients MRI images. So I need to predict these parameters using regression with deep learning. I need help on how to approach. No. Of parameters to predict are 3 and each patient has T1, T2 and there segmentation image files. Kindly help",t2_3stufnwj,False,,0,False,Multivariate regression using deep learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gdqxja,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588680853.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project in where I need to use regression method on MRI images of patient. There is a csv file containing certain parameters related to each patients MRI images. So I need to predict these parameters using regression with deep learning. I need help on how to approach. No. Of parameters to predict are 3 and each patient has T1, T2 and there segmentation image files. Kindly help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdqxja,True,,Troy1729,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdqxja/multivariate_regression_using_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdqxja/multivariate_regression_using_deep_learning/,155203,1588652053.0,0,,False,,,,
,learnmachinelearning,"I'm working on a project in where I need to use regression method on MRI images of patient. There is a csv file containing certain parameters related to each patients MRI images. So I need to predict these parameters using regression with deep learning. I need help on how to approach. No. Of parameters to predict are 3 and each patient has T1, T2 and there segmentation image files. Kindly help",t2_3stufnwj,False,,0,False,Multivariate regression using deep learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gdqxj9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588680853.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project in where I need to use regression method on MRI images of patient. There is a csv file containing certain parameters related to each patients MRI images. So I need to predict these parameters using regression with deep learning. I need help on how to approach. No. Of parameters to predict are 3 and each patient has T1, T2 and there segmentation image files. Kindly help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdqxj9,True,,Troy1729,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdqxj9/multivariate_regression_using_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdqxj9/multivariate_regression_using_deep_learning/,155203,1588652053.0,0,,False,,,,
,learnmachinelearning,,t2_2vj8x0za,False,,0,False,I'm trying the KMeans Clustering algorithm. I couldn't decide the elbow for the optimal K. Is it 2 or 8?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,81.0,False,t3_gdqvef,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/oYaX9a-DVt-1fFH8n0ddlERbtaHcQ3v-tGXX0FlzAXg.jpg,False,,[],{},image,,False,,1588680585.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/7ydagje6fvw41.png?auto=webp&amp;s=74e060cbb4d83cd32181345798dc9e004811a9cd', 'width': 903, 'height': 528}, 'resolutions': [{'url': 'https://preview.redd.it/7ydagje6fvw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bc9c804e1b1b141f159b93c86d7a38a44176760', 'width': 108, 'height': 63}, {'url': 'https://preview.redd.it/7ydagje6fvw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4fb0232da0d4ca69d60d84d22ad2c07231f0828', 'width': 216, 'height': 126}, {'url': 'https://preview.redd.it/7ydagje6fvw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c09036b9db90461aec9cc9d575a243dea94935cf', 'width': 320, 'height': 187}, {'url': 'https://preview.redd.it/7ydagje6fvw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6f257c03820a30d7153b83c5e1ad0a9ac10f7e63', 'width': 640, 'height': 374}], 'variants': {}, 'id': 'unrhhS8Jj5SvEyG9bhQeVb_YBRV-AcMV8mVK7MEHq4E'}], 'enabled': True}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdqvef,True,,wade_wilson2120,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdqvef/im_trying_the_kmeans_clustering_algorithm_i/,all_ads,False,https://i.redd.it/7ydagje6fvw41.png,155203,1588651785.0,0,,False,,,,
,learnmachinelearning,"Assume my input size is 100, and I set my batch to 20. Is the neural network going to behave in teh following manner?

1. Weights and Biases and randomly initiated

2. It takes input 1 of 20 and feeds the network obviously ending up with the wrong output 

3. It takes input 2 of 20 and does the same

skip buncha steps and then finally:

- It takes input 20 of 20, receives a wrong answer AND THEN does backpropagation to update the parameters?

Is that correct? How is this helpful? Woudn't it be better if it took one input, updated the weights and then took another (stochastic GD)?

Or is my understanding of batch incorrect? Thanks in advance. Im assuming it keeps getting a bad result because the weights are never updated after each instance, only after the entire batch of 20 are passed. Meaning that the weights and biases only get updated 5 times in this example.",t2_5923b7w,False,,0,False,Question on Batch gradient descent,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdmzne,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588664946.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Assume my input size is 100, and I set my batch to 20. Is the neural network going to behave in teh following manner?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Weights and Biases and randomly initiated&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It takes input 1 of 20 and feeds the network obviously ending up with the wrong output &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It takes input 2 of 20 and does the same&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;skip buncha steps and then finally:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It takes input 20 of 20, receives a wrong answer AND THEN does backpropagation to update the parameters?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is that correct? How is this helpful? Woudn&amp;#39;t it be better if it took one input, updated the weights and then took another (stochastic GD)?&lt;/p&gt;

&lt;p&gt;Or is my understanding of batch incorrect? Thanks in advance. Im assuming it keeps getting a bad result because the weights are never updated after each instance, only after the entire batch of 20 are passed. Meaning that the weights and biases only get updated 5 times in this example.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gdmzne,True,,cheechuu,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdmzne/question_on_batch_gradient_descent/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdmzne/question_on_batch_gradient_descent/,155203,1588636146.0,0,,False,,,,
,learnmachinelearning," Next fall semester I will be taking my first Machine Learning course that will be going over applications specifically in the field of robotics. I wanted to know some of the best resources I can use to better prepare myself for a course like this. Here is a list of the topics:

Online learning, Online Convex Optimization, Online Supervised Learning, Multi-Armed Bandit, Reinforcement Learning, Inverse Reinforcement Learning, POMDP, Gaussian Processes, Conditional Random Fields, Recurrent Neural Networks, Variational Autoencoders, Generative Adversarial Networks.",t2_3qup4b7w,False,,0,False,Advice for first ML course,[],r/learnmachinelearning,False,6,,0,,False,t3_gdpejw,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588674334.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Next fall semester I will be taking my first Machine Learning course that will be going over applications specifically in the field of robotics. I wanted to know some of the best resources I can use to better prepare myself for a course like this. Here is a list of the topics:&lt;/p&gt;

&lt;p&gt;Online learning, Online Convex Optimization, Online Supervised Learning, Multi-Armed Bandit, Reinforcement Learning, Inverse Reinforcement Learning, POMDP, Gaussian Processes, Conditional Random Fields, Recurrent Neural Networks, Variational Autoencoders, Generative Adversarial Networks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdpejw,True,,addem26,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdpejw/advice_for_first_ml_course/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdpejw/advice_for_first_ml_course/,155203,1588645534.0,0,,False,,,,True
,learnmachinelearning,,t2_eedd5,False,,0,False,Hi r/learnmachinelearning! I built a visual clustering algo that organizes a random sets of images into visually similar groups. What cool things could I use this for?,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gcqphx,False,dark,0.99,,public,693,1,{},140.0,,False,[],,True,False,,{},,False,693,,False,https://a.thumbs.redditmedia.com/IF_juKnWDQ5r9GLUaoj5PBneJho2l3ZOg9tw3WKtYo4.jpg,False,,[],{'gid_1': 1},image,,False,,1588541808.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?auto=webp&amp;s=21e448b772086a7219373d35cdef862e3dee8184', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fc9329213dc9427c72d2857ba7ad0055965de738', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=324740cb3110d4dbf50a04715459c4c5b50315ec', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=568b0989c2128c9fde5bc5d6379e7d995bb76882', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6316cbf152a140103edb8e22ede6711b2529b858', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=abd56901f54fe018624ea3f3f9f520c013851d2e', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/vn1ori4xyjw41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=133745fe8885ba9eef5436a79665fdf1d3f8a06f', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'sXcSD40_E0DYm2-rTNLV0cgeqbt_-wSUCU14gZ3yiIc'}], 'enabled': True}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 100, 'id': 'gid_1', 'penny_donate': None, 'coin_reward': 0, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/silver_512.png', 'days_of_premium': 0, 'icon_height': 512, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/silver_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/silver_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'start_date': None, 'is_enabled': True, 'description': ""Shows the Silver Award... and that's it."", 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'name': 'Silver', 'icon_format': None, 'award_sub_type': 'GLOBAL', 'penny_price': None, 'award_type': 'global'}]",[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcqphx,True,,_conquistador,,86,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcqphx/hi_rlearnmachinelearning_i_built_a_visual/,all_ads,False,https://i.redd.it/vn1ori4xyjw41.png,155203,1588513008.0,1,,False,,,,
,learnmachinelearning,"I'm a beginner at ML/AI. The only idea I can think of as a project that intrigues me is having  
two FPGAs compete with one another in a game.  
To do this, do I need to have two FPGAs? Or can I divide one into two?

Do I need it to be PCIe to ""view"" the game output?

I'm considering getting this:

[https://www.xilinx.com/products/boards-and-kits/ek-k7-kc705-g.html#overview](https://www.xilinx.com/products/boards-and-kits/ek-k7-kc705-g.html#overview)

I understand this might be a big project and ""not for beginners""... but I'm motivated and willing to spend a lot of time on this. Any help is appreciated.",t2_13g9il,False,,0,False,What do I need to have two FPGAs compete with one another in a game?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdnrmb,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588667881.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a beginner at ML/AI. The only idea I can think of as a project that intrigues me is having&lt;br/&gt;
two FPGAs compete with one another in a game.&lt;br/&gt;
To do this, do I need to have two FPGAs? Or can I divide one into two?&lt;/p&gt;

&lt;p&gt;Do I need it to be PCIe to &amp;quot;view&amp;quot; the game output?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m considering getting this:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.xilinx.com/products/boards-and-kits/ek-k7-kc705-g.html#overview""&gt;https://www.xilinx.com/products/boards-and-kits/ek-k7-kc705-g.html#overview&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I understand this might be a big project and &amp;quot;not for beginners&amp;quot;... but I&amp;#39;m motivated and willing to spend a lot of time on this. Any help is appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BdK7Lj_Yndkp_-EUE8DZSaWyp6wuPPR2u4c4j1cxwLs.jpg?auto=webp&amp;s=629bc54c5c06d8c0845fc68a4298d3c8d652f613', 'width': 720, 'height': 531}, 'resolutions': [{'url': 'https://external-preview.redd.it/BdK7Lj_Yndkp_-EUE8DZSaWyp6wuPPR2u4c4j1cxwLs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f843a72fb2e7f5c4d54511d3dd7876f08191f40d', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/BdK7Lj_Yndkp_-EUE8DZSaWyp6wuPPR2u4c4j1cxwLs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b173b45dc746d9668e407b8b0621b105c2f8f862', 'width': 216, 'height': 159}, {'url': 'https://external-preview.redd.it/BdK7Lj_Yndkp_-EUE8DZSaWyp6wuPPR2u4c4j1cxwLs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db07ec4e5aee0ab5d2d3563a0f83a4e3ef56cc8b', 'width': 320, 'height': 236}, {'url': 'https://external-preview.redd.it/BdK7Lj_Yndkp_-EUE8DZSaWyp6wuPPR2u4c4j1cxwLs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=038c96c7e15db27e17a167dce50b2776f8427b6c', 'width': 640, 'height': 472}], 'variants': {}, 'id': 'CpgFH2IoKMX6aPe6O653E_5CaLVdNc8zucbH2xCuPxc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdnrmb,True,,linuxman1929,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdnrmb/what_do_i_need_to_have_two_fpgas_compete_with_one/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdnrmb/what_do_i_need_to_have_two_fpgas_compete_with_one/,155203,1588639081.0,0,,False,,,,
,learnmachinelearning,,t2_6d2bvtf3,False,,0,False,"Enter the new era of Hybrid AI Models optimized by Deep NeuroEvolution, with a complete toolkit of ML, DL &amp; AI models","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_gdjce2,False,light,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/zIyafkUVjLZPri8c0QpyAeqAvsJ0PkuO8yMN2ViDD1Y.jpg,False,,[],{},link,,False,,1588652591.0,richtext,6,,,text,theroboticszone.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lxgyjUgT96Vi9-jzpzVVrLZDuq7TpJZmF_Q_OGciPjM.jpg?auto=webp&amp;s=79c9bdb877cd6b3afed62515267b5a4b6aa84558', 'width': 750, 'height': 422}, 'resolutions': [{'url': 'https://external-preview.redd.it/lxgyjUgT96Vi9-jzpzVVrLZDuq7TpJZmF_Q_OGciPjM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31bb943da897bdda1def46ac26d491dce72aaff0', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/lxgyjUgT96Vi9-jzpzVVrLZDuq7TpJZmF_Q_OGciPjM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7bf947f3c16e9da4237db6dc23eeb9b2dda25c6', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/lxgyjUgT96Vi9-jzpzVVrLZDuq7TpJZmF_Q_OGciPjM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47603827f76d637bfd5c992d73cca977c257ba6d', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/lxgyjUgT96Vi9-jzpzVVrLZDuq7TpJZmF_Q_OGciPjM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c13f28df952d3201ce4a5f6d920202bb79a3e33a', 'width': 640, 'height': 360}], 'variants': {}, 'id': 'AJjTH46Tng-UlvvlPi21NWrFjmUXEE1MKvig7BmZno0'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gdjce2,True,,TheRoboticsZone,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdjce2/enter_the_new_era_of_hybrid_ai_models_optimized/,all_ads,False,https://theroboticszone.com/2020/05/02/artificial-intelligence-masterclass/,155203,1588623791.0,0,,False,,,,
,learnmachinelearning,"\^\^\^

I'm a complete beginner to machine learning, but I do have some Python knowledge from following DataQuest but I mainly code in C#. I bought both of these books a while ago but I'm not sure with which one to start? I'd say my math skills are pretty average but I've been working through [this book](https://www.manning.com/books/math-for-programmers) just fine.",t2_r1d4m,False,,0,False,"Which book to start with? ""Hands on machine learning"" vs ""data science from scratch""","[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdhezg,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},self,,True,,1588646627.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;^^^&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a complete beginner to machine learning, but I do have some Python knowledge from following DataQuest but I mainly code in C#. I bought both of these books a while ago but I&amp;#39;m not sure with which one to start? I&amp;#39;d say my math skills are pretty average but I&amp;#39;ve been working through &lt;a href=""https://www.manning.com/books/math-for-programmers""&gt;this book&lt;/a&gt; just fine.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4VgHK7CRNUtrr5nlUQNiRIlfKxS4ZsDU90HCEbQnnZE.jpg?auto=webp&amp;s=7f3adb0a66f3009d04286764a183a952bdf8aead', 'width': 274, 'height': 274}, 'resolutions': [{'url': 'https://external-preview.redd.it/4VgHK7CRNUtrr5nlUQNiRIlfKxS4ZsDU90HCEbQnnZE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5151cba1360e724811f13c04739c82199067f0be', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/4VgHK7CRNUtrr5nlUQNiRIlfKxS4ZsDU90HCEbQnnZE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1f3c1e00902068d25a684f2c9e7db4a970279ea', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'zhHGpXS_Ua8ciIh8Rg6Er-c2LI2bX9vslY5V1pqiE9U'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gdhezg,True,,chipsmacoy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdhezg/which_book_to_start_with_hands_on_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdhezg/which_book_to_start_with_hands_on_machine/,155203,1588617827.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,Great dataset recently released for the autonomous vehicle industry: Audi Autonomous Driving Dataset (A2D2)!,[],r/learnmachinelearning,False,6,,0,28.0,False,t3_gdl1of,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/48XKcnSyQujbRVj2JMRREolAjKAhl5Z-3OzeN72W2ls.jpg,False,,[],{},link,,False,,1588658089.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?auto=webp&amp;s=97ca536b8b4be79c0aa7c31dc8379a2a2c7808c1', 'width': 1324, 'height': 548}, 'resolutions': [{'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b588094061f949356b4f866e093d665e4a05e6ba', 'width': 108, 'height': 44}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6be00d301458133b80a18feab73760edf49ccfda', 'width': 216, 'height': 89}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f5308e9ea72b56f91099a67bbb104ff509f77f8', 'width': 320, 'height': 132}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e33d1fbe730295d084bacfa2a0ef0ce362bf43b', 'width': 640, 'height': 264}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=614b711442d480c040f490dec9b0ce8d5192934b', 'width': 960, 'height': 397}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c1c6a1f21b8658bb4b5f837d9083d0c6b89be77', 'width': 1080, 'height': 447}], 'variants': {}, 'id': 'IqkkgDo8hSoesujn1FMfV5iJAsyLVPIStIDT0xRg1qM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdl1of,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdl1of/great_dataset_recently_released_for_the/,all_ads,False,/r/LatestInML/comments/gdkyf6/great_dataset_recently_released_for_the/,155203,1588629289.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Link for project and dataset: [click here](https://www.catalyzex.com/paper/arxiv:2004.06320?fbclid=IwAR0A_PpcPTj13MwuNu0C3L-9KCXglJL-NoIbx221wDIx5Ll_4OudpQ6tIno)\n\nhttps://preview.redd.it/3cbn3h52ktw41.png?width=1920&amp;format=png&amp;auto=webp&amp;s=56e79cbd5717bd7a86072b4ec4c281edaf6be30e\n\nThe dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Great dataset recently released for the autonomous vehicle industry: Audi Autonomous Driving Dataset (A2D2)!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 28, 'hide_score': False, 'media_metadata': {'3cbn3h52ktw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 21, 'x': 108, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=96992d037351b917cf2ca6dd206ee1e49e164d8d'}, {'y': 43, 'x': 216, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e48d5c4b95240abbe0157c4b3b8507153e3087ac'}, {'y': 64, 'x': 320, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=83db95f7a913e824a0e94cb0d7363bdcdf06a004'}, {'y': 128, 'x': 640, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=199d25b9ab29b28e8affc8b39655d6c4d3a3ffc6'}, {'y': 192, 'x': 960, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58bccbfa7645c031fa231b9a112886edd4b2bb5b'}, {'y': 216, 'x': 1080, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bab6596b701077d2406b7344531f49f2d683155a'}], 's': {'y': 384, 'x': 1920, 'u': 'https://preview.redd.it/3cbn3h52ktw41.png?width=1920&amp;format=png&amp;auto=webp&amp;s=56e79cbd5717bd7a86072b4ec4c281edaf6be30e'}, 'id': '3cbn3h52ktw41'}}, 'name': 't3_gdkyf6', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 11, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/48XKcnSyQujbRVj2JMRREolAjKAhl5Z-3OzeN72W2ls.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588657787.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Link for project and dataset: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.06320?fbclid=IwAR0A_PpcPTj13MwuNu0C3L-9KCXglJL-NoIbx221wDIx5Ll_4OudpQ6tIno""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/3cbn3h52ktw41.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56e79cbd5717bd7a86072b4ec4c281edaf6be30e""&gt;https://preview.redd.it/3cbn3h52ktw41.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56e79cbd5717bd7a86072b4ec4c281edaf6be30e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?auto=webp&amp;s=97ca536b8b4be79c0aa7c31dc8379a2a2c7808c1', 'width': 1324, 'height': 548}, 'resolutions': [{'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b588094061f949356b4f866e093d665e4a05e6ba', 'width': 108, 'height': 44}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6be00d301458133b80a18feab73760edf49ccfda', 'width': 216, 'height': 89}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f5308e9ea72b56f91099a67bbb104ff509f77f8', 'width': 320, 'height': 132}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e33d1fbe730295d084bacfa2a0ef0ce362bf43b', 'width': 640, 'height': 264}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=614b711442d480c040f490dec9b0ce8d5192934b', 'width': 960, 'height': 397}, {'url': 'https://external-preview.redd.it/WODzbR6CMBBfnFEqlytZ7W0RsXQ6DcAMaQ14ecB4wZg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c1c6a1f21b8658bb4b5f837d9083d0c6b89be77', 'width': 1080, 'height': 447}], 'variants': {}, 'id': 'IqkkgDo8hSoesujn1FMfV5iJAsyLVPIStIDT0xRg1qM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gdkyf6', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gdkyf6/great_dataset_recently_released_for_the/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gdkyf6/great_dataset_recently_released_for_the/', 'subreddit_subscribers': 3386, 'created_utc': 1588628987.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gdkyf6,,
,learnmachinelearning,"Sorry if this is a noob question, but in every single research paper, blog post, or forum I read, when it gets to training, they always say that they trained on an even number of GPUs like 2, 4, 8, 16, 256, 1024, etc. (Of course training on 1 GPU is the exception).

Why don't they train on, say, 3 GPUs or 11 GPUs or 57 GPUs or 123 GPUs? Does an even number of GPUs provide some special kind of benefit?",t2_slnv7,False,,0,False,Why does everyone train on an even number of GPUs?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd5x6w,False,dark,0.89,,public,18,0,{},,,False,[],,False,False,,{},Question,False,18,,False,self,False,,[],{},,,True,,1588600251.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this is a noob question, but in every single research paper, blog post, or forum I read, when it gets to training, they always say that they trained on an even number of GPUs like 2, 4, 8, 16, 256, 1024, etc. (Of course training on 1 GPU is the exception).&lt;/p&gt;

&lt;p&gt;Why don&amp;#39;t they train on, say, 3 GPUs or 11 GPUs or 57 GPUs or 123 GPUs? Does an even number of GPUs provide some special kind of benefit?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gd5x6w,True,,parrot15,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd5x6w/why_does_everyone_train_on_an_even_number_of_gpus/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd5x6w/why_does_everyone_train_on_an_even_number_of_gpus/,155203,1588571451.0,0,,False,,,,
,learnmachinelearning,"With most internships being rescinded, I think it's best if I work on enhancing my skill set over summer break. In this spring semester I developed an interest for neural networks using PyTorch and specifically in the field of NLP. I'd love to explore more and I'd like to ask everyone for suggestions on resources like books, blogs, etc. to help me with that. 

Besides that, I want to learn following technologies in the summer: TensorFlow, PySpark, and Advanced SQL. I already know Python, and R very well but if there are other languages that I should know then please let me know about that as well. Also, if there are use cases that I should read then I'd love suggestions on that too. Thanks in advance!",t2_4hrgsaa6,False,,0,False,Suggestions for skill development over summer break,[],r/learnmachinelearning,False,6,,0,,False,t3_gddpx0,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588635174.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;With most internships being rescinded, I think it&amp;#39;s best if I work on enhancing my skill set over summer break. In this spring semester I developed an interest for neural networks using PyTorch and specifically in the field of NLP. I&amp;#39;d love to explore more and I&amp;#39;d like to ask everyone for suggestions on resources like books, blogs, etc. to help me with that. &lt;/p&gt;

&lt;p&gt;Besides that, I want to learn following technologies in the summer: TensorFlow, PySpark, and Advanced SQL. I already know Python, and R very well but if there are other languages that I should know then please let me know about that as well. Also, if there are use cases that I should read then I&amp;#39;d love suggestions on that too. Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gddpx0,True,,justARegularGuy_95,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gddpx0/suggestions_for_skill_development_over_summer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gddpx0/suggestions_for_skill_development_over_summer/,155203,1588606374.0,0,,False,,,,
,learnmachinelearning,"Once upon a time, there was a model. It already had many features, but it wanted more. Then PCA came along

[https://towardsdatascience.com/dont-get-cursed-by-dimensionality-629bca5de3a5](https://towardsdatascience.com/dont-get-cursed-by-dimensionality-629bca5de3a5)

To break the curse do one of the following:

* use dimensionality reduction algorithms: Use [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) or [t-distributed stochastic neighbor embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding),
* calculate Pearson linear correlation or Spearman non-linear correlation and remove correlated features,
* use[ feature selection algorithms ](https://scikit-learn.org/stable/modules/feature_selection.html)and keep only n-most important features,
* get more training data if possible.",t2_k9eg8,False,,0,False,Donâ€™t get cursed by dimensionality,[],r/learnmachinelearning,False,6,,0,,False,t3_gdby0w,False,dark,0.67,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},self,,True,,1588628965.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Once upon a time, there was a model. It already had many features, but it wanted more. Then PCA came along&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/dont-get-cursed-by-dimensionality-629bca5de3a5""&gt;https://towardsdatascience.com/dont-get-cursed-by-dimensionality-629bca5de3a5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To break the curse do one of the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;use dimensionality reduction algorithms: Use &lt;a href=""https://en.wikipedia.org/wiki/Principal_component_analysis""&gt;Principal Component Analysis (PCA)&lt;/a&gt; or &lt;a href=""https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding""&gt;t-distributed stochastic neighbor embedding (t-SNE)&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;calculate Pearson linear correlation or Spearman non-linear correlation and remove correlated features,&lt;/li&gt;
&lt;li&gt;use&lt;a href=""https://scikit-learn.org/stable/modules/feature_selection.html""&gt; feature selection algorithms &lt;/a&gt;and keep only n-most important features,&lt;/li&gt;
&lt;li&gt;get more training data if possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?auto=webp&amp;s=cafca7e453b92c4b22156eb13bc0fd5cd9e70458', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4265bcb7823a141a9876205b7606c185a4c7c1ca', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef207c80bf65866a3fa40d685f004392169834e7', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22bbc98921c3d3181d42d945cfd771602b78cfa6', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fc52f700f17286a1517ee6297b491c559956032', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ed826c8867c3197cc6a5a69f6a82088ca1d6600', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/IYhad8DLfm34tfld8l366QyQ4qme5LlMUP1RqFqyu9g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98908bbef7bbf3fb1411825991bba5b94ac924c3', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'PWbuUsozhPIn-qezRIiDUpR7KCaFYY_p2ZQD0EDsFjY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdby0w,True,,hiphop1987,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdby0w/dont_get_cursed_by_dimensionality/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdby0w/dont_get_cursed_by_dimensionality/,155203,1588600165.0,0,,False,,,,
,learnmachinelearning,,t2_51mclnu7,False,,0,False,Machine Learning Tutorials - From Novice To Pro-#14-Performance Metrics for Classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gd97yb,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,default,False,,[],{},,,False,,1588617312.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd97yb,True,,TheNerdyDevYT,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd97yb/machine_learning_tutorials_from_novice_to/,all_ads,False,https://www.youtube.com/watch?v=dlLHN7rL03w,155203,1588588512.0,0,,False,,,,
,learnmachinelearning,"Hello r/learnmachinelearning! If you have been searching for an open-source, [shinyapps.io](https://shinyapps.io) or Shiny Server Pro alternative for deploying your Shiny-based machine learning models/apps, you are not alone. As a data scientist working for a boutique consulting firm, I was facing the same problem as the budget for model deployment can be quite tight. My solution is to use ShinyProxy together with Docker and a few open-source services and software to build a production-quality, SSL-secured app with monitoring capacity (see the bottom of the post). I primarily use this framework to build apps based on statistical models to make prediction and dashboards that allow the clients to explore different scenarios, and it has been working quite well.

The process was a bit of trial-and-error so I decided to make a series of tutorials to help fellow data scientists (who also moonlight as DevOps) in similar situations. Below is the latest one:

[Securing and monitoring ShinyProxy deployment of R Shiny apps](https://www.databentobox.com/2020/05/03/secure-shinyproxy/)

If you are new to ShinyProxy and Docker, check out the one below:

[Deploying R Shiny apps using ShinyProxy on Windows 10](https://www.databentobox.com/2019/11/05/deploy-r-app-with-shinyproxy/)

Note that although the tutorial is based on R Shiny, this framework can be pivoted to serve other apps such as Dash (Python). If there is enough interest, I will make one for another popular app framework. Hope it helps and looking forward to your feedback and comments.

&amp;#x200B;

https://preview.redd.it/0wzfo4jxhrw41.png?width=812&amp;format=png&amp;auto=webp&amp;s=f4a3cd77839a8a0d3c981abb0693e36fcc22b6cf",t2_4iyfuor6,False,,0,False,Deploying Machine Learning Apps with ShinyProxy (Tutorials),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,69.0,False,t3_gdd5fu,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},Project,False,3,,False,https://b.thumbs.redditmedia.com/zio5S4jIKU589WSuoGU1eXqtOCITglrnAQ_lZvzWHdo.jpg,False,,[],{},,,True,,1588633290.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello &lt;a href=""/r/learnmachinelearning""&gt;r/learnmachinelearning&lt;/a&gt;! If you have been searching for an open-source, &lt;a href=""https://shinyapps.io""&gt;shinyapps.io&lt;/a&gt; or Shiny Server Pro alternative for deploying your Shiny-based machine learning models/apps, you are not alone. As a data scientist working for a boutique consulting firm, I was facing the same problem as the budget for model deployment can be quite tight. My solution is to use ShinyProxy together with Docker and a few open-source services and software to build a production-quality, SSL-secured app with monitoring capacity (see the bottom of the post). I primarily use this framework to build apps based on statistical models to make prediction and dashboards that allow the clients to explore different scenarios, and it has been working quite well.&lt;/p&gt;

&lt;p&gt;The process was a bit of trial-and-error so I decided to make a series of tutorials to help fellow data scientists (who also moonlight as DevOps) in similar situations. Below is the latest one:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.databentobox.com/2020/05/03/secure-shinyproxy/""&gt;Securing and monitoring ShinyProxy deployment of R Shiny apps&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are new to ShinyProxy and Docker, check out the one below:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.databentobox.com/2019/11/05/deploy-r-app-with-shinyproxy/""&gt;Deploying R Shiny apps using ShinyProxy on Windows 10&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that although the tutorial is based on R Shiny, this framework can be pivoted to serve other apps such as Dash (Python). If there is enough interest, I will make one for another popular app framework. Hope it helps and looking forward to your feedback and comments.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/0wzfo4jxhrw41.png?width=812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4a3cd77839a8a0d3c981abb0693e36fcc22b6cf""&gt;https://preview.redd.it/0wzfo4jxhrw41.png?width=812&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4a3cd77839a8a0d3c981abb0693e36fcc22b6cf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gdd5fu,True,,presstofan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdd5fu/deploying_machine_learning_apps_with_shinyproxy/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdd5fu/deploying_machine_learning_apps_with_shinyproxy/,155203,1588604490.0,0,,False,,,"{'0wzfo4jxhrw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/0wzfo4jxhrw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d3b12e4d3e3b092552a802fb923d254efaa9336'}, {'y': 106, 'x': 216, 'u': 'https://preview.redd.it/0wzfo4jxhrw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=306f6efcfd23ff7fb4972cf3a0a5f1b56c227077'}, {'y': 158, 'x': 320, 'u': 'https://preview.redd.it/0wzfo4jxhrw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e61c252e8a27c075b9ca6723f717cc37c203a7d'}, {'y': 316, 'x': 640, 'u': 'https://preview.redd.it/0wzfo4jxhrw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fef908d1292183e816046e0e80f43816fdd2f4d9'}], 's': {'y': 401, 'x': 812, 'u': 'https://preview.redd.it/0wzfo4jxhrw41.png?width=812&amp;format=png&amp;auto=webp&amp;s=f4a3cd77839a8a0d3c981abb0693e36fcc22b6cf'}, 'id': '0wzfo4jxhrw41'}}",
,learnmachinelearning,"Hi, I'm new to ML right now but I want to build on my skills and build a model that is able to recognise branding's on photos.

Any help would be appreciated.",t2_5d5y99zn,False,,0,False,Recognise branding in photos,[],r/learnmachinelearning,False,6,,0,,False,t3_gdjlqx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588653391.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m new to ML right now but I want to build on my skills and build a model that is able to recognise branding&amp;#39;s on photos.&lt;/p&gt;

&lt;p&gt;Any help would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdjlqx,True,,Totach3,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdjlqx/recognise_branding_in_photos/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdjlqx/recognise_branding_in_photos/,155203,1588624591.0,0,,False,,,,
,learnmachinelearning,"Hello community!

I'm new at applied machine learning. I read some machine learning and data science books for last one and half year and thought ML is really a cool domain for working. But I actually didn't work on any real life project.

Last month I started competing on kaggle and surely I'm not performing well. I know it'll take time but my tension is somewhere else...

Honestly speaking, I don't understand math deeply. Where my perception is ML/DL is a math heavy domain. I'm not sure that I'll improve at math in near future.

Is there any chances that without having excellency in math, I could become a good ML engineer?

Should I still continue this path or should I quite. If you suggest to quite me, what else career you would like to suggest me to explore?",t2_2fnh2985,False,,0,False,Is it too quick to quit ML journey?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdjksk,False,light,0.6,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588653313.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello community!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new at applied machine learning. I read some machine learning and data science books for last one and half year and thought ML is really a cool domain for working. But I actually didn&amp;#39;t work on any real life project.&lt;/p&gt;

&lt;p&gt;Last month I started competing on kaggle and surely I&amp;#39;m not performing well. I know it&amp;#39;ll take time but my tension is somewhere else...&lt;/p&gt;

&lt;p&gt;Honestly speaking, I don&amp;#39;t understand math deeply. Where my perception is ML/DL is a math heavy domain. I&amp;#39;m not sure that I&amp;#39;ll improve at math in near future.&lt;/p&gt;

&lt;p&gt;Is there any chances that without having excellency in math, I could become a good ML engineer?&lt;/p&gt;

&lt;p&gt;Should I still continue this path or should I quite. If you suggest to quite me, what else career you would like to suggest me to explore?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdjksk,True,,hpdipto,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdjksk/is_it_too_quick_to_quit_ml_journey/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdjksk/is_it_too_quick_to_quit_ml_journey/,155203,1588624513.0,0,,False,,,,
,learnmachinelearning,"I am trying to make this [repo work](https://github.com/YadiraF/PRNet#joint-3d-face-reconstruction-and-dense-alignment-with-position-map-regression-network).   
Also, I have to install some specific version of TensorFlow, so how can I build my whole environment from scratch in colab.  
By Building environment I mean like we do with conda.",t2_4r7m7how,False,,0,False,How can I run python2.7 on colab?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdbzkq,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588629127.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to make this &lt;a href=""https://github.com/YadiraF/PRNet#joint-3d-face-reconstruction-and-dense-alignment-with-position-map-regression-network""&gt;repo work&lt;/a&gt;.&lt;br/&gt;
Also, I have to install some specific version of TensorFlow, so how can I build my whole environment from scratch in colab.&lt;br/&gt;
By Building environment I mean like we do with conda.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YaeemPiQWqUvKF3JmexXsMUBWqHea6mMuG82BpC7BDA.jpg?auto=webp&amp;s=980c3bff9d0f1fca809b67ab1bc99633d405a910', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/YaeemPiQWqUvKF3JmexXsMUBWqHea6mMuG82BpC7BDA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a58a90388e37f55fad2ff5e05638d4dba77b587c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/YaeemPiQWqUvKF3JmexXsMUBWqHea6mMuG82BpC7BDA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=93ed58761d4ce0ecccc079ac5190922ad8c2fef7', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/YaeemPiQWqUvKF3JmexXsMUBWqHea6mMuG82BpC7BDA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f8c97743750cd6ea7360e6e571fbdfba42da6d3', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'LdR0JGe5cyWZpmhdDghhOa9zUTPaqJbfeNfSx2s0_BQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdbzkq,True,,sparsh-gupta,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdbzkq/how_can_i_run_python27_on_colab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdbzkq/how_can_i_run_python27_on_colab/,155203,1588600327.0,0,,False,,,,
,learnmachinelearning,"I'm sure this question has been asked a million times.  I'm sure some of you are angered I'm even asking it again.. but I am getting discouraged with the wide array of answers I am finding.

How can I work towards getting a job in machine learning with no experience and only a 4 year college degree in business management?

I am willing to invest a large amount of time over the next year or 2.  I can commit up to around 40 hours a week towards this outside of my current full time job.

I know how to code in Java a tiny bit.  I can probably code up a simple calculator app in a few hours.

The highest math I know is precalc.

I am finding a huge range of answers to this question and want to make sure I am spending my time as efficiently as possible.  Right now I work a sales job and would even be willing to change jobs to something that would help push me forward in my path to becoming a machine learning engineer.

Any help is appreciated here.",t2_3nb15290,False,,0,False,Looking for Guidance,[],r/learnmachinelearning,False,6,,0,,False,t3_gdicfu,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1588629211.0,,[],{},,,True,,1588649488.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m sure this question has been asked a million times.  I&amp;#39;m sure some of you are angered I&amp;#39;m even asking it again.. but I am getting discouraged with the wide array of answers I am finding.&lt;/p&gt;

&lt;p&gt;How can I work towards getting a job in machine learning with no experience and only a 4 year college degree in business management?&lt;/p&gt;

&lt;p&gt;I am willing to invest a large amount of time over the next year or 2.  I can commit up to around 40 hours a week towards this outside of my current full time job.&lt;/p&gt;

&lt;p&gt;I know how to code in Java a tiny bit.  I can probably code up a simple calculator app in a few hours.&lt;/p&gt;

&lt;p&gt;The highest math I know is precalc.&lt;/p&gt;

&lt;p&gt;I am finding a huge range of answers to this question and want to make sure I am spending my time as efficiently as possible.  Right now I work a sales job and would even be willing to change jobs to something that would help push me forward in my path to becoming a machine learning engineer.&lt;/p&gt;

&lt;p&gt;Any help is appreciated here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdicfu,True,,deviant191,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdicfu/looking_for_guidance/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdicfu/looking_for_guidance/,155203,1588620688.0,0,,False,,,,
,learnmachinelearning,,t2_koa9o,False,,0,False,The Ultimate Guide to Linear Regression,[],r/learnmachinelearning,False,6,,0,,False,t3_gdblwa,False,dark,0.58,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,default,False,,[],{},,,False,,1588627707.0,text,6,,,text,learningwithdata.com,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdblwa,True,,syrios12,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdblwa/the_ultimate_guide_to_linear_regression/,all_ads,False,https://learningwithdata.com/posts/tylerfolkman/the-ultimate-guide-to-linear-regression/,155203,1588598907.0,0,,False,,,,
,learnmachinelearning,"A write-up about how we used the maiot Core Engine machine learning platform to train on a dataset of 33,000,000 examples using only a few lines of YAML config. Comments and feedback are welcome.

[https://blog.maiot.io/deep\_learning\_33\_million\_with\_few\_lines\_yaml/](https://blog.maiot.io/deep_learning_33_million_with_few_lines_yaml/)

Hope to see many people use the platform to create large scale deep learning pipelines!",t2_64zw96wq,False,,0,False,"Deep Learning 33,000,000 data points using a few lines of YAML",[],r/learnmachinelearning,False,6,,0,,False,t3_gdi2oa,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588648663.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A write-up about how we used the maiot Core Engine machine learning platform to train on a dataset of 33,000,000 examples using only a few lines of YAML config. Comments and feedback are welcome.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.maiot.io/deep_learning_33_million_with_few_lines_yaml/""&gt;https://blog.maiot.io/deep_learning_33_million_with_few_lines_yaml/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hope to see many people use the platform to create large scale deep learning pipelines!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdi2oa,True,,maiot_io,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdi2oa/deep_learning_33000000_data_points_using_a_few/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdi2oa/deep_learning_33000000_data_points_using_a_few/,155203,1588619863.0,0,,False,,,,
,learnmachinelearning,"Hi,

I have to choose the lessons I'll take for the next two years in my university, and I know for sure that I want to take the Machine learning courses. The second field that interests me the most is Embed systems. So I wonder if those two field matches.

\- edited out -

I have a lot of doubts if it has a lot of jobs prospectives.

So I'm here to have your opinions. Thanks :)",t2_2zkqlvun,False,,0,False,Does it make sense to choose machine learning and embed system courses at university,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdhw49,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1588621304.0,,[],{},,,True,,1588648089.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have to choose the lessons I&amp;#39;ll take for the next two years in my university, and I know for sure that I want to take the Machine learning courses. The second field that interests me the most is Embed systems. So I wonder if those two field matches.&lt;/p&gt;

&lt;p&gt;- edited out -&lt;/p&gt;

&lt;p&gt;I have a lot of doubts if it has a lot of jobs prospectives.&lt;/p&gt;

&lt;p&gt;So I&amp;#39;m here to have your opinions. Thanks :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdhw49,True,,Garnaa,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdhw49/does_it_make_sense_to_choose_machine_learning_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdhw49/does_it_make_sense_to_choose_machine_learning_and/,155203,1588619289.0,0,,False,,,,
,learnmachinelearning,I'm in high school right now and I want to learn about this subject over  the summer. I just want to get the best course/certificate possible.,t2_3nb7kkrl,False,,0,False,what is the most sought after machine/deep learning course right now,[],r/learnmachinelearning,False,6,,0,,False,t3_gdgstf,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588644743.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in high school right now and I want to learn about this subject over  the summer. I just want to get the best course/certificate possible.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdgstf,True,,BensonandEdgar,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdgstf/what_is_the_most_sought_after_machinedeep/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdgstf/what_is_the_most_sought_after_machinedeep/,155203,1588615943.0,0,,False,,,,
,learnmachinelearning,"Iâ€™m releasing a new AI music series on my YouTube channel. It teaches you how to generate melodies with neural networks (specifically, Long Short-Term Memory networks). In the process, you'll also be learning about TensorFlow/Keras, time-series data, and symbolic music representations.

Iâ€™m sharing the first video of the course, which provides an overview of the series and introduces some fundamental concepts.

Here's the video: [https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;feature=youtu.be](https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;feature=youtu.be)

Enjoy!",t2_12ahau,False,,0,False,Automatic music generation with neural networks,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdah7s,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},Project,False,3,,False,self,False,,[],{},self,,True,,1588623179.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m releasing a new AI music series on my YouTube channel. It teaches you how to generate melodies with neural networks (specifically, Long Short-Term Memory networks). In the process, you&amp;#39;ll also be learning about TensorFlow/Keras, time-series data, and symbolic music representations.&lt;/p&gt;

&lt;p&gt;Iâ€™m sharing the first video of the course, which provides an overview of the series and introduces some fundamental concepts.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the video: &lt;a href=""https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;amp;feature=youtu.be""&gt;https://www.youtube.com/watch?v=FLr0r-QhqH0&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/seQTA1faKos-dHAYx3eFyQ7hWyxz25hWsyvn-zZyFXw.jpg?auto=webp&amp;s=2b37e434ae9d942e320f990d8fe2031475e324ff', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/seQTA1faKos-dHAYx3eFyQ7hWyxz25hWsyvn-zZyFXw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14b426931929d64ff64d55d406e743e26883a1b1', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/seQTA1faKos-dHAYx3eFyQ7hWyxz25hWsyvn-zZyFXw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=268ad75c10e5fa977ee26e739d7129cdd259e0f9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/seQTA1faKos-dHAYx3eFyQ7hWyxz25hWsyvn-zZyFXw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c0b448087e2898604a6b467db29faa35d802b75', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'hoMVN8WE991tmH9W9xfM883R3GR9mZ3HDupKvsObf5E'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gdah7s,True,,diabulusInMusica,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdah7s/automatic_music_generation_with_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdah7s/automatic_music_generation_with_neural_networks/,155203,1588594379.0,0,,False,,,,
,learnmachinelearning,"Udacity is offering various nano degrees in ML, DS with the collaboration of  AWS, Kaggle but they are charging a lot even though now they are one month off. Do they really worth the spending and how good are the chances of getting any advantage in the job market.  I understand the need for a good portfolio but the course be any advantage?

It will be really helpful if anyone who has taken such course gives their feedback",t2_4mezd8mm,False,,0,False,Udacity Nano Degree - Worth for the money?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdga3b,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588643171.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Udacity is offering various nano degrees in ML, DS with the collaboration of  AWS, Kaggle but they are charging a lot even though now they are one month off. Do they really worth the spending and how good are the chances of getting any advantage in the job market.  I understand the need for a good portfolio but the course be any advantage?&lt;/p&gt;

&lt;p&gt;It will be really helpful if anyone who has taken such course gives their feedback&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdga3b,True,,shuvob4,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdga3b/udacity_nano_degree_worth_for_the_money/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdga3b/udacity_nano_degree_worth_for_the_money/,155203,1588614371.0,0,,False,,,,
,learnmachinelearning,,t2_59kxi4ej,False,,0,False,How to Apply Machine Learning in Demand Forecasting for Retail?,[],r/learnmachinelearning,False,6,,0,79.0,False,t3_gdfzzc,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/ecvj55teJ9CSqNZ-UxZ2nEnWFLx24Ck0PKDdIldk1Yk.jpg,False,,[],{},link,,False,,1588642320.0,text,6,,,text,mobidev.biz,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?auto=webp&amp;s=0be78357f5d688674d284cc9ea7889b1bf1747ad', 'width': 1488, 'height': 840}, 'resolutions': [{'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a3ce46847234f2e03c7625bd15b22d47cd9b8b2', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=60db0730940aa2d10e51cebcff9ef031316d8700', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09ca53d2e9c2eb0062d542bf1fc73edb96ba9166', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee9606d9da930556531635927d55c5910781c817', 'width': 640, 'height': 361}, {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef8cac593efa11da23144b3c75f0435046d68f9c', 'width': 960, 'height': 541}, {'url': 'https://external-preview.redd.it/15fz0RYpGiUqbQxPCSJXvlH5Xwr1H2Zh_YvtGc3B1n0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=25104ada681de2e4f3fc76a680ca6b0eebdb5dfa', 'width': 1080, 'height': 609}], 'variants': {}, 'id': 'fmEKu70f-Iy7dophPGytZWvF-mXsuVkQdk6iuA2YenU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdfzzc,True,,Data-Power,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdfzzc/how_to_apply_machine_learning_in_demand/,all_ads,False,https://mobidev.biz/blog/machine-learning-methods-demand-forecasting-retail?utm_source=reddit&amp;utm_medium=reddit-ds&amp;utm_campaign=reddit-demand,155203,1588613520.0,0,,False,,,,
,learnmachinelearning,,t2_jg7fk,False,,0,False,Iâ€™m trying to see how the prediction output of a NN would change given a change in one input. Would it make more sense to calculate the Jacobian or just change the input and re-predict the output?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdfpy2,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588641476.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gdfpy2,True,,musicman0326,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdfpy2/im_trying_to_see_how_the_prediction_output_of_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdfpy2/im_trying_to_see_how_the_prediction_output_of_a/,155203,1588612676.0,0,,False,,,,
,learnmachinelearning,"So guys, i want to get into and better at ML. I am interested what sources would you recommend for learning theory and practicing? I am interested in podcasts, videos, books, online courses, basically everything. Of course the cheaper it is the better but if you think something is worth the money please recommend it too. I just want to learn as much as possible.",t2_96cokjj,False,,0,False,learning ML resources,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdezel,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588639228.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So guys, i want to get into and better at ML. I am interested what sources would you recommend for learning theory and practicing? I am interested in podcasts, videos, books, online courses, basically everything. Of course the cheaper it is the better but if you think something is worth the money please recommend it too. I just want to learn as much as possible.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdezel,True,,K-S-C-H-I,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdezel/learning_ml_resources/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdezel/learning_ml_resources/,155203,1588610428.0,0,,False,,,,
,learnmachinelearning,I have 2000 one channel 900 x 900 train images and mask (900x900 binary images also) for all images show building. I want to use this mask images for label (1 is building 0 is non-building). And train my pixel values.  How can i use this mask images for a labels in tensorflow ?,t2_3dwu4yfm,False,,0,False,Pixels using for a label on tensorflow,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdehn3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588637667.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have 2000 one channel 900 x 900 train images and mask (900x900 binary images also) for all images show building. I want to use this mask images for label (1 is building 0 is non-building). And train my pixel values.  How can i use this mask images for a labels in tensorflow ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdehn3,True,,cartwhell07,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdehn3/pixels_using_for_a_label_on_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdehn3/pixels_using_for_a_label_on_tensorflow/,155203,1588608867.0,0,,False,,,,
,learnmachinelearning,"Alexa can play our favorite music, read news to us, keep us organized, and assist with shopping. And last, but not least, we can play games on Alexa. 

Currently, games is one of the most popular categories on Amazonâ€™s Alexa skills store. But is the voice assistant's profitability potential limited to entertaining through games? No. Brands can raise brand awareness and drive consumer engagement with the help of such content.

[Find out how voice games are made and see some prominent examples of Alexa games](https://onix-systems.com/blog/exciting-voice-gaming-experiences-with-amazon-alexa)",t2_4q5pfd01,False,,0,False,Games on Alexa,[],r/learnmachinelearning,False,6,,0,,False,t3_gd6w19,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1588605302.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Alexa can play our favorite music, read news to us, keep us organized, and assist with shopping. And last, but not least, we can play games on Alexa. &lt;/p&gt;

&lt;p&gt;Currently, games is one of the most popular categories on Amazonâ€™s Alexa skills store. But is the voice assistant&amp;#39;s profitability potential limited to entertaining through games? No. Brands can raise brand awareness and drive consumer engagement with the help of such content.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://onix-systems.com/blog/exciting-voice-gaming-experiences-with-amazon-alexa""&gt;Find out how voice games are made and see some prominent examples of Alexa games&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?auto=webp&amp;s=1bcf6de541d1c9f2a296e0b0e1c0d07fc7ddee16', 'width': 3542, 'height': 1229}, 'resolutions': [{'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ad8c3952a02a57e7585dc3a1c9dd38baf0374d2', 'width': 108, 'height': 37}, {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83ec41fd603216e101deea90811fca70c6e4fa78', 'width': 216, 'height': 74}, {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=964848fa9054c895d85d1e1196ec27843bf9fe33', 'width': 320, 'height': 111}, {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b18232117d1e32d7f290e3767d4282ff23db3ddd', 'width': 640, 'height': 222}, {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44d67d5f337f16ebfe51a6b69aa22ec0d31f712d', 'width': 960, 'height': 333}, {'url': 'https://external-preview.redd.it/BiL9juY4GIko-YqQuuPHOqi4Xh0SF-B74uLl8KMtjG8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1c79f5aac2b50f664232b8223559da241bd7b35', 'width': 1080, 'height': 374}], 'variants': {}, 'id': 'oLKX4uY25v_MJZfv7SUc46P_MuZRJrcUnadVTOOXr3M'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd6w19,True,,alexandra_moroz,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd6w19/games_on_alexa/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd6w19/games_on_alexa/,155203,1588576502.0,0,,False,,,,
,learnmachinelearning,"As the title says, I have dyscalculia but been a good programmer for 4 years. That's my strength and my weakness is Maths and Statistics.

I'm interested to use Linear Regression and GANs and already have good enough abstract level understanding of how they algorithms works however I stuck at putting them into practice. Mainly because I don't understand the functions of scikit-learn and find it to be math heavy.  I also good at getting data and preparation. 

Is there another library or path any one can suggest? 

Please stop gate keeping, I need a solution.  I hate hearing lin\*!\*r alg\*bra all the times.",t2_5ji12juq,False,,0,False,I have dyscalculia. Please suggest me a path to learn Machine Learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_gde48k,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588636491.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title says, I have dyscalculia but been a good programmer for 4 years. That&amp;#39;s my strength and my weakness is Maths and Statistics.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested to use Linear Regression and GANs and already have good enough abstract level understanding of how they algorithms works however I stuck at putting them into practice. Mainly because I don&amp;#39;t understand the functions of scikit-learn and find it to be math heavy.  I also good at getting data and preparation. &lt;/p&gt;

&lt;p&gt;Is there another library or path any one can suggest? &lt;/p&gt;

&lt;p&gt;Please stop gate keeping, I need a solution.  I hate hearing lin*!*r alg*bra all the times.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gde48k,True,,Silly-Purple,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gde48k/i_have_dyscalculia_please_suggest_me_a_path_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gde48k/i_have_dyscalculia_please_suggest_me_a_path_to/,155203,1588607691.0,0,,False,,,,
,learnmachinelearning,,t2_20sul3l3,False,,0,False,Most Downloaded Artificial Intelligence Research Articles,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_gddtbu,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,False,https://b.thumbs.redditmedia.com/tyJFt3HOoXhuc1eUPwbirks_S6uReMSc_rqpebjRrHc.jpg,False,,[],{},link,,False,,1588635497.0,richtext,6,,,text,brainstormingbox.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?auto=webp&amp;s=321d90a4e70d961876c0caf6e5fc34b76ec9ac16', 'width': 1068, 'height': 601}, 'resolutions': [{'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4f058a9618e67e1745d77d5d1a97affe9ddd722', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=28c78809b952d4d6d5b7917057e4f11731ae1661', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c52015b346f02d130e62aeccb844bf1bd2b1c6aa', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0348b57b326bfbd0118a1baa10cd9ae2b0df2bde', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/8hE4MUl-R6MIvA6fz8Qxagci7dn03aCWaiKxETfb_gg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5947073aeb82e7b1250a21fbef8511c267067d5', 'width': 960, 'height': 540}], 'variants': {}, 'id': 'y-EfDRG_5XOuV68L2Ohuby1oxKpXpBJZ1sCRXobPgkU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gddtbu,True,,saik2363,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gddtbu/most_downloaded_artificial_intelligence_research/,all_ads,False,http://brainstormingbox.org/most-downloaded-artificial-intelligence-research-articles/,155203,1588606697.0,0,,False,,,,
,learnmachinelearning,"I'm interested in machine learning so and I was told to install Tensorflow GPU first, so I followed [This](https://www.youtube.com/watch?v=tPq6NIboLSc) video tutorial and I was getting this error:

    The ordinal 242 could not be located in the dynamic link library C:\Users\username\Anaconda3\Library\bin\mkl_intel_thread.dll

But then I found a stackoverflow thread that said you have to do the command below before starting jupyter notebook, and it worked.

    set CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1

Now I'm getting this error when trying to run something in Jupyter:

    anaconda3\envs\gputest\lib\site-packages\ipykernel_launcher.py:11: UserWarning: No GPU found. Please ensure you have installed TensorFlow correctly
      # This is added back by InteractiveShellApp.init_path()

I've tried to do ""conda install tensorflow-gpu"" again but it days everything is already installed.",t2_ne48c,False,,0,False,Having issues trying to install tensorflow,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gddcx9,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1588633980.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m interested in machine learning so and I was told to install Tensorflow GPU first, so I followed &lt;a href=""https://www.youtube.com/watch?v=tPq6NIboLSc""&gt;This&lt;/a&gt; video tutorial and I was getting this error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The ordinal 242 could not be located in the dynamic link library C:\Users\username\Anaconda3\Library\bin\mkl_intel_thread.dll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But then I found a stackoverflow thread that said you have to do the command below before starting jupyter notebook, and it worked.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;set CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I&amp;#39;m getting this error when trying to run something in Jupyter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;anaconda3\envs\gputest\lib\site-packages\ipykernel_launcher.py:11: UserWarning: No GPU found. Please ensure you have installed TensorFlow correctly
  # This is added back by InteractiveShellApp.init_path()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;#39;ve tried to do &amp;quot;conda install tensorflow-gpu&amp;quot; again but it days everything is already installed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/saILtSWY8H9KkTbUWPLfRewlQMwn7dcQ4I_kcrPbLcU.jpg?auto=webp&amp;s=c19929b17051080c9af5eb7ac4f459fa88e15bf2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/saILtSWY8H9KkTbUWPLfRewlQMwn7dcQ4I_kcrPbLcU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cdf12f8957c3f2f74a1ca63c7746bee6b017cf5d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/saILtSWY8H9KkTbUWPLfRewlQMwn7dcQ4I_kcrPbLcU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c771931f40517d27e05d172896f6c891076b4927', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/saILtSWY8H9KkTbUWPLfRewlQMwn7dcQ4I_kcrPbLcU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6691006c8262dc859375da1f58e4a50845e63f66', 'width': 320, 'height': 240}], 'variants': {}, 'id': '1FBUonhnijn_dgQGTwjK1n2KwFdhOcutiauI60KwvRs'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gddcx9,True,,obliveater95,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gddcx9/having_issues_trying_to_install_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gddcx9/having_issues_trying_to_install_tensorflow/,155203,1588605180.0,0,,False,,,,True
,learnmachinelearning,"I sincerely couldn't find the right subreddit for this, so I decided to post it here. I need a portable laptop. Those of you who for some reason or another have to train simple neural networks locally on a laptop, what laptop do you use, along with what OS? I know Windows supports it but dual-booting Ubuntu does require you to disable nouveau drivers and then install the proprietary NVIDIA drivers but then configuring CUDA requires some tweaks and when you switch to the NVIDIA GPU it drains your battery very fast.",t2_6dqulggm,False,,0,False,Which laptop for training neural networks locally?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdd4e7,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1588633204.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I sincerely couldn&amp;#39;t find the right subreddit for this, so I decided to post it here. I need a portable laptop. Those of you who for some reason or another have to train simple neural networks locally on a laptop, what laptop do you use, along with what OS? I know Windows supports it but dual-booting Ubuntu does require you to disable nouveau drivers and then install the proprietary NVIDIA drivers but then configuring CUDA requires some tweaks and when you switch to the NVIDIA GPU it drains your battery very fast.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gdd4e7,True,,BriefHighlight4,,16,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdd4e7/which_laptop_for_training_neural_networks_locally/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdd4e7/which_laptop_for_training_neural_networks_locally/,155203,1588604404.0,0,,False,,,,
,learnmachinelearning,"I have a vision of using ML to do address matching for a school project. I've taken a few courses in ML but never implemented anything and at a very beginner level.

Originally I was thinking of just doing k-nearest neighbor but with 3,5m + addresses in my country I think the dataset size would quickly be a problem in terms of computational time?

Another approach (arguably more complicated) would be word2vec which I don't completely understand how would be used here. Would I still segment addresses into features as zipcode, city etc and only do the word2vec on the streetname or on the full text address thus only having 1 feature?

&amp;#x200B;

Let's assume I have a labelled training set, what kind of approach is reasonable to take for someone like me with limited experience?

&amp;#x200B;

Thanks for any input!",t2_14ch82,False,,0,False,What approach should I take to address matching?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdcorc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588631675.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a vision of using ML to do address matching for a school project. I&amp;#39;ve taken a few courses in ML but never implemented anything and at a very beginner level.&lt;/p&gt;

&lt;p&gt;Originally I was thinking of just doing k-nearest neighbor but with 3,5m + addresses in my country I think the dataset size would quickly be a problem in terms of computational time?&lt;/p&gt;

&lt;p&gt;Another approach (arguably more complicated) would be word2vec which I don&amp;#39;t completely understand how would be used here. Would I still segment addresses into features as zipcode, city etc and only do the word2vec on the streetname or on the full text address thus only having 1 feature?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s assume I have a labelled training set, what kind of approach is reasonable to take for someone like me with limited experience?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any input!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gdcorc,True,,maldini94,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdcorc/what_approach_should_i_take_to_address_matching/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdcorc/what_approach_should_i_take_to_address_matching/,155203,1588602875.0,0,,False,,,,
,learnmachinelearning,,t2_dlbdc61,False,,0,False,Using ML for copy and paste.,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_gdcftv,False,dark,0.67,,public,1,0,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;4/10 - Cut &amp;amp; paste your surroundings to Photoshop&lt;br&gt;&lt;br&gt;Code: &lt;a href=""https://t.co/cVddH3u3ik""&gt;https://t.co/cVddH3u3ik&lt;/a&gt;&lt;br&gt;&lt;br&gt;Book: &lt;a href=""https://twitter.com/HOLOmagazine?ref_src=twsrc%5Etfw""&gt;@HOLOmagazine&lt;/a&gt;&lt;br&gt;Garment: SS17 by &lt;a href=""https://twitter.com/thekarentopacio?ref_src=twsrc%5Etfw""&gt;@thekarentopacio&lt;/a&gt; &lt;br&gt;Type: Sainte Colombe by &lt;a href=""https://twitter.com/MinetYoann?ref_src=twsrc%5Etfw""&gt;@MinetYoann&lt;/a&gt; &lt;a href=""https://twitter.com/ProductionType?ref_src=twsrc%5Etfw""&gt;@ProductionType&lt;/a&gt;&lt;br&gt;Technical Insights: â†“&lt;a href=""https://twitter.com/hashtag/ML?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#ML&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AR?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AR&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AI&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AIUX?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AIUX&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Adobe?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Adobe&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Photoshop?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Photoshop&lt;/a&gt; &lt;a href=""https://t.co/LkTBe0t0rF""&gt;pic.twitter.com/LkTBe0t0rF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Cyril Diagne (@cyrildiagne) &lt;a href=""https://twitter.com/cyrildiagne/status/1256916982764646402?ref_src=twsrc%5Etfw""&gt;May 3, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'height': 642}",140.0,,False,[],"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/cyrildiagne/status/1256916982764646402', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;4/10 - Cut &amp;amp; paste your surroundings to Photoshop&lt;br&gt;&lt;br&gt;Code: &lt;a href=""https://t.co/cVddH3u3ik""&gt;https://t.co/cVddH3u3ik&lt;/a&gt;&lt;br&gt;&lt;br&gt;Book: &lt;a href=""https://twitter.com/HOLOmagazine?ref_src=twsrc%5Etfw""&gt;@HOLOmagazine&lt;/a&gt;&lt;br&gt;Garment: SS17 by &lt;a href=""https://twitter.com/thekarentopacio?ref_src=twsrc%5Etfw""&gt;@thekarentopacio&lt;/a&gt; &lt;br&gt;Type: Sainte Colombe by &lt;a href=""https://twitter.com/MinetYoann?ref_src=twsrc%5Etfw""&gt;@MinetYoann&lt;/a&gt; &lt;a href=""https://twitter.com/ProductionType?ref_src=twsrc%5Etfw""&gt;@ProductionType&lt;/a&gt;&lt;br&gt;Technical Insights: â†“&lt;a href=""https://twitter.com/hashtag/ML?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#ML&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AR?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AR&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AI&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AIUX?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AIUX&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Adobe?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Adobe&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Photoshop?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Photoshop&lt;/a&gt; &lt;a href=""https://t.co/LkTBe0t0rF""&gt;pic.twitter.com/LkTBe0t0rF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Cyril Diagne (@cyrildiagne) &lt;a href=""https://twitter.com/cyrildiagne/status/1256916982764646402?ref_src=twsrc%5Etfw""&gt;May 3, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Cyril Diagne', 'height': 642, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/cyrildiagne', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,False,,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;4/10 - Cut &amp;amp; paste your surroundings to Photoshop&lt;br&gt;&lt;br&gt;Code: &lt;a href=""https://t.co/cVddH3u3ik""&gt;https://t.co/cVddH3u3ik&lt;/a&gt;&lt;br&gt;&lt;br&gt;Book: &lt;a href=""https://twitter.com/HOLOmagazine?ref_src=twsrc%5Etfw""&gt;@HOLOmagazine&lt;/a&gt;&lt;br&gt;Garment: SS17 by &lt;a href=""https://twitter.com/thekarentopacio?ref_src=twsrc%5Etfw""&gt;@thekarentopacio&lt;/a&gt; &lt;br&gt;Type: Sainte Colombe by &lt;a href=""https://twitter.com/MinetYoann?ref_src=twsrc%5Etfw""&gt;@MinetYoann&lt;/a&gt; &lt;a href=""https://twitter.com/ProductionType?ref_src=twsrc%5Etfw""&gt;@ProductionType&lt;/a&gt;&lt;br&gt;Technical Insights: â†“&lt;a href=""https://twitter.com/hashtag/ML?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#ML&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AR?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AR&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AI&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AIUX?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AIUX&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Adobe?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Adobe&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Photoshop?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Photoshop&lt;/a&gt; &lt;a href=""https://t.co/LkTBe0t0rF""&gt;pic.twitter.com/LkTBe0t0rF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Cyril Diagne (@cyrildiagne) &lt;a href=""https://twitter.com/cyrildiagne/status/1256916982764646402?ref_src=twsrc%5Etfw""&gt;May 3, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdcftv', 'height': 642}",,False,1,,False,https://b.thumbs.redditmedia.com/APfVcc8W4vxWFGMCcXMUVDnbg6T9iN3ulzR560fN-uw.jpg,False,,[],{},link,,False,,1588630791.0,text,6,,,text,twitter.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qajweRIc_PHaoa_U1L6lfoVh-C0NSEH5haeCfD7i3w0.jpg?auto=webp&amp;s=178990c2557ab651175e7713763532fa4e7503a2', 'width': 810, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/qajweRIc_PHaoa_U1L6lfoVh-C0NSEH5haeCfD7i3w0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ab37b123148b9150dfec60ce22c844052566717', 'width': 108, 'height': 160}, {'url': 'https://external-preview.redd.it/qajweRIc_PHaoa_U1L6lfoVh-C0NSEH5haeCfD7i3w0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f863018b12bb689bb045e1e877c1c35d9953185e', 'width': 216, 'height': 320}, {'url': 'https://external-preview.redd.it/qajweRIc_PHaoa_U1L6lfoVh-C0NSEH5haeCfD7i3w0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fa92f2fd152fe9f4b78833a55e8e46565f6249c', 'width': 320, 'height': 474}, {'url': 'https://external-preview.redd.it/qajweRIc_PHaoa_U1L6lfoVh-C0NSEH5haeCfD7i3w0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8aeb1b35e842523d92f6b7a5d5d1ab0cc1f3db41', 'width': 640, 'height': 948}], 'variants': {}, 'id': 'leBkBotvtT_UuZ1GvDfbHa-wl9MSCs-VXBbEsWluGrE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdcftv,True,,jarmex,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdcftv/using_ml_for_copy_and_paste/,all_ads,False,https://twitter.com/cyrildiagne/status/1256916982764646402?s=21,155203,1588601991.0,0,"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/cyrildiagne/status/1256916982764646402', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;4/10 - Cut &amp;amp; paste your surroundings to Photoshop&lt;br&gt;&lt;br&gt;Code: &lt;a href=""https://t.co/cVddH3u3ik""&gt;https://t.co/cVddH3u3ik&lt;/a&gt;&lt;br&gt;&lt;br&gt;Book: &lt;a href=""https://twitter.com/HOLOmagazine?ref_src=twsrc%5Etfw""&gt;@HOLOmagazine&lt;/a&gt;&lt;br&gt;Garment: SS17 by &lt;a href=""https://twitter.com/thekarentopacio?ref_src=twsrc%5Etfw""&gt;@thekarentopacio&lt;/a&gt; &lt;br&gt;Type: Sainte Colombe by &lt;a href=""https://twitter.com/MinetYoann?ref_src=twsrc%5Etfw""&gt;@MinetYoann&lt;/a&gt; &lt;a href=""https://twitter.com/ProductionType?ref_src=twsrc%5Etfw""&gt;@ProductionType&lt;/a&gt;&lt;br&gt;Technical Insights: â†“&lt;a href=""https://twitter.com/hashtag/ML?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#ML&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AR?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AR&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AI&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/AIUX?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#AIUX&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Adobe?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Adobe&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/Photoshop?src=hash&amp;amp;ref_src=twsrc%5Etfw""&gt;#Photoshop&lt;/a&gt; &lt;a href=""https://t.co/LkTBe0t0rF""&gt;pic.twitter.com/LkTBe0t0rF&lt;/a&gt;&lt;/p&gt;&amp;mdash; Cyril Diagne (@cyrildiagne) &lt;a href=""https://twitter.com/cyrildiagne/status/1256916982764646402?ref_src=twsrc%5Etfw""&gt;May 3, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Cyril Diagne', 'height': 642, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/cyrildiagne', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,,,,
,learnmachinelearning,"The deployment of a machine learning (ML) model to production starts with actually building the model, which can be done in several ways and with many tools.

The approach and tools used at the development stage are very important in ensuring a smooth integration of the basic units that make up the machine learning pipeline. If these are not put into consideration before starting a project, thereâ€™s a good chance youâ€™ll end up with an ML system with low efficiency and high latency.

here is the link to the full article:  [https://heartbeat.fritz.ai/deploying-machine-learning-models-on-google-cloud-platform-gcp-7b1ff8140144](https://heartbeat.fritz.ai/deploying-machine-learning-models-on-google-cloud-platform-gcp-7b1ff8140144) ",t2_65e9orsm,False,,0,False,Deploying Machine Learning Models on Google Cloud Platform (GCP),[],r/learnmachinelearning,False,6,,0,,False,t3_gdcff2,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588630752.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The deployment of a machine learning (ML) model to production starts with actually building the model, which can be done in several ways and with many tools.&lt;/p&gt;

&lt;p&gt;The approach and tools used at the development stage are very important in ensuring a smooth integration of the basic units that make up the machine learning pipeline. If these are not put into consideration before starting a project, thereâ€™s a good chance youâ€™ll end up with an ML system with low efficiency and high latency.&lt;/p&gt;

&lt;p&gt;here is the link to the full article:  &lt;a href=""https://heartbeat.fritz.ai/deploying-machine-learning-models-on-google-cloud-platform-gcp-7b1ff8140144""&gt;https://heartbeat.fritz.ai/deploying-machine-learning-models-on-google-cloud-platform-gcp-7b1ff8140144&lt;/a&gt; &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?auto=webp&amp;s=ca0addc48a25ee5b04c92d2cea0f985653f417fc', 'width': 1200, 'height': 818}, 'resolutions': [{'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3f1e5f050888cf8efc96c0a97a8f7776d1628a9', 'width': 108, 'height': 73}, {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d4e2a7f99dedb7ed4394a49d5d9dddae4051775', 'width': 216, 'height': 147}, {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4611dfbdadb70a48d1f8368c898d625d3b3d533', 'width': 320, 'height': 218}, {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba293ad227a05a2655c1db269964f106d7d379f9', 'width': 640, 'height': 436}, {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6559bdac209c705c7eaf5ecd1602476e57546c01', 'width': 960, 'height': 654}, {'url': 'https://external-preview.redd.it/GZojgdJrAMfrCJNfeL-aPvvuC5yDMUYJaCLYFXTcrgM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ee7a311fb9082aa02ce1287b1075c9fa73eaca81', 'width': 1080, 'height': 736}], 'variants': {}, 'id': 'pRzWdJ7MYZujR2Z8y3JZI2MiI0na1acHusB0LaNAfM0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdcff2,True,,bamigbadeopeyemi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdcff2/deploying_machine_learning_models_on_google_cloud/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdcff2/deploying_machine_learning_models_on_google_cloud/,155203,1588601952.0,0,,False,,,,
,learnmachinelearning,"Confused about classification metrics? I just published a post in Towards Data Science on the foundations to help you keep things straight:

[https://towardsdatascience.com/classification-metrics-everyone-should-know-b67fd0044c0c?source=friends\_link&amp;sk=e24aee1dec0a870f749ba3e7f775b431](https://towardsdatascience.com/classification-metrics-everyone-should-know-b67fd0044c0c?source=friends_link&amp;sk=e24aee1dec0a870f749ba3e7f775b431)

Constructive feedback appreciated!",t2_t04vn,False,,0,False,Classification metric foundations explained,[],r/learnmachinelearning,False,6,,0,,False,t3_gdc2b0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588629414.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Confused about classification metrics? I just published a post in Towards Data Science on the foundations to help you keep things straight:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/classification-metrics-everyone-should-know-b67fd0044c0c?source=friends_link&amp;amp;sk=e24aee1dec0a870f749ba3e7f775b431""&gt;https://towardsdatascience.com/classification-metrics-everyone-should-know-b67fd0044c0c?source=friends_link&amp;amp;sk=e24aee1dec0a870f749ba3e7f775b431&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Constructive feedback appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?auto=webp&amp;s=de34e360220fa8b6045e2c2475b49d9a3c9f4e52', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f7e373363ec27607ba02d37a7d586d59aca84c50', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d33e4caee9d3e9a61a1d68bbbcfcdc465a00facf', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bb2a384bc933a14f555acde611ead83882cac25', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7579e1b197306e6564578bcf79aa92e6ca54eb65', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e2bee50f1a8cc8934ac59016552258d1b933bfb3', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/i6A_s3n8_MVMRaLxO1FjGnYqg8FubT3PRTpslTmkovI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bcaf552ed8026ee0ef39a5abd3f855ca34c8ab2', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'txqDeQfhp2EEQS_qY6sW7qGFn6U6q72QHvouZQyERBg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdc2b0,True,,discdiver,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdc2b0/classification_metric_foundations_explained/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdc2b0/classification_metric_foundations_explained/,155203,1588600614.0,0,,False,,,,
,learnmachinelearning,"Let's say I have some text that has vector V1 and some subset of the same text that has vector V2.

I could multiply a few parameters (some are boolean, some are decimal, some are integers) by some manually written weights and remove some parts of the text based on the result. Can this optimization be automated? Maybe a MLP or a more complex model?

Could I have a vector as the label?

&amp;#x200B;

Thanks =)",t2_kzzlt,False,,0,False,Would there be a way to optimize weights to transform a vector in another vector?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdbmyv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588627824.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I have some text that has vector V1 and some subset of the same text that has vector V2.&lt;/p&gt;

&lt;p&gt;I could multiply a few parameters (some are boolean, some are decimal, some are integers) by some manually written weights and remove some parts of the text based on the result. Can this optimization be automated? Maybe a MLP or a more complex model?&lt;/p&gt;

&lt;p&gt;Could I have a vector as the label?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks =)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdbmyv,True,,Samygabriel,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdbmyv/would_there_be_a_way_to_optimize_weights_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdbmyv/would_there_be_a_way_to_optimize_weights_to/,155203,1588599024.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,Top 3 Artificial Intelligence Research Papers â€“ April 2020,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gd6rdb,False,dark,0.58,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/JFB1oUJj4IvRHG7t4JCzS2dkNumRFOJ7YqGCapGP54E.jpg,False,,[],{},link,,False,,1588604650.0,text,6,,,text,rubikscode.net,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?auto=webp&amp;s=09749d9a7b1d66dfe0cdd39ea59eced328f3befa', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ec41c3962c646c0e3926b2d07ca09620bc6ecf7', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64a3b0ebbea48e15a41f50c04c65b7e5e55875ff', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b0c965d6174cba09fbba6ca15b2a51591126894', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb8c9cf6862a079ded94de89ae11c670f3b39c70', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b6c5f91afb4350536ec21ebbe00c9bdc7b3af93', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/uOkNXGre62VhQZELf8PlFKocZyHbD3pCYZH2wCdleX0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf172fd709e910281cf7b610c20d8c35ead8405a', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'AHDNEJRkyd83a4EEQf3oHiGwH8zikjd4XlMIVSpyYvk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd6rdb,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd6rdb/top_3_artificial_intelligence_research_papers/,all_ads,False,https://rubikscode.net/2020/05/04/top-3-artificial-intelligence-research-papers-april-2020/,155203,1588575850.0,0,,False,,,,
,learnmachinelearning,"Can anyone please me by providing the useful links or material from youtube or from elsewhere so that i can learn tha ML  and apply side by side 
I know little bit of basic....but i want the material which help with the basic programming skill needed in ML with the theory",t2_5j3u26zh,False,,0,False,Wants to learn machine learning,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gdaz65,False,light,0.4,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588625263.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can anyone please me by providing the useful links or material from youtube or from elsewhere so that i can learn tha ML  and apply side by side 
I know little bit of basic....but i want the material which help with the basic programming skill needed in ML with the theory&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gdaz65,True,,sickwalker,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdaz65/wants_to_learn_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdaz65/wants_to_learn_machine_learning/,155203,1588596463.0,0,,False,,,,
,learnmachinelearning,"My current pc build:   
[https://pcpartpicker.com/list/r8FmvW](https://pcpartpicker.com/list/r8FmvW)  


I am doing a project in which I will need to do quite some computing. I have a budget of about 1000 euro's to upgrade/build. Overall, I would like to get into machine learning in general, but feel like perhaps my budget doesn't allow for a second pc.   


Should I go with a completely new build or should I upgrade my current pc? (I assume the upgrade would be mobo + second gpu)  


Something to keep in mind is that I have an i5-8400 and 2x8gb ram spare. These could possibly be used in the second build.",t2_4cmzw2g6,False,,0,False,Upgrade or start from scratch?,[],r/learnmachinelearning,False,6,,0,,False,t3_gdasid,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588624507.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My current pc build:&lt;br/&gt;
&lt;a href=""https://pcpartpicker.com/list/r8FmvW""&gt;https://pcpartpicker.com/list/r8FmvW&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;I am doing a project in which I will need to do quite some computing. I have a budget of about 1000 euro&amp;#39;s to upgrade/build. Overall, I would like to get into machine learning in general, but feel like perhaps my budget doesn&amp;#39;t allow for a second pc.   &lt;/p&gt;

&lt;p&gt;Should I go with a completely new build or should I upgrade my current pc? (I assume the upgrade would be mobo + second gpu)  &lt;/p&gt;

&lt;p&gt;Something to keep in mind is that I have an i5-8400 and 2x8gb ram spare. These could possibly be used in the second build.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/H0Tk1MnFnlAJcB8aLTlO5Pki5V9l4MDQgwoU17ZT96I.jpg?auto=webp&amp;s=4d3848f867568c340e5e11a72a46b1135b65bb3c', 'width': 500, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/H0Tk1MnFnlAJcB8aLTlO5Pki5V9l4MDQgwoU17ZT96I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=932d8a8d3b73da99fd23452c884428698962e50b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/H0Tk1MnFnlAJcB8aLTlO5Pki5V9l4MDQgwoU17ZT96I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed7a7099f9b2fe5ca7fcaaec36e7ccdce5ad1941', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/H0Tk1MnFnlAJcB8aLTlO5Pki5V9l4MDQgwoU17ZT96I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8478e93fba23440bde2cd585da6e3129c1564ff7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'yHKuA1pLjChNKl5saqk57OI0Tc70KBtXWzcetbvCp34'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdasid,True,,YogiTheSaltyBear,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdasid/upgrade_or_start_from_scratch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gdasid/upgrade_or_start_from_scratch/,155203,1588595707.0,0,,False,,,,
,learnmachinelearning,Hi I want to make a model that can build a model that can recite poetries or songs. Is there any work related to this? What kind of technology can we work on?,t2_5husjdgw,False,,0,False,Poetry Reciting Model,[],r/learnmachinelearning,False,6,,0,,False,t3_gd6wdj,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588605353.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi I want to make a model that can build a model that can recite poetries or songs. Is there any work related to this? What kind of technology can we work on?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd6wdj,True,,mr_mercury08,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd6wdj/poetry_reciting_model/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd6wdj/poetry_reciting_model/,155203,1588576553.0,0,,False,,,,
,learnmachinelearning,As a complete beginner in ML I want to create a simple content-filtering based recommendation system based on recommending research papers. But I have no idea how to start! I know Django for backend. Can somebody help me out with how to approach this problem as a complete beginner?,t2_y2rn6,False,,0,False,How to build a recommendation system based Web-App?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd9gh5,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588618496.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As a complete beginner in ML I want to create a simple content-filtering based recommendation system based on recommending research papers. But I have no idea how to start! I know Django for backend. Can somebody help me out with how to approach this problem as a complete beginner?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gd9gh5,True,,megatronus8010,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd9gh5/how_to_build_a_recommendation_system_based_webapp/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd9gh5/how_to_build_a_recommendation_system_based_webapp/,155203,1588589696.0,0,,False,,,,
,learnmachinelearning,"Hello,

I'm using sklearn and trying to automate training and data preprocessing for any CSV file (handling missing values and making the data ready for the model)

I'm using panda and thinking about this approach:

1. Load the data using pandas
2. Use pandas fillna() to handle missing values:

csv\_data.fillna(csv\_data.mean(), inplace=True)

3. Go through each column, if the column is not numeric value, apply sklearn LabelEncoder to that column, if it's numeric, leave it as it's

&amp;#x200B;

1. Create 7 instances of the same dataset and apply the following on each dataset:

Dataset Instance 1 use MinMaxScaler

Dataset Instance 2 use MaxAbsScaler

Dataset Instance 3 use StandardScaler

Dataset Instance 4 use RobustScaler

Dataset Instance 5 use Normalizer

Dataset Instance 6 use QuantileTransformer

Dataset Instance 7 use PowerTransformer

&amp;#x200B;

1. Finally, create 7 instances of the model, train the models based on the dataset instances, and evaluate against test data to determine which one is scoring the highest success.

The question is: Will this work on any data loaded from CSV? Or I'm missing something? Do I need to tweak this for every model, assuming I'm training the following models: Logistic Regression, SVM, Decision Tree, Random Forest, KNearest, KMean Clustering, etc..",t2_4oiur8wv,False,,0,False,Machine Learning Automated CSV Preprocesing and Training,[],r/learnmachinelearning,False,6,,0,,False,t3_gd9dfk,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588618075.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using sklearn and trying to automate training and data preprocessing for any CSV file (handling missing values and making the data ready for the model)&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using panda and thinking about this approach:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Load the data using pandas&lt;/li&gt;
&lt;li&gt;Use pandas fillna() to handle missing values:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;csv_data.fillna(csv_data.mean(), inplace=True)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Go through each column, if the column is not numeric value, apply sklearn LabelEncoder to that column, if it&amp;#39;s numeric, leave it as it&amp;#39;s&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create 7 instances of the same dataset and apply the following on each dataset:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dataset Instance 1 use MinMaxScaler&lt;/p&gt;

&lt;p&gt;Dataset Instance 2 use MaxAbsScaler&lt;/p&gt;

&lt;p&gt;Dataset Instance 3 use StandardScaler&lt;/p&gt;

&lt;p&gt;Dataset Instance 4 use RobustScaler&lt;/p&gt;

&lt;p&gt;Dataset Instance 5 use Normalizer&lt;/p&gt;

&lt;p&gt;Dataset Instance 6 use QuantileTransformer&lt;/p&gt;

&lt;p&gt;Dataset Instance 7 use PowerTransformer&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finally, create 7 instances of the model, train the models based on the dataset instances, and evaluate against test data to determine which one is scoring the highest success.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The question is: Will this work on any data loaded from CSV? Or I&amp;#39;m missing something? Do I need to tweak this for every model, assuming I&amp;#39;m training the following models: Logistic Regression, SVM, Decision Tree, Random Forest, KNearest, KMean Clustering, etc..&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd9dfk,True,,Medo3337,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd9dfk/machine_learning_automated_csv_preprocesing_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd9dfk/machine_learning_automated_csv_preprocesing_and/,155203,1588589275.0,0,,False,,,,
,learnmachinelearning,"Hi Guys!

So I created a time series forecasting tutorial with python by showing how to approach a real use case like the Beijing air pollution dataset. Link here [https://github.com/jiwidi/time-series-forecasting-with-python](https://github.com/jiwidi/time-series-forecasting-with-python)

This is my start on ""writing"" code meant to teach someone else and not just do whatever I want it to do. This means is probably crap right now but would love some feedback on it if you feel like roasting it.",t2_mzgwk,False,,0,False,I created a use case example of time series forecasting with python and I would love some feedback,[],r/learnmachinelearning,False,6,,0,,False,t3_gd97o8,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588617276.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Guys!&lt;/p&gt;

&lt;p&gt;So I created a time series forecasting tutorial with python by showing how to approach a real use case like the Beijing air pollution dataset. Link here &lt;a href=""https://github.com/jiwidi/time-series-forecasting-with-python""&gt;https://github.com/jiwidi/time-series-forecasting-with-python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is my start on &amp;quot;writing&amp;quot; code meant to teach someone else and not just do whatever I want it to do. This means is probably crap right now but would love some feedback on it if you feel like roasting it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/I79N1zycSxIg9V7Tgtb6iWm_Xjug1ns5jjLenKTXWyQ.jpg?auto=webp&amp;s=61c69f9d25ed2ab2ce6faf3fdcd2bc4c02f8bffb', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/I79N1zycSxIg9V7Tgtb6iWm_Xjug1ns5jjLenKTXWyQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7308bb0fd56326b9e651645517fe16e166b3309c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/I79N1zycSxIg9V7Tgtb6iWm_Xjug1ns5jjLenKTXWyQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7343b582c8cdfc21c115cfddc376eeecf594f1a1', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/I79N1zycSxIg9V7Tgtb6iWm_Xjug1ns5jjLenKTXWyQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e80005772d33535afe232157595dacc19361687', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'gyrZI-gLKdX0SYWSpe1bTXr7exlXIRHqobw1m_j5P-E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd97o8,True,,jiwidi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd97o8/i_created_a_use_case_example_of_time_series/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd97o8/i_created_a_use_case_example_of_time_series/,155203,1588588476.0,0,,False,,,,
,learnmachinelearning,"Hello :).

For my bachelor thesis i want to train two CNN's to get a texture descriptor from a given sub-image. 

One CNN is going to be trained, the other one is an auto-encoder. I will compare how well they perform. 

I got some images from my prof. which i will be using, but they aren't labeled. So i need some software to label the data. 

When i researched this, i found that there are many solutions and i didn't know which would be most applicable to my usecase, which is:

&amp;#x200B;

Label the data in a way, such that every sub-image (e.g. 32x32 square) is labeled with exactly one class (or none). 

It would be ok if the image is labeled pixel-wise and the exact outlines of the object are labeled, if i can convert that to the desired structure. (should be quite simple ... e.g. more then 1/4 of pixel labeled as class -&gt; square labeled as class.)

&amp;#x200B;

I hope you guys can recommend me a tool which is suitable. Thanks!",t2_133ztrj9,False,,0,False,Which data Annotation tool for texture recognition?,[],r/learnmachinelearning,False,6,,0,,False,t3_gd8xpl,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588615894.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello :).&lt;/p&gt;

&lt;p&gt;For my bachelor thesis i want to train two CNN&amp;#39;s to get a texture descriptor from a given sub-image. &lt;/p&gt;

&lt;p&gt;One CNN is going to be trained, the other one is an auto-encoder. I will compare how well they perform. &lt;/p&gt;

&lt;p&gt;I got some images from my prof. which i will be using, but they aren&amp;#39;t labeled. So i need some software to label the data. &lt;/p&gt;

&lt;p&gt;When i researched this, i found that there are many solutions and i didn&amp;#39;t know which would be most applicable to my usecase, which is:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Label the data in a way, such that every sub-image (e.g. 32x32 square) is labeled with exactly one class (or none). &lt;/p&gt;

&lt;p&gt;It would be ok if the image is labeled pixel-wise and the exact outlines of the object are labeled, if i can convert that to the desired structure. (should be quite simple ... e.g. more then 1/4 of pixel labeled as class -&amp;gt; square labeled as class.)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I hope you guys can recommend me a tool which is suitable. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd8xpl,True,,Brilliant_Potato,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd8xpl/which_data_annotation_tool_for_texture_recognition/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd8xpl/which_data_annotation_tool_for_texture_recognition/,155203,1588587094.0,0,,False,,,,
,learnmachinelearning," Every time I think about this thing I go in a downward spiral of depression.

I'm at that point in my career where I'm not really sure if I want to go into Data Engineering or Machine Learning. I have a little exposure to both sides and am not sure yet which one I like more.

As we know the whole Machine Learning pipeline has a lost of steps ranging from data collection, aggregation, cleaning, the whole process of creating the model and eventually deployment.

Which part of the Machine Learning process will you think will be automated in the near future?

What should I focus on more right now if I want to be future ready? (I have a background of Bachelor's in Computer Science).",t2_yl8ul14,False,,0,False,Concerned about automation in Machine Learning,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd2u3i,False,light,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,self,False,,[],{},,,True,,1588586394.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Every time I think about this thing I go in a downward spiral of depression.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m at that point in my career where I&amp;#39;m not really sure if I want to go into Data Engineering or Machine Learning. I have a little exposure to both sides and am not sure yet which one I like more.&lt;/p&gt;

&lt;p&gt;As we know the whole Machine Learning pipeline has a lost of steps ranging from data collection, aggregation, cleaning, the whole process of creating the model and eventually deployment.&lt;/p&gt;

&lt;p&gt;Which part of the Machine Learning process will you think will be automated in the near future?&lt;/p&gt;

&lt;p&gt;What should I focus on more right now if I want to be future ready? (I have a background of Bachelor&amp;#39;s in Computer Science).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gd2u3i,True,,MavSidharth,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd2u3i/concerned_about_automation_in_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd2u3i/concerned_about_automation_in_machine_learning/,155203,1588557594.0,0,,False,,,,
,learnmachinelearning,"Edit: No use case given, that we will identify later.",t2_1h8roqwf,False,,0,False,My boss asked me to find courses and or material that we can give to study for 1 month and start implementation after that. How logical and feasible is this?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd8gha,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1588585030.0,,[],{},,,True,,1588613486.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Edit: No use case given, that we will identify later.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gd8gha,True,,amved,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd8gha/my_boss_asked_me_to_find_courses_and_or_material/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd8gha/my_boss_asked_me_to_find_courses_and_or_material/,155203,1588584686.0,0,,False,,,,
,learnmachinelearning,"Available here [http://stonkers.herokuapp.com/](http://stonkers.herokuapp.com/)

Spent a day or two learning [Streamlit](https://www.streamlit.io/) and put together this App. The app pulls data from the Yahoo API (currently only NYSE and NASDAQ options are implemented) and uses the [Prophet](https://github.com/facebook/prophet) framework to generate the Forecasts. The app allows for some tuning, particulary with the `changepoint_prior_scale` which can lead to some interesting results. 

I'd appreciate any feedback or requests for potentially more things to be added.",t2_69qv6jhz,False,,0,False,Designed simple Stock Predictor using FBProphet and Streamlit,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gczuth,False,light,0.9,,public,7,0,{},,,False,[],,False,False,,{},Project,False,7,,False,self,False,,[],{},,,True,,1588574585.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Available here &lt;a href=""http://stonkers.herokuapp.com/""&gt;http://stonkers.herokuapp.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Spent a day or two learning &lt;a href=""https://www.streamlit.io/""&gt;Streamlit&lt;/a&gt; and put together this App. The app pulls data from the Yahoo API (currently only NYSE and NASDAQ options are implemented) and uses the &lt;a href=""https://github.com/facebook/prophet""&gt;Prophet&lt;/a&gt; framework to generate the Forecasts. The app allows for some tuning, particulary with the &lt;code&gt;changepoint_prior_scale&lt;/code&gt; which can lead to some interesting results. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d appreciate any feedback or requests for potentially more things to be added.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gczuth,True,,datapleb,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gczuth/designed_simple_stock_predictor_using_fbprophet/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gczuth/designed_simple_stock_predictor_using_fbprophet/,155203,1588545785.0,0,,False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,TensorFlow.js: Machine Learning in JavaScript by Jason Mayes (Google),[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gd80xu,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/F1AQJWpygfJTsVZuqwep59NDgsttanr4jHbfPlhOi9A.jpg,False,,[],{},link,,False,,1588611289.0,text,6,,,text,meetup.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?auto=webp&amp;s=74a8746b51dd88eb0f4e81db5e57d332f070fffc', 'width': 600, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=86e2230966539ec21f4df4e3a5fd90c67e067a28', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd0963aa755d2f490d6aaa6662b9c055257160ac', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00348597b4d176170405618ee2f8c6b3b48ebc8', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'GmhUoP0Qrw8Z-aDH9xrDZf_Sk6rQlgdVC4pCrET1pRg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd80xu,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd80xu/tensorflowjs_machine_learning_in_javascript_by/,all_ads,False,https://www.meetup.com/TFUG-Mysuru/events/270417132/,155203,1588582489.0,0,,False,,,,
,learnmachinelearning,"I need help to find if the card is rotated / flipped for a business card OCR project. I've gotten to the point where I can seperate the card from background and do perspective correction. A side effect of the correction is that the card can get flipped / rotated.
How do I correct that?",t2_7pk1mg,False,,0,False,Finding card orientation in business card OCR,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd7zby,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588611056.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need help to find if the card is rotated / flipped for a business card OCR project. I&amp;#39;ve gotten to the point where I can seperate the card from background and do perspective correction. A side effect of the correction is that the card can get flipped / rotated.
How do I correct that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gd7zby,True,,HecknBamBoozle,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd7zby/finding_card_orientation_in_business_card_ocr/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd7zby/finding_card_orientation_in_business_card_ocr/,155203,1588582256.0,0,,False,,,,
,learnmachinelearning,"Hello!

Here is a link to a current PC build I plan to make. I am looking for advice and pointers whether this setup is best for ML research using Python and libraries such as pandas, numpy and scipy. I am aware of single core performance with Intel chips which is why I am going with Intel for the build. (Refer to Intel MKL) 

Others in the field of research or environment setup can also provide their opinions.

I'd be curious to know if Intel MKL matters when compared to AMD.

https://pcpartpicker.com/list/NQGnrV",t2_1b1li3iy,False,,0,False,ML does Intel MKL matter vs AMD?,[],r/learnmachinelearning,False,6,,0,,False,t3_gd4vry,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588595213.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!&lt;/p&gt;

&lt;p&gt;Here is a link to a current PC build I plan to make. I am looking for advice and pointers whether this setup is best for ML research using Python and libraries such as pandas, numpy and scipy. I am aware of single core performance with Intel chips which is why I am going with Intel for the build. (Refer to Intel MKL) &lt;/p&gt;

&lt;p&gt;Others in the field of research or environment setup can also provide their opinions.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d be curious to know if Intel MKL matters when compared to AMD.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pcpartpicker.com/list/NQGnrV""&gt;https://pcpartpicker.com/list/NQGnrV&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EqRAwuyKa_qsfrYsG8sWa17x5Ew6UOGALajHOdATAkw.jpg?auto=webp&amp;s=e39311c3ee8306b7816872476174d6418690581e', 'width': 500, 'height': 375}, 'resolutions': [{'url': 'https://external-preview.redd.it/EqRAwuyKa_qsfrYsG8sWa17x5Ew6UOGALajHOdATAkw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ce6e60d1344bf9524ad59106e04711fdcea4e716', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/EqRAwuyKa_qsfrYsG8sWa17x5Ew6UOGALajHOdATAkw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f5c81dc2133730c53b47c417eaaa82a415dd195', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/EqRAwuyKa_qsfrYsG8sWa17x5Ew6UOGALajHOdATAkw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbb773e60dc1522d904cb7102253c8fbadea281d', 'width': 320, 'height': 240}], 'variants': {}, 'id': '1EvyD79zN9GciXb5W77J1w8Q533-BMMEeSoHRqV6qkU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd4vry,True,,antonio_zeus,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd4vry/ml_does_intel_mkl_matter_vs_amd/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd4vry/ml_does_intel_mkl_matter_vs_amd/,155203,1588566413.0,0,,False,,,,
,learnmachinelearning,"I'm a beginner when it comes to coding and machine learning  but desperately want to get basic image captioning working. I came across Uber's Ludwig which looked great, simple and was the only one so far I managed to get running. But after training on the Flicker8k dataset all night, the predictions all appear basically the same for every image...

I must be missing something obvious, does anyone have experience with Ludwig? or could suggest the simplest possible alternative? (preferably using Python, no paid cloud services).

Ran experiment with:

`ludwig experiment --data_csv Flickr8k.token_ludwig.csv  --model_definition_file model_definition.yaml` 

&amp;#x200B;

Flickr8k.token\_ludwig.csv

`image_path,caption`

`images/1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .`

`images/1000268201_693b08cb0e.jpg,A girl going into a wooden building .`

`images/1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .`

`images/1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .`

`images/1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .`

`images/1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting`

`images/1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .`

`images/1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring`

&amp;#x200B;

model\_definition.yaml:

`input_features:`

`-`

`name: image_path`

`type: image`

`encoder: stacked_cnn`

`preprocessing:`

`height: 128`

`width: 128`

`resize_method: interpolate`

&amp;#x200B;

`output_features:`

`-`

`name: caption`

`type: text`

`level: word`

`decoder: generator`

`cell_type: lstm`

&amp;#x200B;

caption\_predictions.csv: 

`a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&lt;PAD&gt;,&lt;PAD&gt;`

`a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&lt;PAD&gt;,&lt;PAD&gt;`

`a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&lt;PAD&gt;,&lt;PAD&gt;`

`a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&lt;PAD&gt;,&lt;PAD&gt;`

`a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&lt;PAD&gt;,&lt;PAD&gt;`",t2_9hpxd,False,,0,False,Struggling with image captioning,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd6x5w,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588605466.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a beginner when it comes to coding and machine learning  but desperately want to get basic image captioning working. I came across Uber&amp;#39;s Ludwig which looked great, simple and was the only one so far I managed to get running. But after training on the Flicker8k dataset all night, the predictions all appear basically the same for every image...&lt;/p&gt;

&lt;p&gt;I must be missing something obvious, does anyone have experience with Ludwig? or could suggest the simplest possible alternative? (preferably using Python, no paid cloud services).&lt;/p&gt;

&lt;p&gt;Ran experiment with:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ludwig experiment --data_csv Flickr8k.token_ludwig.csv  --model_definition_file model_definition.yaml&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Flickr8k.token_ludwig.csv&lt;/p&gt;

&lt;p&gt;&lt;code&gt;image_path,caption&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1000268201_693b08cb0e.jpg,A girl going into a wooden building .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;images/1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;model_definition.yaml:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;input_features:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;-&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;name: image_path&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;type: image&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;encoder: stacked_cnn&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;preprocessing:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;height: 128&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;width: 128&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;resize_method: interpolate&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;output_features:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;-&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;name: caption&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;type: text&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;level: word&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;decoder: generator&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cell_type: lstm&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;caption_predictions.csv: &lt;/p&gt;

&lt;p&gt;&lt;code&gt;a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&amp;lt;PAD&amp;gt;,&amp;lt;PAD&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&amp;lt;PAD&amp;gt;,&amp;lt;PAD&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&amp;lt;PAD&amp;gt;,&amp;lt;PAD&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&amp;lt;PAD&amp;gt;,&amp;lt;PAD&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;a,man,in,a,red,shirt,is,standing,in,front,of,a,crowd,of,people,.,&amp;lt;PAD&amp;gt;,&amp;lt;PAD&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gd6x5w,True,,Vininski,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd6x5w/struggling_with_image_captioning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd6x5w/struggling_with_image_captioning/,155203,1588576666.0,0,,False,,,,
,learnmachinelearning,"I am starting youtube channel dedicates to machine learning and Linux. I would like to focus on news users and people intereated in machine learning. I would like to get your feedback on what topics would be interesting, what would you like to see and if people are intereated in ML workstation builds, cloud formation and enviroment setups.

I work as AI developer in automotive industry for 8 years.",t2_5nej61gz,False,,0,False,"[HELP] Starting machine learning youtube, need help",[],r/learnmachinelearning,False,6,,0,,False,t3_gd6pvg,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588604442.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am starting youtube channel dedicates to machine learning and Linux. I would like to focus on news users and people intereated in machine learning. I would like to get your feedback on what topics would be interesting, what would you like to see and if people are intereated in ML workstation builds, cloud formation and enviroment setups.&lt;/p&gt;

&lt;p&gt;I work as AI developer in automotive industry for 8 years.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd6pvg,True,,eldritch_might,,11,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd6pvg/help_starting_machine_learning_youtube_need_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd6pvg/help_starting_machine_learning_youtube_need_help/,155203,1588575642.0,0,,False,,,,
,learnmachinelearning,"Hey all! I'm looking to start a new career in ML. As of now I'm learning as much as I can via the internet and would like to take some classes preferably online. At the same time I'd like to boost up my resume with projects or better yet an entry level job. 

I would like to ask those who have undertaken a similar journey if they could give me any advice they learned from their journey or anyone that has some thoughtful input on how to best pipeline this problem! 

I have completed the Andrew Ng's Coursera course and right now I'm building a trading bot in Python and attempting to implement the algorithms discussed in the class. 

My background is in Psychology and I am most interested in Neural Networks and any specific information on better understanding them or guided ways to practice building them would be much appreciated! 

Thanks! :)",t2_ls4fh,False,,0,False,New to Machine Learning: Looking for Direction,[],r/learnmachinelearning,False,6,,0,,False,t3_gd4xkg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588595434.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all! I&amp;#39;m looking to start a new career in ML. As of now I&amp;#39;m learning as much as I can via the internet and would like to take some classes preferably online. At the same time I&amp;#39;d like to boost up my resume with projects or better yet an entry level job. &lt;/p&gt;

&lt;p&gt;I would like to ask those who have undertaken a similar journey if they could give me any advice they learned from their journey or anyone that has some thoughtful input on how to best pipeline this problem! &lt;/p&gt;

&lt;p&gt;I have completed the Andrew Ng&amp;#39;s Coursera course and right now I&amp;#39;m building a trading bot in Python and attempting to implement the algorithms discussed in the class. &lt;/p&gt;

&lt;p&gt;My background is in Psychology and I am most interested in Neural Networks and any specific information on better understanding them or guided ways to practice building them would be much appreciated! &lt;/p&gt;

&lt;p&gt;Thanks! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd4xkg,True,,zoltasaur,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd4xkg/new_to_machine_learning_looking_for_direction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd4xkg/new_to_machine_learning_looking_for_direction/,155203,1588566634.0,0,,False,,,,
,learnmachinelearning,"If you want to learn about [JAX](https://github.com/google/jax) and have an interest on Graph Neural Networks, I wrote this post about implementing a couple models (GCN and GAT) using JAX. I start with a small introduction to JAX and then move on to graph neural networks.

Link to the post: [http://gcucurull.github.io/deep-learning/2020/04/20/jax-graph-neural-networks/](http://gcucurull.github.io/deep-learning/2020/04/20/jax-graph-neural-networks/)

Github: [https://github.com/gcucurull/jax-gcn](https://github.com/gcucurull/jax-gcn) and [https://github.com/gcucurull/jax-gat](https://github.com/gcucurull/jax-gat)",t2_113490,False,,0,False,Implementing Graph Convolutional Networks in JAX,[],r/learnmachinelearning,False,6,,0,,False,t3_gcy2k3,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1588577359.0,,[],{},self,,True,,1588568184.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If you want to learn about &lt;a href=""https://github.com/google/jax""&gt;JAX&lt;/a&gt; and have an interest on Graph Neural Networks, I wrote this post about implementing a couple models (GCN and GAT) using JAX. I start with a small introduction to JAX and then move on to graph neural networks.&lt;/p&gt;

&lt;p&gt;Link to the post: &lt;a href=""http://gcucurull.github.io/deep-learning/2020/04/20/jax-graph-neural-networks/""&gt;http://gcucurull.github.io/deep-learning/2020/04/20/jax-graph-neural-networks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github: &lt;a href=""https://github.com/gcucurull/jax-gcn""&gt;https://github.com/gcucurull/jax-gcn&lt;/a&gt; and &lt;a href=""https://github.com/gcucurull/jax-gat""&gt;https://github.com/gcucurull/jax-gat&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?auto=webp&amp;s=e2a0b466eaec0a2eafa4a1ffc660bd07830135e9', 'width': 1880, 'height': 1088}, 'resolutions': [{'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a0e23f669908fc2ce5953b79d21bec110dd9ebd', 'width': 108, 'height': 62}, {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=147c614d72d97f874d1557f53038e80aec3ff97c', 'width': 216, 'height': 125}, {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e229da65e3243fe1e8421cfe949b43fec48cc3e6', 'width': 320, 'height': 185}, {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4d99da412c1708bc0665626d7c8b245c686442b', 'width': 640, 'height': 370}, {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7526e9ea7aec4831329f5b55349861523ea26b7d', 'width': 960, 'height': 555}, {'url': 'https://external-preview.redd.it/suLuzPgkqZUie7zCn01I5KhYvY0zqcBpt2Eueqr6Dao.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9c9b2d9fa20cac316e9d1eeff5663e1e2df7c2f', 'width': 1080, 'height': 625}], 'variants': {}, 'id': 'XDQFaBRreeySjcj1eK3XlkLPWOYakjSgXWw0cwPHI40'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcy2k3,True,,galloguille,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcy2k3/implementing_graph_convolutional_networks_in_jax/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcy2k3/implementing_graph_convolutional_networks_in_jax/,155203,1588539384.0,0,,False,,,,
,learnmachinelearning,"Hello,

&amp;#x200B;

I've got a directory containing several subdirectories that I'm trying to visualize as a dendrogram in python. I would like to also perform some hierarchical clustering to it; however, I'm having issues because some of the my data is text (i.e. the directory or subdirectory name), and this is my first foray into such clustering algorithms. Does anyone have any suggestions for how I can visualize my data? I've tried looking online but haven't been able to find anything that really helps.

&amp;#x200B;

Thanks!",t2_re1il,False,,0,False,Creating dendrogram in python from directory tree data,[],r/learnmachinelearning,False,6,,0,,False,t3_gd4h63,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588593445.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve got a directory containing several subdirectories that I&amp;#39;m trying to visualize as a dendrogram in python. I would like to also perform some hierarchical clustering to it; however, I&amp;#39;m having issues because some of the my data is text (i.e. the directory or subdirectory name), and this is my first foray into such clustering algorithms. Does anyone have any suggestions for how I can visualize my data? I&amp;#39;ve tried looking online but haven&amp;#39;t been able to find anything that really helps.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd4h63,True,,djleviathan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd4h63/creating_dendrogram_in_python_from_directory_tree/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd4h63/creating_dendrogram_in_python_from_directory_tree/,155203,1588564645.0,0,,False,,,,
,learnmachinelearning," Hi,

I've been trying to develop an object counting project. I want high FPS rates. So, I run object detection model not in every frame. I run it in every 5th frame. And I implement object tracking between 5 frames. As a result, I see an object is labeled 5 times. If I run model not in every 5th frame, instead run it every 10th frame, then I see, that an object is labeled 10 times. I know, It sounds weird. So, [here is my result.](https://streamable.com/gvbqi1)

And this my code:

    def saved_model_video(self, video_path: Text, output_video: Text, **kwargs):
        """"""Perform video inference for the given saved model.""""""
        import cv2  # pylint: disable=g-import-not-at-top
    
        driver = ServingDriver(
            self.model_name,
            self.ckpt_path,
            batch_size=1,
            use_xla=self.use_xla,
            model_params=self.model_config.as_dict())
        driver.load(self.saved_model_dir)
    
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
          print('Error opening input video: {}'.format(video_path))
        
        W = None
        H = None
        # instantiate our centroid tracker, then initialize a list to store
        # each of our dlib correlation trackers, followed by a dictionary to
        # map each unique object ID to a TrackableObject
        ct = CentroidTracker(maxDisappeared=40, maxDistance=50)
        trackers = []
        trackableObjects = {}
        totalFrames = 0
        totalDown = 0
        totalUp = 0
        fps = FPS().start()
        label_id_mapping=None
        disable_pyfun=True
        
        while cap.isOpened():
          # Capture frame-by-frame
          ret, frame = cap.read()
          if not ret:
            break
          rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
          (H, W) =frame.shape[:2]  
          #print(H) #680
          #print(W) #150
          status = ""Waiting""
          rects = []
          if totalFrames % 5 == 0:
            status = ""Detecting""
            trackers = []
            raw_frames = [np.array(frame)]
          
            detections_bs = driver.serve_images(raw_frames)
            for i in np.arange(0, detections_bs.shape[1]):   
                prediction = detections_bs[0][i]
                boxes = prediction[1:5]
                classes = prediction[6].astype(int)
                scores = prediction[5]
                print(scores)
                if not disable_pyfun:
        # convert [x, y, width, height] to [y, x, height, width]
                    boxes[:, [0, 1, 2, 3]] = boxes[:, [1, 0, 3, 2]]
                label_id_mapping = label_id_mapping or coco_id_mapping      
                (y, x, height, width) = boxes.astype(""int"")
                (startX, startY, endX, endY) = (x, y, x+width, y+height)            
                tracker = dlib.correlation_tracker()
                rect = dlib.rectangle(startX, startY, endX, endY)
                tracker.start_track(rgb, rect)
                # add the tracker to our list of trackers so we can
                # utilize it during skip frames
                trackers.append(tracker)
          else:
            
            for tracker in trackers:
    			# set the status of our system to be 'tracking' rather
    			# than 'waiting' or 'detecting'
                status = ""Tracking""
                # update the tracker and grab the updated position
                tracker.update(rgb)
                pos = tracker.get_position()
                # unpack the position object
                startX = int(pos.left())
                startY = int(pos.top())
                endX = int(pos.right())
                endY = int(pos.bottom())
                # add the bounding box coordinates to the rectangles list
                rects.append((startX, startY, endX, endY))
          #new_frame = driver.visualize(raw_frames[0], detections_bs[0], **kwargs)
          cv2.line(raw_frames[0], (0, H // 2), (W, H // 2), (0, 255, 255), 2)
          objects = ct.update(rects)
    	# loop over the tracked objects
          for (objectID, centroid) in objects.items():
            to = trackableObjects.get(objectID, None)
    		# if there is no existing trackable object, create one
            if to is None:
                to = TrackableObject(objectID, centroid)
    		# otherwise, there is a trackable object so we can utilize it
    		# to determine direction
            else:
    			# the difference between the y-coordinate of the *current*
    			# centroid and the mean of *previous* centroids will tell
    			# us in which direction the object is moving (negative for
    			# 'up' and positive for 'down')
                y = [c[1] for c in to.centroids]
                direction = centroid[1] - np.mean(y)
                to.centroids.append(centroid)
                if not to.counted:
    				# if the direction is negative (indicating the object
    				# is moving up) AND the centroid is above the center
    				# line, count the object
                    if direction &lt; 0 and centroid[1] &lt; H // 2:
                        totalUp += 1
                        to.counted = True
    				# if the direction is positive (indicating the object
    				# is moving down) AND the centroid is below the
    				# center line, count the object
                    elif direction &gt; 0 and centroid[1] &gt; H // 2:
                        totalDown += 1
                        to.counted = True
            trackableObjects[objectID] = to
            text = ""ID {}"".format(objectID)
            cv2.putText(raw_frames[0], text, (centroid[0] - 10, centroid[1] - 10),
    			cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.circle(raw_frames[0], (centroid[0], centroid[1]), 4, (0, 255, 0), -1)
          info = [
    		(""Up"", totalUp),
    		(""Down"", totalDown),
    		(""Status"", status),
            ]
          for (i, (k, v)) in enumerate(info):
            text = ""{}: {}"".format(k, v)
            cv2.putText(raw_frames[0], text, (10, H - ((i * 20) + 20)),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            # show the frame online, mainly used for real-time speed test.
          cv2.imshow('Frame',raw_frames[0])
            # Press Q on keyboard to  exit
          if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
          totalFrames += 1
          fps.update()
    # stop the timer and display FPS information
        fps.stop()
        print(totalUp)
        print(totalDown)
        print(""[INFO] elapsed time: {:.2f}"".format(fps.elapsed()))
        print(""[INFO] approx. FPS: {:.2f}"".format(fps.fps()))",t2_4aky7agd,False,,0,False,My object tracking project detects objects more than one time,[],r/learnmachinelearning,False,6,,0,,False,t3_gd143e,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588579427.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been trying to develop an object counting project. I want high FPS rates. So, I run object detection model not in every frame. I run it in every 5th frame. And I implement object tracking between 5 frames. As a result, I see an object is labeled 5 times. If I run model not in every 5th frame, instead run it every 10th frame, then I see, that an object is labeled 10 times. I know, It sounds weird. So, &lt;a href=""https://streamable.com/gvbqi1""&gt;here is my result.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And this my code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def saved_model_video(self, video_path: Text, output_video: Text, **kwargs):
    &amp;quot;&amp;quot;&amp;quot;Perform video inference for the given saved model.&amp;quot;&amp;quot;&amp;quot;
    import cv2  # pylint: disable=g-import-not-at-top

    driver = ServingDriver(
        self.model_name,
        self.ckpt_path,
        batch_size=1,
        use_xla=self.use_xla,
        model_params=self.model_config.as_dict())
    driver.load(self.saved_model_dir)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
      print(&amp;#39;Error opening input video: {}&amp;#39;.format(video_path))

    W = None
    H = None
    # instantiate our centroid tracker, then initialize a list to store
    # each of our dlib correlation trackers, followed by a dictionary to
    # map each unique object ID to a TrackableObject
    ct = CentroidTracker(maxDisappeared=40, maxDistance=50)
    trackers = []
    trackableObjects = {}
    totalFrames = 0
    totalDown = 0
    totalUp = 0
    fps = FPS().start()
    label_id_mapping=None
    disable_pyfun=True

    while cap.isOpened():
      # Capture frame-by-frame
      ret, frame = cap.read()
      if not ret:
        break
      rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
      (H, W) =frame.shape[:2]  
      #print(H) #680
      #print(W) #150
      status = &amp;quot;Waiting&amp;quot;
      rects = []
      if totalFrames % 5 == 0:
        status = &amp;quot;Detecting&amp;quot;
        trackers = []
        raw_frames = [np.array(frame)]

        detections_bs = driver.serve_images(raw_frames)
        for i in np.arange(0, detections_bs.shape[1]):   
            prediction = detections_bs[0][i]
            boxes = prediction[1:5]
            classes = prediction[6].astype(int)
            scores = prediction[5]
            print(scores)
            if not disable_pyfun:
    # convert [x, y, width, height] to [y, x, height, width]
                boxes[:, [0, 1, 2, 3]] = boxes[:, [1, 0, 3, 2]]
            label_id_mapping = label_id_mapping or coco_id_mapping      
            (y, x, height, width) = boxes.astype(&amp;quot;int&amp;quot;)
            (startX, startY, endX, endY) = (x, y, x+width, y+height)            
            tracker = dlib.correlation_tracker()
            rect = dlib.rectangle(startX, startY, endX, endY)
            tracker.start_track(rgb, rect)
            # add the tracker to our list of trackers so we can
            # utilize it during skip frames
            trackers.append(tracker)
      else:

        for tracker in trackers:
            # set the status of our system to be &amp;#39;tracking&amp;#39; rather
            # than &amp;#39;waiting&amp;#39; or &amp;#39;detecting&amp;#39;
            status = &amp;quot;Tracking&amp;quot;
            # update the tracker and grab the updated position
            tracker.update(rgb)
            pos = tracker.get_position()
            # unpack the position object
            startX = int(pos.left())
            startY = int(pos.top())
            endX = int(pos.right())
            endY = int(pos.bottom())
            # add the bounding box coordinates to the rectangles list
            rects.append((startX, startY, endX, endY))
      #new_frame = driver.visualize(raw_frames[0], detections_bs[0], **kwargs)
      cv2.line(raw_frames[0], (0, H // 2), (W, H // 2), (0, 255, 255), 2)
      objects = ct.update(rects)
    # loop over the tracked objects
      for (objectID, centroid) in objects.items():
        to = trackableObjects.get(objectID, None)
        # if there is no existing trackable object, create one
        if to is None:
            to = TrackableObject(objectID, centroid)
        # otherwise, there is a trackable object so we can utilize it
        # to determine direction
        else:
            # the difference between the y-coordinate of the *current*
            # centroid and the mean of *previous* centroids will tell
            # us in which direction the object is moving (negative for
            # &amp;#39;up&amp;#39; and positive for &amp;#39;down&amp;#39;)
            y = [c[1] for c in to.centroids]
            direction = centroid[1] - np.mean(y)
            to.centroids.append(centroid)
            if not to.counted:
                # if the direction is negative (indicating the object
                # is moving up) AND the centroid is above the center
                # line, count the object
                if direction &amp;lt; 0 and centroid[1] &amp;lt; H // 2:
                    totalUp += 1
                    to.counted = True
                # if the direction is positive (indicating the object
                # is moving down) AND the centroid is below the
                # center line, count the object
                elif direction &amp;gt; 0 and centroid[1] &amp;gt; H // 2:
                    totalDown += 1
                    to.counted = True
        trackableObjects[objectID] = to
        text = &amp;quot;ID {}&amp;quot;.format(objectID)
        cv2.putText(raw_frames[0], text, (centroid[0] - 10, centroid[1] - 10),
            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        cv2.circle(raw_frames[0], (centroid[0], centroid[1]), 4, (0, 255, 0), -1)
      info = [
        (&amp;quot;Up&amp;quot;, totalUp),
        (&amp;quot;Down&amp;quot;, totalDown),
        (&amp;quot;Status&amp;quot;, status),
        ]
      for (i, (k, v)) in enumerate(info):
        text = &amp;quot;{}: {}&amp;quot;.format(k, v)
        cv2.putText(raw_frames[0], text, (10, H - ((i * 20) + 20)),
            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        # show the frame online, mainly used for real-time speed test.
      cv2.imshow(&amp;#39;Frame&amp;#39;,raw_frames[0])
        # Press Q on keyboard to  exit
      if cv2.waitKey(1) &amp;amp; 0xFF == ord(&amp;#39;q&amp;#39;):
        break
      totalFrames += 1
      fps.update()
# stop the timer and display FPS information
    fps.stop()
    print(totalUp)
    print(totalDown)
    print(&amp;quot;[INFO] elapsed time: {:.2f}&amp;quot;.format(fps.elapsed()))
    print(&amp;quot;[INFO] approx. FPS: {:.2f}&amp;quot;.format(fps.fps()))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?auto=webp&amp;s=cafaf563180665a11ef8189ad2ca4d30979c3be7', 'width': 1354, 'height': 708}, 'resolutions': [{'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3107ba7e2c256efb8570423c75f01229555779c', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f05867716d926e148e7ea4f8ac0892b073b25b83', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6e4d3f4eb21831fe9ae908ca3a767d5c94b6b77', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=848a8e7f3ce68c5cd260af9330b1d164c2c8d0af', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d1f92315a24568d5b588ae3590a1bd8960be81d', 'width': 960, 'height': 501}, {'url': 'https://external-preview.redd.it/fD9OeVIZdYSH-mAzgfcMe0KeR3xGYJqWQmlOhBtFtT8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a3915de81eded15f5615dbd927df1c83de07ab8', 'width': 1080, 'height': 564}], 'variants': {}, 'id': 'Zy5Ur3yjTBYhT6RekLeEGJ7GM4rNfoGWJdyEz4u0NWI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd143e,True,,hernancrespo89,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd143e/my_object_tracking_project_detects_objects_more/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd143e/my_object_tracking_project_detects_objects_more/,155203,1588550627.0,0,,False,,,,
,learnmachinelearning,"I've recently been spending a lot of time reading research papers trying to fully understand them and have spent the better part of the past month on the transformer architecture. I'd like to implement a simple sequence-to-sequence architecture to generate text (based on some example corpus, say all of Shakespeare's work) using a transformer network.

I've delved deep into the [PyTorch documentation](    https://github.com/pytorch/examples/tree/master/word_language_model
) explaining their implementations of `Transformer`, `TransformerEncoder`, `TransformerEncoderLayer`, etc. but I haven't found a concrete example where `TransformerEncoder` is used with `TransformerDecoder`; [PyTorch's example](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) `TransformerEncoder` example doesn't make use of `TransformerDecoder`. I was wondering if anyone knows of any resource that implemented your typical transformer network using these two classes? I'd ultimately like to use a transformer in a coreference resolution task, but that's a topic for another time.",t2_z6umw,False,,0,False,PyTorch Transformer example?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcx4wg,False,light,1.0,,public,5,0,{},,,False,[],,False,False,,{},HELP,False,5,,False,self,False,,[],{},self,,True,,1588564908.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently been spending a lot of time reading research papers trying to fully understand them and have spent the better part of the past month on the transformer architecture. I&amp;#39;d like to implement a simple sequence-to-sequence architecture to generate text (based on some example corpus, say all of Shakespeare&amp;#39;s work) using a transformer network.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve delved deep into the &lt;a href=""https://github.com/pytorch/examples/tree/master/word_language_model""&gt;PyTorch documentation&lt;/a&gt; explaining their implementations of &lt;code&gt;Transformer&lt;/code&gt;, &lt;code&gt;TransformerEncoder&lt;/code&gt;, &lt;code&gt;TransformerEncoderLayer&lt;/code&gt;, etc. but I haven&amp;#39;t found a concrete example where &lt;code&gt;TransformerEncoder&lt;/code&gt; is used with &lt;code&gt;TransformerDecoder&lt;/code&gt;; &lt;a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html""&gt;PyTorch&amp;#39;s example&lt;/a&gt; &lt;code&gt;TransformerEncoder&lt;/code&gt; example doesn&amp;#39;t make use of &lt;code&gt;TransformerDecoder&lt;/code&gt;. I was wondering if anyone knows of any resource that implemented your typical transformer network using these two classes? I&amp;#39;d ultimately like to use a transformer in a coreference resolution task, but that&amp;#39;s a topic for another time.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?auto=webp&amp;s=08c43430ff8793a25b767aa172468acb35be611a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4adad794ed3c0e2f76e2da992ae9eef8c074cae4', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3cb400fea60e0dfe87ebc5314f7db8016f8ab', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/s9U5tBmhB0aKdkcv0XVXtEC6PfG4brvIgmTzc622QvA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f149723b636ef3087022ddc1efa6ab53e3d4bc69', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_YPUxJ8AyYwgpO9r-tD7JUoQ-Nx8IlATLoEfmjW48iQ'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcx4wg,True,,RElliott10,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcx4wg/pytorch_transformer_example/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcx4wg/pytorch_transformer_example/,155203,1588536108.0,1,,False,,,,
,learnmachinelearning,"Hello, I am kinda new to ML/DL and this is my first post on this sub, so apologies for any mistakes that I will make in this post.

I have created a CNN model for an image classifier project that I am doing. The project is mostly complete but when I started off, I installed all the necessary libraries via pip (embarrassing I know). Since the project was mostly working I wanted to finally migrate to using Ananconda Envs, so I did that. I uninstalled TensorFlow2.1.0, sklearn, Keras, opencv etc from pip. I created a new env in conda and installed the tensorflow package and sklearn etc. i found out that TF had absorbed Keras API and it was [recommended to use tf.keras](https://stackoverflow.com/a/58602751) now instead of Keras. So basically I changed all my keras imports to tensorflow.keras.

Code Snippet here - 

    import tensorflow as tf  
    from sklearn.model_selection import train_test_split  
    from tensorflow.keras.utils import to_categorical  
    from tensorflow.keras.models import Sequential  
    from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    

After I ran my model for training I quickly noticed that my training was slower than before, about 10-15s per epoch. But I have [read](https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c) that conda's version of TF is faster than pip's. If anyone could throw some light on this, I would be grateful. Mostly I think I did something wrong because I cant figure out why tf.keras would be slower than keras.

Note: No GPU acceleration was done in either case. I am happy to provide more information if needed.",t2_6pznpxh,False,,0,False,Why is my model training using tf.keras slower than keras?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd3tai,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588590578.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am kinda new to ML/DL and this is my first post on this sub, so apologies for any mistakes that I will make in this post.&lt;/p&gt;

&lt;p&gt;I have created a CNN model for an image classifier project that I am doing. The project is mostly complete but when I started off, I installed all the necessary libraries via pip (embarrassing I know). Since the project was mostly working I wanted to finally migrate to using Ananconda Envs, so I did that. I uninstalled TensorFlow2.1.0, sklearn, Keras, opencv etc from pip. I created a new env in conda and installed the tensorflow package and sklearn etc. i found out that TF had absorbed Keras API and it was &lt;a href=""https://stackoverflow.com/a/58602751""&gt;recommended to use tf.keras&lt;/a&gt; now instead of Keras. So basically I changed all my keras imports to tensorflow.keras.&lt;/p&gt;

&lt;p&gt;Code Snippet here - &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf  
from sklearn.model_selection import train_test_split  
from tensorflow.keras.utils import to_categorical  
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After I ran my model for training I quickly noticed that my training was slower than before, about 10-15s per epoch. But I have &lt;a href=""https://towardsdatascience.com/stop-installing-tensorflow-using-pip-for-performance-sake-5854f9d9eb0c""&gt;read&lt;/a&gt; that conda&amp;#39;s version of TF is faster than pip&amp;#39;s. If anyone could throw some light on this, I would be grateful. Mostly I think I did something wrong because I cant figure out why tf.keras would be slower than keras.&lt;/p&gt;

&lt;p&gt;Note: No GPU acceleration was done in either case. I am happy to provide more information if needed.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gd3tai,True,,KaiBetterThanTyson,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd3tai/why_is_my_model_training_using_tfkeras_slower/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd3tai/why_is_my_model_training_using_tfkeras_slower/,155203,1588561778.0,1,,False,,,,
,learnmachinelearning,"I'm trying to train a DCGAN on the CIFAR10 dataset (in PyTorch). The last layer in my generator model is a convolutional layer with a tanh activation, thus outputting values in the range (-1,1). To then plot the images that have been generated I did `x_creation = (x_creation + 1)/2` where `x_creation` was the output tensor of the generator model to rescale the tensor back to the range (0,1).

  
However, on plotting the [images](https://imgur.com/a/eXLZKXr), I get a whitish layer on top of them and I can't understand where exactly I am going wrong. Should I try using a sigmoid activation and plotting the values as is instead? Or is my scaling the inherent problem? The same model works correctly when implemented in Keras. I used the `permute()` function of PyTorch to manipulate the tensor channels to match the default requirements of the `pyplot.imshow()` function.

  
 Any help regarding the same would be appreciated.",t2_3nec42lu,False,,0,False,Require help to interpret GAN generator output,[],r/learnmachinelearning,False,6,,0,,False,t3_gd3bzz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588588519.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to train a DCGAN on the CIFAR10 dataset (in PyTorch). The last layer in my generator model is a convolutional layer with a tanh activation, thus outputting values in the range (-1,1). To then plot the images that have been generated I did &lt;code&gt;x_creation = (x_creation + 1)/2&lt;/code&gt; where &lt;code&gt;x_creation&lt;/code&gt; was the output tensor of the generator model to rescale the tensor back to the range (0,1).&lt;/p&gt;

&lt;p&gt;However, on plotting the &lt;a href=""https://imgur.com/a/eXLZKXr""&gt;images&lt;/a&gt;, I get a whitish layer on top of them and I can&amp;#39;t understand where exactly I am going wrong. Should I try using a sigmoid activation and plotting the values as is instead? Or is my scaling the inherent problem? The same model works correctly when implemented in Keras. I used the &lt;code&gt;permute()&lt;/code&gt; function of PyTorch to manipulate the tensor channels to match the default requirements of the &lt;code&gt;pyplot.imshow()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;Any help regarding the same would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BiAHdJRUXCHGqu0d4SGo0IPjn9g6SNgE--YceYcDbNg.jpg?auto=webp&amp;s=db431c7bd839b641f7af3134469abed8e099fe67', 'width': 640, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/BiAHdJRUXCHGqu0d4SGo0IPjn9g6SNgE--YceYcDbNg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bc0e82708fcaf07af8fa9e4a3a6170dc03d4b9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/BiAHdJRUXCHGqu0d4SGo0IPjn9g6SNgE--YceYcDbNg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=287888cd66db3220c8af226b39134fb739ca6061', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/BiAHdJRUXCHGqu0d4SGo0IPjn9g6SNgE--YceYcDbNg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31023c98f6d27b9c3525b8b1560a8a49e8000a1a', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/BiAHdJRUXCHGqu0d4SGo0IPjn9g6SNgE--YceYcDbNg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff22bb6d5318968e8e4775eff7d66c3d8b72e263', 'width': 640, 'height': 480}], 'variants': {}, 'id': 'uvohEI6MAbg7MUyI-DRabIuCCCqcWDdXCvRiZeY3_Bc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd3bzz,True,,phoenix__191,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd3bzz/require_help_to_interpret_gan_generator_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gd3bzz/require_help_to_interpret_gan_generator_output/,155203,1588559719.0,0,,False,,,,
,learnmachinelearning,"Greetings,

Is using libraries like Tensor Flow and Scikit-Learn a crutch to actually learning..,machine learning? It appears to me that you can create reasonable models using these libraries without really understanding the math. I'm terrible at math (but trying to learn), but I've been able to make fairly good predictive models using the libraries. Is this a false sense confidence if I don't really understand the formulas involved in the algorithms?",t2_3pnizflv,False,,0,False,Using Tensorflow and Sci-kit Learn a crutch?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcvkm0,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1588559470.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings,&lt;/p&gt;

&lt;p&gt;Is using libraries like Tensor Flow and Scikit-Learn a crutch to actually learning..,machine learning? It appears to me that you can create reasonable models using these libraries without really understanding the math. I&amp;#39;m terrible at math (but trying to learn), but I&amp;#39;ve been able to make fairly good predictive models using the libraries. Is this a false sense confidence if I don&amp;#39;t really understand the formulas involved in the algorithms?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcvkm0,True,,Tyron_Slothrop,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcvkm0/using_tensorflow_and_scikit_learn_a_crutch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcvkm0/using_tensorflow_and_scikit_learn_a_crutch/,155203,1588530670.0,0,,False,,,,
,learnmachinelearning,I've been solving some cool problems using genetic algorithms and sometimes get asked how to create a baseline GA. I wrote a blog post describing it in detail with code [here](https://chrispresso.github.io/Lets_Make_A_Genetic_Algorithm).,t2_t7l1ct1,False,,0,False,Creating a Genetic Algorithm from scratch,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcrsi7,False,light,1.0,,public,7,0,{},,,False,[],,False,False,,{},Project,False,7,,False,self,False,,[],{},self,,True,,1588546034.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been solving some cool problems using genetic algorithms and sometimes get asked how to create a baseline GA. I wrote a blog post describing it in detail with code &lt;a href=""https://chrispresso.github.io/Lets_Make_A_Genetic_Algorithm""&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yhRbcZq0LxLQSPnBVuXMSWQaiEMm8rq8hHf-FFGcFQU.jpg?auto=webp&amp;s=270389d4ee3de5dab685ead2b480041c28c72b27', 'width': 700, 'height': 450}, 'resolutions': [{'url': 'https://external-preview.redd.it/yhRbcZq0LxLQSPnBVuXMSWQaiEMm8rq8hHf-FFGcFQU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59e57be0a24e190162429f3d7a16f23fb01781bc', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/yhRbcZq0LxLQSPnBVuXMSWQaiEMm8rq8hHf-FFGcFQU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb743ca4d7e81968a7f94a530fda74d329814b28', 'width': 216, 'height': 138}, {'url': 'https://external-preview.redd.it/yhRbcZq0LxLQSPnBVuXMSWQaiEMm8rq8hHf-FFGcFQU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce31dedd1f223011a3b21f47b8f2714fb7f4d510', 'width': 320, 'height': 205}, {'url': 'https://external-preview.redd.it/yhRbcZq0LxLQSPnBVuXMSWQaiEMm8rq8hHf-FFGcFQU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4665f314e2124b89c76349a4b1fe3db69b30e985', 'width': 640, 'height': 411}], 'variants': {}, 'id': '36pQPLk_P_ByoRTD48tBPmb0M-Dc2NPJAOKZ5ajC5fU'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gcrsi7,True,,Chrispresso,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcrsi7/creating_a_genetic_algorithm_from_scratch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcrsi7/creating_a_genetic_algorithm_from_scratch/,155203,1588517234.0,0,,False,,,,
,learnmachinelearning,,t2_53t39z3t,False,,0,False,Text Summarization using Python (Flask) in ONE VIDEO | Text summarize flask web app in python,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gd20h0,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/h8fE_G9a_Oo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Text Summarization using Python (Flask) in ONE VIDEO | Text summarize flask web app in python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/h8fE_G9a_Oo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'datageekrj', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/h8fE_G9a_Oo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCeEsbCOjbSXao5dwqXLlJmQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/h8fE_G9a_Oo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gd20h0', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/iQGOUwWcqXnTiBeriR6dDFcMnTer86r3w3ZJW6chj0E.jpg,False,,[],{},rich:video,,False,,1588582957.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZOMXQ_ZygqGW3O3cCqWXMrFww4WS33yXlbhs_XmYtY8.jpg?auto=webp&amp;s=6e24f1697db9b1857c1acae22ef32c620219e3b3', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZOMXQ_ZygqGW3O3cCqWXMrFww4WS33yXlbhs_XmYtY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa4edbe056564edea246ed3d580d4aaa9ace159f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ZOMXQ_ZygqGW3O3cCqWXMrFww4WS33yXlbhs_XmYtY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df76f4db156427b6c0d361985800d380d1f071f7', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ZOMXQ_ZygqGW3O3cCqWXMrFww4WS33yXlbhs_XmYtY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f2257a294bbc9ef44145217ae29a73ec595622b', 'width': 320, 'height': 240}], 'variants': {}, 'id': '3TlA8jRo8qGZgQVxWaNIQ_JCJF-7r3wzUBfpw4viBxk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gd20h0,True,,datageekrj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd20h0/text_summarization_using_python_flask_in_one/,all_ads,False,https://youtu.be/h8fE_G9a_Oo,155203,1588554157.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Text Summarization using Python (Flask) in ONE VIDEO | Text summarize flask web app in python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/h8fE_G9a_Oo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'datageekrj', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/h8fE_G9a_Oo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCeEsbCOjbSXao5dwqXLlJmQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,TensorFlow.js: Machine Learning in JavaScript by Jason Mayes (Google),[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gcp26p,False,dark,0.99,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://b.thumbs.redditmedia.com/F1AQJWpygfJTsVZuqwep59NDgsttanr4jHbfPlhOi9A.jpg,False,,[],{},link,,False,,1588534117.0,text,6,,,text,meetup.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?auto=webp&amp;s=74a8746b51dd88eb0f4e81db5e57d332f070fffc', 'width': 600, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=86e2230966539ec21f4df4e3a5fd90c67e067a28', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd0963aa755d2f490d6aaa6662b9c055257160ac', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Hw88ltqxbQJLgWOjpPLexMYfr1oauDIVVeQUR4prsZg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c00348597b4d176170405618ee2f8c6b3b48ebc8', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'GmhUoP0Qrw8Z-aDH9xrDZf_Sk6rQlgdVC4pCrET1pRg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcp26p,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcp26p/tensorflowjs_machine_learning_in_javascript_by/,all_ads,False,https://www.meetup.com/TFUG-Mysuru/events/270417132/,155203,1588505317.0,0,,False,,,,
,learnmachinelearning,,t2_28bhg59z,False,,0,False,"[R] Searching for Thomas The Tank Engine in StyleGAN2 human face generator's latent space. Not quite getting there, as a trained StyleGAN2 are specified to one type of objects to create images of (.. that they were trained with).",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gcth1k,False,dark,0.83,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'StyleGAN2 generates ""Thomas The Tank Engine"" as Human | N2 Artifical Intelligence', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'N2 - Artifical Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cdWV14W_Kww/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCIkA_Pi0VWSABdyAnmildpg'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcth1k', 'height': 338}",,False,4,,False,https://b.thumbs.redditmedia.com/_S3_djMsJ8MAjZX0nfAZZRRDPuInm4BnQ2dbLgUgg8I.jpg,False,,[],{},rich:video,,False,,1588552182.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?auto=webp&amp;s=facb6efd657d9e8cbc16b63154ae246c69c53703', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3921f7f13f93584fb09b2662beec9f33d7d87e19', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd8fe4095e067125a11c8720f9e947c0ab405767', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a916124c0e3c688968b32e88cda580dfdc3fbac1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'EKnjmCWo0GdIaG8PH2_X_ya2B8qqBL9aEe7_NvLa704'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcth1k,True,,FNBR4,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcth1k/r_searching_for_thomas_the_tank_engine_in/,all_ads,False,https://youtu.be/cdWV14W_Kww,155203,1588523382.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'StyleGAN2 generates ""Thomas The Tank Engine"" as Human | N2 Artifical Intelligence', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'N2 - Artifical Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cdWV14W_Kww/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCIkA_Pi0VWSABdyAnmildpg'}}",False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '', 'author_fullname': 't2_28bhg59z', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""[R] Searching for Thomas The Tank Engine in StyleGAN2 human face generator's latent space. Not quite getting there, as a trained StyleGAN2 are specified to one type of objects to create images of (.. that they were trained with)."", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'three', 'downs': 0, 'thumbnail_height': 105, 'hide_score': False, 'name': 't3_gciikg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 60, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'StyleGAN2 generates ""Thomas The Tank Engine"" as Human | N2 Artifical Intelligence', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'N2 - Artifical Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cdWV14W_Kww/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCIkA_Pi0VWSABdyAnmildpg'}, 'type': 'youtube.com'}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gciikg', 'height': 338}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 60, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/_S3_djMsJ8MAjZX0nfAZZRRDPuInm4BnQ2dbLgUgg8I.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588499984.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?auto=webp&amp;s=facb6efd657d9e8cbc16b63154ae246c69c53703', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3921f7f13f93584fb09b2662beec9f33d7d87e19', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd8fe4095e067125a11c8720f9e947c0ab405767', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/-2eeWiSIRXXI9VAU9adjpBRLjXatMv76Xld31tngPYA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a916124c0e3c688968b32e88cda580dfdc3fbac1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'EKnjmCWo0GdIaG8PH2_X_ya2B8qqBL9aEe7_NvLa704'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gciikg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'FNBR4', 'discussion_type': None, 'num_comments': 22, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/gciikg/r_searching_for_thomas_the_tank_engine_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://youtu.be/cdWV14W_Kww', 'subreddit_subscribers': 1044414, 'created_utc': 1588471184.0, 'num_crossposts': 4, 'media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'StyleGAN2 generates ""Thomas The Tank Engine"" as Human | N2 Artifical Intelligence', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/cdWV14W_Kww?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'N2 - Artifical Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/cdWV14W_Kww/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCIkA_Pi0VWSABdyAnmildpg'}, 'type': 'youtube.com'}, 'is_video': False}]",t3_gciikg,,
,learnmachinelearning,,t2_iy9zket,False,,0,False,When to use standardization and normalisation?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gd0fkn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,default,False,,[],{},,,False,,1588576785.0,richtext,6,,,text,self.MLQuestions,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gd0fkn,True,,cybrotex,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gd0fkn/when_to_use_standardization_and_normalisation/,all_ads,False,/r/MLQuestions/comments/gd0d2m/when_to_use_standardization_and_normalisation/,155203,1588547985.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MLQuestions', 'selftext': 'I explored kernels from the competition ""House Prices: Advanced Regression Techniques"" (link below), and noticed that some use standardization and other use normalization. What I want to know is when to use each one of them.', 'author_fullname': 't2_iy9zket', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'When to use standardization and normalisation?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MLQuestions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gd0d2m', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588576498.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MLQuestions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I explored kernels from the competition &amp;quot;House Prices: Advanced Regression Techniques&amp;quot; (link below), and noticed that some use standardization and other use normalization. What I want to know is when to use each one of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_30rel', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gd0d2m', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cybrotex', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MLQuestions/comments/gd0d2m/when_to_use_standardization_and_normalisation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MLQuestions/comments/gd0d2m/when_to_use_standardization_and_normalisation/', 'subreddit_subscribers': 27178, 'created_utc': 1588547698.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_gd0d2m,,
,learnmachinelearning,,t2_3d8dg3uh,False,,0,False,AI Generates a New Sharingan | Using GAN To Generate SharinGAN,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gc670g,False,light,0.94,,public,434,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8fnynVsR53k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates a New Sharingan | Using GAN To Generate SharinGAN', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8fnynVsR53k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8fnynVsR53k/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8fnynVsR53k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gc670g', 'height': 338}",Project,False,434,,False,https://a.thumbs.redditmedia.com/uWp5NuBm3dRnvHznJk3H1c3ISzcscMtLNJOfpBAVF98.jpg,False,,[],{},rich:video,,False,,1588453754.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6BmlngXBRlvZHISk-_O7c9ZwzSBjT1VfV-U0iM2t05w.jpg?auto=webp&amp;s=31ecf762f07569e86b9d9c35f6c5ebf6a406c22d', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/6BmlngXBRlvZHISk-_O7c9ZwzSBjT1VfV-U0iM2t05w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8b0c33689e6dd224901dad1440fedf273c1993e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/6BmlngXBRlvZHISk-_O7c9ZwzSBjT1VfV-U0iM2t05w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=42ae7ed44a9b1c2f5df8252777691082d7913bd9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/6BmlngXBRlvZHISk-_O7c9ZwzSBjT1VfV-U0iM2t05w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2626b24b880f320e5ad0d1d1fa3e292e18ed396e', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'eaQbVPknNe6jAqcokyJfo4gRSMxU9qzdRYCrQX-0Exw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gc670g,True,,oFlamingo,,58,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc670g/ai_generates_a_new_sharingan_using_gan_to/,all_ads,False,https://youtu.be/8fnynVsR53k,155203,1588424954.0,1,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates a New Sharingan | Using GAN To Generate SharinGAN', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8fnynVsR53k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8fnynVsR53k/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,,,,
,learnmachinelearning,,t2_3tqetf9o,False,,0,False,"Exploratory Data Analysis &amp; Feature Engineering Of Over 2000 OSRS Players | Runescape Bot Classification Project | Tried to make it more educational this time, enjoy!","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gcsxka,False,light,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3LH1TYoqDDI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'The Anatomy Of Runescape Bots | OSRS Bot Classification with Python and Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3LH1TYoqDDI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Chronic Coder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3LH1TYoqDDI/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC04Lk4QIh1DvAOILA098wxw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3LH1TYoqDDI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcsxka', 'height': 338}",Project,False,3,,True,https://a.thumbs.redditmedia.com/604hJIR1dGG9othr4dFXzb2FThCj1089gCwB31wp5D4.jpg,False,,[],{},rich:video,,False,,1588550285.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-rRtI_pZwsczWpi4Dfd9yKWEwPDJnIVlIGmlLfvZKFk.jpg?auto=webp&amp;s=debc747e0a78d7e482bcaebc51532c24ef0d441e', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/-rRtI_pZwsczWpi4Dfd9yKWEwPDJnIVlIGmlLfvZKFk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8fa4fc35b4e4d663d67c04dcb286cf1b9d4497d7', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/-rRtI_pZwsczWpi4Dfd9yKWEwPDJnIVlIGmlLfvZKFk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5bc89229175842a99f23f729030c132784361d9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/-rRtI_pZwsczWpi4Dfd9yKWEwPDJnIVlIGmlLfvZKFk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4830fd983eb38611f64a8ad2d46ac445825d033', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'HPGkrT5hAdDBEWayEzCotrCgQICM7A7sKJWk12L0AJg'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gcsxka,True,,chriskok1337,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcsxka/exploratory_data_analysis_feature_engineering_of/,all_ads,False,https://youtu.be/3LH1TYoqDDI,155203,1588521485.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'The Anatomy Of Runescape Bots | OSRS Bot Classification with Python and Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3LH1TYoqDDI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Chronic Coder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3LH1TYoqDDI/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC04Lk4QIh1DvAOILA098wxw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hi. I am starting with reinforcement learning and have completed several coding by myself. I am replicating the A3C model for my personal interest. However, A3C seem not to converge on PongDeterministic-v4 environment. I try the same model for Cartpole and it works. So I do not know what is wrong with my implementation for Pong. I did let it run for several days and try everything I can but the score average at -20.5.  
[https://github.com/Leonard1904/reinforcement-learning](https://github.com/Leonard1904/reinforcement-learning)

If anyone see this. Please help me.",t2_n31cshn,False,,0,False,[R] A3C LSTM not converge on Pong,[],r/learnmachinelearning,False,6,,0,,False,t3_gcx35a,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588564744.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I am starting with reinforcement learning and have completed several coding by myself. I am replicating the A3C model for my personal interest. However, A3C seem not to converge on PongDeterministic-v4 environment. I try the same model for Cartpole and it works. So I do not know what is wrong with my implementation for Pong. I did let it run for several days and try everything I can but the score average at -20.5.&lt;br/&gt;
&lt;a href=""https://github.com/Leonard1904/reinforcement-learning""&gt;https://github.com/Leonard1904/reinforcement-learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If anyone see this. Please help me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ev01-ezPuss0xa5eMgcPUnU3cSaUdkS1WKpncBbtAO0.jpg?auto=webp&amp;s=0da53295b3de14cf5a5cf2ff805d33165284b8d5', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/ev01-ezPuss0xa5eMgcPUnU3cSaUdkS1WKpncBbtAO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05615275ef1fd8cfa2a95f32d913ed846075d33b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ev01-ezPuss0xa5eMgcPUnU3cSaUdkS1WKpncBbtAO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74dfdf6dd58446890d659e1d0eb27d32b7c5cf6d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ev01-ezPuss0xa5eMgcPUnU3cSaUdkS1WKpncBbtAO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f97a981be56c799c157eadd873f265b006f8c81', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'HF1KimCgi3OCSG6Rdchj6cKDEve0xlQEalXlb_P_QWk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcx35a,True,,lordknight1904,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcx35a/r_a3c_lstm_not_converge_on_pong/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcx35a/r_a3c_lstm_not_converge_on_pong/,155203,1588535944.0,0,,False,,,,
,learnmachinelearning,"I wrote my own neural network library in Rust and I ported it to C++ today. I just want to know if implemented it properly since giving it data from a XOR table gives unpromising results but for anything else Iâ€™ve tried until now it seems ok. Hereâ€™s the links to both projects:
[synapse (Rust)](https://github.com/ptrstr/synapse)
[cynapse (C++)](https://github.com/ptrstr/cynapse)",t2_6d9o5x4r,False,,0,False,I wrote my own neural network library. Not sure my implementation is good though.,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcii5u,False,light,0.97,,public,27,0,{},,,False,[],,False,False,,{},Project,False,27,,False,self,False,,[],{},self,,True,,1588499935.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote my own neural network library in Rust and I ported it to C++ today. I just want to know if implemented it properly since giving it data from a XOR table gives unpromising results but for anything else Iâ€™ve tried until now it seems ok. Hereâ€™s the links to both projects:
&lt;a href=""https://github.com/ptrstr/synapse""&gt;synapse (Rust)&lt;/a&gt;
&lt;a href=""https://github.com/ptrstr/cynapse""&gt;cynapse (C++)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/l6NvBlkyD5Rfa7su17n9Z5IkARosUnoQnYxkpfYI-Hk.jpg?auto=webp&amp;s=79e906981df22440c265c5f64ade10cf68fe6a32', 'width': 275, 'height': 275}, 'resolutions': [{'url': 'https://external-preview.redd.it/l6NvBlkyD5Rfa7su17n9Z5IkARosUnoQnYxkpfYI-Hk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5963bcb9a2d7cb4df7d967651d12b2cafa76722', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/l6NvBlkyD5Rfa7su17n9Z5IkARosUnoQnYxkpfYI-Hk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=523f62d36fe109ac9b8638a17991dc6f7f7bf5d3', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'xvidenbKoL7kWA98nOVBU8WjW822HICdd1mfcbHh3ck'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gcii5u,True,,ptrstr,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcii5u/i_wrote_my_own_neural_network_library_not_sure_my/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcii5u/i_wrote_my_own_neural_network_library_not_sure_my/,155203,1588471135.0,0,,False,,,,
,learnmachinelearning,,t2_702gf,False,,0,False,how to combine recursive feature elimination and grid/random search inside one CV loop?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcvu55,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},link,,False,,1588560365.0,text,6,,,text,self.scikit_learn,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?auto=webp&amp;s=e25be8cb5cf2448d39a7e5ffc877e4d466b776d3', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9645cb9b1f6437724f04c3b63b841cf7766f27d6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7ee570d3611e1ef9314d658423c42908808820', 'width': 216, 'height': 216}], 'variants': {}, 'id': '63C1GbYQbI4tHZMkw99e-qlyoYaGnG58yfnmnFUJ34s'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcvu55,True,,ezeeetm,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcvu55/how_to_combine_recursive_feature_elimination_and/,all_ads,False,/r/scikit_learn/comments/gcvtsm/how_to_combine_recursive_feature_elimination_and/,155203,1588531565.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'scikit_learn', 'selftext': 'I\'ve seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:\n\n[Feature selection and cross-validation](https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751)\n\n[Nested cross-validation and feature selection: when to perform the feature selection?](https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec)\n\n[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)\n\n&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...\n\n[Here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py) is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.\n\nHowever I\'d like to do recursive feature elimination inside random/grid CV, so that ""feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)"", so that data from other folds influence neither feature selection nor hyperparameter optimization.\n\nIs this possible natively with sklearn methods and/or pipelines? Basically, I\'m trying to find an sklearn native way to do this before I go code it from scratch.', 'author_fullname': 't2_702gf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'how to combine recursive feature elimination and grid/random search inside one CV loop?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/scikit_learn', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gcvtsm', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 3, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588560330.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.scikit_learn', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve seen taught several places that feature selection needs to be inside the CV training loop. Here are three examples where I have seen this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation/27751#27751""&gt;Feature selection and cross-validation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://stats.stackexchange.com/questions/223740/nested-cross-validation-and-feature-selection-when-to-perform-the-feature-selec""&gt;Nested cross-validation and feature selection: when to perform the feature selection?&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://machinelearningmastery.com/an-introduction-to-feature-selection/""&gt;https://machinelearningmastery.com/an-introduction-to-feature-selection/&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;...you must include feature selection within the inner-loop when you are using accuracy estimation methods such as cross-validation. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=""https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py""&gt;Here&lt;/a&gt; is an example from the sklearn docs, that shows how to do recursive feature elimination with regular n-fold cross validation.&lt;/p&gt;\n\n&lt;p&gt;However I&amp;#39;d like to do recursive feature elimination inside random/grid CV, so that &amp;quot;feature selection is performed on the prepared fold right before the model is trained (on the random/grid selected params for that fold)&amp;quot;, so that data from other folds influence neither feature selection nor hyperparameter optimization.&lt;/p&gt;\n\n&lt;p&gt;Is this possible natively with sklearn methods and/or pipelines? Basically, I&amp;#39;m trying to find an sklearn native way to do this before I go code it from scratch.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?auto=webp&amp;s=e25be8cb5cf2448d39a7e5ffc877e4d466b776d3', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9645cb9b1f6437724f04c3b63b841cf7766f27d6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7ee570d3611e1ef9314d658423c42908808820', 'width': 216, 'height': 216}], 'variants': {}, 'id': '63C1GbYQbI4tHZMkw99e-qlyoYaGnG58yfnmnFUJ34s'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_338dz', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gcvtsm', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ezeeetm', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/scikit_learn/comments/gcvtsm/how_to_combine_recursive_feature_elimination_and/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/scikit_learn/comments/gcvtsm/how_to_combine_recursive_feature_elimination_and/', 'subreddit_subscribers': 2485, 'created_utc': 1588531530.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_gcvtsm,,
,learnmachinelearning,"I made a numpy matrix X and a vector y

X=[1,2,3,4,5]
Y=[1,4,9,16,25]. 

I want to implement a neural network with 1 hidden layer to predict the value of y for any given value of X. But I cannot implement it for some reason with Keras. 

Can you please write a code  for it? If I succeed then I can implement Neural Networks for other datasets",t2_5fi9eeym,False,,0,False,I can't write a simple pyrhon code to implement a Neural Network with Keras in this problem.,[],r/learnmachinelearning,False,6,,0,,False,t3_gcvhyz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588559220.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I made a numpy matrix X and a vector y&lt;/p&gt;

&lt;p&gt;X=[1,2,3,4,5]
Y=[1,4,9,16,25]. &lt;/p&gt;

&lt;p&gt;I want to implement a neural network with 1 hidden layer to predict the value of y for any given value of X. But I cannot implement it for some reason with Keras. &lt;/p&gt;

&lt;p&gt;Can you please write a code  for it? If I succeed then I can implement Neural Networks for other datasets&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcvhyz,True,,dhokna,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcvhyz/i_cant_write_a_simple_pyrhon_code_to_implement_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcvhyz/i_cant_write_a_simple_pyrhon_code_to_implement_a/,155203,1588530420.0,0,,False,,,,
,learnmachinelearning,,t2_66w2r,False,,0,False,Whatlies: Package to Interactively Visualise Word Embeddings,[],r/learnmachinelearning,False,6,,0,128.0,False,t3_gcog48,False,dark,0.72,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/OumceFTZPnx4aT9pAJaVrzaQ-rxJ_8A2cUWqo5oUy6I.jpg,False,,[],{},link,,False,,1588530885.0,text,6,,,text,rasahq.github.io,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/W6ayOcdf8hkpgMDD63LpU4h4BQYvGjoQh3mNEO7W8xE.jpg?auto=webp&amp;s=f4418ecfa9ffb3759c32983121a8040c7877a0d4', 'width': 369, 'height': 340}, 'resolutions': [{'url': 'https://external-preview.redd.it/W6ayOcdf8hkpgMDD63LpU4h4BQYvGjoQh3mNEO7W8xE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3959504cebcabe72bbfe05aa5d490454040903a', 'width': 108, 'height': 99}, {'url': 'https://external-preview.redd.it/W6ayOcdf8hkpgMDD63LpU4h4BQYvGjoQh3mNEO7W8xE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1729e1aff4ad87c6c769cbab9ea10d94e511bed', 'width': 216, 'height': 199}, {'url': 'https://external-preview.redd.it/W6ayOcdf8hkpgMDD63LpU4h4BQYvGjoQh3mNEO7W8xE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3000aee0b27bbceb1bc02bc1259b2830e70b8422', 'width': 320, 'height': 294}], 'variants': {}, 'id': 'L3XUa7B3bFZHjUEcKhpPA5sE3JmA6_nSRK2DuE-fgmc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcog48,True,,cantdutchthis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcog48/whatlies_package_to_interactively_visualise_word/,all_ads,False,https://rasahq.github.io/whatlies/,155203,1588502085.0,0,,False,,,,
,learnmachinelearning,How do I retrain the Inception model with 10 or so classes and also use it with openCV to make real-time predictions?,t2_5umteyyn,False,,0,False,I am new to ML and wanted help with a Computer Vision project. Was wondering if someone could provide a good tutorial for this?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcu3u2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588554418.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do I retrain the Inception model with 10 or so classes and also use it with openCV to make real-time predictions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcu3u2,True,,LyuSifer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcu3u2/i_am_new_to_ml_and_wanted_help_with_a_computer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcu3u2/i_am_new_to_ml_and_wanted_help_with_a_computer/,155203,1588525618.0,0,,False,,,,
,learnmachinelearning," Hi all!

I'm working with a Deep Learning model which has a ResNet-50 as backbone pretrained on ImageNet. The dataset that I'm using is the CUB-200, which is a set of 200 species of birds. For this reason I think that could be good to have a pretrained model on a dataset that has a similar domain and I found that the iNaturalist one could be the one that I'm looking for.

The problem is that I didn't find any pretrained model for Pytorch, but only a Tensorflow one [here](https://github.com/richardaecn/cvpr18-inaturalist-transfer).

I tried to convert it using the MDNN library, but it needs also the '.ckpt.meta' file extend and I have only the '.ckpt'.

This is an example of how to use the MDNN library to convert a tf model to torch:

    mmconvert -sf tensorflow -in imagenet_resnet_v2_152.ckpt.meta -iw imagenet_resnet_v2_152.ckpt --dstNode MMdnn_Output -df pytorch -om tf_to_pytorch_resnet_152.pth

Could anyone help me with it?",t2_30b4spgy,False,,0,False,How to convert a Tensorflow model to Pytorch?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcqh3y,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588540826.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working with a Deep Learning model which has a ResNet-50 as backbone pretrained on ImageNet. The dataset that I&amp;#39;m using is the CUB-200, which is a set of 200 species of birds. For this reason I think that could be good to have a pretrained model on a dataset that has a similar domain and I found that the iNaturalist one could be the one that I&amp;#39;m looking for.&lt;/p&gt;

&lt;p&gt;The problem is that I didn&amp;#39;t find any pretrained model for Pytorch, but only a Tensorflow one &lt;a href=""https://github.com/richardaecn/cvpr18-inaturalist-transfer""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I tried to convert it using the MDNN library, but it needs also the &amp;#39;.ckpt.meta&amp;#39; file extend and I have only the &amp;#39;.ckpt&amp;#39;.&lt;/p&gt;

&lt;p&gt;This is an example of how to use the MDNN library to convert a tf model to torch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mmconvert -sf tensorflow -in imagenet_resnet_v2_152.ckpt.meta -iw imagenet_resnet_v2_152.ckpt --dstNode MMdnn_Output -df pytorch -om tf_to_pytorch_resnet_152.pth
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Could anyone help me with it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zK9BBsWkuk3WpT17Vn8oQetpkDsFF4a0CwLlrnB92tk.jpg?auto=webp&amp;s=85a5db05b26006250c6a510f8aace3d58a7bf96b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/zK9BBsWkuk3WpT17Vn8oQetpkDsFF4a0CwLlrnB92tk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2836deb44ff80242b9488a944f5dfbf90714dd32', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/zK9BBsWkuk3WpT17Vn8oQetpkDsFF4a0CwLlrnB92tk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12a9119823daf46be6b13fe1438cb7bc8105d951', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/zK9BBsWkuk3WpT17Vn8oQetpkDsFF4a0CwLlrnB92tk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2cb0fc2341b562faa32db348dd13c8d745aaea9', 'width': 320, 'height': 320}], 'variants': {}, 'id': '5W__PXML1xSbFgek13bhx3pjmvwe26MSRzKEUajvBLs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcqh3y,True,,aleflabo,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcqh3y/how_to_convert_a_tensorflow_model_to_pytorch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcqh3y/how_to_convert_a_tensorflow_model_to_pytorch/,155203,1588512026.0,0,,False,,,,
,learnmachinelearning,"Hey everyone! I see a lot of people asking for advice on here about how they should get into ml and data science and what the best ways to learn are, so I thought I should just do one big post instead of replying to every post everyone makes. I don't consider myself to be a master or super expert but I am an ml engineer so I can share some advice I would've loved when I was just getting started, I hope you all find this helpful!

Bite sized tips I would give (feel free to ask me to build on them if I don't explain them properly):

* Learn the math.I know it's tedious but it's something that you'll thank yourself for later. A lot of concepts like linear algebra and mathematical optimization will come in handy for almost any ml problem.
* Don't try to learn &amp; memorize everything.A lot of people try to cram their brains full of ml knowledge in one go and find that they simply can't retain any of it, which can be very frustrating. Download a program like ""Notion"" or ""Evernote"", whatever works for you, and take simple notes and code snippets as you learn.Don't try to memorize those notes, but rather just be aware that they're there. When you want to work some projects using techniques you know you've covered before, you can instantly access code snippets and pointers to refresh your memory. This is going to make learning much easier, because learning the exact syntax for mapping a function onto a matrix in some library is a waste of time for most beginners. It's better to learn what you need to do, and then have a reference for how you need to do it. Things like syntax will come to you over time the more you exercise your skills!Be aware though, some people tend to just repeat code they've written a hundred times before, which isn't always a good thing! The aim is to pick up concepts quickly not just copy and paste code you've written before, as you advance through a subject you should only be using this as a reference, not ever to copy and paste code from!
* Learn by projects!Project based learning IMO is the best way to pick up machine learning, and it works very well with the notes system above. Pick some random arbitrary project that you want to learn, e.g. handwriting recognition, and start a project in it using whatever ml and data techniques you've covered before.By doing this you'll gain a far greater understanding and appreciation of the process, not just working through the same old exercises online courses give you.Don't always try to find datasets to match your models, try to learn when to use each model to match a dataset. In industry, someone will give you data and you will have to analyze it, not give you a model and ask you to apply it to some random data that you think it'll work well with!
* Don't just learn ml.In industry, ml is almost never pure ml. There will always be some stack or some framework they want you to use to develop AND DEPLOY your models with. This is the most neglected thing (I think) by ml beginners.Learning about frameworks like Docker and Flask, as well as systems like AWS will help you build projects that are more compatible with how software is developed in large scale organizations, which will make your life much easier long term if you understand them now.

Of course there are many more points I could go into, but these are the most important so I want people to focus on these. And also of course this is just my opinion, if you don't find these things useful, that's absolutely okay! Everyone has their own system, I'm just sharing mine!

I hope this helps and I'm happy to build more on any of these things! If you have suggestions I'll also add them to the original post for everyone to see.

TL;DR

* Learn the math behind ml, you'll thank yourself for it.
* Don't burn yourself out by truing to remember everything you read or learn, make notes and refer back to them!
* Use projects to learn your way around different models, it'll help you get a feel for when each model and method is useful.
* Don't just learn ml, learn about code deployment and architectures, it'll help you really understand how your code will actually be used in industry.

edit: Let me know if I should do follow up posts on either useful learning content online, or project ideas for ml and data science!",t2_bukiyv9,False,,0,False,Advice for Beginners in ML and Data Science,[],r/learnmachinelearning,False,6,,0,,False,t3_gc5z6b,False,dark,0.98,,public,170,0,{},,,False,[],,False,False,,{},,False,170,,False,self,1588466704.0,,[],{},,,True,,1588452821.0,text,6,,,text,self.learnmachinelearning,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone! I see a lot of people asking for advice on here about how they should get into ml and data science and what the best ways to learn are, so I thought I should just do one big post instead of replying to every post everyone makes. I don&amp;#39;t consider myself to be a master or super expert but I am an ml engineer so I can share some advice I would&amp;#39;ve loved when I was just getting started, I hope you all find this helpful!&lt;/p&gt;

&lt;p&gt;Bite sized tips I would give (feel free to ask me to build on them if I don&amp;#39;t explain them properly):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learn the math.I know it&amp;#39;s tedious but it&amp;#39;s something that you&amp;#39;ll thank yourself for later. A lot of concepts like linear algebra and mathematical optimization will come in handy for almost any ml problem.&lt;/li&gt;
&lt;li&gt;Don&amp;#39;t try to learn &amp;amp; memorize everything.A lot of people try to cram their brains full of ml knowledge in one go and find that they simply can&amp;#39;t retain any of it, which can be very frustrating. Download a program like &amp;quot;Notion&amp;quot; or &amp;quot;Evernote&amp;quot;, whatever works for you, and take simple notes and code snippets as you learn.Don&amp;#39;t try to memorize those notes, but rather just be aware that they&amp;#39;re there. When you want to work some projects using techniques you know you&amp;#39;ve covered before, you can instantly access code snippets and pointers to refresh your memory. This is going to make learning much easier, because learning the exact syntax for mapping a function onto a matrix in some library is a waste of time for most beginners. It&amp;#39;s better to learn what you need to do, and then have a reference for how you need to do it. Things like syntax will come to you over time the more you exercise your skills!Be aware though, some people tend to just repeat code they&amp;#39;ve written a hundred times before, which isn&amp;#39;t always a good thing! The aim is to pick up concepts quickly not just copy and paste code you&amp;#39;ve written before, as you advance through a subject you should only be using this as a reference, not ever to copy and paste code from!&lt;/li&gt;
&lt;li&gt;Learn by projects!Project based learning IMO is the best way to pick up machine learning, and it works very well with the notes system above. Pick some random arbitrary project that you want to learn, e.g. handwriting recognition, and start a project in it using whatever ml and data techniques you&amp;#39;ve covered before.By doing this you&amp;#39;ll gain a far greater understanding and appreciation of the process, not just working through the same old exercises online courses give you.Don&amp;#39;t always try to find datasets to match your models, try to learn when to use each model to match a dataset. In industry, someone will give you data and you will have to analyze it, not give you a model and ask you to apply it to some random data that you think it&amp;#39;ll work well with!&lt;/li&gt;
&lt;li&gt;Don&amp;#39;t just learn ml.In industry, ml is almost never pure ml. There will always be some stack or some framework they want you to use to develop AND DEPLOY your models with. This is the most neglected thing (I think) by ml beginners.Learning about frameworks like Docker and Flask, as well as systems like AWS will help you build projects that are more compatible with how software is developed in large scale organizations, which will make your life much easier long term if you understand them now.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of course there are many more points I could go into, but these are the most important so I want people to focus on these. And also of course this is just my opinion, if you don&amp;#39;t find these things useful, that&amp;#39;s absolutely okay! Everyone has their own system, I&amp;#39;m just sharing mine!&lt;/p&gt;

&lt;p&gt;I hope this helps and I&amp;#39;m happy to build more on any of these things! If you have suggestions I&amp;#39;ll also add them to the original post for everyone to see.&lt;/p&gt;

&lt;p&gt;TL;DR&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learn the math behind ml, you&amp;#39;ll thank yourself for it.&lt;/li&gt;
&lt;li&gt;Don&amp;#39;t burn yourself out by truing to remember everything you read or learn, make notes and refer back to them!&lt;/li&gt;
&lt;li&gt;Use projects to learn your way around different models, it&amp;#39;ll help you get a feel for when each model and method is useful.&lt;/li&gt;
&lt;li&gt;Don&amp;#39;t just learn ml, learn about code deployment and architectures, it&amp;#39;ll help you really understand how your code will actually be used in industry.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;edit: Let me know if I should do follow up posts on either useful learning content online, or project ideas for ml and data science!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc5z6b,True,,Snickypickleton,,72,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc5z6b/advice_for_beginners_in_ml_and_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc5z6b/advice_for_beginners_in_ml_and_data_science/,155203,1588424021.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

I'm using PyTorch as the base framework in one of my first research works in machine learning and I was very glad to find out that [there is a pre-trained model for the Mask RCNN using ResNet50 on it](https://pytorch.org/docs/stable/torchvision/models.html#mask-r-cnn). Although I've had good results with this architecture, I would like to compare the obtained results with the same architecture, but with a deeper backbone (ResNet101). However, there is no pre-trained weights of Mask RCNN with ResNet101 in PyTorch. I tried to download the pre-trained weights of the fpn with a ResNet101 as a backbone and combine with Mask RCNN, but the results were not good. I'm not sure if this was expected nor that the code implementation is correct (the code is bellow if someone would like to see).

I'm trying to avoid to use TensorFlow, [although I know there is a very good implementation of Mask RCNN on it](https://github.com/matterport/Mask_RCNN), because I'm way more comfortable with PyTorch and future parts of the project will not rely on Mask RCNN, but they will rely on transfer learning from the backbone of Mask RCNN. I'm also avoiding at maximum to train the Mask with ResNet101 (I don't even know if I have the computional resources for that). I searched a lot over the internet to find a good implementation of Mask RCNN with ResNet101 as backbone, but I wasn't lucky. So I'm here to ask for help. What are my options now?

Thanks in advance

    # This code is a simple modification of part of the code found in:
    # https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html and
    # https://github.com/pytorch/vision/blob/master/torchvision/models/detection/mask_rcnn.py
    def maskrcnn_resnet101(progress=True, num_classes=91, pretrained_backbone=True):
    	backbone = resnet_fpn_backbone('resnet101', pretrained_backbone)
    	return MaskRCNN(backbone, num_classes)
    
    def get_instance_segmentation_model(num_classes, resnet101=False):
    	# load an instance segmentation model pre-trained on COCO
    	if resnet101:
    		model = maskrcnn_resnet101()
    	else:
    		model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
    
    	# get the number of input features for the classifier
    	in_features = model.roi_heads.box_predictor.cls_score.in_features
    	# replace the pre-trained head with a new one
    	model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    
    	# now get the number of input features for the mask classifier
    	in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    	hidden_layer = 256
    	# and replace the mask predictor with a new one
    	model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)
    
    	return model",t2_9oijw,False,,0,False,What are my options of pre-trained weights for Mask RCNN with ResNet 101 implemented in PyTorch?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcsur0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588550000.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using PyTorch as the base framework in one of my first research works in machine learning and I was very glad to find out that &lt;a href=""https://pytorch.org/docs/stable/torchvision/models.html#mask-r-cnn""&gt;there is a pre-trained model for the Mask RCNN using ResNet50 on it&lt;/a&gt;. Although I&amp;#39;ve had good results with this architecture, I would like to compare the obtained results with the same architecture, but with a deeper backbone (ResNet101). However, there is no pre-trained weights of Mask RCNN with ResNet101 in PyTorch. I tried to download the pre-trained weights of the fpn with a ResNet101 as a backbone and combine with Mask RCNN, but the results were not good. I&amp;#39;m not sure if this was expected nor that the code implementation is correct (the code is bellow if someone would like to see).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to avoid to use TensorFlow, &lt;a href=""https://github.com/matterport/Mask_RCNN""&gt;although I know there is a very good implementation of Mask RCNN on it&lt;/a&gt;, because I&amp;#39;m way more comfortable with PyTorch and future parts of the project will not rely on Mask RCNN, but they will rely on transfer learning from the backbone of Mask RCNN. I&amp;#39;m also avoiding at maximum to train the Mask with ResNet101 (I don&amp;#39;t even know if I have the computional resources for that). I searched a lot over the internet to find a good implementation of Mask RCNN with ResNet101 as backbone, but I wasn&amp;#39;t lucky. So I&amp;#39;m here to ask for help. What are my options now?&lt;/p&gt;

&lt;p&gt;Thanks in advance&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This code is a simple modification of part of the code found in:
# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html and
# https://github.com/pytorch/vision/blob/master/torchvision/models/detection/mask_rcnn.py
def maskrcnn_resnet101(progress=True, num_classes=91, pretrained_backbone=True):
    backbone = resnet_fpn_backbone(&amp;#39;resnet101&amp;#39;, pretrained_backbone)
    return MaskRCNN(backbone, num_classes)

def get_instance_segmentation_model(num_classes, resnet101=False):
    # load an instance segmentation model pre-trained on COCO
    if resnet101:
        model = maskrcnn_resnet101()
    else:
        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)

    # get the number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    # now get the number of input features for the mask classifier
    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    # and replace the mask predictor with a new one
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcsur0,True,,bpmsilva,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcsur0/what_are_my_options_of_pretrained_weights_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcsur0/what_are_my_options_of_pretrained_weights_for/,155203,1588521200.0,0,,False,,,,
,learnmachinelearning,"So far I have programmed a neuron in Python with one weight that calculates the solution by a gradient descent (it should double the input). But how do I calculate the changes of my weights if I have more than one weight? For example, a second input which should also be doubled. So that f(x1, x2) = 2\*x1 + 2\*x2

 [**https://pastebin.com/CF3HtaCC**](https://pastebin.com/CF3HtaCC)",t2_2xz6764h,False,,0,False,Backpropagation/gradient descent with multiple weights?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcskkc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588548943.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So far I have programmed a neuron in Python with one weight that calculates the solution by a gradient descent (it should double the input). But how do I calculate the changes of my weights if I have more than one weight? For example, a second input which should also be doubled. So that f(x1, x2) = 2*x1 + 2*x2&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://pastebin.com/CF3HtaCC""&gt;&lt;strong&gt;https://pastebin.com/CF3HtaCC&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?auto=webp&amp;s=e28867cccd2864fc170848b64bc44e6a778116b9', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b38a59d6140109bc38ed4f88b98bd4436dfe09b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=203ad20eac023d67d122c5f834516a4670799f63', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcskkc,True,,User1377420,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcskkc/backpropagationgradient_descent_with_multiple/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcskkc/backpropagationgradient_descent_with_multiple/,155203,1588520143.0,0,,False,,,,
,learnmachinelearning,Is there any work on Modelling EEG Signals (I. e. generating synthetic EEG)  using Spiking Neural networks?,t2_hkdtkwx,False,,0,False,Spiking Neural Networks EEG Signals Generation,[],r/learnmachinelearning,False,6,,0,,False,t3_gcsf8e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588548391.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any work on Modelling EEG Signals (I. e. generating synthetic EEG)  using Spiking Neural networks?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcsf8e,True,,nuki96,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcsf8e/spiking_neural_networks_eeg_signals_generation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcsf8e/spiking_neural_networks_eeg_signals_generation/,155203,1588519591.0,0,,False,,,,
,learnmachinelearning,"I got an infrared sensor but I can't find the driver for it. Fortunately it plugs into the microphone jack so I am able to access raw data it captures.

But I need to decode this data and I thought of building a classifier to decode IR remote signals, as well as lack of signal.

I was able to determine that it returns data in chunks however these chunks are not fixed-sized. I only know every chunk has an amount of bytes divisible by 4.

I also feel like there's no preprocessing to be done here.

How should I tackle these problems? What kind of algorithm should be the most appropriate?",t2_gkwa4,False,,0,False,How to build a classifier for variable length binary data?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcri72,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1588518391.0,,[],{},,,True,,1588544941.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got an infrared sensor but I can&amp;#39;t find the driver for it. Fortunately it plugs into the microphone jack so I am able to access raw data it captures.&lt;/p&gt;

&lt;p&gt;But I need to decode this data and I thought of building a classifier to decode IR remote signals, as well as lack of signal.&lt;/p&gt;

&lt;p&gt;I was able to determine that it returns data in chunks however these chunks are not fixed-sized. I only know every chunk has an amount of bytes divisible by 4.&lt;/p&gt;

&lt;p&gt;I also feel like there&amp;#39;s no preprocessing to be done here.&lt;/p&gt;

&lt;p&gt;How should I tackle these problems? What kind of algorithm should be the most appropriate?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcri72,True,,_Nexor,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcri72/how_to_build_a_classifier_for_variable_length/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcri72/how_to_build_a_classifier_for_variable_length/,155203,1588516141.0,0,,False,,,,
,learnmachinelearning,"I am trying to identify anomaly in the telecom network, where the supervised data set is not available. So I have tried to cluster the points in the  dataset and then apply some static thresholds on particular features to select the anomaly group.
Now, Is there a way to avoid these thresholds or make them dynamic?",t2_n33qd,False,,0,False,Unsupervised techniques for anomaly,[],r/learnmachinelearning,False,6,,0,,False,t3_gcqypw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588542925.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to identify anomaly in the telecom network, where the supervised data set is not available. So I have tried to cluster the points in the  dataset and then apply some static thresholds on particular features to select the anomaly group.
Now, Is there a way to avoid these thresholds or make them dynamic?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcqypw,True,,allkhush92,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcqypw/unsupervised_techniques_for_anomaly/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcqypw/unsupervised_techniques_for_anomaly/,155203,1588514125.0,0,,False,,,,
,learnmachinelearning,"I am thinking of getting a Tesla k40 GPU in addition to my current AMD Radeon HD 7800 GPU.  
The plan would be to run my PyTorch projects on the Tesla while using the AMD for the video output and general work. My mainboard has 2 more open GPU slots, one more PCI-e cable and around 300 Watts of power left, so this should not be the problem I guess.

Is that approach possible/feasible/recommendable? Has anyone experience with this? Furthermore, I have seen people adding additional fans to the Tesla K40. Doesn't it cool itself enough without a fan?

Ideally, I would just run my PyTorch on the Tesla while continuing to work on other stuff with the other AMD GPU.",t2_3p4d4hht,False,,0,False,"Is it possible to add a Tesla GPU for machine learning purpose only, to my AMD GPU which is used for normal working?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcn47e,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588523085.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am thinking of getting a Tesla k40 GPU in addition to my current AMD Radeon HD 7800 GPU.&lt;br/&gt;
The plan would be to run my PyTorch projects on the Tesla while using the AMD for the video output and general work. My mainboard has 2 more open GPU slots, one more PCI-e cable and around 300 Watts of power left, so this should not be the problem I guess.&lt;/p&gt;

&lt;p&gt;Is that approach possible/feasible/recommendable? Has anyone experience with this? Furthermore, I have seen people adding additional fans to the Tesla K40. Doesn&amp;#39;t it cool itself enough without a fan?&lt;/p&gt;

&lt;p&gt;Ideally, I would just run my PyTorch on the Tesla while continuing to work on other stuff with the other AMD GPU.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcn47e,True,,sh0rt_boy,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcn47e/is_it_possible_to_add_a_tesla_gpu_for_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcn47e/is_it_possible_to_add_a_tesla_gpu_for_machine/,155203,1588494285.0,0,,False,,,,
,learnmachinelearning," I'd like to be using pd.get\_dummies for new data such that its output looks exactly like the output of pd.get\_dummies I get when I run it on my trainset, is there a way to do it and if not, what other alternatives do you guys use",t2_kfdra,False,,0,False,pd.get_dummies on new data,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcpfb9,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588535953.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;d like to be using pd.get_dummies for new data such that its output looks exactly like the output of pd.get_dummies I get when I run it on my trainset, is there a way to do it and if not, what other alternatives do you guys use&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcpfb9,True,,Unchart3disOP,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcpfb9/pdget_dummies_on_new_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcpfb9/pdget_dummies_on_new_data/,155203,1588507153.0,0,,False,,,,
,learnmachinelearning,,t2_2ic7kgpw,False,,0,False,[Career Tip] Should I Choose Path of Software Engineer or Data Scientist,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gcorn1,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/me899cr59ms?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Which Path Should I Choose Software Engineer or Data Scientist?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/me899cr59ms?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Ittwist', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/me899cr59ms/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCkrC9VDBfSuv527u8b1QK9g'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/me899cr59ms?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcorn1', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/nXU1wNgW3oBXeuMpTm7WZbMJwegGI_jDAFtYlCQ0ltg.jpg,False,,[],{},rich:video,,False,,1588532611.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1cS1DgeSh-QXOMJq8TXcy6HBYe8BvhZB9a7gWjn8YbY.jpg?auto=webp&amp;s=1d590641bb5fd6ecd0a4fed3154d92a6cc60181f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/1cS1DgeSh-QXOMJq8TXcy6HBYe8BvhZB9a7gWjn8YbY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a07f4be9334a37b6328390839482aaf51f58a5b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/1cS1DgeSh-QXOMJq8TXcy6HBYe8BvhZB9a7gWjn8YbY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ea32c0554c12a20f7739252189561ed0872f479', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/1cS1DgeSh-QXOMJq8TXcy6HBYe8BvhZB9a7gWjn8YbY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a02b6b1bade77395059e00882ad3a348878313ab', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'o6LEZ6iwje32EALvtyHp2VDfh7FTgA0IWRR_x46llHQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcorn1,True,,gauravlogical,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcorn1/career_tip_should_i_choose_path_of_software/,all_ads,False,https://youtu.be/me899cr59ms,155203,1588503811.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Which Path Should I Choose Software Engineer or Data Scientist?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/me899cr59ms?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Ittwist', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/me899cr59ms/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCkrC9VDBfSuv527u8b1QK9g'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"What happens if it is constantly fed conflicting data? I tried researching it for myself (being only familiar with the concept of machine learning, but not it's actual workings) and only came away with a few articles saying that you just shouldn't do that and that data must be ""cleaned"" before being input for machine learning. Can someone help answer and clarify this for me?",t2_98b3n,False,,0,False,Quick question about Machine Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gclhb8,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588513615.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What happens if it is constantly fed conflicting data? I tried researching it for myself (being only familiar with the concept of machine learning, but not it&amp;#39;s actual workings) and only came away with a few articles saying that you just shouldn&amp;#39;t do that and that data must be &amp;quot;cleaned&amp;quot; before being input for machine learning. Can someone help answer and clarify this for me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gclhb8,True,,smitened,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gclhb8/quick_question_about_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gclhb8/quick_question_about_machine_learning/,155203,1588484815.0,1,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,AI turns humans into animals,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gcc179,False,light,1.0,,public,8,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/1zIdgkl_CEw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI turns humans into animals', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/1zIdgkl_CEw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1zIdgkl_CEw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/1zIdgkl_CEw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcc179', 'height': 344}",Discussion,False,8,,False,https://a.thumbs.redditmedia.com/_comfp-py6p6btE9baZ-G9KQBXsXTUe00GgapU59Zn4.jpg,False,,[],{},rich:video,,False,,1588475587.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aaJF21kURapwVh-X_R4hZgYZzvwLVIntFkF8WtRRwZA.jpg?auto=webp&amp;s=be0e7422968dc101b4e98048587de2850c9144cd', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/aaJF21kURapwVh-X_R4hZgYZzvwLVIntFkF8WtRRwZA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=03b8a4bdad60223ec313b14e7c67dd426ab91186', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/aaJF21kURapwVh-X_R4hZgYZzvwLVIntFkF8WtRRwZA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7506a60f5bf2b92cb4ab735fa64a7f3be793a986', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/aaJF21kURapwVh-X_R4hZgYZzvwLVIntFkF8WtRRwZA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfcae15ec18cc6d8b53671c276998939a16454d7', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zRCZEdlg_kEq558yBXS_v2TZHtTlJ11VoHCJmc75ODU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gcc179,True,,cmillionaire9,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcc179/ai_turns_humans_into_animals/,all_ads,False,https://youtu.be/1zIdgkl_CEw,155203,1588446787.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI turns humans into animals', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/1zIdgkl_CEw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/1zIdgkl_CEw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"So, FaceApp has been pretty popular of late. How do I train a model, given an image outputs the older/younger version of it? I have a dataset of images of people when they were young and now when they are old. What kind of DL technique do I use ?",t2_1w1osf7s,False,,0,False,Training a model to tranform people's age?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcl9eu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588512449.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, FaceApp has been pretty popular of late. How do I train a model, given an image outputs the older/younger version of it? I have a dataset of images of people when they were young and now when they are old. What kind of DL technique do I use ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcl9eu,True,,titian101,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcl9eu/training_a_model_to_tranform_peoples_age/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcl9eu/training_a_model_to_tranform_peoples_age/,155203,1588483649.0,0,,False,,,,
,learnmachinelearning,I'm doing the Neural Networks and Deep Learning book  [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) and I'm trying to clone a github repo for the code used in the book. Here's the github repo  [https://github.com/MichalDanielDobrzanski/DeepLearningPython35](https://github.com/MichalDanielDobrzanski/DeepLearningPython35) . My problem is that I'm trying to clone it on Google Colab but I don't know how to. I'm a newbie in python. Please help.,t2_1mxss6fp,False,,0,False,How to clone a github repo in google colab,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcdb3w,False,light,1.0,,public,5,0,{},,,False,[],,False,False,,{},HELP,False,5,,False,self,False,,[],{},self,,True,,1588480100.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing the Neural Networks and Deep Learning book  &lt;a href=""http://neuralnetworksanddeeplearning.com/""&gt;http://neuralnetworksanddeeplearning.com/&lt;/a&gt; and I&amp;#39;m trying to clone a github repo for the code used in the book. Here&amp;#39;s the github repo  &lt;a href=""https://github.com/MichalDanielDobrzanski/DeepLearningPython35""&gt;https://github.com/MichalDanielDobrzanski/DeepLearningPython35&lt;/a&gt; . My problem is that I&amp;#39;m trying to clone it on Google Colab but I don&amp;#39;t know how to. I&amp;#39;m a newbie in python. Please help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?auto=webp&amp;s=a8cffd3e23c129ddb2dd6c593216a062fb8f1380', 'width': 200, 'height': 214}, 'resolutions': [{'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=06a541c2dd540bb4b74af834f56661b7512cc87c', 'width': 108, 'height': 115}], 'variants': {}, 'id': '_BeH_HLgJJJyvbJXuBeembjdUClSovixHNQF4PeqIeE'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcdb3w,True,,MisfitNJ,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcdb3w/how_to_clone_a_github_repo_in_google_colab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcdb3w/how_to_clone_a_github_repo_in_google_colab/,155203,1588451300.0,0,,False,,,,
,learnmachinelearning,"I'm a CS student. I recently built a powerful desktop system and instead of waiting until 3rd/4th year courses, I'm interested in learning machine learning by running my own models on my PC.  What type of software is useful for getting started? Is there any useful links or resources that would help me?",t2_152bq8,False,,0,False,How can i get started trying/making machine learning models on my desktop PC?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcl0ki,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588511209.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a CS student. I recently built a powerful desktop system and instead of waiting until 3rd/4th year courses, I&amp;#39;m interested in learning machine learning by running my own models on my PC.  What type of software is useful for getting started? Is there any useful links or resources that would help me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcl0ki,True,,Arenevian,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcl0ki/how_can_i_get_started_tryingmaking_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcl0ki/how_can_i_get_started_tryingmaking_machine/,155203,1588482409.0,0,,False,,,,
,learnmachinelearning,"I have a very simple yet scary looking equation which takes x,y,z values from user and gives output on the graph. Before giving any inputs I want to see how its general shapes look like. For example, the graph of log(x), sin(x) etc..

This is my equation. The output ranges between 0-1 as this graph looks like an 'S' shape probability curve.

**HOW DO I VISUALISE IT USING PYTHON?**

https://preview.redd.it/cy6shmbh6hw41.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=a87c310989c27dabf74f74cde44c5cf34b93c731",t2_652qy6ym,False,,0,False,How to generate a graph for this simple equation using Matplotlib,[],r/learnmachinelearning,False,6,,0,96.0,False,t3_gckcn8,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/c5JSL1b183tDSD-y_EEaviIhmEbJchXR_9tLtupSuJ8.jpg,False,,[],{},,,True,,1588507941.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a very simple yet scary looking equation which takes x,y,z values from user and gives output on the graph. Before giving any inputs I want to see how its general shapes look like. For example, the graph of log(x), sin(x) etc..&lt;/p&gt;

&lt;p&gt;This is my equation. The output ranges between 0-1 as this graph looks like an &amp;#39;S&amp;#39; shape probability curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HOW DO I VISUALISE IT USING PYTHON?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/cy6shmbh6hw41.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a87c310989c27dabf74f74cde44c5cf34b93c731""&gt;https://preview.redd.it/cy6shmbh6hw41.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a87c310989c27dabf74f74cde44c5cf34b93c731&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gckcn8,True,,parshu018,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gckcn8/how_to_generate_a_graph_for_this_simple_equation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gckcn8/how_to_generate_a_graph_for_this_simple_equation/,155203,1588479141.0,0,,False,,,"{'cy6shmbh6hw41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 74, 'x': 108, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6927bdda2785fa14e2d93d7ab80f36404fbd3919'}, {'y': 149, 'x': 216, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cc1d5718d381613c7eeb579bc7bae5d44ad219e6'}, {'y': 221, 'x': 320, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdfa8f1d932e2b400d1de14af5bb888696f4a366'}, {'y': 442, 'x': 640, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb8df13799a3a936ed022cfe1ffec030b48ebf5a'}, {'y': 663, 'x': 960, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce88768b652048185b2349f3ec9555d387342cd9'}, {'y': 746, 'x': 1080, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72e945ae71063edf74f5fdbcb02bf6a064ff85d7'}], 's': {'y': 885, 'x': 1280, 'u': 'https://preview.redd.it/cy6shmbh6hw41.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=a87c310989c27dabf74f74cde44c5cf34b93c731'}, 'id': 'cy6shmbh6hw41'}}",
,learnmachinelearning,,t2_zhsd6,False,,0,False,Google Brain : Intro to TFLite and TFLite Micro; Please RSVP here,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gc2bh7,False,dark,0.93,,public,46,0,{},140.0,,False,[],,False,False,,{},,False,46,,False,https://b.thumbs.redditmedia.com/dg-SvvM3FOymqYsg3yT3oE9ef2Sd_mx0B-FTeXuIpfg.jpg,False,,[],{},link,,False,,1588432655.0,text,6,,,text,meetup.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JiaSqrJNGvlrcBQiA7slDGX1jGzxrchGmChJ9GCOt1k.jpg?auto=webp&amp;s=711c96918f72b903f76cd7e8f4501e8d75c0bc18', 'width': 600, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/JiaSqrJNGvlrcBQiA7slDGX1jGzxrchGmChJ9GCOt1k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a367e7253a97531f17d7353e977f448dab46829c', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/JiaSqrJNGvlrcBQiA7slDGX1jGzxrchGmChJ9GCOt1k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7cb706f5ec360de2977b6c4e1681171c4c790023', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/JiaSqrJNGvlrcBQiA7slDGX1jGzxrchGmChJ9GCOt1k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f313786d81dc63438c8397f1e485c1fb590c1ce9', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'elMphgplvzokc5gU16NftyITnDDyFJBXLkiemXqvEZQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc2bh7,True,,gautamrbharadwaj,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc2bh7/google_brain_intro_to_tflite_and_tflite_micro/,all_ads,False,https://www.meetup.com/TFUG-Mysuru/events/270390225/,155203,1588403855.0,1,,False,,,,
,learnmachinelearning,"by Andrew Ng [CS229 from 2018](https://youtu.be/4b4MUYve_U8). First I was extremely exited, because this Course is in Python, unlike the course on Coursera by Andrew Ng that I have started three weeks ago. 
But at the end of the second lecture, things get much more complicated as I am not that familiar with Calculus etc. 

Does the course on YT continue to be so advanced?",t2_5lfdx43m,False,,0,False,Iâ€™ve found a New ML Course,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcgzvq,False,light,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,self,False,,[],{},self,,True,,1588493851.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;by Andrew Ng &lt;a href=""https://youtu.be/4b4MUYve_U8""&gt;CS229 from 2018&lt;/a&gt;. First I was extremely exited, because this Course is in Python, unlike the course on Coursera by Andrew Ng that I have started three weeks ago. 
But at the end of the second lecture, things get much more complicated as I am not that familiar with Calculus etc. &lt;/p&gt;

&lt;p&gt;Does the course on YT continue to be so advanced?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lsZp6UJds8lBufD5un2RuTBCru610yLX15ELoGmwSrM.jpg?auto=webp&amp;s=22ee62dc7ecce5ac3fe08bb589ff00fb0e0dc8ca', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/lsZp6UJds8lBufD5un2RuTBCru610yLX15ELoGmwSrM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdbda74bbbad5d2952fb5ff06639777e9552fe5a', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/lsZp6UJds8lBufD5un2RuTBCru610yLX15ELoGmwSrM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a8f86a2e7d78d950dc220d0eac651cc3e2d921f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/lsZp6UJds8lBufD5un2RuTBCru610yLX15ELoGmwSrM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eba614e9badc29b01240c4b10875e383ba89d6a4', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'GucBEyoRDbuuYubYiKbKTcSgEwU8tpuhkTCp9dNqVOU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gcgzvq,True,,invidae,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcgzvq/ive_found_a_new_ml_course/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcgzvq/ive_found_a_new_ml_course/,155203,1588465051.0,0,,False,,,,
,learnmachinelearning,"Something like [this](https://www.youtube.com/watch?v=MpEbfzztALM) is incredible, it lets you ghostwrite for your favourite artists. What tech-stack/models are likely used, and are there tutorials that exist that could help me out?

I've read the google blog post associated with it, but is it possible to create something similar on your own without advanced degrees in Machine Learning?",t2_bdyeq,False,,0,False,How do people generate voices of celebrities?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcjvdf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588477173.0,,[],{},self,,True,,1588505780.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Something like &lt;a href=""https://www.youtube.com/watch?v=MpEbfzztALM""&gt;this&lt;/a&gt; is incredible, it lets you ghostwrite for your favourite artists. What tech-stack/models are likely used, and are there tutorials that exist that could help me out?&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read the google blog post associated with it, but is it possible to create something similar on your own without advanced degrees in Machine Learning?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Wpbl4rH_kuD_BIrV6FT8ffw7VPUjvi0fTLhJ2JcqAng.jpg?auto=webp&amp;s=6607e2183c68d623fc6962fc2ceb34617cc889b9', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Wpbl4rH_kuD_BIrV6FT8ffw7VPUjvi0fTLhJ2JcqAng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d04c22395b2be7ea5641b1da66f7af8d9b1e0d52', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Wpbl4rH_kuD_BIrV6FT8ffw7VPUjvi0fTLhJ2JcqAng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0dd6b0fca1188dfd85aa1ce684fb6107420d6afc', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Wpbl4rH_kuD_BIrV6FT8ffw7VPUjvi0fTLhJ2JcqAng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6cae3a5cee097de32ce65f375088a472d67fe074', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'VI-5P0d9B-th9QW_cN7BTQgP8dEizTwNVJ40_HH-WUs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcjvdf,True,,BlasphemousBoss,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcjvdf/how_do_people_generate_voices_of_celebrities/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcjvdf/how_do_people_generate_voices_of_celebrities/,155203,1588476980.0,0,,False,,,,
,learnmachinelearning,"I've always thought it would be interesting how dynamic networks would perform. Was dabbling around on Wikipedia and stumbled across CC networks, but couldn't find a lot about how useful they are today as most resources and papers were published 10-20 years ago. I watched [this lecture](https://www.youtube.com/watch?v=k2mPEUZH978) but I dont think he talks about their uses today",t2_25liejo,False,,0,False,How relevant are Cascade Correlation Networks today? i.e. How do they perform compared to state of the art networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_gcfwxb,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588489672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve always thought it would be interesting how dynamic networks would perform. Was dabbling around on Wikipedia and stumbled across CC networks, but couldn&amp;#39;t find a lot about how useful they are today as most resources and papers were published 10-20 years ago. I watched &lt;a href=""https://www.youtube.com/watch?v=k2mPEUZH978""&gt;this lecture&lt;/a&gt; but I dont think he talks about their uses today&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/flvbW5axYLg2ZaECVVEPS4FQnA6gaVKIsxfJBx7FQ4g.jpg?auto=webp&amp;s=3742fe400f3789c39c1af22de1159204c748f3c3', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/flvbW5axYLg2ZaECVVEPS4FQnA6gaVKIsxfJBx7FQ4g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6819ab304b8dc327d3b5ba511f86dcd249499b3b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/flvbW5axYLg2ZaECVVEPS4FQnA6gaVKIsxfJBx7FQ4g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad4d45a435f8679058b0b5f8c6eded8427af8c1d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/flvbW5axYLg2ZaECVVEPS4FQnA6gaVKIsxfJBx7FQ4g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ffd5222f7aebe4bc93797a21816688a3d40a4f0', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Z9u7Gl2oTuGiFm8v5uJa-kcANFTZpgJAcR8oSDwCakc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcfwxb,True,,santoso-sheep,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfwxb/how_relevant_are_cascade_correlation_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcfwxb/how_relevant_are_cascade_correlation_networks/,155203,1588460872.0,0,,False,,,,
,learnmachinelearning,"[https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY](https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY)

**Deep Reinforcement Learning: Hands-on AI Tutorial in Python**

Develop Artificial Intelligence Applications using Reinforcement Learning in Python.

In this course we learn the concepts and fundamentals of reinforcement learning, it's relation to artificial intelligence and machine learning, and how we can formulate a problem in the context of reinforcement learning and Markov Decision Process. We cover different fundamental algorithms including Q-Learning, SARSA as well as Deep Q-Learning. We present the whole implementation of two projects from scratch with Q-learning and Deep Q-Network.

**What you'll learn**

* The concepts and fundamentals of reinforcement learning
* The main algorithms including Q-Learning, SARSA as well as Deep Q-Learning.
* How to formulate a problem in the context of reinforcement learning and MDP.
* Apply the learned techniques to some hands-on experiments and real world projects.
* Develop artificial intelligence applications using reinforcement learning.

[https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY](https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY)",t2_3fne80ni,False,,0,False,Deep Reinforcement Learning: Hands-on AI Tutorial in Python,[],r/learnmachinelearning,False,6,,0,,False,t3_gcfe1a,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588487662.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY""&gt;https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Reinforcement Learning: Hands-on AI Tutorial in Python&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Develop Artificial Intelligence Applications using Reinforcement Learning in Python.&lt;/p&gt;

&lt;p&gt;In this course we learn the concepts and fundamentals of reinforcement learning, it&amp;#39;s relation to artificial intelligence and machine learning, and how we can formulate a problem in the context of reinforcement learning and Markov Decision Process. We cover different fundamental algorithms including Q-Learning, SARSA as well as Deep Q-Learning. We present the whole implementation of two projects from scratch with Q-learning and Deep Q-Network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What you&amp;#39;ll learn&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The concepts and fundamentals of reinforcement learning&lt;/li&gt;
&lt;li&gt;The main algorithms including Q-Learning, SARSA as well as Deep Q-Learning.&lt;/li&gt;
&lt;li&gt;How to formulate a problem in the context of reinforcement learning and MDP.&lt;/li&gt;
&lt;li&gt;Apply the learned techniques to some hands-on experiments and real world projects.&lt;/li&gt;
&lt;li&gt;Develop artificial intelligence applications using reinforcement learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY""&gt;https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=LEARNINMAY&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IQIf2DkgMdVt5wrEbXPrV-Q8vD68Gze7KFUWuOgL0f0.jpg?auto=webp&amp;s=fc5b5574c902b7a13773af901b0bdfe675cc9996', 'width': 480, 'height': 270}, 'resolutions': [{'url': 'https://external-preview.redd.it/IQIf2DkgMdVt5wrEbXPrV-Q8vD68Gze7KFUWuOgL0f0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35726c76067e037f4d48f27d100e031055d8b178', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/IQIf2DkgMdVt5wrEbXPrV-Q8vD68Gze7KFUWuOgL0f0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1770606143f5cbce9b9b2f9ff1fa4cd323b49da', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/IQIf2DkgMdVt5wrEbXPrV-Q8vD68Gze7KFUWuOgL0f0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=95f37695ed0fde856fd1ab7e5c5b08d3499330fe', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'DMla3EvPGoel64edznAZoUKnBQfz-gnXWDecZCi3xSg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcfe1a,True,,mehdi_mka,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfe1a/deep_reinforcement_learning_handson_ai_tutorial/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcfe1a/deep_reinforcement_learning_handson_ai_tutorial/,155203,1588458862.0,0,,False,,,,
,learnmachinelearning,,t2_3n5irqjg,False,,0,False,How financial institutions can benefit from machine learning,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_gcfay0,False,dark,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/G5aUiEgglyEyGGBCbq15yxrKBVNj-9Vhr_7fgQyJxq8.jpg,False,,[],{},link,,False,,1588487343.0,text,6,,,text,thedatascientist.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?auto=webp&amp;s=949f6abae90a12c145941623a8efa8fc6fd1a292', 'width': 1600, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ad836671c40832a1e71975b87e8359d8ad711f7', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=748afd6db41955504866dc5df518d1927fb68ae9', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=721ac77b0f5deaeb0112a181a8b0eaaf07fea1f6', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b47f47e3a82cdc665134e1ec6d882980613e2e3', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=56a378810001af6dd4c3cf9ad4648ed254c086f2', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/zTBvK2vpFcWbihSJX4g8igf0Ur6_GRT8aKowOh1hQyU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9557de868a85315edb8845fc6942ee456a9400a4', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'vB9bFkQacF_WdZepchezB7zN-uoDksnoAO7pILT4zfk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcfay0,True,,TheTesseractAcademy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfay0/how_financial_institutions_can_benefit_from/,all_ads,False,https://thedatascientist.com/financial-institutions-machine-learning/,155203,1588458543.0,0,,False,,,,
,learnmachinelearning,"Hey there, I need help saying I just got into Machine Learning like Neural Networks and Autoencoders, is there a recommended OS/ PC/ Laptop to use? Thanks.",t2_3kvgiyr1,False,,0,False,Computer/ OS,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcip7r,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588500750.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, I need help saying I just got into Machine Learning like Neural Networks and Autoencoders, is there a recommended OS/ PC/ Laptop to use? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcip7r,True,,silverfoxreddits,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcip7r/computer_os/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcip7r/computer_os/,155203,1588471950.0,0,,False,,,,
,learnmachinelearning,,t2_4w138zf2,False,,0,False,Categorizing Products at Scale [Shopify],"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,74.0,False,t3_gcb8re,False,light,1.0,,public,4,0,{},140.0,,False,[],,False,False,,{},Discussion,False,4,,False,https://b.thumbs.redditmedia.com/8XZgVad363z9Ja_9z-cROEk5bpdNLT5WiFCc0ILPY7Q.jpg,False,,[],{},link,,False,,1588472757.0,richtext,6,,,text,engineering.shopify.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9zd14u-_rvg9RO-742i5UDocRC4oMIR9jAaWdT5l6Fs.jpg?auto=webp&amp;s=da77c44fbbb8a1b4a26e70d062b0cfd92ca8050b', 'width': 924, 'height': 494}, 'resolutions': [{'url': 'https://external-preview.redd.it/9zd14u-_rvg9RO-742i5UDocRC4oMIR9jAaWdT5l6Fs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4aa299898f32558fd3211562cc09d869f5d2aca2', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/9zd14u-_rvg9RO-742i5UDocRC4oMIR9jAaWdT5l6Fs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d8f7a41ceda622f4a2e468ffd77d974d1360c0b', 'width': 216, 'height': 115}, {'url': 'https://external-preview.redd.it/9zd14u-_rvg9RO-742i5UDocRC4oMIR9jAaWdT5l6Fs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27b7e30d1394cc42df32da373e44a452af6af6cf', 'width': 320, 'height': 171}, {'url': 'https://external-preview.redd.it/9zd14u-_rvg9RO-742i5UDocRC4oMIR9jAaWdT5l6Fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c2de5f567b4a9cc9b30e705b7ef07d49c194bb4', 'width': 640, 'height': 342}], 'variants': {}, 'id': 'QIIEykTP0QsA01Oss8zmQbqNb5CZYJTu8WNi7MMb0NI'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gcb8re,True,,ygunna12,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcb8re/categorizing_products_at_scale_shopify/,all_ads,False,https://engineering.shopify.com/blogs/engineering/categorizing-products-at-scale,155203,1588443957.0,0,,False,,,,
,learnmachinelearning,"I'm new to machine learning, my problem is I'm able to understand all the concepts clearly but I'm very confused with the programming part. I can see other's code and understand what's what but when I sit and try to work on a project I'm just tied up not knowing what or where to start. Can anyone help me find the best resources to understand the programming part behind the scenes?",t2_2jidkwsz,False,,0,False,Need help with programming part,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcegm9,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588484203.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m new to machine learning, my problem is I&amp;#39;m able to understand all the concepts clearly but I&amp;#39;m very confused with the programming part. I can see other&amp;#39;s code and understand what&amp;#39;s what but when I sit and try to work on a project I&amp;#39;m just tied up not knowing what or where to start. Can anyone help me find the best resources to understand the programming part behind the scenes?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcegm9,True,,boys4jesus3,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcegm9/need_help_with_programming_part/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcegm9/need_help_with_programming_part/,155203,1588455403.0,0,,False,,,,
,learnmachinelearning,"Hello, i am computer science student, just started learning AI, i am want to know what the best programming language (Tool), for me to learn artificial inteligence:

Python
R
Julia
Others...",t2_5nuzb584,False,,0,False,What should i use for IA,[],r/learnmachinelearning,False,6,,0,,False,t3_gcg89g,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588490886.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, i am computer science student, just started learning AI, i am want to know what the best programming language (Tool), for me to learn artificial inteligence:&lt;/p&gt;

&lt;p&gt;Python
R
Julia
Others...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcg89g,True,,felipep31,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcg89g/what_should_i_use_for_ia/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcg89g/what_should_i_use_for_ia/,155203,1588462086.0,0,,False,,,,
,learnmachinelearning,"Hello, im fairly new to machine learning, and i'd like to uptimize my model, to perform better.

    grid = {
        'penalty' : ['l1', 'l2'],
    }
    
    
    grid_search = GridSearchCV(lr, param_grid=grid, scoring='accuracy', n_jobs=-1, cv=5)
    grid_search.fit(X=X_train, y=y_train)
    print(""-----------"")
    print(grid_search.best_score_)
    print(grid_search.best_params_)

Above prints out  0.79 {'penalty': 'l1'}

In this instance lr is my model that i've already created and got a score of 0,75.It turns out that grid\_search.best\_score\_ comes out with 0,79. However when i pass the new parameters in my model, it still gives me 0,75. Is there something i'm misunderstanding about this whole process?

    lr.set_params(penalty='l1')
    lr.score(X_test,y_test)

Above prints out 0,75 still",t2_dnqaa,False,,0,False,A Question about GridSearchCV in sklearn,[],r/learnmachinelearning,False,6,,0,,False,t3_gc9r0m,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588467399.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, im fairly new to machine learning, and i&amp;#39;d like to uptimize my model, to perform better.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grid = {
    &amp;#39;penalty&amp;#39; : [&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;],
}


grid_search = GridSearchCV(lr, param_grid=grid, scoring=&amp;#39;accuracy&amp;#39;, n_jobs=-1, cv=5)
grid_search.fit(X=X_train, y=y_train)
print(&amp;quot;-----------&amp;quot;)
print(grid_search.best_score_)
print(grid_search.best_params_)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above prints out  0.79 {&amp;#39;penalty&amp;#39;: &amp;#39;l1&amp;#39;}&lt;/p&gt;

&lt;p&gt;In this instance lr is my model that i&amp;#39;ve already created and got a score of 0,75.It turns out that grid_search.best_score_ comes out with 0,79. However when i pass the new parameters in my model, it still gives me 0,75. Is there something i&amp;#39;m misunderstanding about this whole process?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lr.set_params(penalty=&amp;#39;l1&amp;#39;)
lr.score(X_test,y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Above prints out 0,75 still&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc9r0m,True,,Berthelmaster,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc9r0m/a_question_about_gridsearchcv_in_sklearn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc9r0m/a_question_about_gridsearchcv_in_sklearn/,155203,1588438599.0,0,,False,,,,
,learnmachinelearning,,t2_4amqi1oj,False,,0,False,Python Coding - Spam Detection using Machine Learning,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gcfkju,False,dark,0.66,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Python Coding - Spam Detection using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ov8ABk4_3d4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcfkju', 'height': 338}",,False,1,,True,https://b.thumbs.redditmedia.com/mX-dh8SV1z5JQ1p_lweYN4Zz9Z1Mm8urC0daqoaIT1k.jpg,False,,[],{},rich:video,,False,,1588488353.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?auto=webp&amp;s=0cb217b9656054ae28c504e83b19c040a94842de', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=88402a600c454af2d6a3073161f1c8bf6e230856', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad2f947960745ad195a3d290717b0373b213309b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf94cffc6c8e1f49ee1859d831dfcd090b58f18', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zqPG334zq6oYtPVMyqN_9nWHAB2c0p748pGyPevvtgY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcfkju,True,,burdin271,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfkju/python_coding_spam_detection_using_machine/,all_ads,False,https://youtu.be/Ov8ABk4_3d4,155203,1588459553.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Python Coding - Spam Detection using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ov8ABk4_3d4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hello,

I want to write an email bot that will compose messages for me. I would train it by feeding it email threads.

I know Python and Google Apps Script (i.e. Javascript). Any suggestions on what frameworks I should look at to get started?

Thanks!",t2_1674cb,False,,0,False,Learning to write my own email bot,[],r/learnmachinelearning,False,6,,0,,False,t3_gcfhfb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588488023.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I want to write an email bot that will compose messages for me. I would train it by feeding it email threads.&lt;/p&gt;

&lt;p&gt;I know Python and Google Apps Script (i.e. Javascript). Any suggestions on what frameworks I should look at to get started?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcfhfb,True,,justshowingup,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfhfb/learning_to_write_my_own_email_bot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcfhfb/learning_to_write_my_own_email_bot/,155203,1588459223.0,0,,False,,,,
,learnmachinelearning,"Hi everybody , sorry in advance for my english i'm french and new on reddit and in the world of machine learning(it's a lot).

So i did (followed a tutorial )a programme for recognize numbers, it works very well but i would like to implement my own images of numbers for example on paint. 

my code:

    import tensorflow as tf
    from tensorflow import keras
    import numpy as np
    import matplotlib.pyplot as plt
    
    data = keras.datasets.mnist
    
    (train_images, train_labels), (test_images, test_labels) = data.load_data()
    
    class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    train_images = train_images/255.0
    test_images = test_images/255.0
    
    model = keras.Sequential([
            keras.layers.Flatten(input_shape=(28, 28)),
            keras.layers.Dense(128, activation=""relu""),
            keras.layers.Dense(10, activation=""softmax"")
    ])
    
    model.compile(optimizer=""adam"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])
    
    model.fit(train_images, train_labels, epochs=5)
    
    prediction = model.predict(test_images)
    
    for i in range(10):
        plt.grid(False)
        plt.imshow(test_images[i], cmap=plt.cm.binary)
        plt.xlabel(""Actual:""+ class_names[test_labels[i]])
        plt.title(""Prediction""+ class_names[np.argmax(prediction[i])])
        plt.show()

thank you for your precious help.",t2_68adp3xo,False,,0,False,Add my own images to my neural network,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcfb8c,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588487369.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everybody , sorry in advance for my english i&amp;#39;m french and new on reddit and in the world of machine learning(it&amp;#39;s a lot).&lt;/p&gt;

&lt;p&gt;So i did (followed a tutorial )a programme for recognize numbers, it works very well but i would like to implement my own images of numbers for example on paint. &lt;/p&gt;

&lt;p&gt;my code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

data = keras.datasets.mnist

(train_images, train_labels), (test_images, test_labels) = data.load_data()

class_names = [&amp;#39;0&amp;#39;, &amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;, &amp;#39;3&amp;#39;, &amp;#39;4&amp;#39;, &amp;#39;5&amp;#39;, &amp;#39;6&amp;#39;, &amp;#39;7&amp;#39;, &amp;#39;8&amp;#39;, &amp;#39;9&amp;#39;]

train_images = train_images/255.0
test_images = test_images/255.0

model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation=&amp;quot;relu&amp;quot;),
        keras.layers.Dense(10, activation=&amp;quot;softmax&amp;quot;)
])

model.compile(optimizer=&amp;quot;adam&amp;quot;, loss=&amp;quot;sparse_categorical_crossentropy&amp;quot;, metrics=[&amp;quot;accuracy&amp;quot;])

model.fit(train_images, train_labels, epochs=5)

prediction = model.predict(test_images)

for i in range(10):
    plt.grid(False)
    plt.imshow(test_images[i], cmap=plt.cm.binary)
    plt.xlabel(&amp;quot;Actual:&amp;quot;+ class_names[test_labels[i]])
    plt.title(&amp;quot;Prediction&amp;quot;+ class_names[np.argmax(prediction[i])])
    plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;thank you for your precious help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcfb8c,True,,HugoGEII,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcfb8c/add_my_own_images_to_my_neural_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcfb8c/add_my_own_images_to_my_neural_network/,155203,1588458569.0,0,,False,,,,
,learnmachinelearning," I have a *n* x *m* one hot encoded matrix of a number of units, where n corresponds to the number of units, and m is the number of total sub-components. This corresponds to a *n* x 5 matrix which has the 5 attribute values for each of these units.

If I want to create clusters of these sub-components corresponding to these 5 attributes (basically see which sub-component is associated with which attribute), which technique should I use? Would something like PCA or t-SNE work or does this problem require a different approach and why should I use what I use?

I am quite new to this, I would appreciate any help on this matter.",t2_6c316kor,False,,0,False,What algorithm should I use?,[],r/learnmachinelearning,False,6,,0,,False,t3_gc3zev,False,dark,0.86,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1588442485.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a &lt;em&gt;n&lt;/em&gt; x &lt;em&gt;m&lt;/em&gt; one hot encoded matrix of a number of units, where n corresponds to the number of units, and m is the number of total sub-components. This corresponds to a &lt;em&gt;n&lt;/em&gt; x 5 matrix which has the 5 attribute values for each of these units.&lt;/p&gt;

&lt;p&gt;If I want to create clusters of these sub-components corresponding to these 5 attributes (basically see which sub-component is associated with which attribute), which technique should I use? Would something like PCA or t-SNE work or does this problem require a different approach and why should I use what I use?&lt;/p&gt;

&lt;p&gt;I am quite new to this, I would appreciate any help on this matter.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc3zev,True,,ml_orange,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc3zev/what_algorithm_should_i_use/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc3zev/what_algorithm_should_i_use/,155203,1588413685.0,0,,False,,,,
,learnmachinelearning,What are some good courses which teaches scikit-learn and do live coding.,t2_47jpmh5m,False,,0,False,SciKit-Learn tutorials,[],r/learnmachinelearning,False,6,,0,,False,t3_gc6cof,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1588454438.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are some good courses which teaches scikit-learn and do live coding.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc6cof,True,,ItisAhmad,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc6cof/scikitlearn_tutorials/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc6cof/scikitlearn_tutorials/,155203,1588425638.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is Pose Estimation | introduction to Human Pose Estimation With Deep Learning 2D / 3D 28,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gcdanz,False,dark,0.66,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_sobpAW16c0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Pose Estimation ? | introduction to Human Pose Estimation With Deep Learning 2D / 3D | 28', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_sobpAW16c0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_sobpAW16c0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_sobpAW16c0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcdanz', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/X0vVbnSFSwiU8WjRr5uG1jJMuAyes0RNhGKtpfcAVIA.jpg,False,,[],{},rich:video,,False,,1588480058.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4aRZnrjhJsMJbfMnx9GnO2fPF75b3g3OAd0VaobZZ7s.jpg?auto=webp&amp;s=c4cc682c4323a95ce6e563693ff82d8900dae4f3', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/4aRZnrjhJsMJbfMnx9GnO2fPF75b3g3OAd0VaobZZ7s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=511a613bb84a29a5cb72e94bae4b8c53fbc52bd8', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/4aRZnrjhJsMJbfMnx9GnO2fPF75b3g3OAd0VaobZZ7s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7b29b9c09d97436a65e94e598e44fbd1ead0fe23', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/4aRZnrjhJsMJbfMnx9GnO2fPF75b3g3OAd0VaobZZ7s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e04a795525804c0ac4986dc658bc66a087b00b7', 'width': 320, 'height': 240}], 'variants': {}, 'id': '-JlXAI4t_z4eUGMunVaeaZxGX1DMUIZ8yOKo90sCNbA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcdanz,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcdanz/what_is_pose_estimation_introduction_to_human/,all_ads,False,https://youtu.be/_sobpAW16c0,155203,1588451258.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Pose Estimation ? | introduction to Human Pose Estimation With Deep Learning 2D / 3D | 28', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_sobpAW16c0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_sobpAW16c0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_2hvgs2te,False,,0,False,"""/b"" is used to run a command in the background in a bat file. Can someone explain why I would want to do this?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcchem,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588477224.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gcchem,True,,Daveboi7,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcchem/b_is_used_to_run_a_command_in_the_background_in_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcchem/b_is_used_to_run_a_command_in_the_background_in_a/,155203,1588448424.0,0,,False,,,,
,learnmachinelearning,"Greetings.
I have read Phillipe Remyâ€™s famous post dedicated to LSTM stateful mode, but my brain power is not enough to comprehend it fully.
Does anyone have a link to an article, which explains it in really layman terms?
I would much appreciate if someone shares it with me.

I am using LSTM to predict stock prices and I noticed that on the Internet everyone use â€˜statefulâ€™ mode of the LSTM when approaching this task.
I am trying to understand why.

Thanks in advance.",t2_3v76dvmu,False,,0,False,What is â€˜statefulâ€™ mode in Keras LSTM in layman terms?,[],r/learnmachinelearning,False,6,,0,,False,t3_gc8p3o,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588463526.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings.
I have read Phillipe Remyâ€™s famous post dedicated to LSTM stateful mode, but my brain power is not enough to comprehend it fully.
Does anyone have a link to an article, which explains it in really layman terms?
I would much appreciate if someone shares it with me.&lt;/p&gt;

&lt;p&gt;I am using LSTM to predict stock prices and I noticed that on the Internet everyone use â€˜statefulâ€™ mode of the LSTM when approaching this task.
I am trying to understand why.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc8p3o,True,,19Summer,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc8p3o/what_is_stateful_mode_in_keras_lstm_in_layman/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc8p3o/what_is_stateful_mode_in_keras_lstm_in_layman/,155203,1588434726.0,0,,False,,,,
,learnmachinelearning,"Hi all!

I am a newbie in machine learning and tensorflow and I could use your guidance in my project.

I am currently working on my master's degree final thesis in which I have to develop a custom object detector and build a REST API (with Django) to be the interface between the object detector and, for proof-of-concept, an Android app.The idea of this proof-of-concept is for the user to take a picture in the Android app, POST it to the API, which in turn returns the category of the detected object(s).

I have already trained a custom Faster-RCNN object detector using the Tensorflow Object Detection API and exported the frozen inference graph and I have tested the model using a python script. Now, I would like to know your opinion on the best way to deploy it and to make the API ""talk"" to it. Since I have this frozen graph, can I simply include it in my project and use it like I did in the test script? I have searched about this, and didn't find anything specific to models from the Object Detection API, but I have seen it generally is done using a Docker container and Tensorflow serving. However, I know almost nothing about these tools, and couldn't really understand their purpose/usefulness in this case.

Another question i have is that right now, I can post an image to the API and call the testing script locally to obtain the detections, but I have to run the entire code for each image posted (start tensorflow, ""activate"" the gpu...) which consumes a lot of time, especially since Faster-RCNN already isn't the fastest of models. Is there a way I can make ""the service keep running"" and just make it wait to receive new images when no new images arrive?

I am sorry if I was not as clear as I should, but English isn't my first language. If you think you can help me but didn't understand my problems/questions, please tell me and I will try to clarify as best as i can.

Thanks in advance!

Edit: formatting",t2_2yw9imkq,False,,0,False,Deploy custom object detector from Tensorflow Object Detection API,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcc10y,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588475572.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all!&lt;/p&gt;

&lt;p&gt;I am a newbie in machine learning and tensorflow and I could use your guidance in my project.&lt;/p&gt;

&lt;p&gt;I am currently working on my master&amp;#39;s degree final thesis in which I have to develop a custom object detector and build a REST API (with Django) to be the interface between the object detector and, for proof-of-concept, an Android app.The idea of this proof-of-concept is for the user to take a picture in the Android app, POST it to the API, which in turn returns the category of the detected object(s).&lt;/p&gt;

&lt;p&gt;I have already trained a custom Faster-RCNN object detector using the Tensorflow Object Detection API and exported the frozen inference graph and I have tested the model using a python script. Now, I would like to know your opinion on the best way to deploy it and to make the API &amp;quot;talk&amp;quot; to it. Since I have this frozen graph, can I simply include it in my project and use it like I did in the test script? I have searched about this, and didn&amp;#39;t find anything specific to models from the Object Detection API, but I have seen it generally is done using a Docker container and Tensorflow serving. However, I know almost nothing about these tools, and couldn&amp;#39;t really understand their purpose/usefulness in this case.&lt;/p&gt;

&lt;p&gt;Another question i have is that right now, I can post an image to the API and call the testing script locally to obtain the detections, but I have to run the entire code for each image posted (start tensorflow, &amp;quot;activate&amp;quot; the gpu...) which consumes a lot of time, especially since Faster-RCNN already isn&amp;#39;t the fastest of models. Is there a way I can make &amp;quot;the service keep running&amp;quot; and just make it wait to receive new images when no new images arrive?&lt;/p&gt;

&lt;p&gt;I am sorry if I was not as clear as I should, but English isn&amp;#39;t my first language. If you think you can help me but didn&amp;#39;t understand my problems/questions, please tell me and I will try to clarify as best as i can.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;Edit: formatting&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gcc10y,True,,fnunogomes,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcc10y/deploy_custom_object_detector_from_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcc10y/deploy_custom_object_detector_from_tensorflow/,155203,1588446772.0,0,,False,,,,
,learnmachinelearning,"After tons of research, here is the list of courses that I think are good for me. I want to make my own AI projects to help society benefit from it. 

1. [Algorithm about machine learning](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)
2. [Python course](https://www.coursera.org/specializations/python?skipBrowseRedirect=true)
3. [Mathematics for machine learning](https://www.coursera.org/specializations/mathematics-machine-learning)
4. [Applied data science with python](https://www.coursera.org/specializations/data-science-python)
5. [Deep learning](https://www.coursera.org/specializations/deep-learning)
6. [TensorFlow in practice](https://www.coursera.org/specializations/tensorflow-in-practice)
7.  [Advance Machine learning](https://www.coursera.org/specializations/aml)

After these, I'm planning to do some of my own ML projects. Can you suggest to me if the order of the courses is good or suggestions on which course should I apply or not?

Thanks!",t2_f90f0r,False,,0,False,"Suggestions, please","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gcbf7y,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1588473412.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After tons of research, here is the list of courses that I think are good for me. I want to make my own AI projects to help society benefit from it. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=""https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/""&gt;Algorithm about machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.coursera.org/specializations/python?skipBrowseRedirect=true""&gt;Python course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.coursera.org/specializations/mathematics-machine-learning""&gt;Mathematics for machine learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.coursera.org/specializations/data-science-python""&gt;Applied data science with python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.coursera.org/specializations/deep-learning""&gt;Deep learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""https://www.coursera.org/specializations/tensorflow-in-practice""&gt;TensorFlow in practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; &lt;a href=""https://www.coursera.org/specializations/aml""&gt;Advance Machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After these, I&amp;#39;m planning to do some of my own ML projects. Can you suggest to me if the order of the courses is good or suggestions on which course should I apply or not?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gcbf7y,True,,mallasahaj,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcbf7y/suggestions_please/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcbf7y/suggestions_please/,155203,1588444612.0,0,,False,,,,
,learnmachinelearning,,t2_51mclnu7,False,,0,False,Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multiple Linear Regression,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gcau54,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multiple Linear Regression', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uEe3PxQmHyM/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gcau54', 'height': 338}",Project,False,1,,False,https://b.thumbs.redditmedia.com/W6y1G0uuxuk7k20MiPegyd2vJ61Eg3RBdeF83XrlD_E.jpg,False,,[],{},rich:video,,False,,1588471316.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?auto=webp&amp;s=e5477f527433b1e6e30ccf1a7e6d9a9949c6d909', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=66609f7f9ba0202fc7da8265cdbcb8371f908a55', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27896b727bb6120ac04b7e10d7b3d648df442f35', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/TdTOt9AmgEKpyqYW_Nvcatdiiy1sJxsAoL73abZf23o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c78d2a9766046d6948d8d2bb4ced40525f369ce1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'chyDtVjkj-LsnfuAX65adm8NKwUrwSuSUJW8095gQRI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gcau54,True,,TheNerdyDevYT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcau54/machine_learning_tutorials_from_novice_to_pro_13/,all_ads,False,https://www.youtube.com/watch?v=uEe3PxQmHyM,155203,1588442516.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning Tutorials - From Novice To Pro - #13 - Project 2: Multiple Linear Regression', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uEe3PxQmHyM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uEe3PxQmHyM/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Here is my python code: [https://pastebin.com/r4UnrHYf](https://pastebin.com/r4UnrHYf) 

I just cant figure out why both points travel to the unweighted mean of all data points? I am a beginner in python and my code is terrible but I would appreciate any help.",t2_2xz6764h,False,,0,False,Need help implementing fuzzy-c-means algorithm,[],r/learnmachinelearning,False,6,,0,,False,t3_gcanvn,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588470698.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is my python code: &lt;a href=""https://pastebin.com/r4UnrHYf""&gt;https://pastebin.com/r4UnrHYf&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;I just cant figure out why both points travel to the unweighted mean of all data points? I am a beginner in python and my code is terrible but I would appreciate any help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?auto=webp&amp;s=e28867cccd2864fc170848b64bc44e6a778116b9', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b38a59d6140109bc38ed4f88b98bd4436dfe09b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=203ad20eac023d67d122c5f834516a4670799f63', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gcanvn,True,,User1377420,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gcanvn/need_help_implementing_fuzzycmeans_algorithm/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gcanvn/need_help_implementing_fuzzycmeans_algorithm/,155203,1588441898.0,0,,False,,,,
,learnmachinelearning,,t2_namkp97,False,,0,False,How's your face categorized?!,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_gc53sh,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/1tORVSNNSXxyRk3wDl_5Z6Js_RHkW7Y1znIUQuNkZmc.jpg,False,,[],{},link,,False,,1588448602.0,text,6,,,text,visage.thevatsalsaglani.xyz,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?auto=webp&amp;s=9776382e6cc0ee4d1baf20775cefdb20cc134e07', 'width': 3000, 'height': 2000}, 'resolutions': [{'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f8520a3c368291d0f5d7a87c0e7f71df28d3cf1a', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41e111c8c46686c0c4d99c5d541ef1b5f140f27f', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dce515f3459aa2dc7588d2d9d84fecde9269f85c', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c14852bd812a5603b54439050006935e8bb99ea', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b1204b0d7276bcb3f7bd5c1cfa8742e79731ac6', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/v0qxgfOvVIawxz3n2LnNA3YVaEHsq1Ms8Dr3m0GiUaA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c2535b47f66dd643620244d1d2165cd1573214b', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'rJp2JfOBLHxvx9V9fn5ybtUVaZ_GsgEvqOiE1m9WJjw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc53sh,True,,thevatsalsaglani,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc53sh/hows_your_face_categorized/,all_ads,False,http://visage.thevatsalsaglani.xyz,155203,1588419802.0,0,,False,,,,
,learnmachinelearning,,t2_3oenc5gi,False,,0,False,"Difference between AI, ML &amp; DP",[],r/learnmachinelearning,False,6,,0,95.0,False,t3_gbcfu4,False,dark,0.87,,public,667,0,{},140.0,,False,[],,True,False,,{},,False,667,,False,https://b.thumbs.redditmedia.com/QgSaYdFB_AmdHNcp07Yykx_tIUe3YiOZJMYb5rb8pKk.jpg,False,,[],{},image,,False,,1588340276.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?auto=webp&amp;s=256f44c8f38b61f401694c76a0e8450a29f18a5f', 'width': 1080, 'height': 733}, 'resolutions': [{'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa6912823f246980ad54c9c04e6097c81d3a65da', 'width': 108, 'height': 73}, {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d8f6d467fb2dcd8ff227bcba4ce5942b018d23e', 'width': 216, 'height': 146}, {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6184650bd2d95fad729f186301d1cd51a15df1dc', 'width': 320, 'height': 217}, {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c832dec02f3c667945228be1ace85402966ccac', 'width': 640, 'height': 434}, {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e909db8a20582c916c51c1d3ae2ed6b2bdfc935e', 'width': 960, 'height': 651}, {'url': 'https://preview.redd.it/3du8nstyb3w41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c098075344a7570340a927a1e2f4d698d099f8e2', 'width': 1080, 'height': 733}], 'variants': {}, 'id': 'ggWzJ0ECG2lu2Yg7emaYIwZAo1VFSbkr6wE2q20f4g0'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbcfu4,True,,InfinityCodeX,,57,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbcfu4/difference_between_ai_ml_dp/,all_ads,False,https://i.redd.it/3du8nstyb3w41.jpg,155203,1588311476.0,1,,False,,,,
,learnmachinelearning," There are end-chapter exercises on the book itself, but I was wondering if anybody has come across other materials to supplement the topics covered.

When I say materials, I mean tutorial videos on Youtube, which discuss the topics in the book, or classes on Coursera, that discuss the data sets and examples in Book, specifically

(I actually found a per-chapter lecture on [Youtube](https://www.youtube.com/playlist?list=PL06ytJZ4Ak1rXmlvxTyAdOEfiVEzH00IK), but I was wondering if there are any more!)",t2_f6qjd,False,,0,False,"Are there any materials available to supplement topics covered in ""An Introduction to Statistical Learning with Application in R""?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc9m32,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588466909.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There are end-chapter exercises on the book itself, but I was wondering if anybody has come across other materials to supplement the topics covered.&lt;/p&gt;

&lt;p&gt;When I say materials, I mean tutorial videos on Youtube, which discuss the topics in the book, or classes on Coursera, that discuss the data sets and examples in Book, specifically&lt;/p&gt;

&lt;p&gt;(I actually found a per-chapter lecture on &lt;a href=""https://www.youtube.com/playlist?list=PL06ytJZ4Ak1rXmlvxTyAdOEfiVEzH00IK""&gt;Youtube&lt;/a&gt;, but I was wondering if there are any more!)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3rrzgrIZHYsMRvOF2u9kuJHA0RaubHQOV498W7MI4bA.jpg?auto=webp&amp;s=b4cceeb46dd184e77e6aa9a0476b6e0f815fcfc5', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/3rrzgrIZHYsMRvOF2u9kuJHA0RaubHQOV498W7MI4bA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=138585c775983634ba05bdffce983c076c8a866e', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'hY8_fiUtWyoIaRYyqqjKHD6S7mPAGw6z-ojfEt0avts'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gc9m32,True,,100007,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc9m32/are_there_any_materials_available_to_supplement/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc9m32/are_there_any_materials_available_to_supplement/,155203,1588438109.0,0,,False,,,,
,learnmachinelearning,"I'm studying linear regression and one part of understanding the fit of the model is 

1.  looking at the summary of the model (standard errors, R2, p value) etc, and

2. analysis of variance of the model (the output that gives you the sum of square, F value, etc etc).

&amp;#x200B;

What is the differencein  interpretation when thinking the Pvalue of the t-statistic vs the p-value in the F-statistic.

Arent they both measuring the same thing?

Here is an output from the summary of a linear regression model (housing prices):  


https://preview.redd.it/wjf7j6bridw41.png?width=911&amp;format=png&amp;auto=webp&amp;s=5c4d1ceec9c57faf906c8fe4c1035064a08b440a

And here is the ANOVA for that table

&amp;#x200B;

https://preview.redd.it/72oqmja1jdw41.png?width=811&amp;format=png&amp;auto=webp&amp;s=315726119bbb45ccf4c99d12b381d5a65069e362

And is the F-statistic in the model summary the same as the F statistic in the anova?

I'm basically trying to understand how to make use of the Anova tables for linear regression but am having trouble undersstanding what benefit it is supposd to provide when talking about fit of the model",t2_i6go6an,False,,0,False,Studying linear regression - differences between meaning of the T statistic vs the F statistic,[],r/learnmachinelearning,False,6,,0,51.0,False,t3_gc8rym,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Ya7iPoYbIQ111O6v_bqGMF-uBt-box11E4kKvvkGAKs.jpg,False,,[],{},,,True,,1588463819.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m studying linear regression and one part of understanding the fit of the model is &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;looking at the summary of the model (standard errors, R2, p value) etc, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;analysis of variance of the model (the output that gives you the sum of square, F value, etc etc).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What is the differencein  interpretation when thinking the Pvalue of the t-statistic vs the p-value in the F-statistic.&lt;/p&gt;

&lt;p&gt;Arent they both measuring the same thing?&lt;/p&gt;

&lt;p&gt;Here is an output from the summary of a linear regression model (housing prices):  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/wjf7j6bridw41.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c4d1ceec9c57faf906c8fe4c1035064a08b440a""&gt;https://preview.redd.it/wjf7j6bridw41.png?width=911&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c4d1ceec9c57faf906c8fe4c1035064a08b440a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here is the ANOVA for that table&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/72oqmja1jdw41.png?width=811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=315726119bbb45ccf4c99d12b381d5a65069e362""&gt;https://preview.redd.it/72oqmja1jdw41.png?width=811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=315726119bbb45ccf4c99d12b381d5a65069e362&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And is the F-statistic in the model summary the same as the F statistic in the anova?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m basically trying to understand how to make use of the Anova tables for linear regression but am having trouble undersstanding what benefit it is supposd to provide when talking about fit of the model&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc8rym,True,,mrdlau,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc8rym/studying_linear_regression_differences_between/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc8rym/studying_linear_regression_differences_between/,155203,1588435019.0,0,,False,,,"{'wjf7j6bridw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 39, 'x': 108, 'u': 'https://preview.redd.it/wjf7j6bridw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a20bb210a95a07a9fdb592d1cc0e00a9baafa65b'}, {'y': 79, 'x': 216, 'u': 'https://preview.redd.it/wjf7j6bridw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=470c3bac694d04e4f657c5e0592f9420b02d902e'}, {'y': 117, 'x': 320, 'u': 'https://preview.redd.it/wjf7j6bridw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f921b42a5d0d9e9f1ec1dfc97bdf7b63954cd41'}, {'y': 234, 'x': 640, 'u': 'https://preview.redd.it/wjf7j6bridw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=52df61335903c6b886dc789fb391837f23cf26d0'}], 's': {'y': 334, 'x': 911, 'u': 'https://preview.redd.it/wjf7j6bridw41.png?width=911&amp;format=png&amp;auto=webp&amp;s=5c4d1ceec9c57faf906c8fe4c1035064a08b440a'}, 'id': 'wjf7j6bridw41'}, '72oqmja1jdw41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 24, 'x': 108, 'u': 'https://preview.redd.it/72oqmja1jdw41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e34b7e0cc6e60ed46623ea2cc29aa4598ebfedeb'}, {'y': 48, 'x': 216, 'u': 'https://preview.redd.it/72oqmja1jdw41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08db8d1a3256fac2fc584cc887e55494fe833ff1'}, {'y': 71, 'x': 320, 'u': 'https://preview.redd.it/72oqmja1jdw41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0931d0b5312b009438fa774d5447106a259a8988'}, {'y': 143, 'x': 640, 'u': 'https://preview.redd.it/72oqmja1jdw41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c378813ce6ba968be5644ce5620cb4a87a00b0'}], 's': {'y': 182, 'x': 811, 'u': 'https://preview.redd.it/72oqmja1jdw41.png?width=811&amp;format=png&amp;auto=webp&amp;s=315726119bbb45ccf4c99d12b381d5a65069e362'}, 'id': '72oqmja1jdw41'}}",
,learnmachinelearning,,t2_5fefq,False,,0,False,"Programming language choices in AI, NLP and machine learning research and in applications?",[],r/learnmachinelearning,False,6,,0,,False,t3_gc84r6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},link,,False,,1588461488.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/y-GeqD-sDv9AvzYc1Re1N00rz5T1s8Gnm6U_21ot8EE.jpg?auto=webp&amp;s=310be0386c40bd27ce7d05cf3df80447c60d1f0d', 'width': 122, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/y-GeqD-sDv9AvzYc1Re1N00rz5T1s8Gnm6U_21ot8EE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6cfaefc24f84d3c8d958e620c6e3b2a29c895a00', 'width': 108, 'height': 141}], 'variants': {}, 'id': '_V0KBwMOLsL46Szbme69J2Re2_YXzPL_B742gP79aKQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc84r6,True,,timlee126,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc84r6/programming_language_choices_in_ai_nlp_and/,all_ads,False,/r/MachineLearning/comments/gc834u/d_programming_language_choices_in_ai_nlp_and/,155203,1588432688.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""http://matt.might.net/articles/best-programming-languages/ says:\n\n&gt; I don't do a lot of artificial intelligence, natural-language processing or machine-learning research, but if I did, Haskell would be my first pick there too. (Scheme would be a strong second.) \n\nWhy are Haskell and Scheme his first and second  picks for AI, NLP, and machine learning research?\n\nFor AI, NLP, and machine learning applications, are Haskell and Scheme top picks  for programming languages?\n\nRight now, for NLP and machine learning research and their applications, Python and R are the most popular languages with richest libraries. \nScala seems also  used heavily in AI and machine learning applications, and is mixture of functional and OO paradigms. \nJulia is also said to be the rising star in these areas.\nHow are Haskell and Scheme compared to them?\n\nThanks."", 'author_fullname': 't2_5fefq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Programming language choices in AI, NLP and machine learning research and in applications?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gc834u', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1588589283.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588461317.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""http://matt.might.net/articles/best-programming-languages/""&gt;http://matt.might.net/articles/best-programming-languages/&lt;/a&gt; says:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I don&amp;#39;t do a lot of artificial intelligence, natural-language processing or machine-learning research, but if I did, Haskell would be my first pick there too. (Scheme would be a strong second.) &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Why are Haskell and Scheme his first and second  picks for AI, NLP, and machine learning research?&lt;/p&gt;\n\n&lt;p&gt;For AI, NLP, and machine learning applications, are Haskell and Scheme top picks  for programming languages?&lt;/p&gt;\n\n&lt;p&gt;Right now, for NLP and machine learning research and their applications, Python and R are the most popular languages with richest libraries. \nScala seems also  used heavily in AI and machine learning applications, and is mixture of functional and OO paradigms. \nJulia is also said to be the rising star in these areas.\nHow are Haskell and Scheme compared to them?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/y-GeqD-sDv9AvzYc1Re1N00rz5T1s8Gnm6U_21ot8EE.jpg?auto=webp&amp;s=310be0386c40bd27ce7d05cf3df80447c60d1f0d', 'width': 122, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/y-GeqD-sDv9AvzYc1Re1N00rz5T1s8Gnm6U_21ot8EE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6cfaefc24f84d3c8d958e620c6e3b2a29c895a00', 'width': 108, 'height': 141}], 'variants': {}, 'id': '_V0KBwMOLsL46Szbme69J2Re2_YXzPL_B742gP79aKQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gc834u', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'timlee126', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/gc834u/d_programming_language_choices_in_ai_nlp_and/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/gc834u/d_programming_language_choices_in_ai_nlp_and/', 'subreddit_subscribers': 1044414, 'created_utc': 1588432517.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_gc834u,,
,learnmachinelearning,"Hi,

Basically, I'm working on a project to predict the ratio of Facebook Reactions ('Haha', 'Love' etc.) for a given post. Then to improve performance, I was wondering if I could use another neural network to predict the sentiment score of the post, and use it as an additional feature for recomputing the predicted ratio of Facebook Reactions. I will use some sort of regression technique or neural network to take the ratios + sentiment score as features for recomputation.

Is this hot garbage? I'm not sure what to Google to know whether this is a good idea or not. Would correlation be an issue? I plotted some graphs and a relationship between sentiment + reactions exists. I've also heard that correlation between input features is bad.

Thanks!",t2_6d2jn3fg,False,,0,False,Recomputing a prediction using additional features?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc4q8u,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,1588418064.0,,[],{},,,True,,1588446614.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;Basically, I&amp;#39;m working on a project to predict the ratio of Facebook Reactions (&amp;#39;Haha&amp;#39;, &amp;#39;Love&amp;#39; etc.) for a given post. Then to improve performance, I was wondering if I could use another neural network to predict the sentiment score of the post, and use it as an additional feature for recomputing the predicted ratio of Facebook Reactions. I will use some sort of regression technique or neural network to take the ratios + sentiment score as features for recomputation.&lt;/p&gt;

&lt;p&gt;Is this hot garbage? I&amp;#39;m not sure what to Google to know whether this is a good idea or not. Would correlation be an issue? I plotted some graphs and a relationship between sentiment + reactions exists. I&amp;#39;ve also heard that correlation between input features is bad.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gc4q8u,True,,sbh116,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc4q8u/recomputing_a_prediction_using_additional_features/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc4q8u/recomputing_a_prediction_using_additional_features/,155203,1588417814.0,0,,False,,,,
,learnmachinelearning,,t2_xf2t5,False,,0,False,Data Science. Bayes theorem,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_gc7occ,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Ce6z9yMj2FkJF8hdF1oE3MkGtf3cTAtHE8FGWCIHNMU.jpg,False,,[],{},link,,False,,1588459775.0,text,6,,,text,luminousmen.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?auto=webp&amp;s=051606fa10335d9dbcd97728c06b2894853312c4', 'width': 1400, 'height': 1049}, 'resolutions': [{'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf775e8b7ad6e70779fab043c940ad7226a8c45c', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b80dc192efe7f6e7ab7f8cae88a23b840e37636', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88d021c095649ac06f5445e28baf70edb9f06c8a', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b25959d4294aae27aa2a70f758d5b3bbc75fb7fc', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc918eb45ea0de5293b6f953bae4776e7b2447b5', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=116ddd61c5f9de2d0da4a6a15b0db04c3c797452', 'width': 1080, 'height': 809}], 'variants': {}, 'id': '1jYcsUcnKCWVynpCtNXuF3QYUAsWpKbDM4oRP8DxhXc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc7occ,True,,luminoumen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc7occ/data_science_bayes_theorem/,all_ads,False,https://luminousmen.com/post/data-science-bayes-theorem,155203,1588430975.0,0,,False,,,,
,learnmachinelearning,"So since I'm math and CS and going phd route, and am working on research a lot. I invariably end up in Latex. I dont know about you guys but I would like to make it easier to write up a lot in latex. My idea is creating a pipeline that scans handwriting and then makes it a pdf or a work doc then makes that into a latex document. From a brief understanding the most difficult part is the formatting on the latex side. I have found an app someone made for a doctoral thesis in 2004 that seems to be dead now, but that app makes handwritten math into latex. Regardless I'm wondering how much support such a project would have. 

Any feedback for thoughts or use cases or functionality would be greatly appreciated.",t2_1mt6zlx5,False,,0,False,App/research Idea,[],r/learnmachinelearning,False,6,,0,,False,t3_gc7fi8,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588458819.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So since I&amp;#39;m math and CS and going phd route, and am working on research a lot. I invariably end up in Latex. I dont know about you guys but I would like to make it easier to write up a lot in latex. My idea is creating a pipeline that scans handwriting and then makes it a pdf or a work doc then makes that into a latex document. From a brief understanding the most difficult part is the formatting on the latex side. I have found an app someone made for a doctoral thesis in 2004 that seems to be dead now, but that app makes handwritten math into latex. Regardless I&amp;#39;m wondering how much support such a project would have. &lt;/p&gt;

&lt;p&gt;Any feedback for thoughts or use cases or functionality would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc7fi8,True,,ixw123,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc7fi8/appresearch_idea/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc7fi8/appresearch_idea/,155203,1588430019.0,0,,False,,,,
,learnmachinelearning,,t2_1ekfyorf,False,,0,False,"Hi I'm a noobie, I have very little experience in programming (little bit of C) and I want to get started with ML. How do I go about this?",[],r/learnmachinelearning,False,6,,0,,False,t3_gc6n6u,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588455640.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc6n6u,True,,harrisakins1,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc6n6u/hi_im_a_noobie_i_have_very_little_experience_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc6n6u/hi_im_a_noobie_i_have_very_little_experience_in/,155203,1588426840.0,0,,False,,,,
,learnmachinelearning,,t2_54gaydu7,False,,0,False,I am interested in pursuing masters in Network Security domain but I am very much interested in machine learning and it's application in network security. Is there any masters program in US which provides this?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc6hyc,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588455050.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gc6hyc,True,,avikram1712,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc6hyc/i_am_interested_in_pursuing_masters_in_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc6hyc/i_am_interested_in_pursuing_masters_in_network/,155203,1588426250.0,0,,False,,,,
,learnmachinelearning,"Who has done this book http://neuralnetworksanddeeplearning.com/
I'm struggling to find Python 3 code for Chapter 1. I'm a newbie to neural networks. Please help",t2_1mxss6fp,False,,0,False,Neural Networks and Deep Learning by Michael Nielsen,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc6d2d,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1588454486.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Who has done this book &lt;a href=""http://neuralnetworksanddeeplearning.com/""&gt;http://neuralnetworksanddeeplearning.com/&lt;/a&gt;
I&amp;#39;m struggling to find Python 3 code for Chapter 1. I&amp;#39;m a newbie to neural networks. Please help&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?auto=webp&amp;s=a8cffd3e23c129ddb2dd6c593216a062fb8f1380', 'width': 200, 'height': 214}, 'resolutions': [{'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=06a541c2dd540bb4b74af834f56661b7512cc87c', 'width': 108, 'height': 115}], 'variants': {}, 'id': '_BeH_HLgJJJyvbJXuBeembjdUClSovixHNQF4PeqIeE'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gc6d2d,True,,MisfitNJ,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc6d2d/neural_networks_and_deep_learning_by_michael/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc6d2d/neural_networks_and_deep_learning_by_michael/,155203,1588425686.0,0,,False,,,,
,learnmachinelearning,,t2_167nx2qq,False,,0,False,Springer releases 65 Machine Learning and Data Science books for free,[],r/learnmachinelearning,False,6,,0,,False,t3_gbi2c1,False,dark,0.93,,public,81,0,{},,,False,[],,False,False,,{},,False,81,,False,default,False,,[],{},,,False,,1588368398.0,text,6,,,text,techgrabyte.com,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbi2c1,True,,mrsailor23,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbi2c1/springer_releases_65_machine_learning_and_data/,all_ads,False,https://techgrabyte.com/springer-released-65-machine-learning-data-science-books-free,155203,1588339598.0,2,,False,,,,
,learnmachinelearning,I am training a CNN and want to know what kinds of preprocessing does ConvNet allow me to skip? For example I know that I don't need to centre my image during training to get a better loss as the filters don't care if the image is in the centre or not. Just like this I skipped upscaling some photos in training(128) and got a great result validation result for 480p. Why did this happen? And what other preprocessing can be skipped because of this great convolutions which are so helpful.,t2_405ucsno,False,,0,False,Preprocessing for CNN,[],r/learnmachinelearning,False,6,,0,,False,t3_gc5r8l,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588451839.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am training a CNN and want to know what kinds of preprocessing does ConvNet allow me to skip? For example I know that I don&amp;#39;t need to centre my image during training to get a better loss as the filters don&amp;#39;t care if the image is in the centre or not. Just like this I skipped upscaling some photos in training(128) and got a great result validation result for 480p. Why did this happen? And what other preprocessing can be skipped because of this great convolutions which are so helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gc5r8l,True,,euqroto,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc5r8l/preprocessing_for_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc5r8l/preprocessing_for_cnn/,155203,1588423039.0,0,,False,,,,
,learnmachinelearning,"I have read through a few articles online about these 2 models, some suggest that in the case of ordinal dependent variable, it is better to use the former one. Why is that? Why treating the dependent variable as categorical is no better than ordinal? Exactly in what way is ordinal logistic regression better?",t2_11o8lx,False,,0,False,How exactly is Ordinal Logistic Regression better than Multinomial Logistic Regression in the case of ordinal dependent variable?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc2nna,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588434611.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have read through a few articles online about these 2 models, some suggest that in the case of ordinal dependent variable, it is better to use the former one. Why is that? Why treating the dependent variable as categorical is no better than ordinal? Exactly in what way is ordinal logistic regression better?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gc2nna,True,,kingfung1120,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc2nna/how_exactly_is_ordinal_logistic_regression_better/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc2nna/how_exactly_is_ordinal_logistic_regression_better/,155203,1588405811.0,0,,False,,,,
,learnmachinelearning,"Hi there,   
Hope everyone is doing fine.   
I'm thinking of starting a AI ML &amp; DS study group to learn and collaborate accordingly. Everyone is welcome to join and collaborate. One who knows explains concepts for others and the rest learns by posting doubts and questions.   
Interested please comment. All the ideas &amp; opinions are welcome.   
If there is already an existing group, then please share that as well.   
Thank you ðŸ˜Š",t2_2osgxnz,False,,0,False,AI ML Study Group,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc289x,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,self,False,,[],{},,,True,,1588432158.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;br/&gt;
Hope everyone is doing fine.&lt;br/&gt;
I&amp;#39;m thinking of starting a AI ML &amp;amp; DS study group to learn and collaborate accordingly. Everyone is welcome to join and collaborate. One who knows explains concepts for others and the rest learns by posting doubts and questions.&lt;br/&gt;
Interested please comment. All the ideas &amp;amp; opinions are welcome.&lt;br/&gt;
If there is already an existing group, then please share that as well.&lt;br/&gt;
Thank you ðŸ˜Š&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gc289x,True,,vinay26k,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc289x/ai_ml_study_group/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc289x/ai_ml_study_group/,155203,1588403358.0,0,,False,,,,
,learnmachinelearning,"Let's have a  meeting!

1. What have you accomplished since last week?
2. What are your goals for next week?
3. Do you have any blockers that need helps from the /r/LearnMachineLearning community?

Don't be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.",t2_6l4z3,False,,0,False,"Weekly Status Check Meeting - Share your progress, your goals, and whatever is stopping you from achieving those goals!",[],r/learnmachinelearning,False,6,,0,,False,t3_gc26he,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588431871.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s have a  meeting!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What have you accomplished since last week?&lt;/li&gt;
&lt;li&gt;What are your goals for next week?&lt;/li&gt;
&lt;li&gt;Do you have any blockers that need helps from the &lt;a href=""/r/LearnMachineLearning""&gt;/r/LearnMachineLearning&lt;/a&gt; community?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Don&amp;#39;t be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gc26he,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc26he/weekly_status_check_meeting_share_your_progress/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc26he/weekly_status_check_meeting_share_your_progress/,155203,1588403071.0,0,,False,,,,
,learnmachinelearning,I have list of pages/actions that user perform on a website (25K unique items) and let's say I have a success criteria which is a form fill. I want to analyze paths taken by user to reach this form fill. What will be the possible algorithms I can use. I have tried FpGROWTH. But looking for more. (Please note mostly they are sequence of actions),t2_689s8lfz,False,,0,False,Best algorithms for path analysis,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gc1tcl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588429846.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have list of pages/actions that user perform on a website (25K unique items) and let&amp;#39;s say I have a success criteria which is a form fill. I want to analyze paths taken by user to reach this form fill. What will be the possible algorithms I can use. I have tried FpGROWTH. But looking for more. (Please note mostly they are sequence of actions)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gc1tcl,True,,prkohli1992,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gc1tcl/best_algorithms_for_path_analysis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gc1tcl/best_algorithms_for_path_analysis/,155203,1588401046.0,0,,False,,,,
,learnmachinelearning,,t2_cvc9f,False,,0,False,Funny Mirrors Using OpenCV,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,52.0,False,t3_gbm92k,False,light,0.62,,public,13,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/bfcvnp50t6w41/DASH_480?source=fallback', 'height': 320, 'width': 854, 'scrubber_media_url': 'https://v.redd.it/bfcvnp50t6w41/DASH_96', 'dash_url': 'https://v.redd.it/bfcvnp50t6w41/DASHPlaylist.mpd', 'duration': 31, 'hls_url': 'https://v.redd.it/bfcvnp50t6w41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},Project,False,13,,False,https://a.thumbs.redditmedia.com/cIuzoRqJtuzXppjp0px1r3KKCEDubB5zA69bYLo2380.jpg,False,,[],{},hosted:video,,False,,1588382407.0,richtext,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?format=pjpg&amp;auto=webp&amp;s=c3a23a053ab6c4ec57fc74192da55dff969147c4', 'width': 1280, 'height': 480}, 'resolutions': [{'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dbaa7078a86b89c2fadc0512d1a3133ffbf97fe3', 'width': 108, 'height': 40}, {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7ec111cf02e9704b5e770d7f20cdef8cb85b778f', 'width': 216, 'height': 81}, {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=13a8426ec4df71a1ba71c2413e0f3cd9e4b99fec', 'width': 320, 'height': 120}, {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fbe1ec915fd1df0aa8084202254566d8f8cc2af5', 'width': 640, 'height': 240}, {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=11dbd42b210617229b3e6eec082e50f03b84b46d', 'width': 960, 'height': 360}, {'url': 'https://external-preview.redd.it/FdgIG5_WCZ9W6_vwhS9_Y_nNxgKA4vXGoKbu9__PzbA.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c8fffddcca9458629ac9b53bc40bc481c4c5000', 'width': 1080, 'height': 405}], 'variants': {}, 'id': 'U_cvxwfdcPCM9tvecUYso88Q89Td5WzOdO1_En_othw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gbm92k,True,,spmallick,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbm92k/funny_mirrors_using_opencv/,all_ads,False,https://v.redd.it/bfcvnp50t6w41,155203,1588353607.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/bfcvnp50t6w41/DASH_480?source=fallback', 'height': 320, 'width': 854, 'scrubber_media_url': 'https://v.redd.it/bfcvnp50t6w41/DASH_96', 'dash_url': 'https://v.redd.it/bfcvnp50t6w41/DASHPlaylist.mpd', 'duration': 31, 'hls_url': 'https://v.redd.it/bfcvnp50t6w41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,,t2_643sg3ws,False,,0,False,Does google speech to text allow developers to build custom models?,[],r/learnmachinelearning,False,6,,0,,False,t3_gbvt49,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588409725.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbvt49,True,,TobinC1,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbvt49/does_google_speech_to_text_allow_developers_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbvt49/does_google_speech_to_text_allow_developers_to/,155203,1588380925.0,0,,False,,,,
,learnmachinelearning,"Hi there,

I am a recent graduate in Physics, and I would like to enter the fields of Data Scientist and Machine Learning. My aim is to self-study in this period of quarantine and hopefully land a intern or entry level job in this field. I would like advices from you guys regarding what I need to learn and do in order to make this happen. I have recently completed the ML course by Ng, implementing it in Python rather than Matlab. I plan to familiar myself with the Scikit learn library, as well as SQL, and start doing kaggle competitions. Also, I am going to start the [Advanced Machine Learning specialization](https://www.coursera.org/specializations/aml) on coursera.

Furthermore, to suppliment the practical ML with theory, I am reading All of Statistics by Wasserman, and I plan to also read Element of Statistical Learning after.

Do you think what I am doing right now is a good use of my time? Do you think this is enough for me to land a entry level job? It seems that even ""entry level"" job requires a good amount experience and degrees like CS, ML or mathematics. Do you think what I am doing will make me standout against the masters and PhDs?

Thank you!",t2_47dqttzy,False,,0,False,Machine Learning Engineer career advice,[],r/learnmachinelearning,False,6,,0,,False,t3_gbpnep,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1588393507.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I am a recent graduate in Physics, and I would like to enter the fields of Data Scientist and Machine Learning. My aim is to self-study in this period of quarantine and hopefully land a intern or entry level job in this field. I would like advices from you guys regarding what I need to learn and do in order to make this happen. I have recently completed the ML course by Ng, implementing it in Python rather than Matlab. I plan to familiar myself with the Scikit learn library, as well as SQL, and start doing kaggle competitions. Also, I am going to start the &lt;a href=""https://www.coursera.org/specializations/aml""&gt;Advanced Machine Learning specialization&lt;/a&gt; on coursera.&lt;/p&gt;

&lt;p&gt;Furthermore, to suppliment the practical ML with theory, I am reading All of Statistics by Wasserman, and I plan to also read Element of Statistical Learning after.&lt;/p&gt;

&lt;p&gt;Do you think what I am doing right now is a good use of my time? Do you think this is enough for me to land a entry level job? It seems that even &amp;quot;entry level&amp;quot; job requires a good amount experience and degrees like CS, ML or mathematics. Do you think what I am doing will make me standout against the masters and PhDs?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?auto=webp&amp;s=dbf67b5b767e33b3f57c82ee1740f8b12cc6fe6a', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76cf741edf36ba1daabda43903aff6701ee9ff2f', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91dff6c55c62f6c90848c99ef27aee718a22e157', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=46bf9a61b5a59245316b60bd8306184799125e30', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=be1d70685ca6cf448bef335bbb15f34111fa8ce0', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd29a251fde1a6782cc5863416ab87221717044b', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/p9joXGatHld84Wl_zr4I_UomOa5M90G3GgOQbQMxnak.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9317991d10164b3ea43b69e6e9e60bd1c425ad1a', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '-B2RYnvKjSBs2Ixt2k7Wo34JLLs72aNpuisbBK3i_vw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbpnep,True,,quantum_booty,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbpnep/machine_learning_engineer_career_advice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbpnep/machine_learning_engineer_career_advice/,155203,1588364707.0,0,,False,,,,
,learnmachinelearning,"Hi! Im completely new to neural networks and am trying to build a convolutional neural network in keras with tensorflow. I'm running about 8 categories seperated by race and gender and it seems to not be learning anything. After running around anywhere from 4 epochs to 15 epochs, it stops reducing its loss and accuracy is capped at about 13%.  I'm using the UTK dataset with but filtered to sort by gender and race, and cut anyone under 20 and over 50. I have about 1000 images per dataset. I built a car-model detecting model before this and it was also random guessing (I decided to try something completely new after), [this](https://pastebin.com/pYggMjwt) is the code I used to build the dataset and [this](https://pastebin.com/7yEDP3CK) is the code I used to build the network, [here](https://drive.google.com/file/d/1vJoaZFzhPKVS0kI2d_ufljWEMPN_7DRa/view?usp=sharing) is a google link to my dataset. I feel like it must have something to do with the data as I followed two seperate tutorials on building a CNN, and the data is somewhat noisy. If anyone has any info on suggestions please let me know. I know just a giant code dump isnt helpful and I'm really sorry for that, but I seriously dont know what could be going on. I heard that it could be from the amount of layers I have or something, I'm not sure. Something I noticed is I get the same validation accuracy every time. Any help would be super appreciated, I'm  planning on going into machine learning and am self teaching right now, thank you so much!

&amp;#x200B;

\*\*NOTE extract all files into one folder so you should have the data builder, network and folder named TRAINING in one folder",t2_n3wq5,False,,0,False,"Convolutional NN not learning, seems to be random guessing.","[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbqzpp,False,light,0.8,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,1588370778.0,,[],{},self,,True,,1588397878.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! Im completely new to neural networks and am trying to build a convolutional neural network in keras with tensorflow. I&amp;#39;m running about 8 categories seperated by race and gender and it seems to not be learning anything. After running around anywhere from 4 epochs to 15 epochs, it stops reducing its loss and accuracy is capped at about 13%.  I&amp;#39;m using the UTK dataset with but filtered to sort by gender and race, and cut anyone under 20 and over 50. I have about 1000 images per dataset. I built a car-model detecting model before this and it was also random guessing (I decided to try something completely new after), &lt;a href=""https://pastebin.com/pYggMjwt""&gt;this&lt;/a&gt; is the code I used to build the dataset and &lt;a href=""https://pastebin.com/7yEDP3CK""&gt;this&lt;/a&gt; is the code I used to build the network, &lt;a href=""https://drive.google.com/file/d/1vJoaZFzhPKVS0kI2d_ufljWEMPN_7DRa/view?usp=sharing""&gt;here&lt;/a&gt; is a google link to my dataset. I feel like it must have something to do with the data as I followed two seperate tutorials on building a CNN, and the data is somewhat noisy. If anyone has any info on suggestions please let me know. I know just a giant code dump isnt helpful and I&amp;#39;m really sorry for that, but I seriously dont know what could be going on. I heard that it could be from the amount of layers I have or something, I&amp;#39;m not sure. Something I noticed is I get the same validation accuracy every time. Any help would be super appreciated, I&amp;#39;m  planning on going into machine learning and am self teaching right now, thank you so much!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;**NOTE extract all files into one folder so you should have the data builder, network and folder named TRAINING in one folder&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?auto=webp&amp;s=e28867cccd2864fc170848b64bc44e6a778116b9', 'width': 250, 'height': 250}, 'resolutions': [{'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b38a59d6140109bc38ed4f88b98bd4436dfe09b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/rc2Fna9bJ_J6PFVDuZUOKJl5CH9700Qyi0-ll1mn4C4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=203ad20eac023d67d122c5f834516a4670799f63', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'tEFaKdpbTuSBBWpWQ-kmQ1l_KwNUpQtPpUtOwmLiL-A'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gbqzpp,True,,10macattack,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbqzpp/convolutional_nn_not_learning_seems_to_be_random/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbqzpp/convolutional_nn_not_learning_seems_to_be_random/,155203,1588369078.0,0,,False,,,,
,learnmachinelearning,"I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.

One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.

What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?

Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.",t2_zmqho4m,False,,0,False,How to write a scikit-learn estimator in PyTorch,[],r/learnmachinelearning,False,6,,0,,False,t3_gbxx54,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588405809.0,,[],{},,,True,,1588414311.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had developed an estimator in Scikit-learn but because of performance issues (both speed and memory usage) I am thinking of making the estimator to run using GPU.&lt;/p&gt;

&lt;p&gt;One way I can think of to do this is to write the estimator in PyTorch (so I can use GPU processing) and then use Google Colab to leverage on their cloud GPUs and memory capacity.&lt;/p&gt;

&lt;p&gt;What would be the best way to write an estimator which is already scikit-learn compatible in PyTorch?&lt;/p&gt;

&lt;p&gt;Any pointers or hints pointing to the right direction would really be appreciated. Many thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbxx54,True,,leockl,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbxx54/how_to_write_a_scikitlearn_estimator_in_pytorch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbxx54/how_to_write_a_scikitlearn_estimator_in_pytorch/,155203,1588385511.0,0,,False,,,,
,learnmachinelearning,"Hello! 

I was reading about GMM: https://scikit-learn.org/stable/modules/mixture.html

And read how the performance of GMM can be improved by introducing the dirichlet process. I am not quite sure I understand the rationale behind this. Could someone please give me a hand?

Thanks!",t2_xtuyc,False,,0,False,Dirichlet process and Gaussian Mixture Models (GMM),[],r/learnmachinelearning,False,6,,0,,False,t3_gbox61,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588391079.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! &lt;/p&gt;

&lt;p&gt;I was reading about GMM: &lt;a href=""https://scikit-learn.org/stable/modules/mixture.html""&gt;https://scikit-learn.org/stable/modules/mixture.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And read how the performance of GMM can be improved by introducing the dirichlet process. I am not quite sure I understand the rationale behind this. Could someone please give me a hand?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?auto=webp&amp;s=60b67c7f7821ca0ece95403af3e2dafe9178633e', 'width': 1410, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f31c907f836ff6cf9a0c1c6ab884a0616d04b07', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f080162f726ecdc663e280b9cb680c0a7f376b95', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=54c109dbfddf4ea419f6d788217ce541b638ac4d', 'width': 320, 'height': 181}, {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c5816def29c5922c1e47767712d9884e31aab3a', 'width': 640, 'height': 363}, {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=676f5b5d7c5a4bf70786695b56d8008b6d1b1b3a', 'width': 960, 'height': 544}, {'url': 'https://external-preview.redd.it/OSZZgWpEd40lBc3_qvyq80fQP-skj9MtGpSdAHQtOmA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d386f2caf41dd382164f6b8e2c4c3b7c7331d6b', 'width': 1080, 'height': 612}], 'variants': {}, 'id': 'X62Gd1IffbA9RdMRn4VQnPnr2YIEg_8oSKLd12-azH0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbox61,True,,ottawalanguages,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbox61/dirichlet_process_and_gaussian_mixture_models_gmm/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbox61/dirichlet_process_and_gaussian_mixture_models_gmm/,155203,1588362279.0,0,,False,,,,True
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Video Depth Estimation,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gbhg1x,False,light,0.82,,public,7,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/51CQObCd_K0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Estimated video depth without Lidar', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/51CQObCd_K0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/51CQObCd_K0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/51CQObCd_K0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gbhg1x', 'height': 338}",Discussion,False,7,,False,https://b.thumbs.redditmedia.com/K4aS-mntApmJPyRtS8lIHJVW0iXcQQ0qg2QKw1vbxbA.jpg,False,,[],{},rich:video,,False,,1588365954.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sByOqGTa99Pi7mII8xmytjlPBYbb-Ie2I5H2we56pjE.jpg?auto=webp&amp;s=79a799c16ff1c048c462886ca2dd88bcbb944fa8', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/sByOqGTa99Pi7mII8xmytjlPBYbb-Ie2I5H2we56pjE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26005c2cb0635d5eca37343806ab691f7161a717', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/sByOqGTa99Pi7mII8xmytjlPBYbb-Ie2I5H2we56pjE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=26e0834cc8ff1ea3821afd9fee9f1feb5120ae99', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/sByOqGTa99Pi7mII8xmytjlPBYbb-Ie2I5H2we56pjE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1844665e8088ea008db0584390661593b1c2dd31', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Q20-PzEpYb21EbFO2ICDW4yhAHLQ3w8aOaAzONtBkFc'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gbhg1x,True,,cmillionaire9,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbhg1x/video_depth_estimation/,all_ads,False,https://youtu.be/51CQObCd_K0,155203,1588337154.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Estimated video depth without Lidar', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/51CQObCd_K0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/51CQObCd_K0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Beginners in Andew Ng course ""Machine Learning"" can join discord server for effective communication and sharing problems and solutions.

[https://discord.gg/dp34tS](https://discord.gg/dp34tS)

Machine Learning Coursera (v) Discord server, Lets be classmates !

&amp;#x200B;

\#coursera #machinelearning",t2_5se67uaf,False,,0,False,Coursera Machine Learning by Andew Ng - Unofficial Discord server,[],r/learnmachinelearning,False,6,,0,,False,t3_gbpbqc,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1588364077.0,,[],{},self,,True,,1588392436.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Beginners in Andew Ng course &amp;quot;Machine Learning&amp;quot; can join discord server for effective communication and sharing problems and solutions.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://discord.gg/dp34tS""&gt;https://discord.gg/dp34tS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Machine Learning Coursera (v) Discord server, Lets be classmates !&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;#coursera #machinelearning&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/u7y4xb4sS-j2Xegu7slh-x2R_kS26jsuVfyRkdsoipU.jpg?auto=webp&amp;s=58cd0859cdc7b95cc15e2c6105b60b63bbfd3935', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/u7y4xb4sS-j2Xegu7slh-x2R_kS26jsuVfyRkdsoipU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=75b2d7910741ffe27ff8bc04fb34969674a81b93', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/u7y4xb4sS-j2Xegu7slh-x2R_kS26jsuVfyRkdsoipU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1ed20c63a50c5c054a70d5cb103531be7ea58de', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'rv_kWzg8Yd6PWRvJZzHRhV8-s2fZquom1BiLnEgyAPo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbpbqc,True,,vasik_vasik,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbpbqc/coursera_machine_learning_by_andew_ng_unofficial/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbpbqc/coursera_machine_learning_by_andew_ng_unofficial/,155203,1588363636.0,0,,False,,,,
,learnmachinelearning," 

I've recently enrolled in the Coursera machine learning, and am working my way through making my own classifier for the [Iris dataset](https://www.kaggle.com/uciml/iris) problem using matlab. I'm training a classifier for each species (the one-vs-all method). The code runs smoothly without any errors, but the results are not quite what I was expecting. Here's my matlab code:

    clear, clc
    
    % loading the data
    data = csvread('data.csv');
    
    % extracting the features
    X = data(:, (1:end-3));
    X = [ones(size(X, 1), 1) X];
    
    % computing the number of training examples (m) and features (n)
    [m, n] = size(X);
    
    % extracting the labels
    y_setosa = data(:, end-2);
    y_versicolor = data(:, end-1);
    y_virginica = data(:, end);
    
    % initializing the options for optimization algorithm
    options = optimset('GradObj', 'on', 'MaxIter', 10);
    
    % creating the initial theta vector
    iniTheta = zeros(n, 1);
    
    % tarining a classifier for each label
    [theta_setosa, min_setosa] = fminunc(@(theta) costFunction(theta, m, X, y_setosa), iniTheta, options);
    [theta_versicolor, min_versicolor] = fminunc(@(theta) costFunction(theta, m, X, y_versicolor), iniTheta, options);
    [theta_virginica, min_virginica] = fminunc(@(theta) costFunction(theta, m, X, y_virginica), iniTheta, options);

Here's the code for my costFunction:

    function [jValue, gradient] = costFunction(theta, m, X, Y)
        V = X*theta;
        jValue = (-1/m)*(Y'*log(sigmoid(V)) + (1-Y)'*log(sigmoid(1-V)));
        gradient = (-1/m)*X'*(sigmoid(V)-Y);
    end

And finally my sigmoid function:

    function sigX = sigmoid(X)
        sigX = arrayfun(@(n) 1/(1+exp(-n)), X);
    end

The resulting theta\_setosa and theta\_versicolor vectors are all zeros, and the costFunction has the same minimum for both of them. the virginica classifier seems to be working fine, however, although for each fminunc I get the following message displayed

    Local minimum possible.
    
    fminunc stopped because it cannot decrease the objective function
    along the current search direction.
    
    &lt;stopping criteria details&gt;",t2_nfd6t23,False,,0,False,Optimization function returns null optimal parameters for two different labels,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbt6u7,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1588405424.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently enrolled in the Coursera machine learning, and am working my way through making my own classifier for the &lt;a href=""https://www.kaggle.com/uciml/iris""&gt;Iris dataset&lt;/a&gt; problem using matlab. I&amp;#39;m training a classifier for each species (the one-vs-all method). The code runs smoothly without any errors, but the results are not quite what I was expecting. Here&amp;#39;s my matlab code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clear, clc

% loading the data
data = csvread(&amp;#39;data.csv&amp;#39;);

% extracting the features
X = data(:, (1:end-3));
X = [ones(size(X, 1), 1) X];

% computing the number of training examples (m) and features (n)
[m, n] = size(X);

% extracting the labels
y_setosa = data(:, end-2);
y_versicolor = data(:, end-1);
y_virginica = data(:, end);

% initializing the options for optimization algorithm
options = optimset(&amp;#39;GradObj&amp;#39;, &amp;#39;on&amp;#39;, &amp;#39;MaxIter&amp;#39;, 10);

% creating the initial theta vector
iniTheta = zeros(n, 1);

% tarining a classifier for each label
[theta_setosa, min_setosa] = fminunc(@(theta) costFunction(theta, m, X, y_setosa), iniTheta, options);
[theta_versicolor, min_versicolor] = fminunc(@(theta) costFunction(theta, m, X, y_versicolor), iniTheta, options);
[theta_virginica, min_virginica] = fminunc(@(theta) costFunction(theta, m, X, y_virginica), iniTheta, options);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;#39;s the code for my costFunction:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function [jValue, gradient] = costFunction(theta, m, X, Y)
    V = X*theta;
    jValue = (-1/m)*(Y&amp;#39;*log(sigmoid(V)) + (1-Y)&amp;#39;*log(sigmoid(1-V)));
    gradient = (-1/m)*X&amp;#39;*(sigmoid(V)-Y);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally my sigmoid function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;function sigX = sigmoid(X)
    sigX = arrayfun(@(n) 1/(1+exp(-n)), X);
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting theta_setosa and theta_versicolor vectors are all zeros, and the costFunction has the same minimum for both of them. the virginica classifier seems to be working fine, however, although for each fminunc I get the following message displayed&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Local minimum possible.

fminunc stopped because it cannot decrease the objective function
along the current search direction.

&amp;lt;stopping criteria details&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aqQ5L-zBsVB2UIyAOF6BitSRs3ef2O2lktn_f3RuMpw.jpg?auto=webp&amp;s=43d4fbb606aebbaf393b30154eec74b4f82abf76', 'width': 600, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/aqQ5L-zBsVB2UIyAOF6BitSRs3ef2O2lktn_f3RuMpw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f401bae33b314513c9f1ea3ff4241a4470ff645f', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/aqQ5L-zBsVB2UIyAOF6BitSRs3ef2O2lktn_f3RuMpw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8bceac26175d7fe3d4108e41a5cecb4722a0b8c9', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/aqQ5L-zBsVB2UIyAOF6BitSRs3ef2O2lktn_f3RuMpw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=62971ee065331a9fe286ae383f849c25f2d298c9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'QAP4pnQsaGWOvlphHT67uTuD_urHloGxcgJkipYvliM'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gbt6u7,True,,EOmar4TW,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbt6u7/optimization_function_returns_null_optimal/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbt6u7/optimization_function_returns_null_optimal/,155203,1588376624.0,0,,False,,,,
,learnmachinelearning,"Yes sorry for the repetitive question.! Didn't want to revive old threads so wanted to ask this again suitable for 2020.

So I finished the introduction to machine learning Andrew Ng coursera course. So I am implementing those exercises in python now as practice too..
So after reading most old threads I've found some answers from what I have seen
1. Practice on kaggle
2. Deep learning ai course Andrew ng coursera
3. Fastai deep course

Honestly I want to learn a bit more before trying on kaggle.
But the specialisation course of deep learning ai coursera seems to be quite large and take a bit time so I'm trying out the Fastai deep learning course now.

So is the Fastai course a good choice to continue now or please recommended if there is a better approach to learn now? (want to utilise my lockdown time now!)

Thank you!",t2_10itsw,False,,0,False,After machine learning course of coursera?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbmpuz,False,light,0.8,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588383904.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Yes sorry for the repetitive question.! Didn&amp;#39;t want to revive old threads so wanted to ask this again suitable for 2020.&lt;/p&gt;

&lt;p&gt;So I finished the introduction to machine learning Andrew Ng coursera course. So I am implementing those exercises in python now as practice too..
So after reading most old threads I&amp;#39;ve found some answers from what I have seen
1. Practice on kaggle
2. Deep learning ai course Andrew ng coursera
3. Fastai deep course&lt;/p&gt;

&lt;p&gt;Honestly I want to learn a bit more before trying on kaggle.
But the specialisation course of deep learning ai coursera seems to be quite large and take a bit time so I&amp;#39;m trying out the Fastai deep learning course now.&lt;/p&gt;

&lt;p&gt;So is the Fastai course a good choice to continue now or please recommended if there is a better approach to learn now? (want to utilise my lockdown time now!)&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gbmpuz,True,,udayuk,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbmpuz/after_machine_learning_course_of_coursera/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbmpuz/after_machine_learning_course_of_coursera/,155203,1588355104.0,0,,False,,,,
,learnmachinelearning,"As the title says, I would like to learn about GAN (and whatever structure that derives from it) in order to do projects like improving image resolution. Any type of resource is welcome (books, papers, online courses, youtube videos, etc) and I hope this thread will help other people too!

Thank you in advance!

PS: I have started by the section about GANs in â€œDeep Learning with Pythonâ€ by Francois Chollet",t2_3qhx565a,False,,0,False,Looking for learning about GAN,[],r/learnmachinelearning,False,6,,0,,False,t3_gbslee,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588403362.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title says, I would like to learn about GAN (and whatever structure that derives from it) in order to do projects like improving image resolution. Any type of resource is welcome (books, papers, online courses, youtube videos, etc) and I hope this thread will help other people too!&lt;/p&gt;

&lt;p&gt;Thank you in advance!&lt;/p&gt;

&lt;p&gt;PS: I have started by the section about GANs in â€œDeep Learning with Pythonâ€ by Francois Chollet&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbslee,True,,Legnaru,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbslee/looking_for_learning_about_gan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbslee/looking_for_learning_about_gan/,155203,1588374562.0,0,,False,,,,
,learnmachinelearning,,t2_5yd6bcu6,False,,0,False,I have prepared a list of TOP 10 Python Libraries You Could Have MISSED,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gbyugx,False,dark,0.43,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FFEVAZhT7iw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TOP 10 Python Libraries You Could Have MISSED', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FFEVAZhT7iw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFEVAZhT7iw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FFEVAZhT7iw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gbyugx', 'height': 338}",,False,0,,False,https://a.thumbs.redditmedia.com/vFyiVWDpr-iEZ9vWDtzDfPzPbj-_xTdkIBzV4hj2uY4.jpg,False,,[],{},rich:video,,False,,1588416541.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/m-ft60K7MhEDSqx4AeVAbFDTuPJFICPIvsyDbvEtYAs.jpg?auto=webp&amp;s=577c76a903e9908d3fd84045afa6b4d1cce4b4a5', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/m-ft60K7MhEDSqx4AeVAbFDTuPJFICPIvsyDbvEtYAs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1321239ae0692f47a13ac72084639307809682a7', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/m-ft60K7MhEDSqx4AeVAbFDTuPJFICPIvsyDbvEtYAs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a276683c335bd5f5ee6bad4a0d83885c8d52d6f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/m-ft60K7MhEDSqx4AeVAbFDTuPJFICPIvsyDbvEtYAs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=437ea7d6fc33d61d4282a200a55d813e3bcce82e', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'AwVtSs6-qMU-DSk8pa6zG7yhGW_hGox3bJX28oaNbW4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbyugx,True,,JK_Bielan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbyugx/i_have_prepared_a_list_of_top_10_python_libraries/,all_ads,False,https://www.youtube.com/watch?v=FFEVAZhT7iw,155203,1588387741.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TOP 10 Python Libraries You Could Have MISSED', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FFEVAZhT7iw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FFEVAZhT7iw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_jh4n9,False,,0,False,Deep Learning with PyTorch - Full Course on freeCodeCamp.org,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gbno8c,False,dark,1.0,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/GIsg-ZUy0MY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Deep Learning with PyTorch - Full Course', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/GIsg-ZUy0MY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'freeCodeCamp.org', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GIsg-ZUy0MY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/GIsg-ZUy0MY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gbno8c', 'height': 338}",,False,2,,False,https://a.thumbs.redditmedia.com/jDlR2hx56R1h4LFxQzD8BEnyhfZl--azgXCeg75GbZ4.jpg,False,,[],{},rich:video,,False,,1588387019.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Sn9lK0Hf2KCtkhrpMiCYXhoPG_AwpvAyLIm6LGITCJU.jpg?auto=webp&amp;s=8c29f26ad3b013aa65b99ad3525ecabf522584d9', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Sn9lK0Hf2KCtkhrpMiCYXhoPG_AwpvAyLIm6LGITCJU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2821e15850670d4b09088e57d1e7b03a1d708c28', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Sn9lK0Hf2KCtkhrpMiCYXhoPG_AwpvAyLIm6LGITCJU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2c879eaed92c6b17b267cb46e56a9fd0221c7e9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Sn9lK0Hf2KCtkhrpMiCYXhoPG_AwpvAyLIm6LGITCJU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ccabb078750bbfaf87258c04c388595c543a2fd', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ngdrU1ID1Vg3xC7NfaylkrgJuBwyWuP1wkluipAUWk4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbno8c,True,,vishalpathikgupta,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbno8c/deep_learning_with_pytorch_full_course_on/,all_ads,False,https://www.youtube.com/watch?v=GIsg-ZUy0MY,155203,1588358219.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Deep Learning with PyTorch - Full Course', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/GIsg-ZUy0MY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'freeCodeCamp.org', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/GIsg-ZUy0MY/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Looking to make a slack group where we work on a new kaggle challenge every week.

Everyone posts their solutions and we discuss best practices and how to improve our models.

This would be a learning group where everyone can focus on improving.

Join if you are looking to get better at Data Science / Machine Learning

Edit: Upvote when you join so we can reach more people

[https://join.slack.com/t/kaggle-emg4622/shared\_invite/zt-dzc1o4h1-y8\_yY1qFipJHzLFkbTb1iA](https://join.slack.com/t/kaggle-emg4622/shared_invite/zt-dzc1o4h1-y8_yY1qFipJHzLFkbTb1iA)",t2_1dwdg6ns,False,,0,False,Kaggle group chat,[],r/learnmachinelearning,False,6,,0,,False,t3_gb05h2,False,dark,0.98,,public,191,0,{},,,False,[],,False,False,,{},,False,191,,False,self,1588298802.0,,[],{},self,,True,,1588296577.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking to make a slack group where we work on a new kaggle challenge every week.&lt;/p&gt;

&lt;p&gt;Everyone posts their solutions and we discuss best practices and how to improve our models.&lt;/p&gt;

&lt;p&gt;This would be a learning group where everyone can focus on improving.&lt;/p&gt;

&lt;p&gt;Join if you are looking to get better at Data Science / Machine Learning&lt;/p&gt;

&lt;p&gt;Edit: Upvote when you join so we can reach more people&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://join.slack.com/t/kaggle-emg4622/shared_invite/zt-dzc1o4h1-y8_yY1qFipJHzLFkbTb1iA""&gt;https://join.slack.com/t/kaggle-emg4622/shared_invite/zt-dzc1o4h1-y8_yY1qFipJHzLFkbTb1iA&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/IT8MxtWsZ2QGVSGf2bRpBk66RtYo1TSfZM_hqpEjeNs.jpg?auto=webp&amp;s=f45b3c869ea7abdb2c1fb35d18104bfd2deb4c23', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/IT8MxtWsZ2QGVSGf2bRpBk66RtYo1TSfZM_hqpEjeNs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=757734b67cae7ba812e3f80cce97dec2a6f8bfb2', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/IT8MxtWsZ2QGVSGf2bRpBk66RtYo1TSfZM_hqpEjeNs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33e8ccd255433292903f699291339d720ecfce6', 'width': 216, 'height': 216}], 'variants': {}, 'id': '__CrVUpNwgKv0cvyAzo600DPJbSZEDXS0xQHtefkw2Y'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb05h2,True,,Anunoby3,,21,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb05h2/kaggle_group_chat/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb05h2/kaggle_group_chat/,155203,1588267777.0,0,,False,,,,
,learnmachinelearning,"When running code on Google Colabâ€™s GPU, will we be using Google Colabâ€™s GPU memory too (rather than our local deviceâ€™s RAM or hard disk)?",t2_zmqho4m,False,,0,False,Memory usage when using Google Colab GPU,[],r/learnmachinelearning,False,6,,0,,False,t3_gbkfyv,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588376516.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;When running code on Google Colabâ€™s GPU, will we be using Google Colabâ€™s GPU memory too (rather than our local deviceâ€™s RAM or hard disk)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbkfyv,True,,leockl,,9,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbkfyv/memory_usage_when_using_google_colab_gpu/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbkfyv/memory_usage_when_using_google_colab_gpu/,155203,1588347716.0,0,,False,,,,
,learnmachinelearning,"Hey, so I am quite new to machine learning and I am currently looking for books to read. So I already bought two books, one on how to apply machine learning in python using scikit and the other one named The hundred-page machine learning book. I read the second book and currently am reading the first one. However, I am wondering if anyone has a good book on the theory behind the different algorithms used in machine learning. The problem is I am reading about all these algorithm in the first book, but they only give a short description and then show how to use functions in scikit in order to use them. That is cool and all, but my goal is not to learn these functions and know how to make a certain algorithm, my goal is to understand them. Which this book doesn't do at all. Hence I am looking for book(s) that would explain me these things in greater detail. Then I would feel comfortable to learn how to apply these algorithm using scikit or any other package. I have a good level of mathematics so I am not afraid to jump into a more mathematical book (in fact I would prefer that over a book that doesn't cover the mathematical part). So if you have a good book to recommend me, please let me know! Thanks in advance:)!",t2_38eeu360,False,,0,False,Looking for books,[],r/learnmachinelearning,False,6,,0,,False,t3_gbprtj,False,dark,0.4,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588393911.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, so I am quite new to machine learning and I am currently looking for books to read. So I already bought two books, one on how to apply machine learning in python using scikit and the other one named The hundred-page machine learning book. I read the second book and currently am reading the first one. However, I am wondering if anyone has a good book on the theory behind the different algorithms used in machine learning. The problem is I am reading about all these algorithm in the first book, but they only give a short description and then show how to use functions in scikit in order to use them. That is cool and all, but my goal is not to learn these functions and know how to make a certain algorithm, my goal is to understand them. Which this book doesn&amp;#39;t do at all. Hence I am looking for book(s) that would explain me these things in greater detail. Then I would feel comfortable to learn how to apply these algorithm using scikit or any other package. I have a good level of mathematics so I am not afraid to jump into a more mathematical book (in fact I would prefer that over a book that doesn&amp;#39;t cover the mathematical part). So if you have a good book to recommend me, please let me know! Thanks in advance:)!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbprtj,True,,IKnowPythagoras,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbprtj/looking_for_books/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbprtj/looking_for_books/,155203,1588365111.0,0,,False,,,,
,learnmachinelearning,"
Hi !  I have one question what are the Pros and cons  of Automated Machine learning tools like pycaret ,Tpot H20 .Do they affect Data science and Machine Learning Job roles ? Can they perform all the pipeline of Data science ?

There is very little information on web , I just read a blog post that said googles Auto Ml outperformed any of the Human based models",t2_2qt6i581,False,,0,False,Auto Ml vs Dl/ ML engineer's,[],r/learnmachinelearning,False,6,,0,,False,t3_gbj52x,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1588372286.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi !  I have one question what are the Pros and cons  of Automated Machine learning tools like pycaret ,Tpot H20 .Do they affect Data science and Machine Learning Job roles ? Can they perform all the pipeline of Data science ?&lt;/p&gt;

&lt;p&gt;There is very little information on web , I just read a blog post that said googles Auto Ml outperformed any of the Human based models&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbj52x,True,,shaelander1990,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbj52x/auto_ml_vs_dl_ml_engineers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbj52x/auto_ml_vs_dl_ml_engineers/,155203,1588343486.0,0,,False,,,,
,learnmachinelearning,"I have code that estimates RMSE for k-fold cross-validation and I think it is correct (from book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by AurÃ©lien GÃ©ron)

    scores = cross_val_score(forest_reg, a, b, scoring=""neg_mean_squared_error"", cv=10)
    print(pd.Series(np.sqrt(-scores)).describe())

&lt;br&gt;&lt;br&gt;

So what about MAE? Should I use (with `sqrt`):

    scores = cross_val_score(forest_reg, a, b, scoring=""neg_mean_absolute_error"", cv=10)
    print(pd.Series(np.sqrt(-scores)).describe())

or this (without `sqrt`):

    scores = cross_val_score(forest_reg, a, b, scoring=""neg_mean_absolute_error"", cv=10)
    print(pd.Series(-scores).describe())



Also for MAE estimation, it should be  `-scores` or `scores`?",t2_an3zj,False,,0,False,MAE estimation for k-fold cross-validation,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbtt06,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588406812.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have code that estimates RMSE for k-fold cross-validation and I think it is correct (from book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by AurÃ©lien GÃ©ron)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scores = cross_val_score(forest_reg, a, b, scoring=&amp;quot;neg_mean_squared_error&amp;quot;, cv=10)
print(pd.Series(np.sqrt(-scores)).describe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&lt;/p&gt;

&lt;p&gt;So what about MAE? Should I use (with &lt;code&gt;sqrt&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scores = cross_val_score(forest_reg, a, b, scoring=&amp;quot;neg_mean_absolute_error&amp;quot;, cv=10)
print(pd.Series(np.sqrt(-scores)).describe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or this (without &lt;code&gt;sqrt&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scores = cross_val_score(forest_reg, a, b, scoring=&amp;quot;neg_mean_absolute_error&amp;quot;, cv=10)
print(pd.Series(-scores).describe())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also for MAE estimation, it should be  &lt;code&gt;-scores&lt;/code&gt; or &lt;code&gt;scores&lt;/code&gt;?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gbtt06,True,,vasili111,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbtt06/mae_estimation_for_kfold_crossvalidation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbtt06/mae_estimation_for_kfold_crossvalidation/,155203,1588378012.0,0,,False,,,,
,learnmachinelearning,Iâ€™m writing a report on mathematics behind naive bayes classifier and i want to pick an interesting topic that can be used as an example. (Preferably not female/male classification or spam) Any ideas?,t2_68mb86mo,False,,0,False,Naive Bayes Classifier - creative application,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbp3wt,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588391721.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m writing a report on mathematics behind naive bayes classifier and i want to pick an interesting topic that can be used as an example. (Preferably not female/male classification or spam) Any ideas?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gbp3wt,True,,hniemczyk,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbp3wt/naive_bayes_classifier_creative_application/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbp3wt/naive_bayes_classifier_creative_application/,155203,1588362921.0,0,,False,,,,
,learnmachinelearning," Hello,

I'm trying to create training data for an LSTM, but I'm stuck at reshaping the data to a tensor which I can feed the model with. Right now I have 5000 different dataframes, where:

* Each dataframe has 10 features and 200 timesteps (each row is a timestep, each column is a feature).
* Each dataframe represents a different sample (so each sample has 200 timesteps and 10 features).

How do I go about at reshaping these dataframes to a 3-dimensional tensor?

Thanks for the help.",t2_5g33rwd0,False,,0,False,Reshaping multiple dataframes to a 3-dimensional tensor,[],r/learnmachinelearning,False,6,,0,,False,t3_gbol3r,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588389990.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to create training data for an LSTM, but I&amp;#39;m stuck at reshaping the data to a tensor which I can feed the model with. Right now I have 5000 different dataframes, where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each dataframe has 10 features and 200 timesteps (each row is a timestep, each column is a feature).&lt;/li&gt;
&lt;li&gt;Each dataframe represents a different sample (so each sample has 200 timesteps and 10 features).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How do I go about at reshaping these dataframes to a 3-dimensional tensor?&lt;/p&gt;

&lt;p&gt;Thanks for the help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbol3r,True,,TheYodeler4,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbol3r/reshaping_multiple_dataframes_to_a_3dimensional/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbol3r/reshaping_multiple_dataframes_to_a_3dimensional/,155203,1588361190.0,0,,False,,,,
,learnmachinelearning,I know that you can pass inputs into a neural network in batches as matrixes. But couldn't the same result be achieved by feeding each set of inputs through the network and puting the results in a matrix.,t2_49xlrsbx,False,,0,False,Ann Batch size question,[],r/learnmachinelearning,False,6,,0,,False,t3_gbnulp,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588387598.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know that you can pass inputs into a neural network in batches as matrixes. But couldn&amp;#39;t the same result be achieved by feeding each set of inputs through the network and puting the results in a matrix.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbnulp,True,,willbill4197,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbnulp/ann_batch_size_question/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbnulp/ann_batch_size_question/,155203,1588358798.0,0,,False,,,,
,learnmachinelearning,"Hello!  


I wish to know how do people normally deploy their CNN models and others alike using JS?   
I'm familiar with both JS and Python, and have used Django before, but I wanted to try something in JS just to get an idea of how differently we do things in the other language.   
I've found Python to be okay-ish for admin pages using Django, but it's not really ideal for building web applications - not even close to the long list of frameworks and technologies available in JS, in my opinion.  


Any guidance would be appreciated. Thank you, and have a nice day!",t2_3viml784,False,,0,False,What's A Good Way To Deploy CNN Models Using JS?,[],r/learnmachinelearning,False,6,,0,,False,t3_gbn25c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588385019.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!  &lt;/p&gt;

&lt;p&gt;I wish to know how do people normally deploy their CNN models and others alike using JS?&lt;br/&gt;
I&amp;#39;m familiar with both JS and Python, and have used Django before, but I wanted to try something in JS just to get an idea of how differently we do things in the other language.&lt;br/&gt;
I&amp;#39;ve found Python to be okay-ish for admin pages using Django, but it&amp;#39;s not really ideal for building web applications - not even close to the long list of frameworks and technologies available in JS, in my opinion.  &lt;/p&gt;

&lt;p&gt;Any guidance would be appreciated. Thank you, and have a nice day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbn25c,True,,Are_We_There_Yet256,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbn25c/whats_a_good_way_to_deploy_cnn_models_using_js/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbn25c/whats_a_good_way_to_deploy_cnn_models_using_js/,155203,1588356219.0,0,,False,,,,
,learnmachinelearning,"Hey there, As beginner, I have completed 2 weeks of study with Andrew Ng popular Machine Learning course after seeing a lot of recommendation in this sub. Courser content is engaging (i am not adverting it). However this course is 4+ year, should i continue this, please guide.",t2_13npf5,False,,0,False,Is Andrew Ng Machine learning course too old?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbm5lv,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588382097.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, As beginner, I have completed 2 weeks of study with Andrew Ng popular Machine Learning course after seeing a lot of recommendation in this sub. Courser content is engaging (i am not adverting it). However this course is 4+ year, should i continue this, please guide.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gbm5lv,True,,HackSpirit,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbm5lv/is_andrew_ng_machine_learning_course_too_old/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbm5lv/is_andrew_ng_machine_learning_course_too_old/,155203,1588353297.0,0,,False,,,,
,learnmachinelearning,"I want to build a web app where users who register/log in can rate items and  then get  personalized recommendations. I've tried running different algorithms in python notebook environments and I have a basic idea of how they work. I was able to get either predictions for estimated ratings or top-n recommended items for users that are part of the training/testing dataset using the library surprise. However, it seems to not be able to provide such predictions or top-n recommendations for new users that are not part of the dataset, even though they **rated** items.

&amp;#x200B;

I think this process is called online learning, from what I could find on the internet these past couple days, but I can't find a way to implement this, either via a library/api or by myself. Can anyone give me some pointers?",t2_4tvmvl2p,False,,0,False,Question about recommender systems ready for production/deployment,[],r/learnmachinelearning,False,6,,0,,False,t3_gblzq9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588381564.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to build a web app where users who register/log in can rate items and  then get  personalized recommendations. I&amp;#39;ve tried running different algorithms in python notebook environments and I have a basic idea of how they work. I was able to get either predictions for estimated ratings or top-n recommended items for users that are part of the training/testing dataset using the library surprise. However, it seems to not be able to provide such predictions or top-n recommendations for new users that are not part of the dataset, even though they &lt;strong&gt;rated&lt;/strong&gt; items.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I think this process is called online learning, from what I could find on the internet these past couple days, but I can&amp;#39;t find a way to implement this, either via a library/api or by myself. Can anyone give me some pointers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gblzq9,True,,machinelearner10,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gblzq9/question_about_recommender_systems_ready_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gblzq9/question_about_recommender_systems_ready_for/,155203,1588352764.0,0,,False,,,,
,learnmachinelearning,"Hey guys thought I'd just share with you a personal project I created over the last couple fo weeks in lockdown. I'm very new to machine learning and coding in general so am super proud of it!

It's a twitterbot comprised of a first order markov chain using tweets scraped from \\@elonmusk and 500 megabytes of the most popular childrens books from the last 50 years. Set up in an AWS architecture using Lambda functions and cloudwatch events.

Follow at your own leisure (or detriment)

[https://twitter.com/ElonsRhymes](https://twitter.com/ElonsRhymes)",t2_639wxm4b,False,,0,False,Personal project twitterbot,[],r/learnmachinelearning,False,6,,0,,False,t3_gbgoot,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588362719.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys thought I&amp;#39;d just share with you a personal project I created over the last couple fo weeks in lockdown. I&amp;#39;m very new to machine learning and coding in general so am super proud of it!&lt;/p&gt;

&lt;p&gt;It&amp;#39;s a twitterbot comprised of a first order markov chain using tweets scraped from \@elonmusk and 500 megabytes of the most popular childrens books from the last 50 years. Set up in an AWS architecture using Lambda functions and cloudwatch events.&lt;/p&gt;

&lt;p&gt;Follow at your own leisure (or detriment)&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://twitter.com/ElonsRhymes""&gt;https://twitter.com/ElonsRhymes&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3drs0dVIfIKxOPdyztQD3kuW55bdRoc1GmwamfGuuhM.jpg?auto=webp&amp;s=8a613f7227bef10eb64b095745ae72cabd8704a2', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3drs0dVIfIKxOPdyztQD3kuW55bdRoc1GmwamfGuuhM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a26bb5ddaaa4512ea1b40c010a14504e0ffc344', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3drs0dVIfIKxOPdyztQD3kuW55bdRoc1GmwamfGuuhM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=676dedbad503032ec35f22e4b57e2ca3e68d2c8a', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3drs0dVIfIKxOPdyztQD3kuW55bdRoc1GmwamfGuuhM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b031f177a8004d7604f1c816f504ab431134358e', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'U4aQCYzUvLl7iNhJog57ZnuKbrkgUTk4_JSKupNOK-w'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbgoot,True,,97thdimension,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbgoot/personal_project_twitterbot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbgoot/personal_project_twitterbot/,155203,1588333919.0,0,,False,,,,
,learnmachinelearning,,t2_x8ze3l8,False,,0,False,Illustrated guide to transformers: step by step explanation,[],r/learnmachinelearning,False,6,,0,,False,t3_gbjgjt,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588373354.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbjgjt,True,,LearnedVector,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbjgjt/illustrated_guide_to_transformers_step_by_step/,all_ads,False,https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?source=linkShare-1bdc81ea939d-1588344499,155203,1588344554.0,0,,False,,,,
,learnmachinelearning,"Hey everyone, I started Andrew Ng's machine learning course on coursera but then I found out about CS229 so now I'm doing both and this is where the problem arises. 

In CS229, gradient ascent has been used which I totally understood but in coursera gradient descent has been used. I understand that there is a difference in the update rule and the cost function but I don't understand it.  Are they essentially the same?  Also, can anyone tell me where to find the derivation of the cost function of gradient descent? I googled it but only found the derivative of the cost function. 

Thank you for taking the time to read this! :)",t2_21m2ydux,False,,0,False,Gradient descent vs Gradient ascent in Logistic Regression,[],r/learnmachinelearning,False,6,,0,,False,t3_gbejf6,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588351594.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I started Andrew Ng&amp;#39;s machine learning course on coursera but then I found out about CS229 so now I&amp;#39;m doing both and this is where the problem arises. &lt;/p&gt;

&lt;p&gt;In CS229, gradient ascent has been used which I totally understood but in coursera gradient descent has been used. I understand that there is a difference in the update rule and the cost function but I don&amp;#39;t understand it.  Are they essentially the same?  Also, can anyone tell me where to find the derivation of the cost function of gradient descent? I googled it but only found the derivative of the cost function. &lt;/p&gt;

&lt;p&gt;Thank you for taking the time to read this! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbejf6,True,,aaadi30,,9,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbejf6/gradient_descent_vs_gradient_ascent_in_logistic/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbejf6/gradient_descent_vs_gradient_ascent_in_logistic/,155203,1588322794.0,0,,False,,,,
,learnmachinelearning,"Recently I have been reading a paper ([Generative Modeling for Protein Structures](https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures.pdf)) that I have also been working on implementing to teach myself more about GANs. One questions I have is; why do they specify in the [supplement](https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures) that the matrices created by the generator have to be made symmetric and clamp above zero when passing from the generator to the discriminator?
  
In this case the model being used is DCGAN but making the generator output symmetric usually doesn't apply to DCGANs from what I've read. When I implement clamping and symmetry, the generator doesn't learn at all why the discriminator converges almost immediately to zero. The model seems to learn better without the use of symmetry and clamping (although it seems to converge and overfit quickly).

If you want to have a look at my current implementation you can find my current training loop below:
```python
# Training Loop
# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0

print(""Starting Training Loop..."")
# For each epoch
for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):
        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        # Unsqueezed dim one to convert [128, 64, 64] to [128, 1, 64, 64] to conform to D architecture 
        real_cpu = (data.unsqueeze(dim=1).type(torch.FloatTensor)).to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate batch of latent vectors
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)
        # Make Symmetric
        sym_fake = (fake.detach().clamp(min=0) + fake.detach().clamp(min=0).permute(0, 1, 3, 2)) / 2
        # Classify all fake batch with D
        output = netD(sym_fake).view(-1)
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()
        #adjust_optim(optimizerD, iters)
        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = netD(fake.detach()).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()
        adjust_optim(optimizerG, iters)

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                  % (epoch, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())

        # Check how the generator is doing by saving G's output on fixed_noise
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

        iters += 1
```",t2_edffv,False,,0,False,Why do the paper's authors suggest using symmetric matrices in DCGAN?,[],r/learnmachinelearning,False,6,,0,,False,t3_gbhzvq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588368128.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recently I have been reading a paper (&lt;a href=""https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures.pdf""&gt;Generative Modeling for Protein Structures&lt;/a&gt;) that I have also been working on implementing to teach myself more about GANs. One questions I have is; why do they specify in the &lt;a href=""https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures""&gt;supplement&lt;/a&gt; that the matrices created by the generator have to be made symmetric and clamp above zero when passing from the generator to the discriminator?&lt;/p&gt;

&lt;p&gt;In this case the model being used is DCGAN but making the generator output symmetric usually doesn&amp;#39;t apply to DCGANs from what I&amp;#39;ve read. When I implement clamping and symmetry, the generator doesn&amp;#39;t learn at all why the discriminator converges almost immediately to zero. The model seems to learn better without the use of symmetry and clamping (although it seems to converge and overfit quickly).&lt;/p&gt;

&lt;p&gt;If you want to have a look at my current implementation you can find my current training loop below:
```python&lt;/p&gt;

&lt;h1&gt;Training Loop&lt;/h1&gt;

&lt;h1&gt;Lists to keep track of progress&lt;/h1&gt;

&lt;p&gt;img_list = []
G_losses = []
D_losses = []
iters = 0&lt;/p&gt;

&lt;p&gt;print(&amp;quot;Starting Training Loop...&amp;quot;)&lt;/p&gt;

&lt;h1&gt;For each epoch&lt;/h1&gt;

&lt;p&gt;for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):
        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        # Unsqueezed dim one to convert [128, 64, 64] to [128, 1, 64, 64] to conform to D architecture 
        real_cpu = (data.unsqueeze(dim=1).type(torch.FloatTensor)).to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    ## Train with all-fake batch
    # Generate batch of latent vectors
    noise = torch.randn(b_size, nz, 1, 1, device=device)
    # Generate fake image batch with G
    fake = netG(noise)
    label.fill_(fake_label)
    # Make Symmetric
    sym_fake = (fake.detach().clamp(min=0) + fake.detach().clamp(min=0).permute(0, 1, 3, 2)) / 2
    # Classify all fake batch with D
    output = netD(sym_fake).view(-1)
    # Calculate D&amp;#39;s loss on the all-fake batch
    errD_fake = criterion(output, label)
    # Calculate the gradients for this batch
    errD_fake.backward()
    D_G_z1 = output.mean().item()
    # Add the gradients from the all-real and all-fake batches
    errD = errD_real + errD_fake
    # Update D
    optimizerD.step()
    #adjust_optim(optimizerD, iters)
    ############################
    # (2) Update G network: maximize log(D(G(z)))
    ###########################
    netG.zero_grad()
    label.fill_(real_label)  # fake labels are real for generator cost
    # Since we just updated D, perform another forward pass of all-fake batch through D
    output = netD(fake.detach()).view(-1)
    # Calculate G&amp;#39;s loss based on this output
    errG = criterion(output, label)
    # Calculate gradients for G
    errG.backward()
    D_G_z2 = output.mean().item()
    # Update G
    optimizerG.step()
    adjust_optim(optimizerG, iters)

    # Output training stats
    if i % 50 == 0:
        print(&amp;#39;[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f&amp;#39;
              % (epoch, num_epochs, i, len(dataloader),
                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

    # Save Losses for plotting later
    G_losses.append(errG.item())
    D_losses.append(errD.item())

    # Check how the generator is doing by saving G&amp;#39;s output on fixed_noise
    if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
        with torch.no_grad():
            fake = netG(fixed_noise).detach().cpu()
        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

    iters += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbhzvq,True,,trexd___,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbhzvq/why_do_the_papers_authors_suggest_using_symmetric/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbhzvq/why_do_the_papers_authors_suggest_using_symmetric/,155203,1588339328.0,0,,False,,,,
,learnmachinelearning,"I've completed the machine learning and deep learning courses on Coursera by Andrew NG. But apart from some simple classification models, I'm unable to develop other deep learning models from scratch. Currently I'm watching cs231n video lectures. What other resources should I cover so that I can implement the research papers on my own or do some project on my own?",t2_5o7kd0ho,False,,0,False,"Resources to learn, to implement papers.",[],r/learnmachinelearning,False,6,,0,,False,t3_gbgm58,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588362387.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve completed the machine learning and deep learning courses on Coursera by Andrew NG. But apart from some simple classification models, I&amp;#39;m unable to develop other deep learning models from scratch. Currently I&amp;#39;m watching cs231n video lectures. What other resources should I cover so that I can implement the research papers on my own or do some project on my own?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbgm58,True,,slayerfx132,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbgm58/resources_to_learn_to_implement_papers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbgm58/resources_to_learn_to_implement_papers/,155203,1588333587.0,0,,False,,,,
,learnmachinelearning,,t2_o0pbd,False,,0,False,A Gentle Introduction to Cross-Entropy for Machine Learning,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gar9u4,False,dark,0.98,,public,141,0,{},140.0,,False,[],,False,False,,{},,False,141,,False,https://b.thumbs.redditmedia.com/vvznfMuZz6xffTzPlkycz1qzbQaQVlPwdbW2JLbsHuQ.jpg,False,,[],{},link,,False,,1588261152.0,text,6,,,text,machinelearningmastery.com,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?auto=webp&amp;s=f87ff8dad35c2b82f5972a5d67bf7d33bb5a2dbf', 'width': 1280, 'height': 960}, 'resolutions': [{'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64f7cd30ff1bd2a6f0ebc396f6b946126e4531f2', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=807be3833a8ef1fc5446c611cc859b3749e3a2ba', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71fae35ba8cbc28811453491feaa4cc57cb15413', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd5c3558bcc1886932da3420106caf71dcd09063', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c3d440e212236781718fa98c49cb596c1b96641e', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/fiQ566NQ-krEtVzaxOgYokPOWo1wwxBRN8o-4CHyl6k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33e99e141677194f126dc3104311e5e9df09345d', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'Vj-0WklMRhm-uz0Usbm6-1lkkBiuyUKZEHQTDJrsAfw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gar9u4,True,,datapablo,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gar9u4/a_gentle_introduction_to_crossentropy_for_machine/,all_ads,False,https://machinelearningmastery.com/cross-entropy-for-machine-learning/,155203,1588232352.0,1,,False,,,,
,learnmachinelearning,,t2_173c6i,False,,0,False,"Where do you practice ML, i used google colab notebook which crashed when i used mxnet function.plz suggest some place to practice ML for free","[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbflwc,False,light,0.4,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588357411.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gbflwc,True,,tech_HACKS,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbflwc/where_do_you_practice_ml_i_used_google_colab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbflwc/where_do_you_practice_ml_i_used_google_colab/,155203,1588328611.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,AI creates high-quality 3D avatars from a single image.,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gb104e,False,light,0.84,,public,19,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aWP9484BuG0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI creates high-quality 3D avatars from a single image.', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aWP9484BuG0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/aWP9484BuG0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aWP9484BuG0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gb104e', 'height': 338}",Discussion,False,19,,False,https://b.thumbs.redditmedia.com/h5LcD21_BzZXNCF4XhGL88yqIocJJUfaMBw5u_budqo.jpg,False,,[],{},rich:video,,False,,1588299232.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/L2JngMk7BE95UZQ_y0iNid5vujoFh9fU9njPH6wKY8k.jpg?auto=webp&amp;s=3bb1cd6574de49275b344415ba8a75a5e0f84430', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/L2JngMk7BE95UZQ_y0iNid5vujoFh9fU9njPH6wKY8k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2dd84da868ee03db9e30a9d8fc8625dfedc79efe', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/L2JngMk7BE95UZQ_y0iNid5vujoFh9fU9njPH6wKY8k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=62f5dbf04b32ab15b90b8782aca601ff5acae820', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/L2JngMk7BE95UZQ_y0iNid5vujoFh9fU9njPH6wKY8k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d6548a3e84ef7b3653218aabb876165aa9cbb79', 'width': 320, 'height': 240}], 'variants': {}, 'id': '9Tbq50urPBhN00XPeYlq4j1KJkJfHMe0klNmbyNOKt8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gb104e,True,,cmillionaire9,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb104e/ai_creates_highquality_3d_avatars_from_a_single/,all_ads,False,https://youtu.be/aWP9484BuG0,155203,1588270432.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI creates high-quality 3D avatars from a single image.', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aWP9484BuG0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/aWP9484BuG0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,How Do You Crack an AI Engineerâ€™s Interview at Companies Like Google and Amazon?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,58.0,False,t3_gbfbw9,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,False,https://b.thumbs.redditmedia.com/hGZocrTIZqjsjMg00z0z5QdvxCA6Ou_RwPLJ4UFA8sI.jpg,False,,[],{},link,,False,,1588356004.0,richtext,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?auto=webp&amp;s=52250d08974dfb7e44ba7a2f15dac6a9f21fbe80', 'width': 1200, 'height': 504}, 'resolutions': [{'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=885d6bbe22fa2bcfc06d16a47de1d6b09413a18e', 'width': 108, 'height': 45}, {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f1e10ef265ca946137cc596192c0eb62bcff76e', 'width': 216, 'height': 90}, {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0e1ecf7b9f8add01a1e826b0128a16b22aa2b32', 'width': 320, 'height': 134}, {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=27965c99e30e18fc72cfd4adc2cd9c061fb16377', 'width': 640, 'height': 268}, {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0279c303bb82433c24ddcfc032e56fad2837fefd', 'width': 960, 'height': 403}, {'url': 'https://external-preview.redd.it/i2tq7nW9VxusN7g1B_2PXknGEhx-AKqU6l0quQKFZy8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7bf71200eabe4c2e691bd4f9bb780131f2d30f3', 'width': 1080, 'height': 453}], 'variants': {}, 'id': '2FJsv6UdFk1E3ugJoPpaUFubCivBzZV_6mjtJe4lQv8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gbfbw9,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbfbw9/how_do_you_crack_an_ai_engineers_interview_at/,all_ads,False,https://medium.com/@albertchristopherr/how-do-you-crack-an-ai-engineers-interview-at-companies-like-google-and-amazon-56c6e5d1b513,155203,1588327204.0,0,,False,,,,
,learnmachinelearning,"Hey guys,

in our last lesson (I'm studying CS) we were talking about the **perceptron**. This is basically an algorithm to **separate two classes with a straight line** **through the origin**. I can understand this. 

**But here comes my problem**: He then showed us a little trick to be able to separate any given set on separable data by adding a dimension to the data (aka. adding a 1 to every data vector).   
So from 2d -&gt; 3d your data set is on one plane and you try to separate the data with a line, which actually is the intersection of the data plane and another plane rotating around the origin.

I hope you heard about this trick. I just can't wrap my head around it. Is there any clear visualization/explanation of this trick (mathematically and from a visual point of view)? I already searched for it but can't find anything. I think I kind of get the idea, but something is missing.

Thanks in advance!",t2_swisye,False,,0,False,Make perceptron work again!,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbfa5o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588355738.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;in our last lesson (I&amp;#39;m studying CS) we were talking about the &lt;strong&gt;perceptron&lt;/strong&gt;. This is basically an algorithm to &lt;strong&gt;separate two classes with a straight line&lt;/strong&gt; &lt;strong&gt;through the origin&lt;/strong&gt;. I can understand this. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But here comes my problem&lt;/strong&gt;: He then showed us a little trick to be able to separate any given set on separable data by adding a dimension to the data (aka. adding a 1 to every data vector).&lt;br/&gt;
So from 2d -&amp;gt; 3d your data set is on one plane and you try to separate the data with a line, which actually is the intersection of the data plane and another plane rotating around the origin.&lt;/p&gt;

&lt;p&gt;I hope you heard about this trick. I just can&amp;#39;t wrap my head around it. Is there any clear visualization/explanation of this trick (mathematically and from a visual point of view)? I already searched for it but can&amp;#39;t find anything. I think I kind of get the idea, but something is missing.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gbfa5o,True,,tarrexoTaste,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbfa5o/make_perceptron_work_again/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbfa5o/make_perceptron_work_again/,155203,1588326938.0,0,,False,,,,
,learnmachinelearning,,t2_jh4n9,False,,0,False,How to start a career in Data Science in 2020,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gb0gdp,False,dark,0.81,,public,18,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Dn2DBnQxA2k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to start a career in Data Science in 2020 | Jovian.ml', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Dn2DBnQxA2k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'JovianML', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Dn2DBnQxA2k/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Dn2DBnQxA2k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gb0gdp', 'height': 338}",,False,18,,False,https://b.thumbs.redditmedia.com/8mXUsVXPmyRo2zMW77HfA7UJwTWmtf-Nu_UHaNVaf9o.jpg,False,,[],{},rich:video,,False,,1588297512.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vWjOGRQGSpeE0oaRYf9D_wyZjiymyDGD0U7d8dbdsBM.jpg?auto=webp&amp;s=e35fa7bd27ed37b6d974bad4877c35543c1bbb62', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/vWjOGRQGSpeE0oaRYf9D_wyZjiymyDGD0U7d8dbdsBM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d4f7760ac4a600e5949dbb46c131d00e08cbc5d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/vWjOGRQGSpeE0oaRYf9D_wyZjiymyDGD0U7d8dbdsBM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad2706de8934aaccb57d8a9365f7736ee10485b4', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/vWjOGRQGSpeE0oaRYf9D_wyZjiymyDGD0U7d8dbdsBM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abd9600a91e93f47491b236026f0baf81d6428c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'BOib8n6hXuhu0X2C88LsF3kYObpDQizu9ytwgXuGtgY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb0gdp,True,,vishalpathikgupta,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb0gdp/how_to_start_a_career_in_data_science_in_2020/,all_ads,False,https://www.youtube.com/watch?v=Dn2DBnQxA2k,155203,1588268712.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to start a career in Data Science in 2020 | Jovian.ml', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Dn2DBnQxA2k?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'JovianML', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Dn2DBnQxA2k/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCmKaoNn0OvxVAe7f_8sXYNQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #16 | Rubik's Code,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gbefkj,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/ngk7y3icFzj_oCSl-PpaqNKy_pdavZrzbOqPULk6TYA.jpg,False,,[],{},link,,False,,1588350960.0,text,6,,,text,rubikscode.net,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?auto=webp&amp;s=4fc2388485a5af518b7b13c619d6265d8915a227', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49fb44c09ea1d75c80742ad6bc54ebb741b8d4ab', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95e50a45ca2321a0b7cccbb4cf731ad0895f6ed4', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2d03ca6ed8f5856de5e6729cd1459ab8519f17', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1d316de8feec9554b9ec14fe94a909d77ec3fef8', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=37c5479177c9da05a2d4ced8870362cd74d9749d', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/-JrRfwF5Ck94BEao_jkvPrWvsRrqUibiZxwLEeJoTHE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6cff518e8405de9615f19176b9480a79870ab1ec', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'wFrM5mQRhJrE5CJ6g9DmlDHLfiukllE9wG7IP7t6JdU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbefkj,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbefkj/this_week_in_ai_issue_16_rubiks_code/,all_ads,False,https://rubikscode.net/2020/05/01/this-week-in-ai-issue-16-2/,155203,1588322160.0,0,,False,,,,
,learnmachinelearning,"Numerically what actually is the bottleneck layer (z) in variational autoencoder? Is it just a latent variable or some sort of distribution? And in case of reparameterization trick, the mean (Î¼) and variance (Ïƒ) is used to calculate z with the equation : z = Î¼ + Ïƒ â€¢ N(0,1). Here , the mean and variance of what thing is being used?",t2_4w7oamkl,False,,0,False,Can somebody briefly explain what is meant by latent loss in variational autoencoder?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbe84u,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588349750.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Numerically what actually is the bottleneck layer (z) in variational autoencoder? Is it just a latent variable or some sort of distribution? And in case of reparameterization trick, the mean (Î¼) and variance (Ïƒ) is used to calculate z with the equation : z = Î¼ + Ïƒ â€¢ N(0,1). Here , the mean and variance of what thing is being used?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gbe84u,True,,HTKasd,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbe84u/can_somebody_briefly_explain_what_is_meant_by/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbe84u/can_somebody_briefly_explain_what_is_meant_by/,155203,1588320950.0,0,,False,,,,
,learnmachinelearning,"After performing cross validation, do I use the model that was trained on the train data  and test this model on the validation set?",t2_55qnkwk,False,,0,False,Cross validation,[],r/learnmachinelearning,False,6,,0,,False,t3_gbbd17,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588335306.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After performing cross validation, do I use the model that was trained on the train data  and test this model on the validation set?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbbd17,True,,kbabqiqja,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbbd17/cross_validation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbbd17/cross_validation/,155203,1588306506.0,0,,False,,,,
,learnmachinelearning,"Hi everyone!

So I took it upon myself to start a machine learning project and I decided to try and build a basic content based music recommendation system using Python.

So basically what I do is get one of my playlists using the Spotify API and extract the acoustic features of every song from that playlist into a csv file.   
So what I have in the csv file is the id and name of the song, the artist and album name and a bunch of acoustic features like acousticness, danceability, energy, key, etc. (you can see all of the features [here!](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)).

Also I found a dataset on the internet that consists of 340 000 songs and their acoustic features and they are exactly in the same format as my playlist data as they are also gathered using the Spotify API. I thought that this would be a good dataset from where to get the the song recommendations.

So basically what I want to do is to get maybe 30-40 songs from the 340 000 song dataset that have similar acoustic features as the songs on my own playlist dataset (350 songs are in my playlist dataset). However I have no idea from where I should start from and how to approach this problem. I'm guessing I should somehow use classification models like Logistic regression, KNN, Random forest or maybe something more complex that I don't know about yet.

Any suggestions, tips and hints are welcome!

Thanks in advance!",t2_biwlg,False,,0,False,Building a recommendation system,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb25pe,False,light,0.79,,public,10,0,{},,,False,[],,False,False,,{},HELP,False,10,,False,self,False,,[],{},self,,True,,1588302852.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;So I took it upon myself to start a machine learning project and I decided to try and build a basic content based music recommendation system using Python.&lt;/p&gt;

&lt;p&gt;So basically what I do is get one of my playlists using the Spotify API and extract the acoustic features of every song from that playlist into a csv file.&lt;br/&gt;
So what I have in the csv file is the id and name of the song, the artist and album name and a bunch of acoustic features like acousticness, danceability, energy, key, etc. (you can see all of the features &lt;a href=""https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/""&gt;here!&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Also I found a dataset on the internet that consists of 340 000 songs and their acoustic features and they are exactly in the same format as my playlist data as they are also gathered using the Spotify API. I thought that this would be a good dataset from where to get the the song recommendations.&lt;/p&gt;

&lt;p&gt;So basically what I want to do is to get maybe 30-40 songs from the 340 000 song dataset that have similar acoustic features as the songs on my own playlist dataset (350 songs are in my playlist dataset). However I have no idea from where I should start from and how to approach this problem. I&amp;#39;m guessing I should somehow use classification models like Logistic regression, KNN, Random forest or maybe something more complex that I don&amp;#39;t know about yet.&lt;/p&gt;

&lt;p&gt;Any suggestions, tips and hints are welcome!&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?auto=webp&amp;s=2ab4a117cdfd4fb7dd714be60e49d26ee05a36eb', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35926c1ff84af55925921fc672fc2cdc11786e90', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=400490238a4accfdf9058481b84406cc0d7313fb', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c2abc11cd8377ad69ac176c42dd3b56f2ede9a3', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9312250857953fbcb70f1b283c6868d2a5ca965', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=42f184e6829237c6ba2925253c8444a48afd3933', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/BShrsgs7G-l3wrDuiwKus10HtaEHneXrjyDeHQMNofY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9dbe36647e26b3c417fd2da94f334396495c5f27', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'pfFz7Phif4Sq5o9ssUtrFBxiaVCaDWr24rt3HhOyDKU'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gb25pe,True,,UkmDamn,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb25pe/building_a_recommendation_system/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb25pe/building_a_recommendation_system/,155203,1588274052.0,0,,False,,,,
,learnmachinelearning,"Hey!, I was a practice with LinearRegression Algorithm ""Multiple Linear Regression""  and that's the result using MSE metric: 0.00031507176851662625 is that good result or it just overfitting ??

https://preview.redd.it/5jcpdq7sx3w41.jpg?width=1120&amp;format=pjpg&amp;auto=webp&amp;s=233affe4fedc0a92d812c5f242a31445cdc01bf7",t2_1xmkpo1v,False,,0,False,I need help to better understand in Linear Regression,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gbdu6s,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/FTsEffray3r79pOHKOaULfVnCIqNC-41cTTQJelv-Os.jpg,False,,[],{},,,True,,1588347627.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey!, I was a practice with LinearRegression Algorithm &amp;quot;Multiple Linear Regression&amp;quot;  and that&amp;#39;s the result using MSE metric: 0.00031507176851662625 is that good result or it just overfitting ??&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/5jcpdq7sx3w41.jpg?width=1120&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=233affe4fedc0a92d812c5f242a31445cdc01bf7""&gt;https://preview.redd.it/5jcpdq7sx3w41.jpg?width=1120&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=233affe4fedc0a92d812c5f242a31445cdc01bf7&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbdu6s,True,,meliodes00,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbdu6s/i_need_help_to_better_understand_in_linear/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbdu6s/i_need_help_to_better_understand_in_linear/,155203,1588318827.0,0,,False,,,"{'5jcpdq7sx3w41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14be3619c3fb51e6c1966a420bcbfe50f66cb285'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2de6e7a8bdb1e01689ef2bd656810224b1efad7'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a61d3245acea6462d123f9d3af221f69acca3a77'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6661b4dbfd1055e5f75971698ed6b412be280cd3'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f81e56bcf488602f65ba655d745b944858b38ca7'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f7e4c6c7e607861ed72922b296e5eb8e1e494746'}], 's': {'y': 630, 'x': 1120, 'u': 'https://preview.redd.it/5jcpdq7sx3w41.jpg?width=1120&amp;format=pjpg&amp;auto=webp&amp;s=233affe4fedc0a92d812c5f242a31445cdc01bf7'}, 'id': '5jcpdq7sx3w41'}}",
,learnmachinelearning,,t2_mmnaisi,False,,0,False,Introduction to Support Vector Machines in Machine Learning using Sklearn,[],r/learnmachinelearning,False,6,,0,111.0,False,t3_gb5wnz,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/0_5z2FuZFUE_UT1NzyybcrsxY98THFV1QL9tD35ffYM.jpg,False,,[],{},link,,False,,1588314775.0,text,6,,,text,ranvir.xyz,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QSyp5gfokEq0UkcK0RNITGfEVCQ5b19YBtu-mCoqUDQ.jpg?auto=webp&amp;s=edf0ba17a06047c0f3d234788a2f2f3a34059b8d', 'width': 806, 'height': 644}, 'resolutions': [{'url': 'https://external-preview.redd.it/QSyp5gfokEq0UkcK0RNITGfEVCQ5b19YBtu-mCoqUDQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a2abf152b29ea45a9bd0904f60544363b4d8ff1', 'width': 108, 'height': 86}, {'url': 'https://external-preview.redd.it/QSyp5gfokEq0UkcK0RNITGfEVCQ5b19YBtu-mCoqUDQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3e467eb8c11a5cc3d11186a72c20a1bb256a14f', 'width': 216, 'height': 172}, {'url': 'https://external-preview.redd.it/QSyp5gfokEq0UkcK0RNITGfEVCQ5b19YBtu-mCoqUDQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7539c6aa936e1ade2ad17024961e5889af903880', 'width': 320, 'height': 255}, {'url': 'https://external-preview.redd.it/QSyp5gfokEq0UkcK0RNITGfEVCQ5b19YBtu-mCoqUDQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=67d8c14d02e6983607f377a36cbc3e36b5f9686e', 'width': 640, 'height': 511}], 'variants': {}, 'id': 'm8kluehH6LonPRD9q2D0h6Sb3zV9urQ-UsAx9IlEReY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb5wnz,True,,rangerranvir,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb5wnz/introduction_to_support_vector_machines_in/,all_ads,False,https://ranvir.xyz/blog/svm-support-vector-machines-in-machine-learning/,155203,1588285975.0,0,,False,,,,
,learnmachinelearning,"Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.",t2_6l4z3,False,,0,False,Weekly Show-off!,[],r/learnmachinelearning,False,6,,0,,False,t3_gbdg7v,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588345495.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gbdg7v,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbdg7v/weekly_showoff/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbdg7v/weekly_showoff/,155203,1588316695.0,0,,False,,,,
,learnmachinelearning,"3 months ago, I created and posted videos from my Python machine learning course on the learnProgramming subreddit. So far they have been received very well! I have also posted some of the videos on this Subreddit but would like to do a combined post as weekend is approaching. So if someone is going to dig into Machine Learning here are some initial resources. For people who are already well digged into Machine Learning please share your ressources in the comment section below, for me and others to benefit from.

* **Part 1 - Machine Learning For Beginners - Basics**

[https://youtu.be/E3l\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)

* **Part 2 - MI environment**

[https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)

* **Part 3 - Python Decision Tree (Theory)**

[https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)

* **Part 4 - Python Decision Tree (Coding)**

[https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)

* **Part 5 - Python Decision Tree (Graphiviz)**

[https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)

* **Part 6 - Knn(Friend Recommender)**

[https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)

* **Part 7- 5-Fold Cross Validation**

[https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)",t2_3m3wzbiw,False,,0,False,Machine Learning Weekend - Sharing is Caring (Post your resources),[],r/learnmachinelearning,False,6,,0,,False,t3_gbd9ne,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588344524.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;3 months ago, I created and posted videos from my Python machine learning course on the learnProgramming subreddit. So far they have been received very well! I have also posted some of the videos on this Subreddit but would like to do a combined post as weekend is approaching. So if someone is going to dig into Machine Learning here are some initial resources. For people who are already well digged into Machine Learning please share your ressources in the comment section below, for me and others to benefit from.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 1 - Machine Learning For Beginners - Basics&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/E3l_aeGjkeI""&gt;https://youtu.be/E3l_aeGjkeI&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 2 - MI environment&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/HqyrqxyDwPU""&gt;https://youtu.be/HqyrqxyDwPU&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 3 - Python Decision Tree (Theory)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/8isUCINSmys""&gt;https://youtu.be/8isUCINSmys&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 4 - Python Decision Tree (Coding)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/24mxQzd3EsU""&gt;https://youtu.be/24mxQzd3EsU&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 5 - Python Decision Tree (Graphiviz)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/aVEfKRfWjHc""&gt;https://youtu.be/aVEfKRfWjHc&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 6 - Knn(Friend Recommender)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/LK0zgA6Mr6k""&gt;https://youtu.be/LK0zgA6Mr6k&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Part 7- 5-Fold Cross Validation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://youtu.be/Zx5cz8pXnOM""&gt;https://youtu.be/Zx5cz8pXnOM&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_Qyh0jrvMFPuXpRu2JnFunI9_eOslYJsr9C8lmXgdK8.jpg?auto=webp&amp;s=2fb25da6d41bc434b2c6409af674cb67c2bf7963', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/_Qyh0jrvMFPuXpRu2JnFunI9_eOslYJsr9C8lmXgdK8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d24cfbd3b26832d88e5e856a8284c0858179f0f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/_Qyh0jrvMFPuXpRu2JnFunI9_eOslYJsr9C8lmXgdK8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf363eaa3092b9328fef7aa1e379d62e3a3caa8c', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/_Qyh0jrvMFPuXpRu2JnFunI9_eOslYJsr9C8lmXgdK8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d9e9b5864c2888409cca6427278de4d9877fd50', 'width': 320, 'height': 240}], 'variants': {}, 'id': '7F-j1Cmd2rMHRRilQKcX5NC4jHyI0uHH1FoiArrnEzI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbd9ne,True,,lukescriptwalker,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbd9ne/machine_learning_weekend_sharing_is_caring_post/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbd9ne/machine_learning_weekend_sharing_is_caring_post/,155203,1588315724.0,0,,False,,,,
,learnmachinelearning,"The title is pretty much the question. I'm wondering if I can use the BERT model without pretraining it.

I'm a bit confused because as far as I'm aware the entire framework of the BERT model itself is that it adopts a two-stage approach of pretraining and fine-tuning. If the first stage of pretraining is taken out, would that still be a BERT architecture?",t2_m8kccne,False,,0,False,Is it possible to use BERT without pretraining it?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbcz9s,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588342994.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The title is pretty much the question. I&amp;#39;m wondering if I can use the BERT model without pretraining it.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a bit confused because as far as I&amp;#39;m aware the entire framework of the BERT model itself is that it adopts a two-stage approach of pretraining and fine-tuning. If the first stage of pretraining is taken out, would that still be a BERT architecture?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gbcz9s,True,,Seankala,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbcz9s/is_it_possible_to_use_bert_without_pretraining_it/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbcz9s/is_it_possible_to_use_bert_without_pretraining_it/,155203,1588314194.0,0,,False,,,,
,learnmachinelearning,,t2_6akogtwr,False,,0,False,I've scraped around 800 million characters worth of comments from the top 50 subreddits,[],r/learnmachinelearning,False,6,,0,,False,t3_gaunbt,False,dark,0.89,,public,26,0,{},,,False,[],,False,False,,{},,False,26,,False,default,False,,[],{},link,,False,,1588278175.0,text,6,,,text,self.datasets,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?auto=webp&amp;s=763bbdcdd1bab7141966bd41927af8e8703dce0f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4f6aba2724ea57b25949152be8b9c85ab9c1636', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2a0cfe5fb6362d5d9a67719d2f72f007fd6bb7d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9adccc192c5ba0845e2db37f581c623cdabc5120', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'aea2ShDiYSvdhmoo1eJs8CZpeV1VC8CTcVqd1fbW4FE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaunbt,True,,PapaCraken,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaunbt/ive_scraped_around_800_million_characters_worth/,all_ads,False,/r/datasets/comments/gaukz5/ive_scraped_around_800_million_characters_worth/,155203,1588249375.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'datasets', 'selftext': ""Hi,\n\n&amp;#x200B;\n\nI've been working on a machine learning side project amidst the quarantine, and for that, I have scraped around the 1000 top posts from the top 50 most subscribed subreddits, and saved 100 comments of each into a data set.\n\nI ended up going with different data for my project, but decided that I might as well share it.\n\n[You can find the dataset here](https://github.com/CrakenHUN/RedditCommentsDataset)\n\n&amp;#x200B;\n\n[And in case you want to toy around with the scraper scripts I used to gather the comments, here they are](https://github.com/CrakenHUN/RedditScraperScripts)  (this includes two scripts, one optimized to save comments from one subreddit in a really user friendly way, and one of them to do so for a list of subreddits. All necessary data for setting them up is included in the repository)"", 'author_fullname': 't2_6akogtwr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""I've scraped around 800 million characters worth of comments from the top 50 subreddits"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datasets', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'dataset', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gaukz5', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 113, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'dataset', 'can_mod_post': False, 'score': 113, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588277911.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.datasets', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a machine learning side project amidst the quarantine, and for that, I have scraped around the 1000 top posts from the top 50 most subscribed subreddits, and saved 100 comments of each into a data set.&lt;/p&gt;\n\n&lt;p&gt;I ended up going with different data for my project, but decided that I might as well share it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/CrakenHUN/RedditCommentsDataset""&gt;You can find the dataset here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://github.com/CrakenHUN/RedditScraperScripts""&gt;And in case you want to toy around with the scraper scripts I used to gather the comments, here they are&lt;/a&gt;  (this includes two scripts, one optimized to save comments from one subreddit in a really user friendly way, and one of them to do so for a list of subreddits. All necessary data for setting them up is included in the repository)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?auto=webp&amp;s=763bbdcdd1bab7141966bd41927af8e8703dce0f', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d4f6aba2724ea57b25949152be8b9c85ab9c1636', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2a0cfe5fb6362d5d9a67719d2f72f007fd6bb7d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/CA2bf3UGOH_lWMgKp7pGZcC0dJHvwrDy5NoYK6zo3gc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9adccc192c5ba0845e2db37f581c623cdabc5120', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'aea2ShDiYSvdhmoo1eJs8CZpeV1VC8CTcVqd1fbW4FE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'f074feb2-5bcd-11e6-8c1d-0e0220cd4035', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r97t', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gaukz5', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'PapaCraken', 'discussion_type': None, 'num_comments': 41, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datasets/comments/gaukz5/ive_scraped_around_800_million_characters_worth/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/datasets/comments/gaukz5/ive_scraped_around_800_million_characters_worth/', 'subreddit_subscribers': 111359, 'created_utc': 1588249111.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_gaukz5,,
,learnmachinelearning,,t2_72x0kqf,False,,0,False,A Complete Guide to Time Series Modelling and Forecasting with ARIMA,[],r/learnmachinelearning,False,6,,0,55.0,False,t3_gb0bv8,False,dark,0.83,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/QFW-qQ_i4YxB3txvX4deA7lLMjrM7Ye7QMr4LvGpYaE.jpg,False,,[],{},link,,False,,1588297134.0,text,6,,,text,kanoki.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/L7f-kbDEsY3-XAST7Yfa8QUq0wCntksPvfE8vXNN39I.jpg?auto=webp&amp;s=93265a69a2ff3b0401708885a9a57f9cc3614042', 'width': 733, 'height': 290}, 'resolutions': [{'url': 'https://external-preview.redd.it/L7f-kbDEsY3-XAST7Yfa8QUq0wCntksPvfE8vXNN39I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2b894e4aebc94e87d2cd3289e6a83dd5f669738b', 'width': 108, 'height': 42}, {'url': 'https://external-preview.redd.it/L7f-kbDEsY3-XAST7Yfa8QUq0wCntksPvfE8vXNN39I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01048cec83809ec8864a683aad35cd78fe0a36a4', 'width': 216, 'height': 85}, {'url': 'https://external-preview.redd.it/L7f-kbDEsY3-XAST7Yfa8QUq0wCntksPvfE8vXNN39I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e3f9ec75b6ab42209f8f00b482ccb33ffa57c30', 'width': 320, 'height': 126}, {'url': 'https://external-preview.redd.it/L7f-kbDEsY3-XAST7Yfa8QUq0wCntksPvfE8vXNN39I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4deaddcfbfb0a34f83a589ff1677c987ce764a0d', 'width': 640, 'height': 253}], 'variants': {}, 'id': 'SM1R5yCcWvKu4zm4aWMmi3-GBQlBaiXPLUIvnOiMgME'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb0bv8,True,,caroleber,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb0bv8/a_complete_guide_to_time_series_modelling_and/,all_ads,False,https://kanoki.org/2020/04/30/time-series-analysis-and-forecasting-with-arima-python/,155203,1588268334.0,0,,False,,,,
,learnmachinelearning,"Currently, I'm practicing deep learning on Keras and Tensorflow and interested in learning PyTorch and Fastai but not sure which one to learn first.",t2_1yrzrqn5,False,,0,False,What to learn first Fastai or PyTorch ?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbc7bj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588339132.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently, I&amp;#39;m practicing deep learning on Keras and Tensorflow and interested in learning PyTorch and Fastai but not sure which one to learn first.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gbc7bj,True,,Shushrut,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbc7bj/what_to_learn_first_fastai_or_pytorch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbc7bj/what_to_learn_first_fastai_or_pytorch/,155203,1588310332.0,0,,False,,,,
,learnmachinelearning,,t2_28nup7z5,False,,0,False,How to get started with Github (Ubuntu),"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbc6li,False,light,0.33,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,default,False,,[],{},,,False,,1588339034.0,richtext,6,,,text,link.medium.com,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gbc6li,True,,internet----explorer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbc6li/how_to_get_started_with_github_ubuntu/,all_ads,False,https://link.medium.com/5YO9aCCN75,155203,1588310234.0,0,,False,,,,
,learnmachinelearning,"Hi!

I'd appreciate a little help in finding a more efficient way of dummy encoding all of the columns in my data frame (using Python). The example below currently works for dummy encoding my columns if I name each column individually.

`pd.get_dummies(df, columns=['1', '2', '3', '4'], drop_first=True, prefix = 'Pos')`

Does anybody have any guidance as to how I can apply this to all the columns in my data frame (except column 0), so column 1 to 100 for example?  

I think in R I'd try to do something like ""columns = \[1:100\]"" to get all columns between 1 and 100, I'm basically trying to find out if there is a way of doing something like this in python? 

Thank you!",t2_47imqrkz,False,,0,False,How to iterate through and dummy encode all columns in a data frame?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb8725,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588322516.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;I&amp;#39;d appreciate a little help in finding a more efficient way of dummy encoding all of the columns in my data frame (using Python). The example below currently works for dummy encoding my columns if I name each column individually.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pd.get_dummies(df, columns=[&amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;, &amp;#39;3&amp;#39;, &amp;#39;4&amp;#39;], drop_first=True, prefix = &amp;#39;Pos&amp;#39;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Does anybody have any guidance as to how I can apply this to all the columns in my data frame (except column 0), so column 1 to 100 for example?  &lt;/p&gt;

&lt;p&gt;I think in R I&amp;#39;d try to do something like &amp;quot;columns = [1:100]&amp;quot; to get all columns between 1 and 100, I&amp;#39;m basically trying to find out if there is a way of doing something like this in python? &lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gb8725,True,,CGBio,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb8725/how_to_iterate_through_and_dummy_encode_all/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb8725/how_to_iterate_through_and_dummy_encode_all/,155203,1588293716.0,1,,False,,,,
,learnmachinelearning,,t2_4iowanqo,False,,0,False,"Any chance for me to work in machine learning with absolutely no background in programming, mathematics, statistics, or anything stem related?",[],r/learnmachinelearning,False,6,,0,,False,t3_gbbffj,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588335585.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbbffj,True,,innervestibule,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbbffj/any_chance_for_me_to_work_in_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbbffj/any_chance_for_me_to_work_in_machine_learning/,155203,1588306785.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: Robust 3D Self-portraits in Seconds,[],r/learnmachinelearning,False,6,,0,88.0,False,t3_gbbcve,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/gQ0KqnkeOkgfisynU6pcSO8n-6GE81gCZNuV4Q3p2Os.jpg,False,,[],{},link,,False,,1588335283.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?auto=webp&amp;s=088f66eef5a14e7c9e8f03bc3417d59e3cbaff0f', 'width': 658, 'height': 418}, 'resolutions': [{'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43555858a28daff57b43910b30dbcaf29a5864ac', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4feaf7644a7967d0679094bd5eb8812ce2c9a7a5', 'width': 216, 'height': 137}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7011b98e68f4b4e68dea091bc856e3ca72d7a433', 'width': 320, 'height': 203}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe7872300970e6112c73dc7c3c128384556e5037', 'width': 640, 'height': 406}], 'variants': {}, 'id': 'PC26OZC29lZWn_zTuE-KxCwjORWGCB5JN3dlNzonzi0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gbbcve,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbbcve/from_cvpr_20_robust_3d_selfportraits_in_seconds/,all_ads,False,/r/LatestInML/comments/gbb1mn/from_cvpr_20_robust_3d_selfportraits_in_seconds/,155203,1588306483.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""From CVPR '20: Robust 3D Self-portraits in Seconds\n\nProject and code/request: [click here](https://www.catalyzex.com/paper/arxiv:2004.02460)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/97ixlq7ct2w41.png?width=1750&amp;format=png&amp;auto=webp&amp;s=1696d65f7e2a8fb75cb45e122721ea42b2cfc734\n\nThe results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods."", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: Robust 3D Self-portraits in Seconds"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 88, 'hide_score': False, 'media_metadata': {'97ixlq7ct2w41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 68, 'x': 108, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fa8bf4b80ec4bfeac75f8ea90042f3a8d18361cb'}, {'y': 136, 'x': 216, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=60f4a5a7adcb32987c8e2ce74c803cd979006a3f'}, {'y': 202, 'x': 320, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c7e750da5c8696a1e0690429d0c7ad74b9fdf2e'}, {'y': 405, 'x': 640, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2379be71cac2c41d0f37d763471ffa4a26d4dd'}, {'y': 607, 'x': 960, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f034937cf5ef0d0d7da65bd2aea7277134f7929'}, {'y': 683, 'x': 1080, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19f3cb9bde7ea3c34f0d28dadc237a8e2e268f8a'}], 's': {'y': 1108, 'x': 1750, 'u': 'https://preview.redd.it/97ixlq7ct2w41.png?width=1750&amp;format=png&amp;auto=webp&amp;s=1696d65f7e2a8fb75cb45e122721ea42b2cfc734'}, 'id': '97ixlq7ct2w41'}}, 'name': 't3_gbb1mn', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/gQ0KqnkeOkgfisynU6pcSO8n-6GE81gCZNuV4Q3p2Os.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588334015.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From CVPR &amp;#39;20: Robust 3D Self-portraits in Seconds&lt;/p&gt;\n\n&lt;p&gt;Project and code/request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.02460""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/97ixlq7ct2w41.png?width=1750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1696d65f7e2a8fb75cb45e122721ea42b2cfc734""&gt;https://preview.redd.it/97ixlq7ct2w41.png?width=1750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1696d65f7e2a8fb75cb45e122721ea42b2cfc734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?auto=webp&amp;s=088f66eef5a14e7c9e8f03bc3417d59e3cbaff0f', 'width': 658, 'height': 418}, 'resolutions': [{'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=43555858a28daff57b43910b30dbcaf29a5864ac', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4feaf7644a7967d0679094bd5eb8812ce2c9a7a5', 'width': 216, 'height': 137}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7011b98e68f4b4e68dea091bc856e3ca72d7a433', 'width': 320, 'height': 203}, {'url': 'https://external-preview.redd.it/kn8NIskseyB3eQVhfFWKhFILta98_Rb_WzQpZ0xuL6I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe7872300970e6112c73dc7c3c128384556e5037', 'width': 640, 'height': 406}], 'variants': {}, 'id': 'PC26OZC29lZWn_zTuE-KxCwjORWGCB5JN3dlNzonzi0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gbb1mn', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gbb1mn/from_cvpr_20_robust_3d_selfportraits_in_seconds/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gbb1mn/from_cvpr_20_robust_3d_selfportraits_in_seconds/', 'subreddit_subscribers': 3386, 'created_utc': 1588305215.0, 'num_crossposts': 11, 'media': None, 'is_video': False}]",t3_gbb1mn,,
,learnmachinelearning,"Hello, thanks for taking the time to read this.

I'm putting together a Keras regression model from NBA data. Validation loss decreases initially then turns upward indicating overfitting (see pic). I've been tuning params for about two weeks and now it seems I can't get the loss any lower.

My model currently has two hidden layers.

Here is what I have tried so far to mitigate overfitting:

* reduce learning rate - helped
* dropout layers - helped
* batch normalization before first dropout layer - made things much worse
* l2 regularization - didn't change the validation minimum much
* l1 regularization - made things worse

Is there anything else that I can try or do you think I have exhausted all the tricks and should accept that there is not enough correlation in the data?

Side question: why is the training loss so noisy? Is there something to be done here?

I kept this brief but am happy to provide more specifics.

Thanks!

https://preview.redd.it/raobe05r31w41.png?width=800&amp;format=png&amp;auto=webp&amp;s=90bd7e9a59321114783d747cb50ccbec66264bde",t2_5w5epazw,False,,0,False,Stuck in an intense battle with overfitting,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gb5fik,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/6NFz4Lbe7DXyRBFk9gdAvTvzX26byXDIOF6rFoTn9Zc.jpg,1588284701.0,,[],{},,,True,,1588313236.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, thanks for taking the time to read this.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m putting together a Keras regression model from NBA data. Validation loss decreases initially then turns upward indicating overfitting (see pic). I&amp;#39;ve been tuning params for about two weeks and now it seems I can&amp;#39;t get the loss any lower.&lt;/p&gt;

&lt;p&gt;My model currently has two hidden layers.&lt;/p&gt;

&lt;p&gt;Here is what I have tried so far to mitigate overfitting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;reduce learning rate - helped&lt;/li&gt;
&lt;li&gt;dropout layers - helped&lt;/li&gt;
&lt;li&gt;batch normalization before first dropout layer - made things much worse&lt;/li&gt;
&lt;li&gt;l2 regularization - didn&amp;#39;t change the validation minimum much&lt;/li&gt;
&lt;li&gt;l1 regularization - made things worse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is there anything else that I can try or do you think I have exhausted all the tricks and should accept that there is not enough correlation in the data?&lt;/p&gt;

&lt;p&gt;Side question: why is the training loss so noisy? Is there something to be done here?&lt;/p&gt;

&lt;p&gt;I kept this brief but am happy to provide more specifics.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/raobe05r31w41.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90bd7e9a59321114783d747cb50ccbec66264bde""&gt;https://preview.redd.it/raobe05r31w41.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90bd7e9a59321114783d747cb50ccbec66264bde&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb5fik,True,,lilsniffles206,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb5fik/stuck_in_an_intense_battle_with_overfitting/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb5fik/stuck_in_an_intense_battle_with_overfitting/,155203,1588284436.0,0,,False,,,"{'raobe05r31w41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/raobe05r31w41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4beb1e17b86c364996f78698cfeb329ba1f34585'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/raobe05r31w41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c70f669c3b5b86c6aff3007605804d7b3735e21f'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/raobe05r31w41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec5df023802e329b4c81b5f9f5c6471fab3f5cb2'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/raobe05r31w41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45328e94d7f31458df922a0087d036217ae6c64b'}], 's': {'y': 600, 'x': 800, 'u': 'https://preview.redd.it/raobe05r31w41.png?width=800&amp;format=png&amp;auto=webp&amp;s=90bd7e9a59321114783d747cb50ccbec66264bde'}, 'id': 'raobe05r31w41'}}",
,learnmachinelearning,,t2_173c6i,False,,0,False,Is it relavent to learn matlab??,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gbav4s,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588333270.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gbav4s,True,,tech_HACKS,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gbav4s/is_it_relavent_to_learn_matlab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gbav4s/is_it_relavent_to_learn_matlab/,155203,1588304470.0,0,,False,,,,
,learnmachinelearning,"Hey there, how are you? I'm stuck at week 4 on Andrew ng course, precisely on the code part. I just don't understand how to code the one Vs all part, and I don't understand what X(matrix) contains. I've understand the intuition behind the multiclass classification, but I just don't understand what is going on here, what he is asking to do. Thank you for your time, appreciate it",t2_51w9vxrk,False,,0,False,Andrew ng one Vs all classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gb4ut1,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588311390.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, how are you? I&amp;#39;m stuck at week 4 on Andrew ng course, precisely on the code part. I just don&amp;#39;t understand how to code the one Vs all part, and I don&amp;#39;t understand what X(matrix) contains. I&amp;#39;ve understand the intuition behind the multiclass classification, but I just don&amp;#39;t understand what is going on here, what he is asking to do. Thank you for your time, appreciate it&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb4ut1,True,,Z4glis,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb4ut1/andrew_ng_one_vs_all_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb4ut1/andrew_ng_one_vs_all_classification/,155203,1588282590.0,0,,False,,,,
,learnmachinelearning,"Hi. I want to train a BERT model on a text summarization task without the pre-training.

When I say ""normal"" in the title, I mean a framework without the pre-training portion that is one of BERT's selling points.

Is this pointless? And is this even possible? I've been reading blog posts on how to train BERT from scratch, and all of them go through a pre-training step.",t2_m8kccne,False,,0,False,"Is it pointless (or even possible) to train BERT from scratch like a ""normal"" model?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb9rjq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588328714.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I want to train a BERT model on a text summarization task without the pre-training.&lt;/p&gt;

&lt;p&gt;When I say &amp;quot;normal&amp;quot; in the title, I mean a framework without the pre-training portion that is one of BERT&amp;#39;s selling points.&lt;/p&gt;

&lt;p&gt;Is this pointless? And is this even possible? I&amp;#39;ve been reading blog posts on how to train BERT from scratch, and all of them go through a pre-training step.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gb9rjq,True,,Seankala,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb9rjq/is_it_pointless_or_even_possible_to_train_bert/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb9rjq/is_it_pointless_or_even_possible_to_train_bert/,155203,1588299914.0,0,,False,,,,
,learnmachinelearning,"i like both fields but i dont know what to chose or start with and can i learn both of them in the same time or no ?

and i have the roadmap for Artificial Intelligence  only",t2_2psmfnmi,False,,0,False,I'm Lost Should I Learn Artificial Intelligence or Cyber Security ?,[],r/learnmachinelearning,False,6,,0,,False,t3_gb9kb3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588327875.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i like both fields but i dont know what to chose or start with and can i learn both of them in the same time or no ?&lt;/p&gt;

&lt;p&gt;and i have the roadmap for Artificial Intelligence  only&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb9kb3,True,,MarcoAcrono,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb9kb3/im_lost_should_i_learn_artificial_intelligence_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb9kb3/im_lost_should_i_learn_artificial_intelligence_or/,155203,1588299075.0,0,,False,,,,
,learnmachinelearning,Generally do all scikit-learn packages and the parallelization package joblib work in Google Colab?,t2_zmqho4m,False,,0,False,Do all scikit-learn packages and the parallelization package joblib work in Google Colab?,[],r/learnmachinelearning,False,6,,0,,False,t3_gb8tda,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588324922.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Generally do all scikit-learn packages and the parallelization package joblib work in Google Colab?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb8tda,True,,leockl,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb8tda/do_all_scikitlearn_packages_and_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb8tda/do_all_scikitlearn_packages_and_the/,155203,1588296122.0,0,,False,,,,
,learnmachinelearning,"Does anyone have experience using the [Stable Baselines library](https://stable-baselines.readthedocs.io/) with [Cleverhans](https://github.com/tensorflow/cleverhans)?

I'm trying to do research on performing adversarial attacks on DQN. Stable Baselines comes with a pretrained DQN on the Atari environment, which I was hoping to utilize. However, I cannot figure out how to get Cleverhans working with it.

From what I can tell, Cleverhans requires me to write a Model object that implements the fprop method, but I'm not sure how to extract the logit layer from the Stable Baselines DQN, required in fprop if I wanted to conduct attacks like FGSM or C&amp;W.

I've already looked into the [rl-attack](https://github.com/behzadanksu/rl-attack/) library, but this one doesn't work as well, presumably because rl-attack requires OpenAI-baselines, not Stable Baselines

I'm new to Tensorflow development would greatly appreciate any sort of help. Thank you in advance!",t2_5qo06mn0,False,,0,False,How to use Stable Baselines with Cleverhans? Or implementing any sort of adversarial attacks on DRL,[],r/learnmachinelearning,False,6,,0,,False,t3_gb8r6d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588324685.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone have experience using the &lt;a href=""https://stable-baselines.readthedocs.io/""&gt;Stable Baselines library&lt;/a&gt; with &lt;a href=""https://github.com/tensorflow/cleverhans""&gt;Cleverhans&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to do research on performing adversarial attacks on DQN. Stable Baselines comes with a pretrained DQN on the Atari environment, which I was hoping to utilize. However, I cannot figure out how to get Cleverhans working with it.&lt;/p&gt;

&lt;p&gt;From what I can tell, Cleverhans requires me to write a Model object that implements the fprop method, but I&amp;#39;m not sure how to extract the logit layer from the Stable Baselines DQN, required in fprop if I wanted to conduct attacks like FGSM or C&amp;amp;W.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve already looked into the &lt;a href=""https://github.com/behzadanksu/rl-attack/""&gt;rl-attack&lt;/a&gt; library, but this one doesn&amp;#39;t work as well, presumably because rl-attack requires OpenAI-baselines, not Stable Baselines&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to Tensorflow development would greatly appreciate any sort of help. Thank you in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb8r6d,True,,dub_chaeng,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb8r6d/how_to_use_stable_baselines_with_cleverhans_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb8r6d/how_to_use_stable_baselines_with_cleverhans_or/,155203,1588295885.0,0,,False,,,,
,learnmachinelearning,"Hi everyone, sorry if this has already been asked, I canâ€™t find an answer to this anywhere: 

when relative importance is derived using C-tree  conditional permutation, how do you interpret the differences in relative importance numbers between different variables? 

For example, if one variable has an importance twice that of another, is it twice as important, 4x?... etc.

Any help is appreciated",t2_me5qmw0,False,,0,False,C-tree Relative Importance Interpretation?,[],r/learnmachinelearning,False,6,,0,,False,t3_gb8ewv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588323358.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, sorry if this has already been asked, I canâ€™t find an answer to this anywhere: &lt;/p&gt;

&lt;p&gt;when relative importance is derived using C-tree  conditional permutation, how do you interpret the differences in relative importance numbers between different variables? &lt;/p&gt;

&lt;p&gt;For example, if one variable has an importance twice that of another, is it twice as important, 4x?... etc.&lt;/p&gt;

&lt;p&gt;Any help is appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb8ewv,True,,Sturm_Liouville2,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb8ewv/ctree_relative_importance_interpretation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb8ewv/ctree_relative_importance_interpretation/,155203,1588294558.0,0,,False,,,,
,learnmachinelearning,"So this is my first machine learning project, I decided to make something simple so I'm having it classify different classes of cars in python. I found a good database for car models from stanford and with some Regex magic made 3 different categories of cars. I built it in tensorflow, with each category having about 1500 images and 2 hidden layers. For some reason, I cant get more than say 34.5% accuracy on my data after many epochs and a few hours of tuning. I don't know if the data I'm feeding it is too bad to actually load the data or if there is something wrong with my code. Changing the loss function from sparse categorical to KL Divergence had some positive results on one run (about 40%), but I think there is something wrong with my code. If someone could look it over and help me out with what might be going on would be greatly appreciated. Reducing the batch size helped and sometimes changing the number of neurons per layer helped. A link to the github repo I've shared everything on is [here](https://github.com/10macattack/CarsNeuralNetwork). Thank you!",t2_n3wq5,False,,0,False,My CNN is brain dead,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb3vbe,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},self,,True,,1588308265.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So this is my first machine learning project, I decided to make something simple so I&amp;#39;m having it classify different classes of cars in python. I found a good database for car models from stanford and with some Regex magic made 3 different categories of cars. I built it in tensorflow, with each category having about 1500 images and 2 hidden layers. For some reason, I cant get more than say 34.5% accuracy on my data after many epochs and a few hours of tuning. I don&amp;#39;t know if the data I&amp;#39;m feeding it is too bad to actually load the data or if there is something wrong with my code. Changing the loss function from sparse categorical to KL Divergence had some positive results on one run (about 40%), but I think there is something wrong with my code. If someone could look it over and help me out with what might be going on would be greatly appreciated. Reducing the batch size helped and sometimes changing the number of neurons per layer helped. A link to the github repo I&amp;#39;ve shared everything on is &lt;a href=""https://github.com/10macattack/CarsNeuralNetwork""&gt;here&lt;/a&gt;. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iDdLRRxib9RCfxw6H5LurDKLqYx4650v0mhJ5kVGBgM.jpg?auto=webp&amp;s=2c3c6ed2af067229ac8c588487da433511688a8c', 'width': 301, 'height': 301}, 'resolutions': [{'url': 'https://external-preview.redd.it/iDdLRRxib9RCfxw6H5LurDKLqYx4650v0mhJ5kVGBgM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb8a35494ad83d486708e43e7f785eeedb406661', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/iDdLRRxib9RCfxw6H5LurDKLqYx4650v0mhJ5kVGBgM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df184fa4d22c0ec70b70cd583629ef561bb02e3f', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'epbDCX2ZQGtwppzzbXhep54X_avz14pMy_S6704BEMg'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gb3vbe,True,,10macattack,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb3vbe/my_cnn_is_brain_dead/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb3vbe/my_cnn_is_brain_dead/,155203,1588279465.0,0,,False,,,,
,learnmachinelearning,"BitHacks is a global online hackathon for high schoolers and undergrads. Build AI-themed projects to compete to win prizes. Learn new skills through our workshops

Dates: June 22nd - 26th

For more information, go to our website: [bithacks.io](https://bithacks.io/)

**Registration open now on** [https://docs.google.com/forms/d/e/1FAIpQLSf6zO8X9t0Edg\_rvn9FnUE4WQAw5rWd4eEEKMOGvYWhhfDwGg/viewform](https://docs.google.com/forms/d/e/1FAIpQLSf6zO8X9t0Edg_rvn9FnUE4WQAw5rWd4eEEKMOGvYWhhfDwGg/viewform)

Instagram: bit.hacks",t2_6byhc6bj,False,,0,False,Online AI Hackathon | BitHacks,[],r/learnmachinelearning,False,6,,0,,False,t3_gb7eqj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588319773.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;BitHacks is a global online hackathon for high schoolers and undergrads. Build AI-themed projects to compete to win prizes. Learn new skills through our workshops&lt;/p&gt;

&lt;p&gt;Dates: June 22nd - 26th&lt;/p&gt;

&lt;p&gt;For more information, go to our website: &lt;a href=""https://bithacks.io/""&gt;bithacks.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Registration open now on&lt;/strong&gt; &lt;a href=""https://docs.google.com/forms/d/e/1FAIpQLSf6zO8X9t0Edg_rvn9FnUE4WQAw5rWd4eEEKMOGvYWhhfDwGg/viewform""&gt;https://docs.google.com/forms/d/e/1FAIpQLSf6zO8X9t0Edg_rvn9FnUE4WQAw5rWd4eEEKMOGvYWhhfDwGg/viewform&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Instagram: bit.hacks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZNjsTTyVAs35JcO_BuSdD0uwYCZWSKoLkxzzgsP8avA.jpg?auto=webp&amp;s=7dd9aa57e9900e2be78b6ce737b6ff5e32cc5fcf', 'width': 260, 'height': 355}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZNjsTTyVAs35JcO_BuSdD0uwYCZWSKoLkxzzgsP8avA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da9d3185a65c2bf9cd4725842ce15149e564b11d', 'width': 108, 'height': 147}, {'url': 'https://external-preview.redd.it/ZNjsTTyVAs35JcO_BuSdD0uwYCZWSKoLkxzzgsP8avA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9dbc3e258964cf4cd7bd38ba9b59f960f189f04e', 'width': 216, 'height': 294}], 'variants': {}, 'id': 'DIU2U8uEDbZBYZpYtrp4VyW14I_waU1PdWQfEsAzcG4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gb7eqj,True,,BitHacksAI,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb7eqj/online_ai_hackathon_bithacks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb7eqj/online_ai_hackathon_bithacks/,155203,1588290973.0,0,,False,,,,
,learnmachinelearning,,t2_4csu169w,False,,0,False,A guide to K-means clustering with implementation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gar4n4,False,light,0.88,,public,17,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IB9WfafBmjk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'K-means Clustering in Python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IB9WfafBmjk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IB9WfafBmjk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IB9WfafBmjk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gar4n4', 'height': 338}",Project,False,17,,False,https://b.thumbs.redditmedia.com/-alC23U5ilN9aL23cCaYh3PgXwSr-GeOkj7PKqBmaiY.jpg,False,,[],{},rich:video,,False,,1588260380.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FuusRPtgDE6E1X6yvUeXR4p6riL7pMdyjQd5qlcCzr0.jpg?auto=webp&amp;s=f3d114337006abe801851339fa6eda82606e0773', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/FuusRPtgDE6E1X6yvUeXR4p6riL7pMdyjQd5qlcCzr0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=33115790d7ed62dd624346c49be43e2485171b38', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/FuusRPtgDE6E1X6yvUeXR4p6riL7pMdyjQd5qlcCzr0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84d4c9ff7518c8c267fb278f7e00fb8a002312b9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/FuusRPtgDE6E1X6yvUeXR4p6riL7pMdyjQd5qlcCzr0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebc635592a0b102cff2de0fcb7531a79d6930e00', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KWO96e1MDyiaZ5_wYjpF_ga6U25fVCxbjEkjPVt2UtQ'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gar4n4,True,,nerdy_wits,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gar4n4/a_guide_to_kmeans_clustering_with_implementation/,all_ads,False,https://youtu.be/IB9WfafBmjk,155203,1588231580.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'K-means Clustering in Python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IB9WfafBmjk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IB9WfafBmjk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Say I want to tune the parameters of a machine learning model in order to reduce its error rate. For an example, let's use a [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine).

Some of the [several parameters it accepts](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) are floats, others are integers or booleans, yet others are strings.

Would  searching over the space of parameter values in order to reduce the  error rate be considered a discrete, or continuous optimization problem?",t2_1kjpn251,False,,0,False,Is Hyperparameter Optimization A Discrete Or Continuous Problem?,[],r/learnmachinelearning,False,6,,0,,False,t3_gavr9z,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1588282457.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Say I want to tune the parameters of a machine learning model in order to reduce its error rate. For an example, let&amp;#39;s use a &lt;a href=""https://en.wikipedia.org/wiki/Support-vector_machine""&gt;support vector machine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the &lt;a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html""&gt;several parameters it accepts&lt;/a&gt; are floats, others are integers or booleans, yet others are strings.&lt;/p&gt;

&lt;p&gt;Would  searching over the space of parameter values in order to reduce the  error rate be considered a discrete, or continuous optimization problem?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?auto=webp&amp;s=78838a66125bf7aa03489f673313601508eae5ad', 'width': 1200, 'height': 545}, 'resolutions': [{'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=648d3b34a2e5f5e6a07f9d274c6223f9e0e611a4', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3483de6dc00e14dac35b27240761b96b09da7496', 'width': 216, 'height': 98}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dba4ab2e8503305436507058815213788d1b53a', 'width': 320, 'height': 145}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=070b91604de8ab8a73274edcc276a23ed194d1e3', 'width': 640, 'height': 290}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c8f5a11df21ad558dbb7aa3d50aa0d478be7321b', 'width': 960, 'height': 436}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21959f74b60bfbf454e05276edb49e37d32f3ccb', 'width': 1080, 'height': 490}], 'variants': {}, 'id': 'othF90FfeIQC66ekAeqIYR_nvXmSV79Jb3-4hkfnoVg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gavr9z,True,,SubstantialRange,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gavr9z/is_hyperparameter_optimization_a_discrete_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gavr9z/is_hyperparameter_optimization_a_discrete_or/,155203,1588253657.0,0,,False,,,,
,learnmachinelearning,"*(You can find the original post at* [*https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65*](https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65)*)*

Time series forecasting is one of the most important topics in data science. Imagine that you are a business owner, you might want to predict different sorts of future events to make better decisions and optimize your resource allocation. Typical examples of time series forecasting use cases are retail sales forecasting, package shipment delay forecasting, energy demand forecasting, and financial forecasting. As you can see, forecasting is everywhere!

Given its ubiquitous nature and wide-ranging business applications, we have developed an open-source [forecasting repo](https://github.com/microsoft/forecasting?WT.mc_id=github-reddit-lazzeri) that puts world-class models and forecasting best practices in the hands of data scientists and industry experts â€” i.e., you!

# Forecasting Best Practices and Solution Accelerators

This repository provides examples of building forecasting solutions presented as Python Jupyter notebooks, R markdown files, and a library of utility functions. Our goal is to help you as a data scientist or machine learning engineer with varying levels of knowledge in forecasting:

* Learn best practices for the development of forecasting solutions in a variety of languages.
* Leverage recent advances in forecasting algorithms to build high-performance solutions and operationalize them.
* Accelerate the solution development process for real-world forecasting problems. With the provided examples, you will be able to significantly reduce the â€œtime to marketâ€ by simplifying the experience from defining the business problem to the development of solutions by orders of magnitude.

In the repository, you will find state-of-the-art (SOAT) forecasting models using traditional machine learning and deep learning approaches. Implementations of SOTA models in this release are centered around retail sales forecasting and are written in Python and R, two of the most popular programming languages in the forecasting domain. To enable high-throughput forecasting scenarios, we have included notebooks for forecasting multiple time series with distributed training techniques such as Ray in Python, the parallel package in R, and multi-threading in LightGBM. The following is a quick summary of forecasting models covered in this repository:

https://preview.redd.it/zur9qp28wyv41.png?width=699&amp;format=png&amp;auto=webp&amp;s=0a70a70e83339384764f3d7b3ab752eef287a9fa

The repository also comes with [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri) ([Azure ML](https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri)) themed notebooks and best practices recipes to accelerate the development of scalable, production-grade forecasting solutions on [Azure](https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri). You will find the following examples for forecasting with [Azure AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?WT.mc_id=medium-article-lazzeri) as well as tuning and deploying a forecasting model on [Azure](https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri).

&amp;#x200B;

https://preview.redd.it/yafcpzqawyv41.png?width=699&amp;format=png&amp;auto=webp&amp;s=17060a140963606405c2e31a0b204f1700ef482f

&amp;#x200B;

Developing an accurate forecasting solution can be a complex and time-consuming process. We hope the forecasting repo will help shorten your development cycle on [Azure.](https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri)

# To Learn More and Contribute

For more information, please visit: [https://github.com/microsoft/forecasting](https://github.com/microsoft/forecasting?WT.mc_id=github-reddit-lazzeri)

Contributions from open-source community are always welcome! Please feel free to check our contribution guide if you would like to contribute to the content and bring in the latest SOTA algorithms.

# Additional Azure resources to learn more

To learn more, you can read the following articles and notebooks:

* Portfolio of Azure Machine Learning Notebooks: [aka.ms/AzureMLServiceGithub](https://aka.ms/AzureMLServiceGithub)
* Azure Machine Learning: [aka.ms/AzureMLservice](https://aka.ms/AzureMLservice)
* Get started with Azure ML: [aka.ms/GetStartedAzureML](https://aka.ms/GetStartedAzureML)
* Automated Machine Learning Documentation: [aka.ms/AutomatedMLDocs](https://aka.ms/AutomatedMLDocs)
* Python Microsoft: [aka.ms/PythonMS](https://aka.ms/PythonMS)
* Azure ML for VS Code: [aka.ms/AzureMLforVSCode](https://aka.ms/AzureMLforVSCode)
* Algorithm Cheat Sheet: [aka.ms/AlgorithmCheatSheet](https://aka.ms/AlgorithmCheatSheet)
* How to Select Machine Learning algorithms: [aka.ms/SelectAlgos](https://aka.ms/SelectAlgos)

*(This post was published at*  [*https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65*](https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65)*)*",t2_3a06zoee,False,,0,False,Open-Source Repository of Forecasting Best Practices for Accelerating Solution Development,[],r/learnmachinelearning,False,6,,0,127.0,False,t3_gawxwn,False,dark,0.84,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/5NLXReprPctrP3KimN2vJ8bUgvK8JbLZBik-r8AhYyc.jpg,False,,[],{},,,True,,1588286617.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;em&gt;(You can find the original post at&lt;/em&gt; &lt;a href=""https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65""&gt;&lt;em&gt;https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Time series forecasting is one of the most important topics in data science. Imagine that you are a business owner, you might want to predict different sorts of future events to make better decisions and optimize your resource allocation. Typical examples of time series forecasting use cases are retail sales forecasting, package shipment delay forecasting, energy demand forecasting, and financial forecasting. As you can see, forecasting is everywhere!&lt;/p&gt;

&lt;p&gt;Given its ubiquitous nature and wide-ranging business applications, we have developed an open-source &lt;a href=""https://github.com/microsoft/forecasting?WT.mc_id=github-reddit-lazzeri""&gt;forecasting repo&lt;/a&gt; that puts world-class models and forecasting best practices in the hands of data scientists and industry experts â€” i.e., you!&lt;/p&gt;

&lt;h1&gt;Forecasting Best Practices and Solution Accelerators&lt;/h1&gt;

&lt;p&gt;This repository provides examples of building forecasting solutions presented as Python Jupyter notebooks, R markdown files, and a library of utility functions. Our goal is to help you as a data scientist or machine learning engineer with varying levels of knowledge in forecasting:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learn best practices for the development of forecasting solutions in a variety of languages.&lt;/li&gt;
&lt;li&gt;Leverage recent advances in forecasting algorithms to build high-performance solutions and operationalize them.&lt;/li&gt;
&lt;li&gt;Accelerate the solution development process for real-world forecasting problems. With the provided examples, you will be able to significantly reduce the â€œtime to marketâ€ by simplifying the experience from defining the business problem to the development of solutions by orders of magnitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the repository, you will find state-of-the-art (SOAT) forecasting models using traditional machine learning and deep learning approaches. Implementations of SOTA models in this release are centered around retail sales forecasting and are written in Python and R, two of the most popular programming languages in the forecasting domain. To enable high-throughput forecasting scenarios, we have included notebooks for forecasting multiple time series with distributed training techniques such as Ray in Python, the parallel package in R, and multi-threading in LightGBM. The following is a quick summary of forecasting models covered in this repository:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/zur9qp28wyv41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a70a70e83339384764f3d7b3ab752eef287a9fa""&gt;https://preview.redd.it/zur9qp28wyv41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a70a70e83339384764f3d7b3ab752eef287a9fa&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The repository also comes with &lt;a href=""https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri""&gt;Azure Machine Learning&lt;/a&gt; (&lt;a href=""https://docs.microsoft.com/en-us/azure/machine-learning/?WT.mc_id=medium-article-lazzeri""&gt;Azure ML&lt;/a&gt;) themed notebooks and best practices recipes to accelerate the development of scalable, production-grade forecasting solutions on &lt;a href=""https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri""&gt;Azure&lt;/a&gt;. You will find the following examples for forecasting with &lt;a href=""https://docs.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?WT.mc_id=medium-article-lazzeri""&gt;Azure AutoML&lt;/a&gt; as well as tuning and deploying a forecasting model on &lt;a href=""https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri""&gt;Azure&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/yafcpzqawyv41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17060a140963606405c2e31a0b204f1700ef482f""&gt;https://preview.redd.it/yafcpzqawyv41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17060a140963606405c2e31a0b204f1700ef482f&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Developing an accurate forecasting solution can be a complex and time-consuming process. We hope the forecasting repo will help shorten your development cycle on &lt;a href=""https://azure.microsoft.com/en-us/?WT.mc_id=medium-article-lazzeri""&gt;Azure.&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;To Learn More and Contribute&lt;/h1&gt;

&lt;p&gt;For more information, please visit: &lt;a href=""https://github.com/microsoft/forecasting?WT.mc_id=github-reddit-lazzeri""&gt;https://github.com/microsoft/forecasting&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Contributions from open-source community are always welcome! Please feel free to check our contribution guide if you would like to contribute to the content and bring in the latest SOTA algorithms.&lt;/p&gt;

&lt;h1&gt;Additional Azure resources to learn more&lt;/h1&gt;

&lt;p&gt;To learn more, you can read the following articles and notebooks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Portfolio of Azure Machine Learning Notebooks: &lt;a href=""https://aka.ms/AzureMLServiceGithub""&gt;aka.ms/AzureMLServiceGithub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure Machine Learning: &lt;a href=""https://aka.ms/AzureMLservice""&gt;aka.ms/AzureMLservice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Get started with Azure ML: &lt;a href=""https://aka.ms/GetStartedAzureML""&gt;aka.ms/GetStartedAzureML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Automated Machine Learning Documentation: &lt;a href=""https://aka.ms/AutomatedMLDocs""&gt;aka.ms/AutomatedMLDocs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python Microsoft: &lt;a href=""https://aka.ms/PythonMS""&gt;aka.ms/PythonMS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Azure ML for VS Code: &lt;a href=""https://aka.ms/AzureMLforVSCode""&gt;aka.ms/AzureMLforVSCode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Algorithm Cheat Sheet: &lt;a href=""https://aka.ms/AlgorithmCheatSheet""&gt;aka.ms/AlgorithmCheatSheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How to Select Machine Learning algorithms: &lt;a href=""https://aka.ms/SelectAlgos""&gt;aka.ms/SelectAlgos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;(This post was published at&lt;/em&gt;  &lt;a href=""https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65""&gt;&lt;em&gt;https://medium.com/microsoftazure/open-source-repository-of-forecasting-best-practices-for-accelerating-solution-development-9272f8f93d65&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gawxwn,True,,frlazzeri,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gawxwn/opensource_repository_of_forecasting_best/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gawxwn/opensource_repository_of_forecasting_best/,155203,1588257817.0,0,,False,,,"{'zur9qp28wyv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 98, 'x': 108, 'u': 'https://preview.redd.it/zur9qp28wyv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e175d55277a7a79c28b1a154cd73571c56d304f'}, {'y': 196, 'x': 216, 'u': 'https://preview.redd.it/zur9qp28wyv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=18a63684b92dab8781f270ad99d3f66b7f1308e4'}, {'y': 290, 'x': 320, 'u': 'https://preview.redd.it/zur9qp28wyv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=814f3bc8a13b05e6ff3c733faf407fe083e43a03'}, {'y': 581, 'x': 640, 'u': 'https://preview.redd.it/zur9qp28wyv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b54278253cfcd356eb127596d340dd0992b6c2f'}], 's': {'y': 635, 'x': 699, 'u': 'https://preview.redd.it/zur9qp28wyv41.png?width=699&amp;format=png&amp;auto=webp&amp;s=0a70a70e83339384764f3d7b3ab752eef287a9fa'}, 'id': 'zur9qp28wyv41'}, 'yafcpzqawyv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 41, 'x': 108, 'u': 'https://preview.redd.it/yafcpzqawyv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2aed251e0ba686ce6fd2c0e60c4abb30418385e0'}, {'y': 83, 'x': 216, 'u': 'https://preview.redd.it/yafcpzqawyv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16fe409da5103230d0a4cdeab0065779f6763b9d'}, {'y': 124, 'x': 320, 'u': 'https://preview.redd.it/yafcpzqawyv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c05850d629bd6c1a8a90f1dee49c35271703d52'}, {'y': 248, 'x': 640, 'u': 'https://preview.redd.it/yafcpzqawyv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bb01a498d5557bd7fe1be7dcdb1ff8fc2323287'}], 's': {'y': 271, 'x': 699, 'u': 'https://preview.redd.it/yafcpzqawyv41.png?width=699&amp;format=png&amp;auto=webp&amp;s=17060a140963606405c2e31a0b204f1700ef482f'}, 'id': 'yafcpzqawyv41'}}",
,learnmachinelearning,"Hey!! I have knowledge on python programming language, a basic algorithms. Although I have been into machine learning for some time, trying to learn it. It hadn't been that great journey yet. 
Hence, I am thinking to start over again. Anyone got any ideas, how to start?",t2_53zytwkv,False,,0,False,Need help to start.,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb4pgs,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588310907.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey!! I have knowledge on python programming language, a basic algorithms. Although I have been into machine learning for some time, trying to learn it. It hadn&amp;#39;t been that great journey yet. 
Hence, I am thinking to start over again. Anyone got any ideas, how to start?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gb4pgs,True,,Pranay_Saha,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb4pgs/need_help_to_start/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb4pgs/need_help_to_start/,155203,1588282107.0,0,,False,,,,
,learnmachinelearning,"Hello guys I am training a deep qlearning model to play atari games. In that i have to pass an image of size 84x84x4 as input to the network. Each frame (84x84) of the game is part of 4 different images. I have to store approximately 1 million of these images on my memory removing the oldest one (like a queue with max length 1 mil). Also the images are produced while the game is running. How do i store these frames effieciently such that each frmae is stored only once? I only have a limited amount of ram of size 8 GB. Any help would be greatly appreciated.

I am doing this in python.",t2_myr70qq,False,,0,False,Effiecient deep q-learning model,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaxoxu,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588289027.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys I am training a deep qlearning model to play atari games. In that i have to pass an image of size 84x84x4 as input to the network. Each frame (84x84) of the game is part of 4 different images. I have to store approximately 1 million of these images on my memory removing the oldest one (like a queue with max length 1 mil). Also the images are produced while the game is running. How do i store these frames effieciently such that each frmae is stored only once? I only have a limited amount of ram of size 8 GB. Any help would be greatly appreciated.&lt;/p&gt;

&lt;p&gt;I am doing this in python.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gaxoxu,True,,rushik_subba,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaxoxu/effiecient_deep_qlearning_model/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaxoxu/effiecient_deep_qlearning_model/,155203,1588260227.0,0,,False,,,,
,learnmachinelearning,"Hi Team,

I was given a take home data science test, and one of the question stumped me.

The question is as follows :

By generation of 1,000, 000 random samples for a random variable X N(u, sigma), find the value of sigma for which P(a &lt; X &lt; b) is maximum, for simplicity, assume u= 0, a = 1, b = 2, set. Seed ( 123)

Any idea on how to approach this? [Discussion]",t2_6ys5mu5,False,,0,False,Estimating the variance of a normal distribution to maximise area,[],r/learnmachinelearning,False,6,,0,,False,t3_gazid0,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588294632.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Team,&lt;/p&gt;

&lt;p&gt;I was given a take home data science test, and one of the question stumped me.&lt;/p&gt;

&lt;p&gt;The question is as follows :&lt;/p&gt;

&lt;p&gt;By generation of 1,000, 000 random samples for a random variable X N(u, sigma), find the value of sigma for which P(a &amp;lt; X &amp;lt; b) is maximum, for simplicity, assume u= 0, a = 1, b = 2, set. Seed ( 123)&lt;/p&gt;

&lt;p&gt;Any idea on how to approach this? [Discussion]&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gazid0,True,,user19911506,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gazid0/estimating_the_variance_of_a_normal_distribution/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gazid0/estimating_the_variance_of_a_normal_distribution/,155203,1588265832.0,0,,False,,,,
,learnmachinelearning," I've recently started the Coursera ML course and took It upon my self to train a linear regression model through gradient descent using octave. Here's the code I came up with: 

    clear, clc
    
    % loading the data and scaling it
    data = csvread('data.csv');
    mu = mean(data);
    dev = std(data);
    for i =1:size(data, 2)
      data(:, i) = (data(:, i) - mu(i)) * (1/dev(i));
    endfor
    
    % extracting the features (X) and labels from the data (Y)
    X = data(:, [1:end-1]);
    Y = data(:, end);
    
    % adding a column of ones to the training data
    X = [ones(size(X, 1), 1) X];
    
    % number of training examples (m) and number of features (n)
    [m, n] = size(X);
    
    % initializing starting theta vector and alpha value
    theta = zeros(n, 1);
    alpha = 1;
    
    % choosing a convergence threshold
    convergence = 0.01;
    
    % initializing convergence condition
    convergent = 0;
    
    % starting gradient descent
    while (convergent == 0)
      
      % updating theta vector
      theta = theta - alpha * (1/m) * (X'*(X*theta-Y));
      
      % computing the new value of the cost function
      cost = (1/2*m)* sum((X*theta - Y).^2)
      
      % evaluating convergence condition
      if (cost &lt; convergence)
        convergent = 1;
      endif
      
    endwhile

 My problem is, no matter what value I use for alpha, the value of the cost function decreases really slowly and at some point, the change is so small that the value displayed stays constant through every iteration. I don't know if the problem stems from my implementation, or my choice of alpha or even the dataset, and would love someone to shed some light as to what I'm doing wrong.

here is a [link](https://www.kaggle.com/mirichoi0218/insurance/data) for the dataset. I've removed all columns except for the age and bmi (so 2 features) and the charged price (label).

Thank you.",t2_nfd6t23,False,,0,False,Can't bring my cost function to minimize,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb3qpq,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1588307851.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently started the Coursera ML course and took It upon my self to train a linear regression model through gradient descent using octave. Here&amp;#39;s the code I came up with: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clear, clc

% loading the data and scaling it
data = csvread(&amp;#39;data.csv&amp;#39;);
mu = mean(data);
dev = std(data);
for i =1:size(data, 2)
  data(:, i) = (data(:, i) - mu(i)) * (1/dev(i));
endfor

% extracting the features (X) and labels from the data (Y)
X = data(:, [1:end-1]);
Y = data(:, end);

% adding a column of ones to the training data
X = [ones(size(X, 1), 1) X];

% number of training examples (m) and number of features (n)
[m, n] = size(X);

% initializing starting theta vector and alpha value
theta = zeros(n, 1);
alpha = 1;

% choosing a convergence threshold
convergence = 0.01;

% initializing convergence condition
convergent = 0;

% starting gradient descent
while (convergent == 0)

  % updating theta vector
  theta = theta - alpha * (1/m) * (X&amp;#39;*(X*theta-Y));

  % computing the new value of the cost function
  cost = (1/2*m)* sum((X*theta - Y).^2)

  % evaluating convergence condition
  if (cost &amp;lt; convergence)
    convergent = 1;
  endif

endwhile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My problem is, no matter what value I use for alpha, the value of the cost function decreases really slowly and at some point, the change is so small that the value displayed stays constant through every iteration. I don&amp;#39;t know if the problem stems from my implementation, or my choice of alpha or even the dataset, and would love someone to shed some light as to what I&amp;#39;m doing wrong.&lt;/p&gt;

&lt;p&gt;here is a &lt;a href=""https://www.kaggle.com/mirichoi0218/insurance/data""&gt;link&lt;/a&gt; for the dataset. I&amp;#39;ve removed all columns except for the age and bmi (so 2 features) and the charged price (label).&lt;/p&gt;

&lt;p&gt;Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?auto=webp&amp;s=25fc65aa5d09148cb5b7919200b47e820375738f', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f25e9a0237d8c363ac779bd01bcfdd101afa49f5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfd2facf87a0e4d0b16cb4368f85d0b21965fed7', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=daacac98892ffa5e146594d54e0b801777481741', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=49c22b2975b498d3d8847087440b4d564c7ca481', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f796df9c606caf550b9cd80c39594a82a32b3718', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/JINjogBvZcvIFMSp3h6yY74Gkn_97aK8OF84FCOcjuI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=038b86db56ad50b19981eff7f11495b5415adfa5', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'F_GjHwHix9aE8IINcahYS7WuagWa0w7QoegUbKTapM4'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gb3qpq,True,,EOmar4TW,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb3qpq/cant_bring_my_cost_function_to_minimize/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb3qpq/cant_bring_my_cost_function_to_minimize/,155203,1588279051.0,0,,False,,,,
,learnmachinelearning,"Hello, I'm Mari and I'm college student during this pandemic. I'm currently researching the effects of data bias and how differing backgrounds influence emotional labeling in images. My team has compiled a database of images and we want to find the major factors that influence data labeling. This survey  only takes 5 minutes, and despite all the chaos going on, if you have a minute to help my group on our research we would greatly appreciate it. 

[https://cuboulder.qualtrics.com/jfe/form/SV\_2odmKsfcVGvEZOB](https://cuboulder.qualtrics.com/jfe/form/SV_2odmKsfcVGvEZOB)

Thank you for supporting machine learning, and future AI work.",t2_2x2trbm0,False,,0,False,Emotional Labeling Research,[],r/learnmachinelearning,False,6,,0,,False,t3_gazebl,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588294296.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m Mari and I&amp;#39;m college student during this pandemic. I&amp;#39;m currently researching the effects of data bias and how differing backgrounds influence emotional labeling in images. My team has compiled a database of images and we want to find the major factors that influence data labeling. This survey  only takes 5 minutes, and despite all the chaos going on, if you have a minute to help my group on our research we would greatly appreciate it. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://cuboulder.qualtrics.com/jfe/form/SV_2odmKsfcVGvEZOB""&gt;https://cuboulder.qualtrics.com/jfe/form/SV_2odmKsfcVGvEZOB&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you for supporting machine learning, and future AI work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gazebl,True,,marigriego,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gazebl/emotional_labeling_research/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gazebl/emotional_labeling_research/,155203,1588265496.0,0,,False,,,,
,learnmachinelearning,,t2_67lldm9q,False,,0,False,Machine Learning and Its Business Applications,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_ga70x7,False,dark,0.93,,public,527,0,{},140.0,,False,[],,True,False,,{},,False,527,,False,https://b.thumbs.redditmedia.com/V3J0pIKSt8EzHN_x8ov9KWmIjjH7LwGxB-ZqthCsrIY.jpg,False,,[],{},image,,False,,1588185081.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?auto=webp&amp;s=579e531f779ce6351e6f7fefcc3fa76f98ba8daa', 'width': 1080, 'height': 5397}, 'resolutions': [{'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7634e6cf7034eb1c7833af7106abd4b5de4d097d', 'width': 108, 'height': 216}, {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6224e3cf8417e8dc7cb2fd2f18f2ba6bf5bf278', 'width': 216, 'height': 432}, {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d41b4e4a41ec2987ad368b8b6cac036e6c9b6ee', 'width': 320, 'height': 640}, {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2fcf912095fc8f5922b924845e010a52817a656', 'width': 640, 'height': 1280}, {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d18dda643af642f507e27d674bf93c98ea9faac5', 'width': 960, 'height': 1920}, {'url': 'https://preview.redd.it/41rqkw3giqv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0b424fe2e063bf3011847a927c66d012fb83646e', 'width': 1080, 'height': 2160}], 'variants': {}, 'id': 'l41-m_rKQatkhDWf7Kk2ltTGZam0MbKEkMQ7ItFDivI'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga70x7,True,,Intagleo,,33,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga70x7/machine_learning_and_its_business_applications/,all_ads,False,https://i.redd.it/41rqkw3giqv41.jpg,155203,1588156281.0,2,,False,,,,
,learnmachinelearning,"Im currently working on project with different kind of sensors that measure different things in the environment. It is more an engineering project than programming projects but I said I'm gonna do the ML part because I have some experience with ML. I didn't expected it to be so difficult though (only have very basic skills).

I want to get a 1 if Event A will take place in the next 5 hours and and a 0 if Event a won't happen in the next 5 hours.
Currently I'm using log regression wich try to predict if Event A will happen based on the values our sensors provide in this very moment. Our sensors get updated 3 times in a Minute.
Now I think it might be useful to include the measurements from the last 90 times it got measured but I have no clue how to implement that.
Is this even possible? Could anyone point me into the right direction?",t2_5dexvvvs,False,,0,False,Is it possible to include time into a log regression or NN?,[],r/learnmachinelearning,False,6,,0,,False,t3_gavvoz,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588282899.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Im currently working on project with different kind of sensors that measure different things in the environment. It is more an engineering project than programming projects but I said I&amp;#39;m gonna do the ML part because I have some experience with ML. I didn&amp;#39;t expected it to be so difficult though (only have very basic skills).&lt;/p&gt;

&lt;p&gt;I want to get a 1 if Event A will take place in the next 5 hours and and a 0 if Event a won&amp;#39;t happen in the next 5 hours.
Currently I&amp;#39;m using log regression wich try to predict if Event A will happen based on the values our sensors provide in this very moment. Our sensors get updated 3 times in a Minute.
Now I think it might be useful to include the measurements from the last 90 times it got measured but I have no clue how to implement that.
Is this even possible? Could anyone point me into the right direction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gavvoz,True,,CrybabyAlien,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gavvoz/is_it_possible_to_include_time_into_a_log/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gavvoz/is_it_possible_to_include_time_into_a_log/,155203,1588254099.0,0,,False,,,,
,learnmachinelearning,"Hi, I'm building an LSTM to classify a time series (classes are largely imbalanced). I used a 1dConv layer  followed by 2 LSTM(40) layers to classify the next point in the time series based on 100 previous time steps.

Unfortunately I get only 0.2 precision and 0.8 recall.

Is there anything I can do to improve my precision?

Thank you!",t2_axav08j,False,,0,False,Any tips to improve precision of LSTM?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gb28kg,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588303112.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m building an LSTM to classify a time series (classes are largely imbalanced). I used a 1dConv layer  followed by 2 LSTM(40) layers to classify the next point in the time series based on 100 previous time steps.&lt;/p&gt;

&lt;p&gt;Unfortunately I get only 0.2 precision and 0.8 recall.&lt;/p&gt;

&lt;p&gt;Is there anything I can do to improve my precision?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gb28kg,True,,everek123,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gb28kg/any_tips_to_improve_precision_of_lstm/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gb28kg/any_tips_to_improve_precision_of_lstm/,155203,1588274312.0,0,,False,,,,
,learnmachinelearning,,t2_bchkxdc,False,,0,False,A blog I wrote summarizing the Fast.ai machine learning course talking about Feature Engineering using Random Forests,[],r/learnmachinelearning,False,6,,0,87.0,False,t3_gash6f,False,dark,0.77,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/15JGEdTGCOWSrQpgO4BmHISOU7UTifHYDhbp1z_TyYc.jpg,False,,[],{},link,,False,,1588267621.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lVQDCUHpAs-9X4dxoaHCuUzkJ9DThfD2qBk98PSkjmY.jpg?auto=webp&amp;s=50fbe033a454803933e5a5949f7ffafc3c5462c9', 'width': 859, 'height': 536}, 'resolutions': [{'url': 'https://external-preview.redd.it/lVQDCUHpAs-9X4dxoaHCuUzkJ9DThfD2qBk98PSkjmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=edcb85e5300893e6de714f7a57e11b2c02a9ee11', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/lVQDCUHpAs-9X4dxoaHCuUzkJ9DThfD2qBk98PSkjmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e011a6c0d90d94cddf736d1411c3883d611f93fd', 'width': 216, 'height': 134}, {'url': 'https://external-preview.redd.it/lVQDCUHpAs-9X4dxoaHCuUzkJ9DThfD2qBk98PSkjmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e23417728e97fc4792dce12ec5dc0f538ced6c03', 'width': 320, 'height': 199}, {'url': 'https://external-preview.redd.it/lVQDCUHpAs-9X4dxoaHCuUzkJ9DThfD2qBk98PSkjmY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ceee99a76889c0acc2c6453e9a46bf48d41bad39', 'width': 640, 'height': 399}], 'variants': {}, 'id': 'JFUqbnbHgaBC38aVzpEOzs9ffeT22xURHd-bOkbM-NI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gash6f,True,,sdhnshu,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gash6f/a_blog_i_wrote_summarizing_the_fastai_machine/,all_ads,False,https://medium.com/@sdhnshu/feature-engineering-with-random-forests-part1-601c66dfb09b,155203,1588238821.0,0,,False,,,,
,learnmachinelearning,"Hey everyone,

A couple of weeks ago I decided to implement core Machine Learning algorithms from scratch in pure Python (using only plotting libraries as dependencies).

While doing so I decided to share the results via dedicated Jupyter notebooks and in-depth blog posts. My overall goal is to explain the underlying mathematics in an intuitive and playful way.

Today Iâ€™ve published the fourth post which explains Linear- and Multiple Regression: [https://philippmuens.com/linear-and-multiple-regression-from-scratch/](https://philippmuens.com/linear-and-multiple-regression-from-scratch/)

All the Jupyter Notebooks can be found here: [https://github.com/pmuens/lab#implementations](https://github.com/pmuens/lab#implementations)

I hope that you enjoy it! More implementations will follow in the upcoming weeks / months.",t2_pfy1e,False,,0,False,Linear- and Multiple Regression from scratch in pure Python,[],r/learnmachinelearning,False,6,,0,,False,t3_gawc0k,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588284508.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;

&lt;p&gt;A couple of weeks ago I decided to implement core Machine Learning algorithms from scratch in pure Python (using only plotting libraries as dependencies).&lt;/p&gt;

&lt;p&gt;While doing so I decided to share the results via dedicated Jupyter notebooks and in-depth blog posts. My overall goal is to explain the underlying mathematics in an intuitive and playful way.&lt;/p&gt;

&lt;p&gt;Today Iâ€™ve published the fourth post which explains Linear- and Multiple Regression: &lt;a href=""https://philippmuens.com/linear-and-multiple-regression-from-scratch/""&gt;https://philippmuens.com/linear-and-multiple-regression-from-scratch/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All the Jupyter Notebooks can be found here: &lt;a href=""https://github.com/pmuens/lab#implementations""&gt;https://github.com/pmuens/lab#implementations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope that you enjoy it! More implementations will follow in the upcoming weeks / months.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?auto=webp&amp;s=e1269ce70b59392ca576f4390cd9edfc399462bb', 'width': 2000, 'height': 1333}, 'resolutions': [{'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=51a0040cd4b82d54c5d38751553172b498a17c15', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4256c53d3493a421be9aa3f414f78894d45e66f', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8b88d312cc0d26cde56131a3bd3a4e2720bd9d9', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=726798e4a76442490ddc6e08ea523f9b01bb6a96', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ad0a6986d1663879251b91aa1ec80199e11b887', 'width': 960, 'height': 639}, {'url': 'https://external-preview.redd.it/mdcKe2U4UcZz7Eg2WuybJXcMV2c5hqp_gPSi2VdmnkU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3101f37a1429f8122f1d3ae98d02afdad95f0ca2', 'width': 1080, 'height': 719}], 'variants': {}, 'id': 'erExeqNV4Tw0A_qU1D7KFtLWXFq-y5nFSeEn5fpANTs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gawc0k,True,,pmuens,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gawc0k/linear_and_multiple_regression_from_scratch_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gawc0k/linear_and_multiple_regression_from_scratch_in/,155203,1588255708.0,0,,False,,,,
,learnmachinelearning,,t2_1myz87vv,False,,0,False,Feature Ranking in ML.NET,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_gavxtp,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/AWDbx1bjpD73HM5SOCfIKImT3rVH76oJ4-wzbTCdYNI.jpg,False,,[],{},link,,False,,1588283115.0,text,6,,,text,bush-dev.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?auto=webp&amp;s=606bacaa47a1858e95c12030f43434ac8c184416', 'width': 1000, 'height': 667}, 'resolutions': [{'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc6bbf3e7a6be7eb547d0ded6b0bda3b0b34748d', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5141c66e1a99f4a77c7e65a3efecd3fd064ebcc', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d347919201e415409968081292c82d6e512cbdb', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ba7ff60d69a2c094bbabc0abbe8df944211d50ec', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/a9tSNuWN4O04fwqfvaFg4P2UPXTYb7JgTsi4REfduGg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c28933871b7a343a87b4b057ac141a3409d29728', 'width': 960, 'height': 640}], 'variants': {}, 'id': 'b5lT6v6rsg6GZykDHkjkrP3Zo4xDYenk9kKwbnPpCGw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gavxtp,True,,bush_dev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gavxtp/feature_ranking_in_mlnet/,all_ads,False,http://bush-dev.com/feature-ranking-in-ml-net/,155203,1588254315.0,0,,False,,,,
,learnmachinelearning,"Hi,

I don't know much about Machine Learning (ML), but I have the following problem. I want to categorize the complexity of an object so that I can understand the time that it takes to produce. Simple objects are fast to produce, but complex objects take more time. I am going to give an example.

E.g., I have a factory that produces several models of bikes. Simple bikes that don't have any particular top-notch piece (like the bikes in the '60s) should take less time to produce, than a bike that is electrical, with the gear change, disk break, and so on... I received a new order from the client for a new model that was never produced in the factory. The client asks me how much time it takes to build this new bike. I don't know the time it takes because we never produced one.

Looking at this problem, it seems that I should classify the complexity of the bike from our previous models to understand how much time it takes to put each piece in the bike. Then, I should extrapolate what are the pieces that I need to produce the new model and predict how much time it takes to build this bike by summing the time to put each piece in place.

I don't know how ML can help me, nor which model should I use, nor if it is better to use Neural Network. Can someone give me a clue what should I learn, so that I can know how to answer the question?",t2_gtpqu,False,,0,False,Categorize the complexity of an object,[],r/learnmachinelearning,False,6,,0,,False,t3_gayrmq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588266639.0,,[],{},,,True,,1588292344.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know much about Machine Learning (ML), but I have the following problem. I want to categorize the complexity of an object so that I can understand the time that it takes to produce. Simple objects are fast to produce, but complex objects take more time. I am going to give an example.&lt;/p&gt;

&lt;p&gt;E.g., I have a factory that produces several models of bikes. Simple bikes that don&amp;#39;t have any particular top-notch piece (like the bikes in the &amp;#39;60s) should take less time to produce, than a bike that is electrical, with the gear change, disk break, and so on... I received a new order from the client for a new model that was never produced in the factory. The client asks me how much time it takes to build this new bike. I don&amp;#39;t know the time it takes because we never produced one.&lt;/p&gt;

&lt;p&gt;Looking at this problem, it seems that I should classify the complexity of the bike from our previous models to understand how much time it takes to put each piece in the bike. Then, I should extrapolate what are the pieces that I need to produce the new model and predict how much time it takes to build this bike by summing the time to put each piece in place.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know how ML can help me, nor which model should I use, nor if it is better to use Neural Network. Can someone give me a clue what should I learn, so that I can know how to answer the question?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gayrmq,True,,xeon1234,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gayrmq/categorize_the_complexity_of_an_object/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gayrmq/categorize_the_complexity_of_an_object/,155203,1588263544.0,0,,False,,,,
,learnmachinelearning,"I was able to find many examples of hybrid CNN / LSTM or CNN / biLSTM models and wanted to try it on a multi-label text classification problem I am working on. I am new to machine learning so I am not very knowledgeable on the subject.. Interestingly, even though I tuned both models, the opposite of that (a bidirectional LSTM layer followed by CNN layer(s)) seems to be doing better ... Now, I can't seem to find any examples of this type of architecture, beside one post on Kaggle from 2 years ago. As I understand, the benefit of using CNN before any RNN layer is that it shortens the input and extracts the important bits for the RNN to process. What could be the benefit of having a convolution layer after a RNN layer?  Is there any?",t2_2js9wngd,False,,0,False,LSTM + CNN and CNN + LSTM,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaypba,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588292142.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was able to find many examples of hybrid CNN / LSTM or CNN / biLSTM models and wanted to try it on a multi-label text classification problem I am working on. I am new to machine learning so I am not very knowledgeable on the subject.. Interestingly, even though I tuned both models, the opposite of that (a bidirectional LSTM layer followed by CNN layer(s)) seems to be doing better ... Now, I can&amp;#39;t seem to find any examples of this type of architecture, beside one post on Kaggle from 2 years ago. As I understand, the benefit of using CNN before any RNN layer is that it shortens the input and extracts the important bits for the RNN to process. What could be the benefit of having a convolution layer after a RNN layer?  Is there any?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gaypba,True,,_Pillars_,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaypba/lstm_cnn_and_cnn_lstm/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaypba/lstm_cnn_and_cnn_lstm/,155203,1588263342.0,0,,False,,,,
,learnmachinelearning,"Hi everyone!

I'm graduating this year from IITG majoring in CS and I want to apply for MS programs soon  
Given the COVID situation and my profile (average GPA and just one research intern at srib), I want to spend the upcoming year working as a research assistant in a good research group in order to build up my profile.

I have some prior experience with Vision (intern I mentioned), Data mining (academic course I did well in). My BTP was a cross-domain analysis of various embedding methods with special attention to code analysis and representation.

Which groups should I apply to?

Any advice on how to go approach professors and application process is greatly appreciated! :)",t2_mfwag2h,False,,0,False,Research Groups to apply to: Need advice (India specific!),[],r/learnmachinelearning,False,6,,0,,False,t3_gayosk,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588292098.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m graduating this year from IITG majoring in CS and I want to apply for MS programs soon&lt;br/&gt;
Given the COVID situation and my profile (average GPA and just one research intern at srib), I want to spend the upcoming year working as a research assistant in a good research group in order to build up my profile.&lt;/p&gt;

&lt;p&gt;I have some prior experience with Vision (intern I mentioned), Data mining (academic course I did well in). My BTP was a cross-domain analysis of various embedding methods with special attention to code analysis and representation.&lt;/p&gt;

&lt;p&gt;Which groups should I apply to?&lt;/p&gt;

&lt;p&gt;Any advice on how to go approach professors and application process is greatly appreciated! :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gayosk,True,,yagyanshbhatia,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gayosk/research_groups_to_apply_to_need_advice_india/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gayosk/research_groups_to_apply_to_need_advice_india/,155203,1588263298.0,0,,False,,,,
,learnmachinelearning,"Hi.

I downloaded ""QSAR fish bioconcentration factor "" from UCI repository. I want to remove the values in the column ""LogKOW"" corresponding to the values starting with ""V-Mey_NA"" in the column ""CAS"". How can I do this?",t2_22zkbwyp,False,,0,False,Removing Values in a Column corresponding to Another Value in a Column.,[],r/learnmachinelearning,False,6,,0,,False,t3_gayl87,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588291787.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;I downloaded &amp;quot;QSAR fish bioconcentration factor &amp;quot; from UCI repository. I want to remove the values in the column &amp;quot;LogKOW&amp;quot; corresponding to the values starting with &amp;quot;V-Mey_NA&amp;quot; in the column &amp;quot;CAS&amp;quot;. How can I do this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gayl87,True,,yinyang6969,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gayl87/removing_values_in_a_column_corresponding_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gayl87/removing_values_in_a_column_corresponding_to/,155203,1588262987.0,0,,False,,,,
,learnmachinelearning,"I am starting off to dive into deep learning. Which is the best free GPU computing platform to go with? I have heard of Google Compute Platform, Paperspace Gradient and Colab or are there others?

Does Anaconda allow for GPU computing for free?

Sorry if I got some of the stuff above wrong as I am new to this.

Thanks in advance!",t2_zmqho4m,False,,0,False,GPU platform for deep learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gaxcgw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588259834.0,,[],{},,,True,,1588287949.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am starting off to dive into deep learning. Which is the best free GPU computing platform to go with? I have heard of Google Compute Platform, Paperspace Gradient and Colab or are there others?&lt;/p&gt;

&lt;p&gt;Does Anaconda allow for GPU computing for free?&lt;/p&gt;

&lt;p&gt;Sorry if I got some of the stuff above wrong as I am new to this.&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaxcgw,True,,leockl,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaxcgw/gpu_platform_for_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaxcgw/gpu_platform_for_deep_learning/,155203,1588259149.0,0,,False,,,,
,learnmachinelearning,"in google deep learning vm image 

    &gt;&gt;&gt; validation = raw_validation.map(format_example)
&gt;&gt;&gt; train = raw_train.map(format_example)
&gt;&gt;&gt; test = raw_test.map(format_example)
&gt;&gt;&gt; BATCH_SIZE = 32
&gt;&gt;&gt; SHUFFLE_BUFFER_SIZE = 1000
&gt;&gt;&gt; train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
&gt;&gt;&gt; validation_batches = validation.batch(BATCH_SIZE)
&gt;&gt;&gt; test_batches = test.batch(BATCH_SIZE) 
 
    &gt;&gt;&gt; for image_batch, label_batch in train_batches.take(1):
    ...    pass
    ...
 
Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9",t2_z39i9,False,,0,False,tensorflow2.1 Corrupt JPEG data,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gar5oa,False,light,0.81,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588260542.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;in google deep learning vm image &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; validation = raw_validation.map(format_example)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;train = raw_train.map(format_example)
test = raw_test.map(format_example)
BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000
train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE) &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; for image_batch, label_batch in train_batches.take(1):
...    pass
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gar5oa,True,,googcheng,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gar5oa/tensorflow21_corrupt_jpeg_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gar5oa/tensorflow21_corrupt_jpeg_data/,155203,1588231742.0,1,,False,,,,
,learnmachinelearning,,t2_2qev4220,False,,0,False,[R] How to do research on machine-learning your own as a Sophomore.,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gax634,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,default,False,,[],{},,,False,,1588287368.0,richtext,6,,,text,self.statistics,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gax634,True,,Playful_Effect,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gax634/r_how_to_do_research_on_machinelearning_your_own/,all_ads,False,/r/statistics/comments/gax2gx/r_how_to_do_research_on_machinelearning_your_own/,155203,1588258568.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'statistics', 'selftext': ""Hello, I hope you are safe and okay. I am a sophomore majoring in statistics. I want to do my master's in machine learning and artificial neural network. Unfortunately, none of my professors are working on that. So, I think I'll have to do research on my own.\n\nAFAIK, you have to have research history(if you have papers it's better) if you want to do a master's abroad(Europe/North America). So I want to know how to do that on my own."", 'author_fullname': 't2_2qev4220', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[R] How to do research on machine-learning your own as a Sophomore.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/statistics', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gax2gx', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Research', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588287049.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.statistics', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I hope you are safe and okay. I am a sophomore majoring in statistics. I want to do my master&amp;#39;s in machine learning and artificial neural network. Unfortunately, none of my professors are working on that. So, I think I&amp;#39;ll have to do research on my own.&lt;/p&gt;\n\n&lt;p&gt;AFAIK, you have to have research history(if you have papers it&amp;#39;s better) if you want to do a master&amp;#39;s abroad(Europe/North America). So I want to know how to do that on my own.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qhfi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gax2gx', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Playful_Effect', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/statistics/comments/gax2gx/r_how_to_do_research_on_machinelearning_your_own/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/statistics/comments/gax2gx/r_how_to_do_research_on_machinelearning_your_own/', 'subreddit_subscribers': 103426, 'created_utc': 1588258249.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_gax2gx,,
,learnmachinelearning,"Some background: I'm pretty much a newbie in NLP and in machine learning in general, I'm currently following some courses in my university about these topics.

I'm working on my first ml project using basic scikit learn tools and some deep learning with Keras. I'm trying to build something that's able to classify the author of a book (just fiction books for the moment) by looking at its text. At first, I just worked with authors that had at least 30 books. In my dataset, that was \~2000 books written by \~30 authors in total, and that went pretty great, so I decided to raise the stakes. By lowering the threshold to 10 books, the database grew to \~400 authors and \~9000 books.

The way I classified the books is as follows: I removed punctuation, newlines, extra spaces, and stopwords from every text, then I extracted the features with sklearn CountVectorizer and TfidfVectorizer. With this approach, the F1 score was 0.95 for the 2000 books db and 0.62 for the 9000 books db.

The training with the larger dataset was much slower, so I decided to get 5000 random words from each book and use only those. This decreased the training time from 20 minutes to only 2 minutes, and enabled me to try something different. There wasn't a big difference in the various metrics, so I decided to try to remove the tfidf features. Using only CountVectorizer I got a 0.87 F1 score.

I'm not sure why this is happening, my expectation was that the tfidf would make things easier and therefore better my score. Do you have any clue on why removing it improved the score instead?

Are there other strategies, maybe better suited to work with long texts?",t2_15wzw44e,False,,0,False,Literary author classification from books content,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gax5r2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588287338.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Some background: I&amp;#39;m pretty much a newbie in NLP and in machine learning in general, I&amp;#39;m currently following some courses in my university about these topics.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on my first ml project using basic scikit learn tools and some deep learning with Keras. I&amp;#39;m trying to build something that&amp;#39;s able to classify the author of a book (just fiction books for the moment) by looking at its text. At first, I just worked with authors that had at least 30 books. In my dataset, that was ~2000 books written by ~30 authors in total, and that went pretty great, so I decided to raise the stakes. By lowering the threshold to 10 books, the database grew to ~400 authors and ~9000 books.&lt;/p&gt;

&lt;p&gt;The way I classified the books is as follows: I removed punctuation, newlines, extra spaces, and stopwords from every text, then I extracted the features with sklearn CountVectorizer and TfidfVectorizer. With this approach, the F1 score was 0.95 for the 2000 books db and 0.62 for the 9000 books db.&lt;/p&gt;

&lt;p&gt;The training with the larger dataset was much slower, so I decided to get 5000 random words from each book and use only those. This decreased the training time from 20 minutes to only 2 minutes, and enabled me to try something different. There wasn&amp;#39;t a big difference in the various metrics, so I decided to try to remove the tfidf features. Using only CountVectorizer I got a 0.87 F1 score.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not sure why this is happening, my expectation was that the tfidf would make things easier and therefore better my score. Do you have any clue on why removing it improved the score instead?&lt;/p&gt;

&lt;p&gt;Are there other strategies, maybe better suited to work with long texts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gax5r2,True,,mikcnt,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gax5r2/literary_author_classification_from_books_content/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gax5r2/literary_author_classification_from_books_content/,155203,1588258538.0,0,,False,,,,
,learnmachinelearning,"Hello guys , I am quite new to Machine Learning. I recently completed a 40hr Udemy course on it. The course covered topics:

Regression :
1. Simple Linear Regression
2. Muliple Linear Regression
3. Polynomial Regression
4. Support Vector Machine (SVR)
5. Decision Tree Regression
6. Random Forest Regression

Classification :
1. Logistic Regression
2. K- Nearest Neighbours
3. Support Vector Machines
4. Kernel Support Vector Machines
5. Naive Based Classification
6. Decision Tree Classification
7. Random Forest Classification

Evaluating Classification model's performance:
1. FALSE positives and False Negatives on Confusion Matrix
2. Accuracy Investment
3. CAP curve

Clustering:
1. K-Means Clustering
2. Hierarchical Clustering

Association :
1. Apriori
2. Eclat

Reinforcement Learning:
1. Upper Confidence Bound
2. Thompson's Sampling

Natural Language Processing (Basic)

Deep Learning:
1. Artificial Neural Networks
2. Convolutional Neural Network

Dimensionality Reduction :
1. Principal Component Analysis (PCA)
2. Linear Discriminant Analysis
3. Kernel PCA

Model Selection and Boosting :
1. K - Fold Cross Validation
2. Grid Search
3. XGBoost

Could you please advise me what do do next ? I guess i should start working on some projects on Kaggle. It looks very daunting to me seeing the big codes in that website. I guess i will start small. Could you please advise what can i do more (Like topics to cover , tools to learn like Power Bi ) to improve myself. I really want to get a job in this. Thank You.",t2_4ziftlig,False,,0,False,What should i do next ?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_garphv,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1588263466.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys , I am quite new to Machine Learning. I recently completed a 40hr Udemy course on it. The course covered topics:&lt;/p&gt;

&lt;p&gt;Regression :
1. Simple Linear Regression
2. Muliple Linear Regression
3. Polynomial Regression
4. Support Vector Machine (SVR)
5. Decision Tree Regression
6. Random Forest Regression&lt;/p&gt;

&lt;p&gt;Classification :
1. Logistic Regression
2. K- Nearest Neighbours
3. Support Vector Machines
4. Kernel Support Vector Machines
5. Naive Based Classification
6. Decision Tree Classification
7. Random Forest Classification&lt;/p&gt;

&lt;p&gt;Evaluating Classification model&amp;#39;s performance:
1. FALSE positives and False Negatives on Confusion Matrix
2. Accuracy Investment
3. CAP curve&lt;/p&gt;

&lt;p&gt;Clustering:
1. K-Means Clustering
2. Hierarchical Clustering&lt;/p&gt;

&lt;p&gt;Association :
1. Apriori
2. Eclat&lt;/p&gt;

&lt;p&gt;Reinforcement Learning:
1. Upper Confidence Bound
2. Thompson&amp;#39;s Sampling&lt;/p&gt;

&lt;p&gt;Natural Language Processing (Basic)&lt;/p&gt;

&lt;p&gt;Deep Learning:
1. Artificial Neural Networks
2. Convolutional Neural Network&lt;/p&gt;

&lt;p&gt;Dimensionality Reduction :
1. Principal Component Analysis (PCA)
2. Linear Discriminant Analysis
3. Kernel PCA&lt;/p&gt;

&lt;p&gt;Model Selection and Boosting :
1. K - Fold Cross Validation
2. Grid Search
3. XGBoost&lt;/p&gt;

&lt;p&gt;Could you please advise me what do do next ? I guess i should start working on some projects on Kaggle. It looks very daunting to me seeing the big codes in that website. I guess i will start small. Could you please advise what can i do more (Like topics to cover , tools to learn like Power Bi ) to improve myself. I really want to get a job in this. Thank You.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,garphv,True,,pranayprasad3,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/garphv/what_should_i_do_next/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/garphv/what_should_i_do_next/,155203,1588234666.0,0,,False,,,,
,learnmachinelearning,,t2_21lknq2x,False,,0,False,Supervised Learning - Linear Regression (Python Series),[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gar4at,False,dark,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/R4j7ki0-Lzk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Supervised Learning - Linear Regression (Python Series)', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/R4j7ki0-Lzk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/R4j7ki0-Lzk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/R4j7ki0-Lzk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gar4at', 'height': 338}",,False,3,,False,https://b.thumbs.redditmedia.com/knFx0KusY7N2PLQz8KscMeHHk9tkU6D_e1ck6GKB4CQ.jpg,False,,[],{},rich:video,,False,,1588260332.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VN0oHM5s_1dHfI_dUBSnOrpujkWT3lHbqq8dZ0njw-c.jpg?auto=webp&amp;s=331bd21c0533ac10f79ac0dabb04c6b811f9f296', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/VN0oHM5s_1dHfI_dUBSnOrpujkWT3lHbqq8dZ0njw-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3feab12c3ffa91e929f17b49fcbc6fcf2c5d352', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/VN0oHM5s_1dHfI_dUBSnOrpujkWT3lHbqq8dZ0njw-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b91baa18a216507082948ff9ec5d6d2cf9a3e7b3', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/VN0oHM5s_1dHfI_dUBSnOrpujkWT3lHbqq8dZ0njw-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=96db7bcec5a780e19fc2e48081363bbde5cf3970', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'R_hDLkYgJrEbPtH0qV7KdeLNOZAIC17R7vLYbFvNd9M'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gar4at,True,,SquareTechAcademy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gar4at/supervised_learning_linear_regression_python/,all_ads,False,https://youtu.be/R4j7ki0-Lzk,155203,1588231532.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Supervised Learning - Linear Regression (Python Series)', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/R4j7ki0-Lzk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/R4j7ki0-Lzk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"
How do you use scikit-learnâ€™s gaussian mixture model (GMM) to detect outliers?

If I am not mistaken, predit_proba gives the probabilities of a data point belonging to each cluster and this sums to 1. If so, how do we detect a data point does not belong to any clusters and is an outlier?

Thank you in advance.",t2_zmqho4m,False,,0,False,Outlier detection using sklearnâ€™s gaussian mixture model,[],r/learnmachinelearning,False,6,,0,,False,t3_gao4hg,False,dark,1.0,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1588246201.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do you use scikit-learnâ€™s gaussian mixture model (GMM) to detect outliers?&lt;/p&gt;

&lt;p&gt;If I am not mistaken, predit_proba gives the probabilities of a data point belonging to each cluster and this sums to 1. If so, how do we detect a data point does not belong to any clusters and is an outlier?&lt;/p&gt;

&lt;p&gt;Thank you in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gao4hg,True,,leockl,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gao4hg/outlier_detection_using_sklearns_gaussian_mixture/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gao4hg/outlier_detection_using_sklearns_gaussian_mixture/,155203,1588217401.0,0,,False,,,,
,learnmachinelearning,"I was making my plans on where could I do an internship (my first) during the summer period and I remembered that my uncle has a Chinese restaurant (given that I eat there every other week, it was dumb from me that I didn't come up with that earlier). What is even more interesting is that he does the data collection (logging) and accounting and I know for a fact that he is not very fond of it and even slacking from time to time. 

This sounds like an awesome opportunity that I could use to jump in. I spent a good half an hour brainstorming some ideas (I planned to plan something more discrete later in the day) but then it came to me. Sure, I can do the analysis, set up an infrastructure for data, maybe even do some ML (it is a small business so I am not hoping on much ML), but what do I do once I get the insights?

I know that he is going to have some kind of idea but I was wondering what can I do about it, just so I complete my Data Science pipeline with a solution/s. Maybe some of you had some experience regarding a situation like this one.

I plan on calling him later in the day to discuss the idea, explain to him what DS is (most of my fam doesn't even know I do this lol) and of course, I am going to ask him for his spreadsheets so I can start forming concrete ideas and goals based on the provided data.

TL;DR So, how can data insights help a small restaurant business, and what improvements could be made? (e.g. most of the traffic happens at 8 o'clock Saturday knight, what could I do to boost the business)

Thank in advance!",t2_b3i55f5,False,,0,False,How to improve a small restaurant business with Data Analytics and ML,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gav27b,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1588279809.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was making my plans on where could I do an internship (my first) during the summer period and I remembered that my uncle has a Chinese restaurant (given that I eat there every other week, it was dumb from me that I didn&amp;#39;t come up with that earlier). What is even more interesting is that he does the data collection (logging) and accounting and I know for a fact that he is not very fond of it and even slacking from time to time. &lt;/p&gt;

&lt;p&gt;This sounds like an awesome opportunity that I could use to jump in. I spent a good half an hour brainstorming some ideas (I planned to plan something more discrete later in the day) but then it came to me. Sure, I can do the analysis, set up an infrastructure for data, maybe even do some ML (it is a small business so I am not hoping on much ML), but what do I do once I get the insights?&lt;/p&gt;

&lt;p&gt;I know that he is going to have some kind of idea but I was wondering what can I do about it, just so I complete my Data Science pipeline with a solution/s. Maybe some of you had some experience regarding a situation like this one.&lt;/p&gt;

&lt;p&gt;I plan on calling him later in the day to discuss the idea, explain to him what DS is (most of my fam doesn&amp;#39;t even know I do this lol) and of course, I am going to ask him for his spreadsheets so I can start forming concrete ideas and goals based on the provided data.&lt;/p&gt;

&lt;p&gt;TL;DR So, how can data insights help a small restaurant business, and what improvements could be made? (e.g. most of the traffic happens at 8 o&amp;#39;clock Saturday knight, what could I do to boost the business)&lt;/p&gt;

&lt;p&gt;Thank in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gav27b,True,,rLoper,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gav27b/how_to_improve_a_small_restaurant_business_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gav27b/how_to_improve_a_small_restaurant_business_with/,155203,1588251009.0,0,,False,,,,
,learnmachinelearning,"Full disclaimer: I work at Nanonets. Nanonets is a Deep Learning API company. We've written some of the most go to blogs on Deep Learning available on the internet. [https://nanonets.com/blog/](https://nanonets.com/blog/)

This post might be a little different so apologies in advance if I'm violating any rules.

We've written a blog that gives an overview of an important but often overlooked process in an organization: 3 way matching of Purchase Orders

We've noticed that Deep learning (GCNs) + OCR can be applied and bring down manual digitization times significantly. i.e the process that typically takes a human 90 seconds can be done within 20 seconds with AI assistance. However there are quite a few caveats and nuances before beginning and this blog covers that aspect.

Link to blog: [https://nanonets.com/blog/how-to-ocr-purchase-orders-for-automation/](https://nanonets.com/blog/how-to-ocr-purchase-orders-for-automation/)",t2_g65cl,False,,0,False,Problems with automating manual data entry using Deep Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gauk0z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588277800.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Full disclaimer: I work at Nanonets. Nanonets is a Deep Learning API company. We&amp;#39;ve written some of the most go to blogs on Deep Learning available on the internet. &lt;a href=""https://nanonets.com/blog/""&gt;https://nanonets.com/blog/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This post might be a little different so apologies in advance if I&amp;#39;m violating any rules.&lt;/p&gt;

&lt;p&gt;We&amp;#39;ve written a blog that gives an overview of an important but often overlooked process in an organization: 3 way matching of Purchase Orders&lt;/p&gt;

&lt;p&gt;We&amp;#39;ve noticed that Deep learning (GCNs) + OCR can be applied and bring down manual digitization times significantly. i.e the process that typically takes a human 90 seconds can be done within 20 seconds with AI assistance. However there are quite a few caveats and nuances before beginning and this blog covers that aspect.&lt;/p&gt;

&lt;p&gt;Link to blog: &lt;a href=""https://nanonets.com/blog/how-to-ocr-purchase-orders-for-automation/""&gt;https://nanonets.com/blog/how-to-ocr-purchase-orders-for-automation/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?auto=webp&amp;s=f3b30b9e784d1b64dc81cdc241fe694dd0cd348e', 'width': 2000, 'height': 1125}, 'resolutions': [{'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d90f15c84a668a1e2883e81ffccf5f3e5b2e27c2', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d19cebbb9ca5f59ed008ea4516a48a5c53bad5d4', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d9c9e4bf6c8593e1c526b853fdff5334a947d816', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4a2ac14a1b8efed278564280133813e3513ae9e', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=00335b11353c786a5605e7297562c40bc88faf37', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/mwNpxfESaa3Gd3TSvqkdWomHcvRruQqx0K-iiQD4M0w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=855ab83dc09496c37cd47a7bd7768159187c9722', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'Y08j0sr2gndTv26a2dsSP1aU5YjmeHe7If2bVrZEUVY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gauk0z,True,,Ole_Gooner,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gauk0z/problems_with_automating_manual_data_entry_using/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gauk0z/problems_with_automating_manual_data_entry_using/,155203,1588249000.0,0,,False,,,,
,learnmachinelearning,,t2_13iu8d,False,,0,False,Settled,[],r/learnmachinelearning,False,6,,0,77.0,False,t3_g9x3y7,False,dark,0.98,,public,1028,0,{},140.0,,False,[],,True,False,,{},,False,1028,,False,https://a.thumbs.redditmedia.com/zwz2eTWZqVu3iVHqsbAGVo0U0vkDWtTt2_bt-ZiveV4.jpg,False,,[],{},link,,False,,1588141538.0,text,6,,,text,v.redd.it,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?auto=webp&amp;s=f73fa6965d9186c16b8ed9425cf327b11f211051', 'width': 636, 'height': 352}, 'resolutions': [{'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=663507347724f1b3c04db6add1ebc331b7db5c32', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac496ea68e6a287ccdb1277510fb0054cf4ed5d5', 'width': 216, 'height': 119}, {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec33c1d1dfac07bc792b7bc30063819c5226c4a0', 'width': 320, 'height': 177}], 'variants': {}, 'id': 'JZoA8Xkw8jwnsVswFnBF5Ia4Vky_-L9vLo5VrU7m6fo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9x3y7,True,,AlexandreFSR,,44,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9x3y7/settled/,all_ads,False,https://v.redd.it/hvxqozsktlv41,155203,1588112738.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ProgrammerHumor', 'selftext': '', 'author_fullname': 't2_2fu1z1g3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""It's Finally Settled"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ProgrammerHumor', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 77, 'hide_score': False, 'name': 't3_g9sx9v', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 440, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'reddit_video': {'fallback_url': 'https://v.redd.it/hvxqozsktlv41/DASH_240?source=fallback', 'height': 236, 'width': 426, 'scrubber_media_url': 'https://v.redd.it/hvxqozsktlv41/DASH_96', 'dash_url': 'https://v.redd.it/hvxqozsktlv41/DASHPlaylist.mpd', 'duration': 18, 'hls_url': 'https://v.redd.it/hvxqozsktlv41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 440, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/zwz2eTWZqVu3iVHqsbAGVo0U0vkDWtTt2_bt-ZiveV4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'hosted:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588128286.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'v.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?format=pjpg&amp;auto=webp&amp;s=da66153155f67ff3ba2ecdc1786c2b67343fc0d5', 'width': 636, 'height': 352}, 'resolutions': [{'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dffaefa3cbca2c0575e8fca43406bb5acd934939', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=107e5005c84fd91f5107642dcb94e85d688798a9', 'width': 216, 'height': 119}, {'url': 'https://external-preview.redd.it/x1f2eaoQ0fu16JUsMDpPWh40SU2wLOGp6s6241cEztw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f26f6fde79ed48b4ccb7ce31e849578ce2bfd17c', 'width': 320, 'height': 177}], 'variants': {}, 'id': 'JZoA8Xkw8jwnsVswFnBF5Ia4Vky_-L9vLo5VrU7m6fo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2tex6', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g9sx9v', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'SpecificTwo', 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ProgrammerHumor/comments/g9sx9v/its_finally_settled/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://v.redd.it/hvxqozsktlv41', 'subreddit_subscribers': 1178180, 'created_utc': 1588099486.0, 'num_crossposts': 4, 'media': {'reddit_video': {'fallback_url': 'https://v.redd.it/hvxqozsktlv41/DASH_240?source=fallback', 'height': 236, 'width': 426, 'scrubber_media_url': 'https://v.redd.it/hvxqozsktlv41/DASH_96', 'dash_url': 'https://v.redd.it/hvxqozsktlv41/DASHPlaylist.mpd', 'duration': 18, 'hls_url': 'https://v.redd.it/hvxqozsktlv41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}, 'is_video': True}]",t3_g9sx9v,,
,learnmachinelearning,"Sorry if it's a repost, I checked the wiki and couldn't find anything. So, I want to learn ML for making marketing and product decisions. Is there a course or curriculum that you guys know of? Or just maybe give me a plan what to do. I can't fully dabble is ML because my job doesn't permit me to. Any help would be greatly appreciated, thanks.

Edit: As naveen suggested - My primary outcomes would be churn prediction, revival prediction, life time value, lead scoring etc.",t2_6b8qq3ac,False,,0,False,Learn ML for marketing,[],r/learnmachinelearning,False,6,,0,,False,t3_gana3b,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1588229338.0,,[],{},,,True,,1588242748.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if it&amp;#39;s a repost, I checked the wiki and couldn&amp;#39;t find anything. So, I want to learn ML for making marketing and product decisions. Is there a course or curriculum that you guys know of? Or just maybe give me a plan what to do. I can&amp;#39;t fully dabble is ML because my job doesn&amp;#39;t permit me to. Any help would be greatly appreciated, thanks.&lt;/p&gt;

&lt;p&gt;Edit: As naveen suggested - My primary outcomes would be churn prediction, revival prediction, life time value, lead scoring etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gana3b,True,,deejayangochained,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gana3b/learn_ml_for_marketing/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gana3b/learn_ml_for_marketing/,155203,1588213948.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

Let's assume I have a data set of employees in finance. For these employees I have gender, age, role (e.g. engineer, trader, etc). Obviously these characteristics are not uniformly distributed (age will be skewed, gender will be skewed, for example).

I want to create a test set from this data set that is balanced in terms of gender, age, role, i.e. I want the distribution of age in the test set to be uniform, same for gender and same for role.

How would I go about doing this?Thanks!",t2_255ub203,False,,0,False,[Question] Sample balanced test set from imbalanced data set,[],r/learnmachinelearning,False,6,,0,,False,t3_gasmvz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588268478.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s assume I have a data set of employees in finance. For these employees I have gender, age, role (e.g. engineer, trader, etc). Obviously these characteristics are not uniformly distributed (age will be skewed, gender will be skewed, for example).&lt;/p&gt;

&lt;p&gt;I want to create a test set from this data set that is balanced in terms of gender, age, role, i.e. I want the distribution of age in the test set to be uniform, same for gender and same for role.&lt;/p&gt;

&lt;p&gt;How would I go about doing this?Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gasmvz,True,,raphael-lenain,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gasmvz/question_sample_balanced_test_set_from_imbalanced/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gasmvz/question_sample_balanced_test_set_from_imbalanced/,155203,1588239678.0,0,,False,,,,
,learnmachinelearning,"Lately, I have started learning about the application of DS in Finance. Created a sample demo showcasing both the metrics and comparing various stocks in parallel.  Check out at - [https://www.linkedin.com/posts/prakhar21\_investmentplanning-investments-datascience-activity-6661350518469861376-\_1u9](https://www.linkedin.com/posts/prakhar21_investmentplanning-investments-datascience-activity-6661350518469861376-_1u9)",t2_hkv9s,False,,0,False,Sortino and Sharpe Ratios - Data Science in Finance,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaq1o6,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},self,,True,,1588254876.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lately, I have started learning about the application of DS in Finance. Created a sample demo showcasing both the metrics and comparing various stocks in parallel.  Check out at - &lt;a href=""https://www.linkedin.com/posts/prakhar21_investmentplanning-investments-datascience-activity-6661350518469861376-_1u9""&gt;https://www.linkedin.com/posts/prakhar21_investmentplanning-investments-datascience-activity-6661350518469861376-_1u9&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5emeEhOJgrmtgkP2hzPPV4hWpamjh1WXcgZsYtj3wPg.jpg?auto=webp&amp;s=da2146815f40835a1967fd74bdbe839e5b92b2ba', 'width': 600, 'height': 421}, 'resolutions': [{'url': 'https://external-preview.redd.it/5emeEhOJgrmtgkP2hzPPV4hWpamjh1WXcgZsYtj3wPg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1bb13e156fcf042d53d23a2899e29c59ec461025', 'width': 108, 'height': 75}, {'url': 'https://external-preview.redd.it/5emeEhOJgrmtgkP2hzPPV4hWpamjh1WXcgZsYtj3wPg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a35eb3a2e9aead07103f1e40b1955e492c538b6b', 'width': 216, 'height': 151}, {'url': 'https://external-preview.redd.it/5emeEhOJgrmtgkP2hzPPV4hWpamjh1WXcgZsYtj3wPg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c7644d1234c1718ee6bf57cd08c143114e6d1f1', 'width': 320, 'height': 224}], 'variants': {}, 'id': 'iMcSYhZRUB4YSu52DEFXeFGdehB1tzUx6OfDFuCXf3c'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gaq1o6,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaq1o6/sortino_and_sharpe_ratios_data_science_in_finance/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaq1o6/sortino_and_sharpe_ratios_data_science_in_finance/,155203,1588226076.0,0,,False,,,,
,learnmachinelearning,"Hey all, so been training a WGAN for a while now.

Training it on three datasets, the first one produces terrible images no matter what I do with my model and hyper parameters. We assume this is because the database is too diverse in its features so generation just can't happen. Similar trends seen across dbs

&amp;#x200B;

As for losses, generator loss needs to start positive and eventually get more and more negative.

&amp;#x200B;

Discriminator loss needs to start negative and seek zero,

&amp;#x200B;

&amp;#x200B;

Now when I deepen the model, I find that the loss trends change drastically, losses that are expected to converge over epochs continue to diverge and take up heavy values. (10\^4) and this is for a fast learning rate of 0.0004. Images for 1/3 datasets are very good though. The other has some noticeable features, but are still very blurry and bad. 

&amp;#x200B;

I read up that learning rate has something to do with it, a diverging loss could mean that the learning rate is too high. So I used 0.00005 like in the paper. But the images are really bad for all three. They are noise with no features whatsoever. As for the losses, they still diverge, but oscillate less and the values are off a lower order. In the previous learning rate though. There looked to be some hope of convergence around epoch 275. But it never converged. Now that doesn't seem to happen.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

So TLDR:

&amp;#x200B;

&amp;#x200B;

Best images--- deeper model, fast learning rate, diverging losses. Some sign of converge. But by the time they converge, images are degrading. 

&amp;#x200B;

&amp;#x200B;

Worst images --- shallower model, slow learning rate, proper losses with drop and rises as expected. 

&amp;#x200B;

&amp;#x200B;

Any advice appreciated:

&amp;#x200B;

&amp;#x200B;

My current ideas:  

&amp;#x200B;

1. Find optimal learning rate for deeper network to converge losses faster? As there is some hope. Hopefully maintain image quality.

2. Or deepen network still more

&amp;#x200B;

3.  Or else change architecture itself. 

Currently generator has four strided conv2DT, and a conv2D output. No dropout or batch Norm

&amp;#x200B;

Critic has 3 strided conv and 1 non strided conv layer.  Dropout and BN",t2_1ckethps,False,,0,False,Loss Stability vs Image quality for a WGAN,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gascer,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588266935.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, so been training a WGAN for a while now.&lt;/p&gt;

&lt;p&gt;Training it on three datasets, the first one produces terrible images no matter what I do with my model and hyper parameters. We assume this is because the database is too diverse in its features so generation just can&amp;#39;t happen. Similar trends seen across dbs&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As for losses, generator loss needs to start positive and eventually get more and more negative.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Discriminator loss needs to start negative and seek zero,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Now when I deepen the model, I find that the loss trends change drastically, losses that are expected to converge over epochs continue to diverge and take up heavy values. (10^4) and this is for a fast learning rate of 0.0004. Images for 1/3 datasets are very good though. The other has some noticeable features, but are still very blurry and bad. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I read up that learning rate has something to do with it, a diverging loss could mean that the learning rate is too high. So I used 0.00005 like in the paper. But the images are really bad for all three. They are noise with no features whatsoever. As for the losses, they still diverge, but oscillate less and the values are off a lower order. In the previous learning rate though. There looked to be some hope of convergence around epoch 275. But it never converged. Now that doesn&amp;#39;t seem to happen.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;So TLDR:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Best images--- deeper model, fast learning rate, diverging losses. Some sign of converge. But by the time they converge, images are degrading. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Worst images --- shallower model, slow learning rate, proper losses with drop and rises as expected. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any advice appreciated:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;My current ideas:  &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Find optimal learning rate for deeper network to converge losses faster? As there is some hope. Hopefully maintain image quality.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Or deepen network still more&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; Or else change architecture itself. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently generator has four strided conv2DT, and a conv2D output. No dropout or batch Norm&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Critic has 3 strided conv and 1 non strided conv layer.  Dropout and BN&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gascer,True,,Transit-Strike,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gascer/loss_stability_vs_image_quality_for_a_wgan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gascer/loss_stability_vs_image_quality_for_a_wgan/,155203,1588238135.0,0,,False,,,,
,learnmachinelearning,"I am doing an online MS with Georgia Tech.  I started as a total noob in anything AI/RL related, but now I've taken 'Introduction to RL' (4 months), 'AI' (4 months as well, basically going over the entire Norvig and Russel book) and in the Fall I'll take ML.  

So far, it's been very interesting, but I can't pretend I am a  pro, and taking ML won't me one either.  I need to get to pro-level, or at least become efficient at solving all sorts of problems to be hireable.

Also, we've been asked to re-develop known algorithms from scratch, like k-means, random forest, EM,  pattern recognition with HMM etc, but never how to use the existing libraries, which is what most ML engineers do.  Correct me if I'm wrong.  

That's why, I'll take only one course, ML, in the Fall, so I can 'complete' it with real-life experience.  So my question is: where can I get this knowledge? Please don't tell me 'read a book'.  I'm looking for a course, with videos, explanations as to why we choose this algorithm instead of this one, why this library or method is better, Tensorflow vs Pytorch, code examples etc by an expert in the field.  

Thanks a lot.",t2_45flj,False,,0,False,What would be the next step after a few introduction classes to RL &amp; AI?,[],r/learnmachinelearning,False,6,,0,,False,t3_gasa0l,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588266583.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing an online MS with Georgia Tech.  I started as a total noob in anything AI/RL related, but now I&amp;#39;ve taken &amp;#39;Introduction to RL&amp;#39; (4 months), &amp;#39;AI&amp;#39; (4 months as well, basically going over the entire Norvig and Russel book) and in the Fall I&amp;#39;ll take ML.  &lt;/p&gt;

&lt;p&gt;So far, it&amp;#39;s been very interesting, but I can&amp;#39;t pretend I am a  pro, and taking ML won&amp;#39;t me one either.  I need to get to pro-level, or at least become efficient at solving all sorts of problems to be hireable.&lt;/p&gt;

&lt;p&gt;Also, we&amp;#39;ve been asked to re-develop known algorithms from scratch, like k-means, random forest, EM,  pattern recognition with HMM etc, but never how to use the existing libraries, which is what most ML engineers do.  Correct me if I&amp;#39;m wrong.  &lt;/p&gt;

&lt;p&gt;That&amp;#39;s why, I&amp;#39;ll take only one course, ML, in the Fall, so I can &amp;#39;complete&amp;#39; it with real-life experience.  So my question is: where can I get this knowledge? Please don&amp;#39;t tell me &amp;#39;read a book&amp;#39;.  I&amp;#39;m looking for a course, with videos, explanations as to why we choose this algorithm instead of this one, why this library or method is better, Tensorflow vs Pytorch, code examples etc by an expert in the field.  &lt;/p&gt;

&lt;p&gt;Thanks a lot.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gasa0l,True,,pikatchum,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gasa0l/what_would_be_the_next_step_after_a_few/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gasa0l/what_would_be_the_next_step_after_a_few/,155203,1588237783.0,0,,False,,,,
,learnmachinelearning,,t2_1bodl8op,False,,0,False,5 Tools To Create A Custom Object Detection Dataset,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_garq69,False,light,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},Discussion,False,1,,False,https://b.thumbs.redditmedia.com/0UUtIKYd2sVCQrhV5GcHrGBAtNhElDUt14f0fRIpBgw.jpg,False,,[],{},image,,False,,1588263563.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/xer0wg2szwv41.png?auto=webp&amp;s=4bb4a1dda0cd9870cd981f0a45408cfbb3e2b082', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c8bf83285ff2daba353d855224b207bdc8a39298', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23046a27672e9afe8799714adf1ec69e34620632', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=757c02a2c82fa54506de00f1458d563e67ee6993', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=010d01322de35407456bf8cb12edd6cb765e0ddd', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=af2773bb649405cb402904201ad48d34a1e8825f', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/xer0wg2szwv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=900f8eb18876672ea86400e9e4fa45c819a26298', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '4gx3adFiJMq_5Afu02yNViM9uFwj0zMv-vpi4aZafd8'}], 'enabled': True}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,garq69,True,,lekorotkov,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/garq69/5_tools_to_create_a_custom_object_detection/,all_ads,False,https://i.redd.it/xer0wg2szwv41.png,155203,1588234763.0,0,,False,,,,
,learnmachinelearning,"hello, wish to make a ""merged syntetized artificial"" picture out of a dataset of pictures of faces I have personally collected, possibly using stylegan2 on runway. Wich model should I use?",t2_10q2z0,False,,0,False,GANs on Runway,[],r/learnmachinelearning,False,6,,0,,False,t3_gaog6z,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588247535.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hello, wish to make a &amp;quot;merged syntetized artificial&amp;quot; picture out of a dataset of pictures of faces I have personally collected, possibly using stylegan2 on runway. Wich model should I use?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaog6z,True,,bodytexture,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaog6z/gans_on_runway/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaog6z/gans_on_runway/,155203,1588218735.0,0,,False,,,,
,learnmachinelearning,"Hello,

This is a problem I'm almost sure there is a term for which I don't know -- if somebody could point me in the direction of some literature (or just the term I need to Google) it'd be greatly appreciated. On to the problem:

I am tasked with building a model that will determine whether or not our business accepts a bet. The calculation is, in theory, simple enough: if predicted\_EV &gt; 0 , accept the bet -- and your problem boils down to building a model that estimates well the probability of winning the bet. 

Here's the hitch: while we have a ton of historical data on how bets we accepted in the past resulted, the domain is such that we have no idea whether we would have won or loss the bets we declined. In other words, in our dataset, we only know the outcomes of bets we actually accepted.

I'm wary of building a model using only the bets we've taken in the past because going forward we will also have to consider bets that don't match the distribution of bets we've historically accepted. Put another way, the distribution of data against which we want to make predictions in the future almost certainly differs from the distribution of data we have in our dataset. I'm worried that using only this data will introduce bias into the resulting model. What's a data scientist to do?

Thanks in advance!

rVE",t2_69ytb2ys,False,,0,False,How to deal with a partially realized target?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gamcix,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1588239062.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;This is a problem I&amp;#39;m almost sure there is a term for which I don&amp;#39;t know -- if somebody could point me in the direction of some literature (or just the term I need to Google) it&amp;#39;d be greatly appreciated. On to the problem:&lt;/p&gt;

&lt;p&gt;I am tasked with building a model that will determine whether or not our business accepts a bet. The calculation is, in theory, simple enough: if predicted_EV &amp;gt; 0 , accept the bet -- and your problem boils down to building a model that estimates well the probability of winning the bet. &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the hitch: while we have a ton of historical data on how bets we accepted in the past resulted, the domain is such that we have no idea whether we would have won or loss the bets we declined. In other words, in our dataset, we only know the outcomes of bets we actually accepted.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wary of building a model using only the bets we&amp;#39;ve taken in the past because going forward we will also have to consider bets that don&amp;#39;t match the distribution of bets we&amp;#39;ve historically accepted. Put another way, the distribution of data against which we want to make predictions in the future almost certainly differs from the distribution of data we have in our dataset. I&amp;#39;m worried that using only this data will introduce bias into the resulting model. What&amp;#39;s a data scientist to do?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;rVE&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gamcix,True,,raiseValueError,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gamcix/how_to_deal_with_a_partially_realized_target/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gamcix/how_to_deal_with_a_partially_realized_target/,155203,1588210262.0,0,,False,,,,
,learnmachinelearning,,t2_2i0qwoem,False,,0,False,[D] We are putting together a list of image segmentation tips and tricks. Anything we missed?,[],r/learnmachinelearning,False,6,,0,,False,t3_gaqooq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},link,,False,,1588258092.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?auto=webp&amp;s=7c7317b2827db8dc146e5405734723d6dd089b7f', 'width': 1093, 'height': 696}, 'resolutions': [{'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80d624b9027a6a35d65a8233d9ed1dd302a413d1', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65dc2546255f192f5ecfd5cb23ac2973f13b607c', 'width': 216, 'height': 137}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6c92011f2492fedeacd8ddd7334321f945dc3fb', 'width': 320, 'height': 203}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c39479432b7fa4f2a529695bf5aa8947bb4f69c', 'width': 640, 'height': 407}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b1754ced1701c8cf22c820982f0eea27d9dec0c', 'width': 960, 'height': 611}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2584fc2a49c1957ff261ebf1e3db428e07b68104', 'width': 1080, 'height': 687}], 'variants': {}, 'id': 'mEZQSqtMpGjAtXRthdDr6QuFsfQEbvMAMCkhQjpsQv0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaqooq,True,,ai_yoda,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaqooq/d_we_are_putting_together_a_list_of_image/,all_ads,False,/r/MachineLearning/comments/g1okir/d_we_are_putting_together_a_list_of_image/,155203,1588229292.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Hi all, \n\nWe are putting together a structured list of things to try when working on image segmentation problems. \n\nWe've already gone through 39 kaggle competitions and put whatever we could find in the discussions and kernels into [this article.](https://neptune.ai/blog/image-segmentation-tips-and-tricks-from-kaggle-competitions?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-image-segmentation-tips-and-tricks-from-kaggle-competitions)\n\nIt is structured into:\n\n* External Data \n* Preprocessing \n* Data Augmentations \n* Modeling \n* Hardware Setups \n* Loss Functions \n* Training Tips \n* Evaluation and Cross-validation \n* Ensembling Methods \n* Post Processing \n\nAnything important that could be added to this from your experience working with image segmentation problems both in the competition, research and industry-projects perspective?"", 'author_fullname': 't2_2i0qwoem', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] We are putting together a list of image segmentation tips and tricks. Anything we missed?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g1okir', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 181, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 181, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1586971066.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;We are putting together a structured list of things to try when working on image segmentation problems. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve already gone through 39 kaggle competitions and put whatever we could find in the discussions and kernels into &lt;a href=""https://neptune.ai/blog/image-segmentation-tips-and-tricks-from-kaggle-competitions?utm_source=reddit&amp;amp;utm_medium=post&amp;amp;utm_campaign=blog-image-segmentation-tips-and-tricks-from-kaggle-competitions""&gt;this article.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is structured into:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;External Data &lt;/li&gt;\n&lt;li&gt;Preprocessing &lt;/li&gt;\n&lt;li&gt;Data Augmentations &lt;/li&gt;\n&lt;li&gt;Modeling &lt;/li&gt;\n&lt;li&gt;Hardware Setups &lt;/li&gt;\n&lt;li&gt;Loss Functions &lt;/li&gt;\n&lt;li&gt;Training Tips &lt;/li&gt;\n&lt;li&gt;Evaluation and Cross-validation &lt;/li&gt;\n&lt;li&gt;Ensembling Methods &lt;/li&gt;\n&lt;li&gt;Post Processing &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anything important that could be added to this from your experience working with image segmentation problems both in the competition, research and industry-projects perspective?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?auto=webp&amp;s=7c7317b2827db8dc146e5405734723d6dd089b7f', 'width': 1093, 'height': 696}, 'resolutions': [{'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80d624b9027a6a35d65a8233d9ed1dd302a413d1', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65dc2546255f192f5ecfd5cb23ac2973f13b607c', 'width': 216, 'height': 137}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6c92011f2492fedeacd8ddd7334321f945dc3fb', 'width': 320, 'height': 203}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c39479432b7fa4f2a529695bf5aa8947bb4f69c', 'width': 640, 'height': 407}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6b1754ced1701c8cf22c820982f0eea27d9dec0c', 'width': 960, 'height': 611}, {'url': 'https://external-preview.redd.it/FKecaMbELDhTf2eERvO3pq2v9ysug6JFsZ16nHTtUic.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2584fc2a49c1957ff261ebf1e3db428e07b68104', 'width': 1080, 'height': 687}], 'variants': {}, 'id': 'mEZQSqtMpGjAtXRthdDr6QuFsfQEbvMAMCkhQjpsQv0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g1okir', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ai_yoda', 'discussion_type': None, 'num_comments': 30, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/g1okir/d_we_are_putting_together_a_list_of_image/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/g1okir/d_we_are_putting_together_a_list_of_image/', 'subreddit_subscribers': 1044415, 'created_utc': 1586942266.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_g1okir,,
,learnmachinelearning,,t2_bmgw35h,False,,0,False,Predicting baseball games with a LSTM,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_ganf50,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/crHGHPNAi12hD0fbt3tw-pvC8k_25cqHmWhjKWauk30.jpg,False,,[],{},link,,False,,1588243323.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/0Qt61ONjEiy-T-09_zZndnH43IdPyKpHxM1eHZhiZJM.jpg?auto=webp&amp;s=6274514ffb7a5ae84a45821998f03734a3210086', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/0Qt61ONjEiy-T-09_zZndnH43IdPyKpHxM1eHZhiZJM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a7779b3c8adfeb304369f393d88a814cee9a707', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/0Qt61ONjEiy-T-09_zZndnH43IdPyKpHxM1eHZhiZJM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63c7a1fd76ddcd8a85d002fa05d928a847f4ce3d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/0Qt61ONjEiy-T-09_zZndnH43IdPyKpHxM1eHZhiZJM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1dbef382f46406804170ad99f6b82817cc7e2a62', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Llg14TkrlLarlLClfVgvIPllEZmN--Cfoj9NbEQSRng'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ganf50,True,,AllWashedOut,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ganf50/predicting_baseball_games_with_a_lstm/,all_ads,False,https://github.com/AllWashedOut/MachineBall,155203,1588214523.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,"From CVPR: Reconstruct photorealistic 3D faces from a single ""in-the-wild"" image with an increasing level of detail",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gan4a3,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/qsLusCEGBFBsat94GMUriHVG3c52xT_aka3RSqwlcBc.jpg,False,,[],{},link,,False,,1588242107.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?auto=webp&amp;s=84a41b9f1b3b1a9fe41397dcb8aa32f50b2d5681', 'width': 626, 'height': 172}, 'resolutions': [{'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=88d2626872e3af7d971576c490e2e764cf81ed73', 'width': 108, 'height': 29}, {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76904a2463f1418dc4eb8ea8e2b1754e62811435', 'width': 216, 'height': 59}, {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=148632a81bdba405226bb9e56e965d1b4b96cb1a', 'width': 320, 'height': 87}], 'variants': {}, 'id': 'Vbwvosk31O2HRGt_D3v0wq5w7YE4fsbr1dU8r8n1E5U'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gan4a3,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gan4a3/from_cvpr_reconstruct_photorealistic_3d_faces/,all_ads,False,/r/LatestInML/comments/gamxtp/from_cvpr_reconstruct_photorealistic_3d_faces/,155203,1588213307.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'From CVPR: Reconstruct photorealistic 3D faces from a single ""in-the-wild"" image with an increasing level of detail \n\nProject and code/requests: [click here](https://www.catalyzex.com/paper/arxiv:2003.13845)\n\n&amp;#x200B;\n\nhttps://i.redd.it/f2xaia9w5vv41.gif\n\nAvatarMe outperforms the existing arts by a significant margin and reconstructs authentic, 4K by 6K-resolution 3D faces from a single low-resolution image that, for the first time, bridges the uncanny valley.', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From CVPR: Reconstruct photorealistic 3D faces from a single ""in-the-wild"" image with an increasing level of detail', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 78, 'hide_score': False, 'media_metadata': {'f2xaia9w5vv41': {'status': 'valid', 'e': 'AnimatedImage', 'm': 'image/gif', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/f2xaia9w5vv41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0f7b6c41a0cba4fbec9a5501d342df38f38ad173'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/f2xaia9w5vv41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=6f2c8252ff520782ba940a87d61751315869d860'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/f2xaia9w5vv41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=407600ccdb5140dfd8433f616ebdb62681cf4ba9'}], 's': {'y': 293, 'gif': 'https://i.redd.it/f2xaia9w5vv41.gif', 'mp4': 'https://preview.redd.it/f2xaia9w5vv41.gif?format=mp4&amp;s=b6a0e1b961e379d108286c3914d0f35c96456b16', 'x': 520}, 'id': 'f2xaia9w5vv41'}}, 'name': 't3_gamxtp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 29, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/qsLusCEGBFBsat94GMUriHVG3c52xT_aka3RSqwlcBc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588241383.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From CVPR: Reconstruct photorealistic 3D faces from a single &amp;quot;in-the-wild&amp;quot; image with an increasing level of detail &lt;/p&gt;\n\n&lt;p&gt;Project and code/requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.13845""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://i.redd.it/f2xaia9w5vv41.gif""&gt;https://i.redd.it/f2xaia9w5vv41.gif&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;AvatarMe outperforms the existing arts by a significant margin and reconstructs authentic, 4K by 6K-resolution 3D faces from a single low-resolution image that, for the first time, bridges the uncanny valley.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?auto=webp&amp;s=84a41b9f1b3b1a9fe41397dcb8aa32f50b2d5681', 'width': 626, 'height': 172}, 'resolutions': [{'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=88d2626872e3af7d971576c490e2e764cf81ed73', 'width': 108, 'height': 29}, {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=76904a2463f1418dc4eb8ea8e2b1754e62811435', 'width': 216, 'height': 59}, {'url': 'https://external-preview.redd.it/TdAcwBZWIoxXbm79oC4V4yiXIXlpaP4SFawKU_u2EZ0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=148632a81bdba405226bb9e56e965d1b4b96cb1a', 'width': 320, 'height': 87}], 'variants': {}, 'id': 'Vbwvosk31O2HRGt_D3v0wq5w7YE4fsbr1dU8r8n1E5U'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gamxtp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gamxtp/from_cvpr_reconstruct_photorealistic_3d_faces/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gamxtp/from_cvpr_reconstruct_photorealistic_3d_faces/', 'subreddit_subscribers': 3386, 'created_utc': 1588212583.0, 'num_crossposts': 10, 'media': None, 'is_video': False}]",t3_gamxtp,,
,learnmachinelearning,,t2_2ic7kgpw,False,,0,False,Django - The Easy Way &amp; ML Models Deployment using DRF,[],r/learnmachinelearning,False,6,,0,63.0,False,t3_gaq64g,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/GyrfFLAmvQ0zyl_HN_cxFcpbRLNP7WQjKuXnB3MF6G8.jpg,False,,[],{},image,,False,,1588255481.0,text,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/hi7p0uisbwv41.png?auto=webp&amp;s=fc7f37b443bd91626b1977e3783f3f59223fb2fe', 'width': 881, 'height': 401}, 'resolutions': [{'url': 'https://preview.redd.it/hi7p0uisbwv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e30603786f6e5ab5182b4e76772dbe2abf04eedb', 'width': 108, 'height': 49}, {'url': 'https://preview.redd.it/hi7p0uisbwv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=496673617233b9bec66b881fe610d34353d5df5d', 'width': 216, 'height': 98}, {'url': 'https://preview.redd.it/hi7p0uisbwv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=744294053f846759432f4682009c5f14654ea799', 'width': 320, 'height': 145}, {'url': 'https://preview.redd.it/hi7p0uisbwv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c18788d127afeb79ca3dfcd8023214c9342620c3', 'width': 640, 'height': 291}], 'variants': {}, 'id': '3bacSfYpUFrRsZWL63jOa3FpU7VFNwdaxEjeMwn5PQs'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaq64g,True,,gauravlogical,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaq64g/django_the_easy_way_ml_models_deployment_using_drf/,all_ads,False,https://i.redd.it/hi7p0uisbwv41.png,155203,1588226681.0,0,,False,,,,
,learnmachinelearning,I have a CNN that can classify images from Reddit but I want to pass the image caption as well because I want to use this to help predict the upvotes on a post. I'm using a post's upvote count after a day but I'm not sure how to connect my image classifier with a caption to output a number (upvotes).,t2_r3kku,False,,0,False,Is there a way to pass an image caption and image to a NN in Tensorflow?,[],r/learnmachinelearning,False,6,,0,,False,t3_gamwr1,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588241262.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a CNN that can classify images from Reddit but I want to pass the image caption as well because I want to use this to help predict the upvotes on a post. I&amp;#39;m using a post&amp;#39;s upvote count after a day but I&amp;#39;m not sure how to connect my image classifier with a caption to output a number (upvotes).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gamwr1,True,,NYGooner17,,3,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gamwr1/is_there_a_way_to_pass_an_image_caption_and_image/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gamwr1/is_there_a_way_to_pass_an_image_caption_and_image/,155203,1588212462.0,0,,False,,,,
,learnmachinelearning,,t2_76phz,False,,0,False,Sound classification with Processing and Wekinator,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,87.0,False,t3_gag20k,False,light,1.0,,public,7,0,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F276021078%3Fapp_id%3D122963&amp;dntp=1&amp;display_name=Vimeo&amp;url=https%3A%2F%2Fvimeo.com%2F276021078&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F708445220_1280.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=vimeo"" width=""600"" height=""375"" scrolling=""no"" title=""Vimeo embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 375}",140.0,,False,[],"{'oembed': {'provider_url': 'https://vimeo.com/', 'description': ""I've created a sound classification tool for the Processing environment that can make live spectrograms of the microphone input, and send the pixel array to Wekinator through OSC. I included controls to easily change analytical parameters such as frequency spectrum, thresholds, and the temporal aspect."", 'title': 'Sound classification with Processing and Wekinator', 'author_name': 'Tore Knudsen', 'height': 375, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F276021078%3Fapp_id%3D122963&amp;dntp=1&amp;display_name=Vimeo&amp;url=https%3A%2F%2Fvimeo.com%2F276021078&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F708445220_1280.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=vimeo"" width=""600"" height=""375"" scrolling=""no"" title=""Vimeo embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 1280, 'version': '1.0', 'provider_name': 'Vimeo', 'thumbnail_url': 'https://i.vimeocdn.com/video/708445220_1280.jpg', 'type': 'video', 'thumbnail_height': 800, 'author_url': 'https://vimeo.com/user45543231'}, 'type': 'vimeo.com'}",False,False,,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F276021078%3Fapp_id%3D122963&amp;dntp=1&amp;display_name=Vimeo&amp;url=https%3A%2F%2Fvimeo.com%2F276021078&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F708445220_1280.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=vimeo"" width=""600"" height=""375"" scrolling=""no"" title=""Vimeo embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gag20k', 'height': 375}",Project,False,7,,False,https://b.thumbs.redditmedia.com/lHrHCnDPKMqrUMBlzvTluLuwK_Wa6wArxgMMTunfDcU.jpg,False,,[],{},rich:video,,False,,1588217920.0,richtext,6,,,text,vimeo.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?auto=webp&amp;s=af1d4771c69c6e33e8b5848981fb4f2767627cf1', 'width': 1280, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fdb33926e896bb4b069b2887edcbbd4db88fb2b', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=60795c9bd5bb295b1c957335e77097ffbaea8f61', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cf6922d4faec73032e9005fa0fd6065f6a646f4', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=22bd32b9e189f3e72e8f7aa807897c13d6621bba', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b6d44ba0713f31e089b53c5e4a9760870816dc48', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/GlwxKIe-_nOWobjuTn1xONGbzJMbPw0Big1vpzcPWnw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=549115fdd16ca51991cb26f05eeb6804b1666ed9', 'width': 1080, 'height': 675}], 'variants': {}, 'id': 'b6dQlR4XdI1fL1gr3wEaCoQvsRf1MPJwigwxkxPqS_c'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gag20k,True,,dackdel,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gag20k/sound_classification_with_processing_and_wekinator/,all_ads,False,https://vimeo.com/276021078,155203,1588189120.0,0,"{'oembed': {'provider_url': 'https://vimeo.com/', 'description': ""I've created a sound classification tool for the Processing environment that can make live spectrograms of the microphone input, and send the pixel array to Wekinator through OSC. I included controls to easily change analytical parameters such as frequency spectrum, thresholds, and the temporal aspect."", 'title': 'Sound classification with Processing and Wekinator', 'author_name': 'Tore Knudsen', 'height': 375, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F276021078%3Fapp_id%3D122963&amp;dntp=1&amp;display_name=Vimeo&amp;url=https%3A%2F%2Fvimeo.com%2F276021078&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F708445220_1280.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=vimeo"" width=""600"" height=""375"" scrolling=""no"" title=""Vimeo embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 1280, 'version': '1.0', 'provider_name': 'Vimeo', 'thumbnail_url': 'https://i.vimeocdn.com/video/708445220_1280.jpg', 'type': 'video', 'thumbnail_height': 800, 'author_url': 'https://vimeo.com/user45543231'}, 'type': 'vimeo.com'}",False,,,,
,learnmachinelearning,Item assignment is not possible in tensorflow. Due to that it seems impossible to implement NLL_loss in tensorflow. Can anybody help me with it.,t2_66zfw4cu,False,,0,False,NLL loss implementation in tensorflow,[],r/learnmachinelearning,False,6,,0,,False,t3_gapimr,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588252330.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Item assignment is not possible in tensorflow. Due to that it seems impossible to implement NLL_loss in tensorflow. Can anybody help me with it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gapimr,True,,mrrohitarora,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gapimr/nll_loss_implementation_in_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gapimr/nll_loss_implementation_in_tensorflow/,155203,1588223530.0,0,,False,,,,
,learnmachinelearning," 

Machine Learning Certification Course by Simplilearn

Machine Learning Specialty by A Cloud Guru (Not much review online)

Machine Learning Training by Edureka

Introduction to Data Science by Metis",t2_2g01fz76,False,,0,False,Which course to pick?,[],r/learnmachinelearning,False,6,,0,,False,t3_gap5id,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588250605.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Machine Learning Certification Course by Simplilearn&lt;/p&gt;

&lt;p&gt;Machine Learning Specialty by A Cloud Guru (Not much review online)&lt;/p&gt;

&lt;p&gt;Machine Learning Training by Edureka&lt;/p&gt;

&lt;p&gt;Introduction to Data Science by Metis&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gap5id,True,,tariksalay,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gap5id/which_course_to_pick/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gap5id/which_course_to_pick/,155203,1588221805.0,0,,False,,,,
,learnmachinelearning,"Hi, I'm soliciting any reactions to Databricks-managed MLflow. I love the idea of managed everything, and Databricks built MLflow, so that seems like a no-brainer. But as far as I can tell, you need to have a running cluster to do that, and that sounds expensive. I basically want to run the  models/mlflow on my local machine or AWS, and only use Databricks for storing model information, hosting the UI, etc. Is it possible to use  MLflow in this way and avoid having a running cluster? If not, thoughts  around managed MLflow on other platforms? I'd note there are other  platforms that seem interesting like [comet.ml](https://comet.ml/), Weights and Balances, etc., but MLflow seems really solid.",t2_11zpdr,False,,0,False,Managed MLFlow on Databricks without spinning up a cluster?,[],r/learnmachinelearning,False,6,,0,,False,t3_galm14,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588236211.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m soliciting any reactions to Databricks-managed MLflow. I love the idea of managed everything, and Databricks built MLflow, so that seems like a no-brainer. But as far as I can tell, you need to have a running cluster to do that, and that sounds expensive. I basically want to run the  models/mlflow on my local machine or AWS, and only use Databricks for storing model information, hosting the UI, etc. Is it possible to use  MLflow in this way and avoid having a running cluster? If not, thoughts  around managed MLflow on other platforms? I&amp;#39;d note there are other  platforms that seem interesting like &lt;a href=""https://comet.ml/""&gt;comet.ml&lt;/a&gt;, Weights and Balances, etc., but MLflow seems really solid.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,galm14,True,,cents_less,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/galm14/managed_mlflow_on_databricks_without_spinning_up/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/galm14/managed_mlflow_on_databricks_without_spinning_up/,155203,1588207411.0,0,,False,,,,
,learnmachinelearning,"[https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd)

Outline:

\- Introduction to reinforcement learning and its framework  
\- RL solutions: model-based methods  
\- RL solutions: model-free methods  
\- Deep reinforcement learning  
\- Real-world applications: Alpha Go, Self-driving cars, Robotics, finance, etc.",t2_5gts1ok8,False,,0,False,Free Online Talk | Reinforcement Learning Explained: Overview and Applications,[],r/learnmachinelearning,False,6,,0,,False,t3_gagt87,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588220312.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd""&gt;https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Outline:&lt;/p&gt;

&lt;p&gt;- Introduction to reinforcement learning and its framework&lt;br/&gt;
- RL solutions: model-based methods&lt;br/&gt;
- RL solutions: model-free methods&lt;br/&gt;
- Deep reinforcement learning&lt;br/&gt;
- Real-world applications: Alpha Go, Self-driving cars, Robotics, finance, etc.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?auto=webp&amp;s=465fd5b4d6443da449eaaed2b7f45245f723b324', 'width': 1000, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77383e5a0bd859336a8bdae0c997860aac3f91ce', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83dfbf3b061aabc85e58d3267846ac3a837b61fe', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2457b14ca8d91942f59db7c6f3a0f611cadb908c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5596ce1acb73972e634b1e2f560c1146f1eff6af', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/pAF7jYzPyG84SRJkXNLb57rLpq82SrdpBBL34y9QQVs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f761aa4f874c21216e513c8cb138d7d70c6219b', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'ri2Xrtm_i9JLtFEIuCrmM46dVbtVpX1-r-w3UzZX-ng'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gagt87,True,,oyolim,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gagt87/free_online_talk_reinforcement_learning_explained/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gagt87/free_online_talk_reinforcement_learning_explained/,155203,1588191512.0,0,,False,,,,
,learnmachinelearning,"So I was building a diagnostic classification model to understand behavioural pattern of shoppers and have about 500 odd features, I have around 500k users to go along with them.

The decision to build a decision tree was because I wanted to visualize how each feature split impact the class distribution. This information is useful in another process to make decisions.

Now obviously with 500 features it will be hard to do this, so will use hyper parameterd like minimum number of samples in a class and max tree depth.

Still I feel the huge number of features can cause issues, the alternative could be to use important variables from a rf output and feed it to the decision tree.

So any thoughts?",t2_6ys5mu5,False,,0,False,Impact of large number of continuous features on decision tree,[],r/learnmachinelearning,False,6,,0,,False,t3_gac7uf,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1588205163.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I was building a diagnostic classification model to understand behavioural pattern of shoppers and have about 500 odd features, I have around 500k users to go along with them.&lt;/p&gt;

&lt;p&gt;The decision to build a decision tree was because I wanted to visualize how each feature split impact the class distribution. This information is useful in another process to make decisions.&lt;/p&gt;

&lt;p&gt;Now obviously with 500 features it will be hard to do this, so will use hyper parameterd like minimum number of samples in a class and max tree depth.&lt;/p&gt;

&lt;p&gt;Still I feel the huge number of features can cause issues, the alternative could be to use important variables from a rf output and feed it to the decision tree.&lt;/p&gt;

&lt;p&gt;So any thoughts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gac7uf,True,,user19911506,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gac7uf/impact_of_large_number_of_continuous_features_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gac7uf/impact_of_large_number_of_continuous_features_on/,155203,1588176363.0,0,,False,,,,
,learnmachinelearning,,t2_167nx2qq,False,,0,False,How an AI Learned to Dominate a Virtual Market,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_ga8wf8,False,dark,0.9,,public,8,0,{},140.0,,False,[],,False,False,,{},,False,8,,False,https://a.thumbs.redditmedia.com/MuQIbTEzevYfSM9pDdqdThKppZi_vzd3AQqD9idFm90.jpg,False,,[],{},link,,False,,1588193655.0,text,6,,,text,theaimango.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?auto=webp&amp;s=340d5ff2ddef2c77ec2c670aff13e9899033d3aa', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3358838c7aaa0d04ea0d92c4f2ec4fd7d7a1328f', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b9bfe4a29e0c27afb6601917478c91fb3b2cd50b', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d7fd523937bb0168eeacfb644d4abe5d9d94fb6', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3eb50e4017638ce4e5708232d2da6914e67b10d', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fdfeb3b14aca70e8cec67f6eb827c9b8bc92869', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/9rAlXd9pm1NKYH52kBwTWsKMUvjsaE_t5mJYEzQfbN4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a5ed08555ef80d78ba9b85db8454817f12bba3e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'DzYch2HVr9U6ja2AGzY1Is3ePx4-DZ9x0tC-upWydI4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga8wf8,True,,mrsailor23,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga8wf8/how_an_ai_learned_to_dominate_a_virtual_market/,all_ads,False,https://theaimango.com/how-an-ai-learned-to-dominate-a-virtual-market/,155203,1588164855.0,0,,False,,,,
,learnmachinelearning,"I've been trying to train a GAN to generate some images, but most of the time when I start training after a few adversarial epochs, for both the Generator and Discriminator, the accuracy goes to 100% and the loss goes to \~0 (including on validation set). After checking the 2 models results, it looks like the discriminator is correctly getting 100% of the images labeled correctly, so the issue seems to be with the generator. Anyone know what the issue could be. 

I tried adding noise to the training labels (which seems to work some), but metrics (acc, TruePos, TrueNeg, FalsePos, FalseNeg) start to think that all the labels = 1 (since 0 labels become \[0, 0.1\]) so the accuracy is always 0 for the discriminator even when its working.

model is built with tensorflow 2.1 keras library

Loss: Binary Cross-entropy

&amp;#x200B;

https://preview.redd.it/wk0qdu46cuv41.png?width=2388&amp;format=png&amp;auto=webp&amp;s=cc37e0c7f849a138fefe3856e591c06a82e02d6f",t2_6umxs,False,,0,False,[GAN] Generator and Discriminator both have ~0 loss and 100% accuracy,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,35.0,False,t3_gakbo9,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},HELP,False,0,,False,https://b.thumbs.redditmedia.com/yO6LFcsRbHaLrHx3wQvqVHk2d34ETzfgwt4192ClvwE.jpg,False,,[],{},,,True,,1588231650.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been trying to train a GAN to generate some images, but most of the time when I start training after a few adversarial epochs, for both the Generator and Discriminator, the accuracy goes to 100% and the loss goes to ~0 (including on validation set). After checking the 2 models results, it looks like the discriminator is correctly getting 100% of the images labeled correctly, so the issue seems to be with the generator. Anyone know what the issue could be. &lt;/p&gt;

&lt;p&gt;I tried adding noise to the training labels (which seems to work some), but metrics (acc, TruePos, TrueNeg, FalsePos, FalseNeg) start to think that all the labels = 1 (since 0 labels become [0, 0.1]) so the accuracy is always 0 for the discriminator even when its working.&lt;/p&gt;

&lt;p&gt;model is built with tensorflow 2.1 keras library&lt;/p&gt;

&lt;p&gt;Loss: Binary Cross-entropy&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/wk0qdu46cuv41.png?width=2388&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc37e0c7f849a138fefe3856e591c06a82e02d6f""&gt;https://preview.redd.it/wk0qdu46cuv41.png?width=2388&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc37e0c7f849a138fefe3856e591c06a82e02d6f&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gakbo9,True,,slothalot,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gakbo9/gan_generator_and_discriminator_both_have_0_loss/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gakbo9/gan_generator_and_discriminator_both_have_0_loss/,155203,1588202850.0,0,,False,,,"{'wk0qdu46cuv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 27, 'x': 108, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1579a743e3f8227ef0e4b48e568a9a4c4044f7c'}, {'y': 54, 'x': 216, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac7aed55c776b8656f3572993c61bedb5b786825'}, {'y': 80, 'x': 320, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6736b662fd078f1a7820161e0d5bbfaa7de6b4e'}, {'y': 161, 'x': 640, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8d65bb59949bfad8c6f1c54a13bc4f93d53dcfc'}, {'y': 242, 'x': 960, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c272228d343fec28424ebd6e5004bf18c18c6132'}, {'y': 272, 'x': 1080, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a262311b8e08cab2364ca3dfbe2ced04bbd3265'}], 's': {'y': 602, 'x': 2388, 'u': 'https://preview.redd.it/wk0qdu46cuv41.png?width=2388&amp;format=png&amp;auto=webp&amp;s=cc37e0c7f849a138fefe3856e591c06a82e02d6f'}, 'id': 'wk0qdu46cuv41'}}",
,learnmachinelearning,"I am training a NN in keras for the first time. I have train data of Titanic dataset from kaggle with
`shape=(712,15)`

What should be the input shape in
`keras.layers.Flatten(input_shape=(?,?))`

I do not seem to get the idea after reading online.",t2_28mfwh9d,False,,0,False,I need help with this keras hyperparameter,[],r/learnmachinelearning,False,6,,0,,False,t3_gafy6t,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588217583.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am training a NN in keras for the first time. I have train data of Titanic dataset from kaggle with
&lt;code&gt;shape=(712,15)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;What should be the input shape in
&lt;code&gt;keras.layers.Flatten(input_shape=(?,?))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I do not seem to get the idea after reading online.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gafy6t,True,,long_legged_nerd,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gafy6t/i_need_help_with_this_keras_hyperparameter/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gafy6t/i_need_help_with_this_keras_hyperparameter/,155203,1588188783.0,0,,False,,,,
,learnmachinelearning,"I have reached so far but I dont know how to save these comments into a plain HTML file or any else format? guide me please

https://preview.redd.it/l80saiwkauv41.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d83fb0ac247eb01d4eb6e72d61bd8dcb66e942df",t2_1keb5vxs,False,,0,False,Fetch Comments,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_gak3pw,False,dark,0.33,,public,0,0,{},140.0,,False,[],,False,False,,{},Request,False,0,,False,https://b.thumbs.redditmedia.com/ix1IfxP58nvdQ_mE_irP1yIZYISzV3e3HmuRo07z4jc.jpg,False,,[],{},,,True,,1588230874.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have reached so far but I dont know how to save these comments into a plain HTML file or any else format? guide me please&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/l80saiwkauv41.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d83fb0ac247eb01d4eb6e72d61bd8dcb66e942df""&gt;https://preview.redd.it/l80saiwkauv41.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d83fb0ac247eb01d4eb6e72d61bd8dcb66e942df&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gak3pw,True,,ionezation,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gak3pw/fetch_comments/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gak3pw/fetch_comments/,155203,1588202074.0,0,,False,,,"{'l80saiwkauv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea590b32d604a7d190950ae1f968837fc23dffdf'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1eea2a9a4ff0d9752c8ebea1c436cfb59f4adc95'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abd8e31ccda960425e2d8a41ab6f33c3fde8de8d'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cf2f9c550e3e5f3539543445d7baab047b1725f'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c087d36d107e2e6e1da00d23307124b86d335a31'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02239dfc1c54a657bc932fb8fb65eeed95ca3d03'}], 's': {'y': 1080, 'x': 1920, 'u': 'https://preview.redd.it/l80saiwkauv41.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d83fb0ac247eb01d4eb6e72d61bd8dcb66e942df'}, 'id': 'l80saiwkauv41'}}",
,learnmachinelearning,Ive sunk days into troubleshooting an installation. Nothing has worked.,,False,,0,False,Are there any foolproof ways to install opencv-CUDA-gpu on Win10?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaj8g6,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,,self,False,,,{},,,True,,1588227936.0,richtext,6,,,,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Ive sunk days into troubleshooting an installation. Nothing has worked.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gaj8g6,True,,[deleted],,3,True,all_ads,False,[],,dark,/r/learnmachinelearning/comments/gaj8g6/are_there_any_foolproof_ways_to_install/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaj8g6/are_there_any_foolproof_ways_to_install/,155203,1588199136.0,0,,False,,,,
,learnmachinelearning,,t2_60xhu,False,,0,False,Train an AI to judge the winner of a marble race!,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaceue,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},Project,False,3,,False,default,False,,[],{},,,False,,1588205851.0,richtext,6,,,text,steamlabs.ca,False,,,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gaceue,True,,DancesWithWhales,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaceue/train_an_ai_to_judge_the_winner_of_a_marble_race/,all_ads,False,https://steamlabs.ca/train-ai-judge/,155203,1588177051.0,0,,False,,,,
,learnmachinelearning,"I confess that I can't do data wrangling without looking at Pandas cheat sheets. And there are quite a few good ones:

* [Data Wrangling](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) by [pandas.pydata.org](https://pandas.pydata.org)
* [Basics](http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3) and [Pandas](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf) by [DataCamp.com](https://DataCamp.com)
* [Data Analysis](http://www.datasciencefree.com/pandas.pdf) by [DataScienceFree.com](https://DataScienceFree.com)

What is your favourite Pandas cheat sheet, that is if you need one :-)  


Updates:

* A gem u/Phaar18 shared in comments: [Plotting with Pandas Series and DataFrames](https://www.enthought.com/wp-content/uploads/Enthought-Python-Pandas-Cheat-Sheets-1-8-v1.0.2.pdf) from [enthought.com](https://enthought.com)",t2_32upp8wh,False,,0,False,Pandas Cheat Sheets,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9pwys,False,light,0.99,,public,254,0,{},,,False,[],,False,False,,{},Discussion,False,254,,False,self,1588127457.0,,[],{},,,True,,1588118620.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I confess that I can&amp;#39;t do data wrangling without looking at Pandas cheat sheets. And there are quite a few good ones:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=""https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf""&gt;Data Wrangling&lt;/a&gt; by &lt;a href=""https://pandas.pydata.org""&gt;pandas.pydata.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""http://datacamp-community-prod.s3.amazonaws.com/dbed353d-2757-4617-8206-8767ab379ab3""&gt;Basics&lt;/a&gt; and &lt;a href=""https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Pandas_Cheat_Sheet_2.pdf""&gt;Pandas&lt;/a&gt; by &lt;a href=""https://DataCamp.com""&gt;DataCamp.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""http://www.datasciencefree.com/pandas.pdf""&gt;Data Analysis&lt;/a&gt; by &lt;a href=""https://DataScienceFree.com""&gt;DataScienceFree.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What is your favourite Pandas cheat sheet, that is if you need one :-)  &lt;/p&gt;

&lt;p&gt;Updates:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A gem &lt;a href=""/u/Phaar18""&gt;u/Phaar18&lt;/a&gt; shared in comments: &lt;a href=""https://www.enthought.com/wp-content/uploads/Enthought-Python-Pandas-Cheat-Sheets-1-8-v1.0.2.pdf""&gt;Plotting with Pandas Series and DataFrames&lt;/a&gt; from &lt;a href=""https://enthought.com""&gt;enthought.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9pwys,True,,satishcgupta,,20,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9pwys/pandas_cheat_sheets/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9pwys/pandas_cheat_sheets/,155203,1588089820.0,0,,False,,,,
,learnmachinelearning,For creating DeepFakes you need about 2000 images per person. So why does it take so long to train? Is the model capacity so big?,t2_16znoi5m,False,,0,False,Why do DeepFakes train so long?,[],r/learnmachinelearning,False,6,,0,,False,t3_gaikzi,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588225839.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For creating DeepFakes you need about 2000 images per person. So why does it take so long to train? Is the model capacity so big?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaikzi,True,,Hollowax,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaikzi/why_do_deepfakes_train_so_long/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaikzi/why_do_deepfakes_train_so_long/,155203,1588197039.0,0,,False,,,,
,learnmachinelearning,"Hi, 

To work on this [https://pypi.org/project/youtube-sentiment](https://pypi.org/project/youtube-sentiment) ... to get the sentiment analysis of the youtube comments ... Do I need Rest Framework in Django or I can directly use YT APIs for fetching comments? Please guide .. I am making a AI ML model in Django that analyze the YT comments from the given link in the text box.",t2_1keb5vxs,False,,0,False,Youtube Sentiment analysis,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gahzdg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588223944.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;To work on this &lt;a href=""https://pypi.org/project/youtube-sentiment""&gt;https://pypi.org/project/youtube-sentiment&lt;/a&gt; ... to get the sentiment analysis of the youtube comments ... Do I need Rest Framework in Django or I can directly use YT APIs for fetching comments? Please guide .. I am making a AI ML model in Django that analyze the YT comments from the given link in the text box.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?auto=webp&amp;s=01b29ed2d2e90d072e1fc7295da2c1cb3797f686', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54fdfef1cb192ed04e4ba25828970287fc1ecde9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/p3HQXQzdkmyXyJ89enL6PkgmSdstFY5z1QkzOzRNUaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22ff535b36cf2b9412be48a21108ba34479e2ba5', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'POVR4AtJHvry29k6WQQwgYhMSdLrOeYwBodMqA6lPGk'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gahzdg,True,,ionezation,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gahzdg/youtube_sentiment_analysis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gahzdg/youtube_sentiment_analysis/,155203,1588195144.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is Data Labeling ? How to attach a meaning to digital data (short video introduction for everyone),[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gagiz0,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lMwX-n_1NXs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Data Labeling ? | Prepare Your Data for ML and AI | Attaching meaning to digital data 27', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lMwX-n_1NXs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lMwX-n_1NXs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lMwX-n_1NXs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gagiz0', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/gISuzz4Kygmrejt2gz3vvlzIfGF6_cG3jxNVdS714wI.jpg,False,,[],{},rich:video,,False,,1588219413.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iE0rwVfCBkHmwX6E4x3P9uxZv83Pc74I37ED8xCBgco.jpg?auto=webp&amp;s=5dd21cb436ea76cc1934751c0bb7cd2bdb735c43', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/iE0rwVfCBkHmwX6E4x3P9uxZv83Pc74I37ED8xCBgco.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8048851c2df216aed1c28ab160550b93f4e137d5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/iE0rwVfCBkHmwX6E4x3P9uxZv83Pc74I37ED8xCBgco.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbfe16681e11b4747d48c9d6626445542af90124', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/iE0rwVfCBkHmwX6E4x3P9uxZv83Pc74I37ED8xCBgco.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d0c60d45a301f33db5fa4fa23989e931585a4d83', 'width': 320, 'height': 240}], 'variants': {}, 'id': '13U21JJTL3LwkA6TrZwDdOnzSauB-lUyP1CMSx8BBnk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gagiz0,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gagiz0/what_is_data_labeling_how_to_attach_a_meaning_to/,all_ads,False,https://www.youtube.com/watch?v=lMwX-n_1NXs,155203,1588190613.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Data Labeling ? | Prepare Your Data for ML and AI | Attaching meaning to digital data 27', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lMwX-n_1NXs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lMwX-n_1NXs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_5pbaz02n,False,,0,False,[ Errno 13] Permission Denied,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_gakd3a,False,light,0.25,,public,0,0,{},140.0,,False,[],,True,False,,{},HELP,False,0,,False,https://b.thumbs.redditmedia.com/NCbVEFN9rxPSPlgv20jF_eGM5c4e6AMheUVBznxCDiM.jpg,False,,[],{},image,,False,,1588231790.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?auto=webp&amp;s=02076c254f07446afad23a696ef097d557ba714f', 'width': 3024, 'height': 4032}, 'resolutions': [{'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c691835646431ab35707a5cdf8becfc24f58e22', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94cdcbe9f8d43cc41a79a6beca8514daee64cef6', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8525c96876a280f798e557087c8c2ecb29e0a51', 'width': 320, 'height': 426}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9c28a778a02251f7f74f3e082a67dad0c4bf527', 'width': 640, 'height': 853}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49ec8f85c5161974d1a03bebd405bc330af15e9f', 'width': 960, 'height': 1280}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e47f4c01533117b47b751b0fdb5ea9d1424baff8', 'width': 1080, 'height': 1440}], 'variants': {}, 'id': 'Mb2OZeLKbLTEEQ48q-q2ZbmWk-M_TMeGVFfNKodqiak'}], 'enabled': True}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gakd3a,True,,ohthatspaul,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gakd3a/errno_13_permission_denied/,all_ads,False,https://i.redd.it/ayotmcfycuv41.jpg,155203,1588202990.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'computervision', 'selftext': '', 'author_fullname': 't2_5pbaz02n', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[ Errno 13] Permission Denied', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/computervision', 'hidden': False, 'pwls': 7, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'hide_score': False, 'name': 't3_gakbto', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.2, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help Required', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/NCbVEFN9rxPSPlgv20jF_eGM5c4e6AMheUVBznxCDiM.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588231665.0, 'link_flair_type': 'text', 'wls': 7, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?auto=webp&amp;s=02076c254f07446afad23a696ef097d557ba714f', 'width': 3024, 'height': 4032}, 'resolutions': [{'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c691835646431ab35707a5cdf8becfc24f58e22', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94cdcbe9f8d43cc41a79a6beca8514daee64cef6', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f8525c96876a280f798e557087c8c2ecb29e0a51', 'width': 320, 'height': 426}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9c28a778a02251f7f74f3e082a67dad0c4bf527', 'width': 640, 'height': 853}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49ec8f85c5161974d1a03bebd405bc330af15e9f', 'width': 960, 'height': 1280}, {'url': 'https://preview.redd.it/ayotmcfycuv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e47f4c01533117b47b751b0fdb5ea9d1424baff8', 'width': 1080, 'height': 1440}], 'variants': {}, 'id': 'Mb2OZeLKbLTEEQ48q-q2ZbmWk-M_TMeGVFfNKodqiak'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '29f7c376-4f5a-11ea-b5e4-0e34260c7df1', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2rfzn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffd635', 'id': 'gakbto', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ohthatspaul', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'some_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/computervision/comments/gakbto/errno_13_permission_denied/', 'parent_whitelist_status': 'some_ads', 'stickied': False, 'url': 'https://i.redd.it/ayotmcfycuv41.jpg', 'subreddit_subscribers': 35033, 'created_utc': 1588202865.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_gakbto,,
,learnmachinelearning,"JupyterLab just became a full-fledged IDE with features like Code Assistance, Debugging and Git â€” welcome to the future of Notebook editing.

[https://towardsdatascience.com/3-must-have-jupyterlab-2-0-extensions-41024fe455cc](https://towardsdatascience.com/3-must-have-jupyterlab-2-0-extensions-41024fe455cc)

TLDR;

\- [JupyterLab-LSP](https://github.com/krassowski/jupyterlab-lsp) for Coding Assistance

\- [debugger](https://github.com/jupyterlab/debugger) for debugging

\- [JupyterLab-Git](https://github.com/jupyterlab/jupyterlab-git) for git blame and diff",t2_k9eg8,False,,0,False,3 Must-have JupyterLab 2.0 extensions,[],r/learnmachinelearning,False,6,,0,,False,t3_ga972w,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1588166234.0,,[],{},self,,True,,1588194748.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;JupyterLab just became a full-fledged IDE with features like Code Assistance, Debugging and Git â€” welcome to the future of Notebook editing.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/3-must-have-jupyterlab-2-0-extensions-41024fe455cc""&gt;https://towardsdatascience.com/3-must-have-jupyterlab-2-0-extensions-41024fe455cc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TLDR;&lt;/p&gt;

&lt;p&gt;- &lt;a href=""https://github.com/krassowski/jupyterlab-lsp""&gt;JupyterLab-LSP&lt;/a&gt; for Coding Assistance&lt;/p&gt;

&lt;p&gt;- &lt;a href=""https://github.com/jupyterlab/debugger""&gt;debugger&lt;/a&gt; for debugging&lt;/p&gt;

&lt;p&gt;- &lt;a href=""https://github.com/jupyterlab/jupyterlab-git""&gt;JupyterLab-Git&lt;/a&gt; for git blame and diff&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?auto=webp&amp;s=9f93af4fce79d3d63ad483b9a4f4ec21b57ce1d1', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6027d3ab5735685c37bbe22fe1ef413bb0a68772', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad794f5635fa866394c0f392b03bc18a08d63219', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91dd07acb892281a6899c0644849ab37956076a6', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=601c14d20738da36c05dc0eff9b75c7935dff8f5', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d2b2e69f7d534228f93aab7d8239cd9cd6e4cda', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/WteXHc5MPIe7UR9FPaCeycANMlcRzilSpVaudxQk-Bo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6b6e3d7b7a9b722470fcc0f24f4656bb93a249f', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'ei46pNkzTwWgkCPHr22tdUL9TUqFvumrySfDaCY8PIg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga972w,True,,hiphop1987,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga972w/3_musthave_jupyterlab_20_extensions/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga972w/3_musthave_jupyterlab_20_extensions/,155203,1588165948.0,0,,False,,,,
,learnmachinelearning,"Hi guys, I am a math major undergrad and I don't have the opportunity to take grad school.
I love ML and I want to know that if I work consistently and take courses like Andrew Ng on coursera and Other courses, will I have the same level if I were at college learning there? College is expensive and if I could have the knowledge without paying it would be great.
Thanks.",t2_5hsfakko,False,,0,False,Self study vs University,[],r/learnmachinelearning,False,6,,0,,False,t3_gaf37a,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588214965.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, I am a math major undergrad and I don&amp;#39;t have the opportunity to take grad school.
I love ML and I want to know that if I work consistently and take courses like Andrew Ng on coursera and Other courses, will I have the same level if I were at college learning there? College is expensive and if I could have the knowledge without paying it would be great.
Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaf37a,True,,momondb,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaf37a/self_study_vs_university/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaf37a/self_study_vs_university/,155203,1588186165.0,0,,False,,,,
,learnmachinelearning,"I would like to be able to input an image file with a hand-drawn UI-wireframe with multiple predefined components (button, title, paragraph) but also more complex elements like a list and a progress bar. 

The output would be a HTML file with CSS styling. Of course I already know what HTML elements to expect and the CSS classes that apply. 

What would be the best way to tackle this problem? 

&amp;#x200B;

I am quite new to Python/ML, but somewhat experienced with developing in general. All help is welcome, thanks!",t2_2f6b0rck,False,,0,False,Best way to convert a paper-drawn UI wireframe into HTML/CSS output?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaeydq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588214553.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would like to be able to input an image file with a hand-drawn UI-wireframe with multiple predefined components (button, title, paragraph) but also more complex elements like a list and a progress bar. &lt;/p&gt;

&lt;p&gt;The output would be a HTML file with CSS styling. Of course I already know what HTML elements to expect and the CSS classes that apply. &lt;/p&gt;

&lt;p&gt;What would be the best way to tackle this problem? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am quite new to Python/ML, but somewhat experienced with developing in general. All help is welcome, thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gaeydq,True,,MrCactus19,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaeydq/best_way_to_convert_a_paperdrawn_ui_wireframe/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaeydq/best_way_to_convert_a_paperdrawn_ui_wireframe/,155203,1588185753.0,0,,False,,,,
,learnmachinelearning,"I know. The title sounds like I haven't googled my problem, but trust me, I did. Maybe my problem has a name and I haven't found it yet. Hoping you can help me wrap my head around it.

Just as a preface I am 'kind of' experienced in ML (mostly time-series prediction problems) but not that much in NLP. I have know started dipping my toes in the hugging face transformers library and have fine tuned a BERT model on a domain-specific corpus for my task at hand.

Now, what I want to do is, given a text, extract all terms from a specific domain. For simplicity let's say, given a list of hard-coded animals, I want my model to extract from an input text all of the animals that are present in the list. Why would I use IA for this? Well e.g. I want to distinguish negations, so I don't want to extract ""lion"" if the text says ""no it a'int a lion m'am"", also I want it to extract ""lion"" if the text says ""lioness"", without having to make a huge list of synonyms.

&amp;#x200B;

I think my problem is kind of like NER, but not exactly, right? I don't just want to say ""lioness"" = animal, I want it to map it to ""lion"" also. So this is basically a classification problem, right? I want it to label all of the animals found in the text, basically label ""lioness"" = lion. But the problem is I may have 100k+ thousands of possible labels, so should I just train a classification model with 100k+ possible outputs? Doesn't seem right.

Thanks in advance for any feedback.",t2_6ujqf,False,,0,False,Extracting domain specific terms from text,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gae6jg,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588212114.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I know. The title sounds like I haven&amp;#39;t googled my problem, but trust me, I did. Maybe my problem has a name and I haven&amp;#39;t found it yet. Hoping you can help me wrap my head around it.&lt;/p&gt;

&lt;p&gt;Just as a preface I am &amp;#39;kind of&amp;#39; experienced in ML (mostly time-series prediction problems) but not that much in NLP. I have know started dipping my toes in the hugging face transformers library and have fine tuned a BERT model on a domain-specific corpus for my task at hand.&lt;/p&gt;

&lt;p&gt;Now, what I want to do is, given a text, extract all terms from a specific domain. For simplicity let&amp;#39;s say, given a list of hard-coded animals, I want my model to extract from an input text all of the animals that are present in the list. Why would I use IA for this? Well e.g. I want to distinguish negations, so I don&amp;#39;t want to extract &amp;quot;lion&amp;quot; if the text says &amp;quot;no it a&amp;#39;int a lion m&amp;#39;am&amp;quot;, also I want it to extract &amp;quot;lion&amp;quot; if the text says &amp;quot;lioness&amp;quot;, without having to make a huge list of synonyms.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I think my problem is kind of like NER, but not exactly, right? I don&amp;#39;t just want to say &amp;quot;lioness&amp;quot; = animal, I want it to map it to &amp;quot;lion&amp;quot; also. So this is basically a classification problem, right? I want it to label all of the animals found in the text, basically label &amp;quot;lioness&amp;quot; = lion. But the problem is I may have 100k+ thousands of possible labels, so should I just train a classification model with 100k+ possible outputs? Doesn&amp;#39;t seem right.&lt;/p&gt;

&lt;p&gt;Thanks in advance for any feedback.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gae6jg,True,,misticfed,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gae6jg/extracting_domain_specific_terms_from_text/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gae6jg/extracting_domain_specific_terms_from_text/,155203,1588183314.0,0,,False,,,,
,learnmachinelearning,"Hey community! 

Full disclosure - I'm writing as the founder of a startup called [maiot](https://maiot.io) , but before you dismiss this as just another BS startup posting for marketing purposes, hear me out! 

We're not looking for monetary gains at all - rather we're looking for open and critical feedback of what we have built. Our platform allows you to write simple a simple YAML file and execute distributed deep learning pipelines completely at scale on a managed service. We think the service is cool for people in this community in particular as it is a more approachable tool than having to learn the internals of Tensorflow/PyTorch from scratch. Right now, you can connect to a BigQuery table, but we plan to include more datasources like images, text and video soon. An example can be found in the [quickstart](https://docs.maiot.io/docs/quickstart), where I connected a [public BigQuery table](https://cloud.google.com/bigquery/public-data) and built a classification model to predict incomes based on census data.

We're currently in [Beta](http://docs.maiot.io/), so you don't have to pay anything at all to try it out (We give you 150 EUR worth of credits to play around as much as you want, and no credits card details or anything are required to sign up). 

I hope I did not break any of the communities rules here. Thank you again and looking forward to your open feedback.

[Use simple YAML and terminal commands to train deep learning models at scale](https://reddit.com/link/gae16s/video/pizucb2vgsv41/player)",t2_64zw96wq,False,,0,False,Use YAML to create production ready deep learning pipelines,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_gae16s,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},Project,False,0,,False,https://b.thumbs.redditmedia.com/VjKu36AQhwfmO77WKFUUtZttVUivIjMjrXB6LunwLJc.jpg,False,,[],{},self,,True,,1588211606.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey community! &lt;/p&gt;

&lt;p&gt;Full disclosure - I&amp;#39;m writing as the founder of a startup called &lt;a href=""https://maiot.io""&gt;maiot&lt;/a&gt; , but before you dismiss this as just another BS startup posting for marketing purposes, hear me out! &lt;/p&gt;

&lt;p&gt;We&amp;#39;re not looking for monetary gains at all - rather we&amp;#39;re looking for open and critical feedback of what we have built. Our platform allows you to write simple a simple YAML file and execute distributed deep learning pipelines completely at scale on a managed service. We think the service is cool for people in this community in particular as it is a more approachable tool than having to learn the internals of Tensorflow/PyTorch from scratch. Right now, you can connect to a BigQuery table, but we plan to include more datasources like images, text and video soon. An example can be found in the &lt;a href=""https://docs.maiot.io/docs/quickstart""&gt;quickstart&lt;/a&gt;, where I connected a &lt;a href=""https://cloud.google.com/bigquery/public-data""&gt;public BigQuery table&lt;/a&gt; and built a classification model to predict incomes based on census data.&lt;/p&gt;

&lt;p&gt;We&amp;#39;re currently in &lt;a href=""http://docs.maiot.io/""&gt;Beta&lt;/a&gt;, so you don&amp;#39;t have to pay anything at all to try it out (We give you 150 EUR worth of credits to play around as much as you want, and no credits card details or anything are required to sign up). &lt;/p&gt;

&lt;p&gt;I hope I did not break any of the communities rules here. Thank you again and looking forward to your open feedback.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/gae16s/video/pizucb2vgsv41/player""&gt;Use simple YAML and terminal commands to train deep learning models at scale&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?auto=webp&amp;s=35d91044ecdb7f01e7bf25ef91ba078c132f0d7c', 'width': 1400, 'height': 1400}, 'resolutions': [{'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=361d1f63385b011b82195483a6ea134cbeb41bec', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b94508fd116f63779d3b502ca8ff403b07f78ab4', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f83841a820bd8431d51002637c2ad5429afa673a', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=71718b5d5efb122659d2ecab4d3d8a013a3cc5c5', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d4eed66ad2ad511cd14c63c153d583c547be3e5b', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/3Q-_YPjuFmr9QerMQVPLwiWSyZwbHy7TKxiJVpg-ytI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c654d905e4371782be9edc58540c4f3b566838e4', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'TzDlkE0tMlnDEI3mZFDbANsVGF8St982lwWqNUCPMXw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gae16s,True,,maiot_io,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gae16s/use_yaml_to_create_production_ready_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gae16s/use_yaml_to_create_production_ready_deep_learning/,155203,1588182806.0,0,,False,,,"{'pizucb2vgsv41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/gae16s/asset/pizucb2vgsv41/DASHPlaylist.mpd', 'x': 640, 'y': 314, 'hlsUrl': 'https://v.redd.it/link/gae16s/asset/pizucb2vgsv41/HLSPlaylist.m3u8', 'id': 'pizucb2vgsv41', 'isGif': True}}",
,learnmachinelearning,"I want something like this:
â€¢ The model will take the sound of the bird as an input
â€¢ Will generate the bird name as an output 

Can this model be created using machine learning?
If yes, what algorithms do I use. 

Note that I am doing linear regression as of now and will move onto logistic regression very soon.",t2_5pfavzcm,False,,0,False,Iâ€™m planning to build a ML model.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga7ual,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588189049.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want something like this:
â€¢ The model will take the sound of the bird as an input
â€¢ Will generate the bird name as an output &lt;/p&gt;

&lt;p&gt;Can this model be created using machine learning?
If yes, what algorithms do I use. &lt;/p&gt;

&lt;p&gt;Note that I am doing linear regression as of now and will move onto logistic regression very soon.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ga7ual,True,,FooFighter39,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga7ual/im_planning_to_build_a_ml_model/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga7ual/im_planning_to_build_a_ml_model/,155203,1588160249.0,0,,False,,,,
,learnmachinelearning,"Hi, does someone wan't to go through the official PyTorch tutorials?",t2_5zclgibe,False,,0,False,PyTorch Tutorials,[],r/learnmachinelearning,False,6,,0,,False,t3_gadnma,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588210296.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, does someone wan&amp;#39;t to go through the official PyTorch tutorials?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gadnma,True,,mihailfomin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gadnma/pytorch_tutorials/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gadnma/pytorch_tutorials/,155203,1588181496.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,AI navigation for blind people,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ga933v,False,light,1.0,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8Ao5qQdGbv4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI navigation for blind people', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8Ao5qQdGbv4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8Ao5qQdGbv4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8Ao5qQdGbv4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ga933v', 'height': 338}",Discussion,False,2,,False,https://b.thumbs.redditmedia.com/LY9uFSbovkwWu6E70RZuXiYygifkPfGikH2YaCTJaTc.jpg,False,,[],{},rich:video,,False,,1588194339.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/aw74au-738UqEHp_r0Rc57p4xx433sUVbgq5w4_xQAs.jpg?auto=webp&amp;s=d4f79b2a35f417b3cc94cd42d7f8ee00d57a1568', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/aw74au-738UqEHp_r0Rc57p4xx433sUVbgq5w4_xQAs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ad21687d041394b0a651fa16123bdda6fb2d520', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/aw74au-738UqEHp_r0Rc57p4xx433sUVbgq5w4_xQAs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7aa44f728e4ab2026679dbc9f289798c6295543', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/aw74au-738UqEHp_r0Rc57p4xx433sUVbgq5w4_xQAs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f494b528759b6e72e3a5cc77306670d8f02e5bc5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'awAlP9B497cPAs6KOjL1kYyr9VEcE7C-syTAlrE45FU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ga933v,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga933v/ai_navigation_for_blind_people/,all_ads,False,https://youtu.be/8Ao5qQdGbv4,155203,1588165539.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'AI navigation for blind people', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8Ao5qQdGbv4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8Ao5qQdGbv4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_itge0,False,,0,False,6 excellent free data science courses. Have fun!,[],r/learnmachinelearning,False,6,,0,,False,t3_gacn7a,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588206697.0,text,6,,,text,www-infoworld-com.cdn.ampproject.org,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gacn7a,True,,Yatima_K,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gacn7a/6_excellent_free_data_science_courses_have_fun/,all_ads,False,https://www-infoworld-com.cdn.ampproject.org/v/s/www.infoworld.com/article/3540434/the-best-free-data-science-courses-during-quarantine.amp.html?amp_js_v=a3&amp;amp_gsa=1&amp;usqp=mq331AQIKAGwASDAAQE%3D#referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.infoworld.com%2Farticle%2F3540434%2Fthe-best-free-data-science-courses-during-quarantine.html,155203,1588177897.0,0,,False,,,,
,learnmachinelearning,"I'm 26 years old with a (pure) math degree and currently with a full time job on a non-CS related position. I would like to get started in ML but I don't know exactly where to start. I have a strong basis on Linear Algebra, Multivariable Calculus, and many other math fields, but when it comes to coding, I only know very basic C,  basic Python and Bash.

I don't think it would be too difficult for me to learn the coding part, but I completely don't know how/where to get started. I was thinking about enrolling on a master's degree, but since a have a full time job I don't think I would be able to follow it properly.

What would be a path I should follow to switch from my non computer science job to a machine learning position? What are the best practices to maximize my chances? Would it be better to land a basic/entry job or wait until I have a strong profile and go for an advanced level job?

I feel like I am at disadvantage to many people coming from a CS degree, as it's my impression their skill set is more convenient than the one you'd get from a math bachelor's degree.",t2_q4y0ppr,False,,0,False,How to get started coming from a Math degree?,[],r/learnmachinelearning,False,6,,0,,False,t3_gacd1p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588177565.0,,[],{},,,True,,1588205672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m 26 years old with a (pure) math degree and currently with a full time job on a non-CS related position. I would like to get started in ML but I don&amp;#39;t know exactly where to start. I have a strong basis on Linear Algebra, Multivariable Calculus, and many other math fields, but when it comes to coding, I only know very basic C,  basic Python and Bash.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t think it would be too difficult for me to learn the coding part, but I completely don&amp;#39;t know how/where to get started. I was thinking about enrolling on a master&amp;#39;s degree, but since a have a full time job I don&amp;#39;t think I would be able to follow it properly.&lt;/p&gt;

&lt;p&gt;What would be a path I should follow to switch from my non computer science job to a machine learning position? What are the best practices to maximize my chances? Would it be better to land a basic/entry job or wait until I have a strong profile and go for an advanced level job?&lt;/p&gt;

&lt;p&gt;I feel like I am at disadvantage to many people coming from a CS degree, as it&amp;#39;s my impression their skill set is more convenient than the one you&amp;#39;d get from a math bachelor&amp;#39;s degree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gacd1p,True,,a-lawliet,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gacd1p/how_to_get_started_coming_from_a_math_degree/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gacd1p/how_to_get_started_coming_from_a_math_degree/,155203,1588176872.0,0,,False,,,,
,learnmachinelearning,"Hi everyone, It is worth converting a ""classic"" decision-making software (i. E. House pricing software) that is already working, into a software based on a machine-learning model? What are the advantages, if there are any?
Thanks",t2_6xheb,False,,0,False,How to know if a project is worth,[],r/learnmachinelearning,False,6,,0,,False,t3_gac1mw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588204539.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, It is worth converting a &amp;quot;classic&amp;quot; decision-making software (i. E. House pricing software) that is already working, into a software based on a machine-learning model? What are the advantages, if there are any?
Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gac1mw,True,,useae,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gac1mw/how_to_know_if_a_project_is_worth/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gac1mw/how_to_know_if_a_project_is_worth/,155203,1588175739.0,0,,False,,,,
,learnmachinelearning,I am more interested on the mathematical books. But even practical books are fine too.,t2_4ziftlig,False,,0,False,Could you please advise me good books on Machine Learning .,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gag1ai,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588217858.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am more interested on the mathematical books. But even practical books are fine too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gag1ai,True,,pranayprasad3,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gag1ai/could_you_please_advise_me_good_books_on_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gag1ai/could_you_please_advise_me_good_books_on_machine/,155203,1588189058.0,0,,False,,,,
,learnmachinelearning,,t2_o0pbd,False,,0,False,Maths behind the perceptron neuron model for machine learning beginners.,[],r/learnmachinelearning,False,6,,0,73.0,False,t3_gabnpe,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/TUG3qYOqgZATDmXq5p_u7JKZbVziSx219La2s3bwCkc.jpg,False,,[],{},link,,False,,1588203191.0,text,6,,,text,neuraldesigner.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?auto=webp&amp;s=3e1749446380da6596586980c323910d2a99faeb', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=acd2bd8ce4cabe159d56383d1dbd2955fd189321', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e92cda09cd7553d5afc146db6ac8708651cbc9c', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24ffc85b1f3bffa7daa9d7f7b687216f57cbd1a9', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8674c91e80c834bb0233a1982357ff1472f435b5', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0cb727a83f8f939db4b0d193c86f9ac6c2a9aba', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/mE1YXxjEOzlozF_5XhxbTFQdu6Ljq1QZW5u_7X8HPLY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1425aa76da31acb036d605fc651be6e507233d20', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'Amn3kkr4PZzTyiN0kJBRAR-K_eyETXGABm6Ut5k6O0E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gabnpe,True,,datapablo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gabnpe/maths_behind_the_perceptron_neuron_model_for/,all_ads,False,https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks,155203,1588174391.0,0,,False,,,,
,learnmachinelearning, i am using tensor flow version 2.0.0 and i know in tensor flow 2.0 the gfile package has been moved into [t](https://tf.io/)f.io.gfile.GFile i have also tried using this but it gives the same error,t2_6akncvc2,False,,0,False,AttributeError: module 'tensorflow' has no attribute 'gfile',[],r/learnmachinelearning,False,6,,0,,False,t3_gabh4u,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588202587.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i am using tensor flow version 2.0.0 and i know in tensor flow 2.0 the gfile package has been moved into &lt;a href=""https://tf.io/""&gt;t&lt;/a&gt;f.io.gfile.GFile i have also tried using this but it gives the same error&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gabh4u,True,,mehmood11,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gabh4u/attributeerror_module_tensorflow_has_no_attribute/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gabh4u/attributeerror_module_tensorflow_has_no_attribute/,155203,1588173787.0,0,,False,,,,
,learnmachinelearning,"So in k-means clustering, when recalculating the centroid location to the mean of it's cluster, what do you do with centroids that don't have any data points assigned to them? Just leave them where they are? I may be misunderstanding something but I'd appreciate any help. Thanks",t2_fe2gw,False,,0,False,K-Means Clustering Question,[],r/learnmachinelearning,False,6,,0,,False,t3_gaarz6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588200322.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So in k-means clustering, when recalculating the centroid location to the mean of it&amp;#39;s cluster, what do you do with centroids that don&amp;#39;t have any data points assigned to them? Just leave them where they are? I may be misunderstanding something but I&amp;#39;d appreciate any help. Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaarz6,True,,juicebox15,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaarz6/kmeans_clustering_question/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaarz6/kmeans_clustering_question/,155203,1588171522.0,0,,False,,,,
,learnmachinelearning,"Hi, I'm doing a project where I'm going to input an image of algebraic equations into the neural network. The output is obviously the network trying to figure out what the equation is. I'm new to machine learning, and I am wondering if there is already test data for this kind of exercise or if I will have to draw all of the test data myself.

I'd like the equations to include addition, subtraction, multiplication, division, roots, brackets, radicals, logarithmic, and 2 variable systems of equations.

Any help would be appreciated to find a data set.  


edit: Wrote radicals twice :/",t2_4o83nuxl,False,,0,False,Is there a online data set for hadnwritten algebreic equations?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga7aky,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1588184876.0,,[],{},,,True,,1588186431.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m doing a project where I&amp;#39;m going to input an image of algebraic equations into the neural network. The output is obviously the network trying to figure out what the equation is. I&amp;#39;m new to machine learning, and I am wondering if there is already test data for this kind of exercise or if I will have to draw all of the test data myself.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like the equations to include addition, subtraction, multiplication, division, roots, brackets, radicals, logarithmic, and 2 variable systems of equations.&lt;/p&gt;

&lt;p&gt;Any help would be appreciated to find a data set.  &lt;/p&gt;

&lt;p&gt;edit: Wrote radicals twice :/&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga7aky,True,,C_morling,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga7aky/is_there_a_online_data_set_for_hadnwritten/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga7aky/is_there_a_online_data_set_for_hadnwritten/,155203,1588157631.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

Wanted to share something with this community that I'm really proud of and excited about. I'm the Head of Community at [Fritz AI](https://fritz.ai), and also someone with practically zero programming/coding experience. 

But I wanted to challenge myself and see if I could use our company's platform for mobile ML to build, test, and improve an object detection model for iOS over a weekend. I tried to be as honest and clear as I could about the opportunities and limitations of our platform, and our engineering team was kind enough to let me work on this independently. Definitely not my intention to spam here, but I figured for a community of folks just getting started with ML, this might be inspiring, helpful, or both. Would appreciate any feedback! 

I've shared the accompanying blog post below:

[https://heartbeat.fritz.ai/mobile-machine-learning-with-fritz-ai-a-non-developers-journey-bb644bbede17](https://heartbeat.fritz.ai/mobile-machine-learning-with-fritz-ai-a-non-developers-journey-bb644bbede17)",t2_y7afdfq,False,,0,False,A non-developers journey in building an on-device (iOS) object detection model,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gaaexc,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},,,True,,1588199110.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;Wanted to share something with this community that I&amp;#39;m really proud of and excited about. I&amp;#39;m the Head of Community at &lt;a href=""https://fritz.ai""&gt;Fritz AI&lt;/a&gt;, and also someone with practically zero programming/coding experience. &lt;/p&gt;

&lt;p&gt;But I wanted to challenge myself and see if I could use our company&amp;#39;s platform for mobile ML to build, test, and improve an object detection model for iOS over a weekend. I tried to be as honest and clear as I could about the opportunities and limitations of our platform, and our engineering team was kind enough to let me work on this independently. Definitely not my intention to spam here, but I figured for a community of folks just getting started with ML, this might be inspiring, helpful, or both. Would appreciate any feedback! &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve shared the accompanying blog post below:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/mobile-machine-learning-with-fritz-ai-a-non-developers-journey-bb644bbede17""&gt;https://heartbeat.fritz.ai/mobile-machine-learning-with-fritz-ai-a-non-developers-journey-bb644bbede17&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gaaexc,True,,austin_kodra,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaaexc/a_nondevelopers_journey_in_building_an_ondevice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaaexc/a_nondevelopers_journey_in_building_an_ondevice/,155203,1588170310.0,0,,False,,,,
,learnmachinelearning,https://heartbeat.fritz.ai/image-compression-using-different-machine-learning-techniques-5787c88515f8,t2_45tb49o2,False,,0,False,Image Compression using different ML Techniques,[],r/learnmachinelearning,False,6,,0,,False,t3_gaa52a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588198174.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/image-compression-using-different-machine-learning-techniques-5787c88515f8""&gt;https://heartbeat.fritz.ai/image-compression-using-different-machine-learning-techniques-5787c88515f8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gaa52a,True,,codegeass30,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gaa52a/image_compression_using_different_ml_techniques/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gaa52a/image_compression_using_different_ml_techniques/,155203,1588169374.0,0,,False,,,,
,learnmachinelearning,"I've just completed the Machine Learning A-Z on Udemy and I just wanted to know if anyone could direct me in the right direction for my first dataset so that I can test my knowledge in the real world. I tried implementing a model on dataset but I got stuck so I looked through a few submissions to check how to go about the problem. The best submission had implemented the problem in a very complex way and it kinda sent me spiralling down. Is there anywhere I can start practicing with small datasets and then gradually go upwards, in terms of complexity?",t2_s9ix22r,False,,0,False,Guidance for a new ML student,[],r/learnmachinelearning,False,6,,0,,False,t3_ga9ptg,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588196697.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve just completed the Machine Learning A-Z on Udemy and I just wanted to know if anyone could direct me in the right direction for my first dataset so that I can test my knowledge in the real world. I tried implementing a model on dataset but I got stuck so I looked through a few submissions to check how to go about the problem. The best submission had implemented the problem in a very complex way and it kinda sent me spiralling down. Is there anywhere I can start practicing with small datasets and then gradually go upwards, in terms of complexity?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga9ptg,True,,_sanskaari,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga9ptg/guidance_for_a_new_ml_student/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga9ptg/guidance_for_a_new_ml_student/,155203,1588167897.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

I'll be doing a master's in artificial intelligence and speech processing (assuming COVID 19 doesn't interfere) this fall. However, I did a bachelor's degree in electrical engineering so I'm only really familiar with the signal processing aspect of my master's and I'm not really that confident in what I know in statistics and probability. My adviser recommended me that I brush up on my statistics so I wouldn't have much difficulty adjusting.

Is there an online course/set of videos that anyone would recommend that would give out the background needed for machine learning? I'm already reviewing books such as Machine Learning: a probabilistic perspective by Murphy, Elements of Statistical Learning, Pattern Recognition by Bishop, but I would also prefer to add videos into the mix so there would be different sources of learning.

Thanks so much :)",t2_66cnc655,False,,0,False,Video Lessons - Statistics Behind Machine Learning (Refresher for MS studies),[],r/learnmachinelearning,False,6,,0,,False,t3_g9nzaf,False,dark,0.97,,public,73,0,{},,,False,[],,False,False,,{},,False,73,,False,self,False,,[],{},,,True,,1588112175.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll be doing a master&amp;#39;s in artificial intelligence and speech processing (assuming COVID 19 doesn&amp;#39;t interfere) this fall. However, I did a bachelor&amp;#39;s degree in electrical engineering so I&amp;#39;m only really familiar with the signal processing aspect of my master&amp;#39;s and I&amp;#39;m not really that confident in what I know in statistics and probability. My adviser recommended me that I brush up on my statistics so I wouldn&amp;#39;t have much difficulty adjusting.&lt;/p&gt;

&lt;p&gt;Is there an online course/set of videos that anyone would recommend that would give out the background needed for machine learning? I&amp;#39;m already reviewing books such as Machine Learning: a probabilistic perspective by Murphy, Elements of Statistical Learning, Pattern Recognition by Bishop, but I would also prefer to add videos into the mix so there would be different sources of learning.&lt;/p&gt;

&lt;p&gt;Thanks so much :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9nzaf,True,,hydrogenken,,22,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9nzaf/video_lessons_statistics_behind_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9nzaf/video_lessons_statistics_behind_machine_learning/,155203,1588083375.0,0,,False,,,,
,learnmachinelearning,Project I'm trying to solve is the feature detection problem for hands. Trying to predict finger tip and 1st knuckle positions. I'm using a CPM. Everythings properly set up. Its probably a minor thing. Would probably take 5 mins. HAAAALP,t2_1hynl93f,False,,0,False,Can somebody help me understand why my model is not training?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga9c8s,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588195291.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Project I&amp;#39;m trying to solve is the feature detection problem for hands. Trying to predict finger tip and 1st knuckle positions. I&amp;#39;m using a CPM. Everythings properly set up. Its probably a minor thing. Would probably take 5 mins. HAAAALP&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga9c8s,True,,hooman_Instance,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga9c8s/can_somebody_help_me_understand_why_my_model_is/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga9c8s/can_somebody_help_me_understand_why_my_model_is/,155203,1588166491.0,0,,False,,,,
,learnmachinelearning,"In company we have lots of different tables on our clients. Most of the tables are relatively sparse. My task is to present advertisement department a lost of clients to target. 


What are the techniques to apply machine learning to this task bearing in mind that information is sparse across multiple tables? How can I become more sure in solution and itâ€™s stability of judgement? 

Most important, how it can be applied to our covid/isolation reality, where customer behavior changes substantially? I understand that model trained on data from a year ago can be vastly different to model on data from past few months. Any tips on extracting features?

Would be great if you could provide resources such as articles/courses/books

Thank you",t2_7lorwum,False,,0,False,Machine learning on a sparse dataset,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga90bf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588194066.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In company we have lots of different tables on our clients. Most of the tables are relatively sparse. My task is to present advertisement department a lost of clients to target. &lt;/p&gt;

&lt;p&gt;What are the techniques to apply machine learning to this task bearing in mind that information is sparse across multiple tables? How can I become more sure in solution and itâ€™s stability of judgement? &lt;/p&gt;

&lt;p&gt;Most important, how it can be applied to our covid/isolation reality, where customer behavior changes substantially? I understand that model trained on data from a year ago can be vastly different to model on data from past few months. Any tips on extracting features?&lt;/p&gt;

&lt;p&gt;Would be great if you could provide resources such as articles/courses/books&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ga90bf,True,,Alkanste,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga90bf/machine_learning_on_a_sparse_dataset/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga90bf/machine_learning_on_a_sparse_dataset/,155203,1588165266.0,1,,False,,,,
,learnmachinelearning," 

I'm a first year PhD student with a CS background but have been on and off with data sci.

I  ran a large metabolomic experiment and am trying to identify  differences in my cells. This is my second time running this and both  time I noticed that the heatmap hierarchal clustering didn't seperate my  groups as nicely as when I tried PCA + kmeans.

The  first time, it appeared that there were some differences when visually  inspecting the map, yet for some of my samples, the hierarchal  clustering mixed some of the groupings up while the rest stayed intact.  It was strange to me because visually the groups that got mixed together  appeared to be distict. I ran heatmap2 on R and just used the default  clustering algorithm.

I ran PCA +  Kmeans later and got very nice seperation between the groups that made  biological sense. So I definitely have a few questions.

1. What  factors should I consider when deciding between the different  hierarchal clustering methods? I'm not sure if there's a reason I need  to switch over from the default method, other than the fact that my  results weren't as expected.
2. I  think I understand how/why we choose certain algorithms for cases where  we are building predictive models. In those cases, we can test  different models on data, look for accurary and performance, etc. But in  this case, where I am doing exploratory data analysis and not planning  on building a deployable machine learning model, how much does it  matter? The reason I chose K-means is because it's something I  understand and also since I have an idea of how many biologically  different groups there are, I also have some logic for choosing a  certain number of clusters. I don't see a need to choose another  clustering algorithm unless things just don't work out for me... which  leads me to 3
3. In  a scientific environment where these techniques are being used to  understand our data, how do you choose between different techniques  especially when they provide conflicting results. I favor the k-means  over the hierarchal clustering, but only because I understand the  biological context. But at that point, am I introducing bias into my  interpretation as well? The only pseudo-mathematical reason I can think  of is that the PCA accounts for variation across the entire set of  variables where as the heatmap looks at things on a more individualistic  basis. So the PCA and subsequent clustering is able to account for the  summation of all the minute variability in my samples.
4. From  what I udnerstand, PCA is great for reducing multidimensional data. The  reason I think it's important to run on my dataset is because without  dimensionality reduction, there would be no way to visually plot my data  since I have about 100 different variables in each cell. However,  technically, k-means could have been run on my samples without PCA, it  might have been more computationally intensive and impossible to plot  visually as I would then have to choose what I believed to be the most 2  or 3 most representative variables from my data.  Do I have the correct  idea?
5. In  my first set of data, my mentor was very happy with my results and  asked me to figure out what variables were most responsible for  seperation. This seemed a little fishy to me... from what I understand  it's really hard to get something meaningful out of the components and  the weights assigned to the variables. I did notice that there were  groups of 5 or 6 variables that were in related pathways that had the  highest absolute weights. Is it therefore correct to say that those  cells are likely to different within those specific pathways (although  of course this must be experimentally verified

Sorry for the lengthy message, but I appreciate your input!",t2_65wva7k2,False,,0,False,Understanding differences in clustering result (PCA + Kmeans and heatmap),[],r/learnmachinelearning,False,6,,0,,False,t3_ga6490,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588180258.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a first year PhD student with a CS background but have been on and off with data sci.&lt;/p&gt;

&lt;p&gt;I  ran a large metabolomic experiment and am trying to identify  differences in my cells. This is my second time running this and both  time I noticed that the heatmap hierarchal clustering didn&amp;#39;t seperate my  groups as nicely as when I tried PCA + kmeans.&lt;/p&gt;

&lt;p&gt;The  first time, it appeared that there were some differences when visually  inspecting the map, yet for some of my samples, the hierarchal  clustering mixed some of the groupings up while the rest stayed intact.  It was strange to me because visually the groups that got mixed together  appeared to be distict. I ran heatmap2 on R and just used the default  clustering algorithm.&lt;/p&gt;

&lt;p&gt;I ran PCA +  Kmeans later and got very nice seperation between the groups that made  biological sense. So I definitely have a few questions.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What  factors should I consider when deciding between the different  hierarchal clustering methods? I&amp;#39;m not sure if there&amp;#39;s a reason I need  to switch over from the default method, other than the fact that my  results weren&amp;#39;t as expected.&lt;/li&gt;
&lt;li&gt;I  think I understand how/why we choose certain algorithms for cases where  we are building predictive models. In those cases, we can test  different models on data, look for accurary and performance, etc. But in  this case, where I am doing exploratory data analysis and not planning  on building a deployable machine learning model, how much does it  matter? The reason I chose K-means is because it&amp;#39;s something I  understand and also since I have an idea of how many biologically  different groups there are, I also have some logic for choosing a  certain number of clusters. I don&amp;#39;t see a need to choose another  clustering algorithm unless things just don&amp;#39;t work out for me... which  leads me to 3&lt;/li&gt;
&lt;li&gt;In  a scientific environment where these techniques are being used to  understand our data, how do you choose between different techniques  especially when they provide conflicting results. I favor the k-means  over the hierarchal clustering, but only because I understand the  biological context. But at that point, am I introducing bias into my  interpretation as well? The only pseudo-mathematical reason I can think  of is that the PCA accounts for variation across the entire set of  variables where as the heatmap looks at things on a more individualistic  basis. So the PCA and subsequent clustering is able to account for the  summation of all the minute variability in my samples.&lt;/li&gt;
&lt;li&gt;From  what I udnerstand, PCA is great for reducing multidimensional data. The  reason I think it&amp;#39;s important to run on my dataset is because without  dimensionality reduction, there would be no way to visually plot my data  since I have about 100 different variables in each cell. However,  technically, k-means could have been run on my samples without PCA, it  might have been more computationally intensive and impossible to plot  visually as I would then have to choose what I believed to be the most 2  or 3 most representative variables from my data.  Do I have the correct  idea?&lt;/li&gt;
&lt;li&gt;In  my first set of data, my mentor was very happy with my results and  asked me to figure out what variables were most responsible for  seperation. This seemed a little fishy to me... from what I understand  it&amp;#39;s really hard to get something meaningful out of the components and  the weights assigned to the variables. I did notice that there were  groups of 5 or 6 variables that were in related pathways that had the  highest absolute weights. Is it therefore correct to say that those  cells are likely to different within those specific pathways (although  of course this must be experimentally verified&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sorry for the lengthy message, but I appreciate your input!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga6490,True,,1099shateme,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga6490/understanding_differences_in_clustering_result/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga6490/understanding_differences_in_clustering_result/,155203,1588151458.0,0,,False,,,,
,learnmachinelearning,,t2_4h5p1eq8,False,,0,False,"Hey Coder! This is my new project FACE PARADISE (Ik it's funny) well it is basically a GUI which you can interact with by selecting different filters , face mask (Basically like Snapchat ~v 0.01) and save them on your desktop . Source code - github.com/Itsaadarsh/Quarantine-Projects/Face Filter","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_ga635s,False,light,1.0,,public,2,0,{},140.0,,False,[],,True,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/_BZZwTyjrxbY8dTnQCqSb3vCuFQVsEd2U8Ami5ZbUrQ.jpg,False,,[],{},image,,False,,1588180091.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?auto=webp&amp;s=f68662d4dd0af0278f7c3b5c8e56b0df330f74cf', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6f9ff2c6822e532ace109bc184a9a3758966c32', 'width': 108, 'height': 60}, {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a40e32319e984f79d9c68f71d201fd17be857b4', 'width': 216, 'height': 121}, {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86bcea344eb59174f9e504390c404a5cc18da4d1', 'width': 320, 'height': 180}, {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff7c84a23f414511ce97f4b7de6619f0f8beefce', 'width': 640, 'height': 360}, {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cbf0ee755112e20a9482f6940a9815007bebebe', 'width': 960, 'height': 540}, {'url': 'https://preview.redd.it/45sc8n4n3qv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=46afad67e43600741578af113f90a4a834fe8a20', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'G3b88trixCo-Klau6AFExTk2oGFknIfEZXguB6bvujs'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ga635s,True,,itsaadarsh,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga635s/hey_coder_this_is_my_new_project_face_paradise_ik/,all_ads,False,https://i.redd.it/45sc8n4n3qv41.jpg,155203,1588151291.0,0,,False,,,,
,learnmachinelearning,,t2_2yrrce83,False,,0,False,Responsible AI for Engineers,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ga8ipv,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HMSmLe-Az3s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'GOTO 2019 â€¢ Responsible AI for Engineers â€¢ Ron Bodkin', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HMSmLe-Az3s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HMSmLe-Az3s/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/GotoConferences'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HMSmLe-Az3s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ga8ipv', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/mLAej2KqA5yz7F3NUD76g2cmHV3XDLlkUjNkOc0A1sA.jpg,False,,[],{},rich:video,,False,,1588192078.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/KPniPClRsXPAbli8EkcwO_jo_En3bL4AqmaIJ3Ensds.jpg?auto=webp&amp;s=eb6b60c18ae59e05036ab554b1c989fec1ba51dc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/KPniPClRsXPAbli8EkcwO_jo_En3bL4AqmaIJ3Ensds.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd66eca384983b6ce34843fcd82519897f39abd3', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/KPniPClRsXPAbli8EkcwO_jo_En3bL4AqmaIJ3Ensds.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e84e7eeace88aa86d3539a3ac4088970d4651d69', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/KPniPClRsXPAbli8EkcwO_jo_En3bL4AqmaIJ3Ensds.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7b1cea8273e17c01ad4a169cbc1e958111e9630', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'rTpLphkvgXuxf4IATjwCl_9k7ER8GsQiVvVNU-FZ_bU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga8ipv,True,,goto-con,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga8ipv/responsible_ai_for_engineers/,all_ads,False,https://youtu.be/HMSmLe-Az3s?list=PLEx5khR4g7PLHBVGOjNbevChU9DOL3Axj,155203,1588163278.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'GOTO 2019 â€¢ Responsible AI for Engineers â€¢ Ron Bodkin', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/HMSmLe-Az3s?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'GOTO Conferences', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/HMSmLe-Az3s/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/GotoConferences'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_4klfxmco,False,,0,False,I don't know whether this is appropriate or not but I was learning Clustering and found this very funny....,[],r/learnmachinelearning,False,6,,0,68.0,False,t3_g9vi8g,False,dark,0.87,,public,17,0,{},140.0,,False,[],,True,False,,{},,False,17,,False,https://a.thumbs.redditmedia.com/QkjLAQz05EdxpePX6eJarWPBKITlt8FMou18V3H9oP0.jpg,False,,[],{},image,,False,,1588136391.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?auto=webp&amp;s=383fe77b660bbe6dddd40861d9acf8ccb99bd8f1', 'width': 4032, 'height': 1960}, 'resolutions': [{'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=763cadfd1d1b3bf5a78faf1d3ce5a99808155d38', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fec30ec3ea993fac24c6c02a108a6ca209682d1b', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab04dbc8a19d68461ea28a0c0736a6fa891ff110', 'width': 320, 'height': 155}, {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b642306c7a5e982a26b0c147ddb3f240e66195e0', 'width': 640, 'height': 311}, {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b9400a8cb9b3968ce2bb6c73a85a79346a899fbd', 'width': 960, 'height': 466}, {'url': 'https://preview.redd.it/0t5z476phmv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=adfe710882f861fd4d974828768870d97f3007d1', 'width': 1080, 'height': 525}], 'variants': {}, 'id': 'YEiniKd4ii1EZ_PDiOXMmAqC8--6Hgfg41Vj6W8F5kY'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9vi8g,True,,siddharth0703,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9vi8g/i_dont_know_whether_this_is_appropriate_or_not/,all_ads,False,https://i.redd.it/0t5z476phmv41.jpg,155203,1588107591.0,0,,False,,,,
,learnmachinelearning,"Hey, I want to build a face detection and recognition project. For face detection, I read on the net that mtcnn is a good option. However, It seems that it is a ready-made system, I just need to upload images. I don't need to put any work. My question is how can I put such a project on my resume if all the work has been done already, am I missing something here?. I am new to the field, so please be patient with me,",t2_6ifad1b,False,,0,False,Face recognition project for the resume?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga85kj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588190510.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I want to build a face detection and recognition project. For face detection, I read on the net that mtcnn is a good option. However, It seems that it is a ready-made system, I just need to upload images. I don&amp;#39;t need to put any work. My question is how can I put such a project on my resume if all the work has been done already, am I missing something here?. I am new to the field, so please be patient with me,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ga85kj,True,,akagami_no_indra,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga85kj/face_recognition_project_for_the_resume/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga85kj/face_recognition_project_for_the_resume/,155203,1588161710.0,0,,False,,,,
,learnmachinelearning,"Hey guys,

I'm working on a NN solving some communication theory tasks. But now I've got the problem, that, when defining a model, I can't add the BatchNormalization layer, as well as the Lambda layer to it. I'm quite new to ML, so maybe one of you can help me?

Thanks in advance! :)

P.S.: Here's the depending part of the code:

`input_signal = Input(shape=(M,))`

`model_transmitter = models.Sequential()`
`model_transmitter.add(layers.Dense(M, activation='relu')(input_signal)) `
`model_transmitter.add(layers.Dense(2*n_channel, activation='linear') `
`model_transmitter = BatchNormalization()(model_transmitter) `
`model_transmitter.layers.BatchNormalization()`",t2_yn9axez,False,,0,False,How to use BatchNormalization Layer in a model?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga7ii6,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,1588173632.0,,[],{},,,True,,1588187467.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a NN solving some communication theory tasks. But now I&amp;#39;ve got the problem, that, when defining a model, I can&amp;#39;t add the BatchNormalization layer, as well as the Lambda layer to it. I&amp;#39;m quite new to ML, so maybe one of you can help me?&lt;/p&gt;

&lt;p&gt;Thanks in advance! :)&lt;/p&gt;

&lt;p&gt;P.S.: Here&amp;#39;s the depending part of the code:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;input_signal = Input(shape=(M,))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model_transmitter = models.Sequential()&lt;/code&gt;
&lt;code&gt;model_transmitter.add(layers.Dense(M, activation=&amp;#39;relu&amp;#39;)(input_signal))&lt;/code&gt;
&lt;code&gt;model_transmitter.add(layers.Dense(2*n_channel, activation=&amp;#39;linear&amp;#39;)&lt;/code&gt;
&lt;code&gt;model_transmitter = BatchNormalization()(model_transmitter)&lt;/code&gt;
&lt;code&gt;model_transmitter.layers.BatchNormalization()&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ga7ii6,True,,Cytosius,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga7ii6/how_to_use_batchnormalization_layer_in_a_model/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga7ii6/how_to_use_batchnormalization_layer_in_a_model/,155203,1588158667.0,0,,False,,,,
,learnmachinelearning,"I am a master's student on a big data program and we touch various technologies and machine learning methods. Recently, I came up a project where electrical characteristics of machines are measured (current, voltages, power, and many more). Basically sensors take the status of the machine every few seconds.

The data is used for monitoring reasons, but also wants to be used to implement some machine learning algorithms and to provide further information about the machine. So I have been researching this and have found various methods. For example:

-RUL (remaining useful life): a regression method to generate a degradation curve over time which you could use to predict where in the curve the machine is.

-Classification method: creating labels from the data as necessary. For example, fail status and have negative labels for normal behaviour and positive fail labels when it gets closer to the failure.

-Anomaly detection: using methods to find instances where the behaviour is not ""normal"" as defined from the data.

These seem to be used for various similar cases and I can see how they could be structured to work on this situation. However, it'd be my first time setting up a project at this scale and on my own, and keep looking for more guidance/examples on how to structure the whole thing. 

I am looking for any advice on things like predictive maintenance, using machine learning for monitoring machinery or anything similar dealing with time series and the like. Even just advice on moving up from machine learning projects where there's a clear defined objective (school, kaggle and things like that), to use cases where it's not so clear how to define the problem.

For now it's just me setting this up along with electrical engineers (don't know machine learning implementation, but have domain knowledge) and we might get some more experienced people in machine learning to help out as well. But I'm hoping to get things rolling.

Any advice would be appreciated!",t2_1uoiganw,False,,0,False,How to get started with predictive maintenance?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga7fqm,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588187111.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a master&amp;#39;s student on a big data program and we touch various technologies and machine learning methods. Recently, I came up a project where electrical characteristics of machines are measured (current, voltages, power, and many more). Basically sensors take the status of the machine every few seconds.&lt;/p&gt;

&lt;p&gt;The data is used for monitoring reasons, but also wants to be used to implement some machine learning algorithms and to provide further information about the machine. So I have been researching this and have found various methods. For example:&lt;/p&gt;

&lt;p&gt;-RUL (remaining useful life): a regression method to generate a degradation curve over time which you could use to predict where in the curve the machine is.&lt;/p&gt;

&lt;p&gt;-Classification method: creating labels from the data as necessary. For example, fail status and have negative labels for normal behaviour and positive fail labels when it gets closer to the failure.&lt;/p&gt;

&lt;p&gt;-Anomaly detection: using methods to find instances where the behaviour is not &amp;quot;normal&amp;quot; as defined from the data.&lt;/p&gt;

&lt;p&gt;These seem to be used for various similar cases and I can see how they could be structured to work on this situation. However, it&amp;#39;d be my first time setting up a project at this scale and on my own, and keep looking for more guidance/examples on how to structure the whole thing. &lt;/p&gt;

&lt;p&gt;I am looking for any advice on things like predictive maintenance, using machine learning for monitoring machinery or anything similar dealing with time series and the like. Even just advice on moving up from machine learning projects where there&amp;#39;s a clear defined objective (school, kaggle and things like that), to use cases where it&amp;#39;s not so clear how to define the problem.&lt;/p&gt;

&lt;p&gt;For now it&amp;#39;s just me setting this up along with electrical engineers (don&amp;#39;t know machine learning implementation, but have domain knowledge) and we might get some more experienced people in machine learning to help out as well. But I&amp;#39;m hoping to get things rolling.&lt;/p&gt;

&lt;p&gt;Any advice would be appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga7fqm,True,,electricIbis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga7fqm/how_to_get_started_with_predictive_maintenance/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga7fqm/how_to_get_started_with_predictive_maintenance/,155203,1588158311.0,0,,False,,,,
,learnmachinelearning,"I'll accept it being taught with any other package/language as long as I'm coding. Obviously I want to learn linear algebra to better understand the underlying math in machine learning. Please and thank you

Extra question: how did you learn linear algebra?",t2_snoik,False,,0,False,Are there any courses/books that teach linear algebra with numpy?,[],r/learnmachinelearning,False,6,,0,,False,t3_g9zv4x,False,dark,0.78,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,True,self,False,,[],{},,,True,,1588151244.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ll accept it being taught with any other package/language as long as I&amp;#39;m coding. Obviously I want to learn linear algebra to better understand the underlying math in machine learning. Please and thank you&lt;/p&gt;

&lt;p&gt;Extra question: how did you learn linear algebra?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9zv4x,True,,Mjjjokes,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9zv4x/are_there_any_coursesbooks_that_teach_linear/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9zv4x/are_there_any_coursesbooks_that_teach_linear/,155203,1588122444.0,0,,False,,,,
,learnmachinelearning,I am first year B. Tech student and was hoping to ge internships in Machine Learning. I have all the basic knowledge in Machine learning(ML and Deep Learning) but i don't have any remarkable project which i can add to my resume...Can someone suggest m a Machine Learning (or Deep learning) project which will help m get a work internship or a Research Internship.,t2_5l3no2oj,False,,0,False,Good Project to write in Resume as an Indian Machine Learning enthusiast for internships,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga6vnh,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},,,True,,1588184311.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am first year B. Tech student and was hoping to ge internships in Machine Learning. I have all the basic knowledge in Machine learning(ML and Deep Learning) but i don&amp;#39;t have any remarkable project which i can add to my resume...Can someone suggest m a Machine Learning (or Deep learning) project which will help m get a work internship or a Research Internship.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ga6vnh,True,,seneark_,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga6vnh/good_project_to_write_in_resume_as_an_indian/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga6vnh/good_project_to_write_in_resume_as_an_indian/,155203,1588155511.0,0,,False,,,,
,learnmachinelearning,"Beginner here. Anyone else overwhelmed by the wealth of knowledge and information for concepts? 

What steps have some of you taken in order to organize material, gauge progress and solidify understanding?",t2_rl2ll,False,,0,False,Too Many Resources and Info,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9w2wk,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},Question,False,10,,False,self,False,,[],{},,,True,,1588138197.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Beginner here. Anyone else overwhelmed by the wealth of knowledge and information for concepts? &lt;/p&gt;

&lt;p&gt;What steps have some of you taken in order to organize material, gauge progress and solidify understanding?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9w2wk,True,,cwoodall6,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9w2wk/too_many_resources_and_info/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9w2wk/too_many_resources_and_info/,155203,1588109397.0,0,,False,,,,
,learnmachinelearning,"Hello all, 

I am trying to build out the solutions for a recently published book called [Mathematics for Machine Learning](https://mml-book.github.io/). I have done most of the solutions on paper and I am in the process of transferring them to Latex. Would anyone be interesting in helping me in this endeavour? I would love for the opportunity to compare my answers to others because there is no solutions to the exercises in the book. I have created a [github repository](https://github.com/jewelltaylor/Mathematics-for-Machine-Learning-Solutions) to help facilitate collaboration. Let me know if any of you are interested!",t2_654bfula,False,,0,False,Mathematics for Machine Learning Solutions Group,[],r/learnmachinelearning,False,6,,0,,False,t3_g9t2j0,False,dark,0.93,,public,13,0,{},,,False,[],,False,False,,{},,False,13,,False,self,False,,[],{},self,,True,,1588128744.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, &lt;/p&gt;

&lt;p&gt;I am trying to build out the solutions for a recently published book called &lt;a href=""https://mml-book.github.io/""&gt;Mathematics for Machine Learning&lt;/a&gt;. I have done most of the solutions on paper and I am in the process of transferring them to Latex. Would anyone be interesting in helping me in this endeavour? I would love for the opportunity to compare my answers to others because there is no solutions to the exercises in the book. I have created a &lt;a href=""https://github.com/jewelltaylor/Mathematics-for-Machine-Learning-Solutions""&gt;github repository&lt;/a&gt; to help facilitate collaboration. Let me know if any of you are interested!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?auto=webp&amp;s=a5a066e892cd1a887ba10e324183d3020e0906fe', 'width': 180, 'height': 261}, 'resolutions': [{'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=324d98b2afe974c9923a90b7fece00bbb8c2357f', 'width': 108, 'height': 156}], 'variants': {}, 'id': 'XzFfeCJvwXSOhVXOWBpN_GqQftnCRE5pJg-f7VJIAm8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9t2j0,True,,jewelltaylor,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9t2j0/mathematics_for_machine_learning_solutions_group/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9t2j0/mathematics_for_machine_learning_solutions_group/,155203,1588099944.0,0,,False,,,,
,learnmachinelearning,"Talked to a couple students a year ahead in my CS program. We're all doing an emphasis in Data Science. 

They said the intro Deep Learning class senior year is great, but the intro Machine Learning class sucks! They were majorly disappointed by how outdated the curriculum was.

Looking for resources to compensate and help prep for the industry... What are some solid options?",t2_4skyz1wi,False,,0,False,Disappointing Intro to ML class,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga5uio,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588178760.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Talked to a couple students a year ahead in my CS program. We&amp;#39;re all doing an emphasis in Data Science. &lt;/p&gt;

&lt;p&gt;They said the intro Deep Learning class senior year is great, but the intro Machine Learning class sucks! They were majorly disappointed by how outdated the curriculum was.&lt;/p&gt;

&lt;p&gt;Looking for resources to compensate and help prep for the industry... What are some solid options?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ga5uio,True,,booreiBlue,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga5uio/disappointing_intro_to_ml_class/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga5uio/disappointing_intro_to_ml_class/,155203,1588149960.0,0,,False,,,,
,learnmachinelearning,,t2_117v1r,False,,0,False,I'll probably hang some pictures my GAN generated on the wall. What do you think?,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g9w7gg,False,dark,0.88,,public,6,0,{},140.0,,False,[],,True,False,,{},,False,6,,False,https://b.thumbs.redditmedia.com/ZzcJpMXv-AWScLFybX5ZrDo90IhaxvMT9igz77stXQI.jpg,False,,[],{},image,,False,,1588138592.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/tmaabfb9omv41.png?auto=webp&amp;s=922ded164784dc9a5b94c55ab7d748dfc412bf06', 'width': 2048, 'height': 2048}, 'resolutions': [{'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b0963a5d7e8db0a46c306cf698695d30672f866', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f24065658e18ba18499e8850eee124e89acce86a', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=59bf85f833c8755555fbfb891eceacb28b0dab42', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9935f32ce6672dc92b09db2e1a1495ea9d3e0192', 'width': 640, 'height': 640}, {'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e119ac6fe0ebbaea0ad6cd6af1f274f822d05b11', 'width': 960, 'height': 960}, {'url': 'https://preview.redd.it/tmaabfb9omv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03e0f330015a22921c12143cc0afce38561e106b', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'Y7osOpXmK7HJIxlvAFel7cNLV2wYRF3xUh7ADbwcKNU'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9w7gg,True,,JackLoStrambo,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9w7gg/ill_probably_hang_some_pictures_my_gan_generated/,all_ads,False,https://i.redd.it/tmaabfb9omv41.png,155203,1588109792.0,0,,False,,,,
,learnmachinelearning," 

Let's say I have a dataset with multiple types of multiple ingredients (salt1, salt2, etc). Each nth variation of each ingredient vs flavor may be represented by an nxk matrix that where an ingredient corresponds with a particular value of ""flavor"".

A recipe consists of a 1xn vector (where n is the number of ingredients) where each value corresponds to the quantity of ingredient in the recipe.

A particular combination of ingredients, with particular weights, with *some* transformation, would result in a particular 1xk ""flavor"" profile, in this simple model.

One approach could be to formulate this as a Probabilistic Matrix Factorization problem (I think), with k being the number of flavor parameters. And combining the recipe vector with the flavor matrix might do the trick.

But the problem is, the flavor value of each ingredient (and each variation of the ingredient) in the ingredient-flavor matrix would be very very limited. The recipe flavor profile might have a corresponding flavor vector, that too would be limited, and would not be available, at the beginning. So in order to capture the relationship between the ingredients and the flavor, the system would be dependent on user-submitted data on recipe/ingredient flavors.

Is there a way I could create clusters of recipes based on user flavor ratings and extrapolate these to the constituent ingredients or vice versa? Could this be done via some unsupervised learning algorithm?

I am quite new to this, I would appreciate some help or some pointers to which mathematical approaches I should be looking at to model this problem.",t2_6c316kor,False,,0,False,How do I approach this problem?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga5b71,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588175887.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s say I have a dataset with multiple types of multiple ingredients (salt1, salt2, etc). Each nth variation of each ingredient vs flavor may be represented by an nxk matrix that where an ingredient corresponds with a particular value of &amp;quot;flavor&amp;quot;.&lt;/p&gt;

&lt;p&gt;A recipe consists of a 1xn vector (where n is the number of ingredients) where each value corresponds to the quantity of ingredient in the recipe.&lt;/p&gt;

&lt;p&gt;A particular combination of ingredients, with particular weights, with &lt;em&gt;some&lt;/em&gt; transformation, would result in a particular 1xk &amp;quot;flavor&amp;quot; profile, in this simple model.&lt;/p&gt;

&lt;p&gt;One approach could be to formulate this as a Probabilistic Matrix Factorization problem (I think), with k being the number of flavor parameters. And combining the recipe vector with the flavor matrix might do the trick.&lt;/p&gt;

&lt;p&gt;But the problem is, the flavor value of each ingredient (and each variation of the ingredient) in the ingredient-flavor matrix would be very very limited. The recipe flavor profile might have a corresponding flavor vector, that too would be limited, and would not be available, at the beginning. So in order to capture the relationship between the ingredients and the flavor, the system would be dependent on user-submitted data on recipe/ingredient flavors.&lt;/p&gt;

&lt;p&gt;Is there a way I could create clusters of recipes based on user flavor ratings and extrapolate these to the constituent ingredients or vice versa? Could this be done via some unsupervised learning algorithm?&lt;/p&gt;

&lt;p&gt;I am quite new to this, I would appreciate some help or some pointers to which mathematical approaches I should be looking at to model this problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga5b71,True,,ml_orange,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga5b71/how_do_i_approach_this_problem/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga5b71/how_do_i_approach_this_problem/,155203,1588147087.0,0,,False,,,,
,learnmachinelearning,"I wanna go deep in DL.That's why, trying to gather as much MATH knowledge as possible. Share any resource about optimization related to DL. 
TIA",t2_3cspjb6o,False,,0,False,"Recommend some notes/videos or any resources about ""Mathematical Optimization for ML/DL""",[],r/learnmachinelearning,False,6,,0,,False,t3_ga2kco,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588162320.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wanna go deep in DL.That&amp;#39;s why, trying to gather as much MATH knowledge as possible. Share any resource about optimization related to DL. 
TIA&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga2kco,True,,Noman_al_a,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga2kco/recommend_some_notesvideos_or_any_resources_about/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga2kco/recommend_some_notesvideos_or_any_resources_about/,155203,1588133520.0,0,,False,,,,
,learnmachinelearning,"Hello all,

I would like to share **classes about Deep Learning for Artists** I am teaching at the **University of the Arts** in London.

The lectures are being recorded and are publicly available on YouTube in playlist: [https://www.youtube.com/playlist?list=PLCIVpmFkFKQ88lzWtYW2MCwqXofhkzqgt](https://www.youtube.com/playlist?list=PLCIVpmFkFKQ88lzWtYW2MCwqXofhkzqgt)

We also have git pages (with practice codes and slides) at:  [https://github.com/previtus/cci\_exploring\_machine\_intelligence/](https://github.com/previtus/cci_exploring_machine_intelligence/) 

&amp;#x200B;

[This was our first introductory lesson -\&gt; https:\/\/youtu.be\/b6bjeelzB5c](https://preview.redd.it/higi4z05rlv41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=e1409a3348819e6b72ba557ecdc286216820c880)

Over the 8 weeks course, I am planning to cover topics from the basics of Machine Learning all the way to critically explore the current state-of-the-art Generative models and algorithms. Currently, we are in the third week.

**\*\*How is the course any different from the other many ML courses out there?\*\***

While I also have to start with some ML basics (what is a neuron network, etc.), I'll focus on generative models, ways of using model embedding creatively (remember *X degrees of separation* for example?) and all that jazz. Already in these early classes we are exploring ways of interacting with neural networks beyond the usual latent vector interpolation. Imagine for example *""NN hacking""* by changing weights of generative models in real time, etc.

**\*\*Is this a legit course :D? Who is the teacher?\*\***

My name is VÃ­t RÅ¯Å¾iÄka and I have been doing research at places such as Carnegie Mellon University in USA or ETH Zurich in Europe (and right now I ended up at UAL).

My current focus is in the category of Creative AI and Computational Arts - and this class is more or less an exploration of this field! This course was officially designed as a Master degree course taught at the Creative Computing Institute at UAL.

I also expect to host  Rebecca Fiebrink as a guest lecturer from UAL who will teach about interaction with ML models (and also her addition into this field, the Wekinator).

**\*\*Why share it?\*\***

In this time of lockdown, I was more or less forced to make my lectures in the form of pre-recorded video lessons. And while this takes about 3x the time usual class would take me to prepare in a classroom, it also allows everyone else around the world to also look at it! So, I want to share this lecture to a larger audience :).

We have weekly live practical sessions which are only for the enrolled students, but these are also being recorded. If anyone wants to reach out to me outside of these students, please do so at my mail: [v.ruzicka@arts.ac.uk](mailto:v.ruzicka@arts.ac.uk)

&amp;#x200B;

**Looking forward to seeing anyone else interested by this field!**

\~ Best, VÃ­t RÅ¯Å¾iÄka",t2_1kqjv5j1,False,,0,False,Course on Machine Learning for Artists at the University of the Arts London - free video lectures,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g9svnu,False,dark,0.92,,public,11,0,{},140.0,,False,[],,False,False,,{},,False,11,,False,https://b.thumbs.redditmedia.com/wCqFRDK6c2gYw_r_6KpTXl8pX8G3bDnJVVDoVRlfnXE.jpg,False,,[],{},self,,True,,1588128137.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;I would like to share &lt;strong&gt;classes about Deep Learning for Artists&lt;/strong&gt; I am teaching at the &lt;strong&gt;University of the Arts&lt;/strong&gt; in London.&lt;/p&gt;

&lt;p&gt;The lectures are being recorded and are publicly available on YouTube in playlist: &lt;a href=""https://www.youtube.com/playlist?list=PLCIVpmFkFKQ88lzWtYW2MCwqXofhkzqgt""&gt;https://www.youtube.com/playlist?list=PLCIVpmFkFKQ88lzWtYW2MCwqXofhkzqgt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We also have git pages (with practice codes and slides) at:  &lt;a href=""https://github.com/previtus/cci_exploring_machine_intelligence/""&gt;https://github.com/previtus/cci_exploring_machine_intelligence/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/higi4z05rlv41.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1409a3348819e6b72ba557ecdc286216820c880""&gt;This was our first introductory lesson -&amp;gt; https://youtu.be/b6bjeelzB5c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Over the 8 weeks course, I am planning to cover topics from the basics of Machine Learning all the way to critically explore the current state-of-the-art Generative models and algorithms. Currently, we are in the third week.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**How is the course any different from the other many ML courses out there?*\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;While I also have to start with some ML basics (what is a neuron network, etc.), I&amp;#39;ll focus on generative models, ways of using model embedding creatively (remember &lt;em&gt;X degrees of separation&lt;/em&gt; for example?) and all that jazz. Already in these early classes we are exploring ways of interacting with neural networks beyond the usual latent vector interpolation. Imagine for example &lt;em&gt;&amp;quot;NN hacking&amp;quot;&lt;/em&gt; by changing weights of generative models in real time, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**Is this a legit course :D? Who is the teacher?*\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;My name is VÃ­t RÅ¯Å¾iÄka and I have been doing research at places such as Carnegie Mellon University in USA or ETH Zurich in Europe (and right now I ended up at UAL).&lt;/p&gt;

&lt;p&gt;My current focus is in the category of Creative AI and Computational Arts - and this class is more or less an exploration of this field! This course was officially designed as a Master degree course taught at the Creative Computing Institute at UAL.&lt;/p&gt;

&lt;p&gt;I also expect to host  Rebecca Fiebrink as a guest lecturer from UAL who will teach about interaction with ML models (and also her addition into this field, the Wekinator).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;**Why share it?*\&lt;/strong&gt;*&lt;/p&gt;

&lt;p&gt;In this time of lockdown, I was more or less forced to make my lectures in the form of pre-recorded video lessons. And while this takes about 3x the time usual class would take me to prepare in a classroom, it also allows everyone else around the world to also look at it! So, I want to share this lecture to a larger audience :).&lt;/p&gt;

&lt;p&gt;We have weekly live practical sessions which are only for the enrolled students, but these are also being recorded. If anyone wants to reach out to me outside of these students, please do so at my mail: [&lt;a href=""mailto:v.ruzicka@arts.ac.uk""&gt;v.ruzicka@arts.ac.uk&lt;/a&gt;](mailto:&lt;a href=""mailto:v.ruzicka@arts.ac.uk""&gt;v.ruzicka@arts.ac.uk&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Looking forward to seeing anyone else interested by this field!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;~ Best, VÃ­t RÅ¯Å¾iÄka&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pRPc6bLb4M-xVTzPtdmktWCQJA5eVU9-cOfW5BLdrHY.jpg?auto=webp&amp;s=2aaf17028f4cb7ebbf74dd11fae230024f0bc47c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/pRPc6bLb4M-xVTzPtdmktWCQJA5eVU9-cOfW5BLdrHY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54b2d09559311557af4e48cf96307bbb35f17016', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'wEZVNk_jLe9I9PPD2SFQEsQf8REr-zv4fpL2k5uxklQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9svnu,True,,VitRuzicka,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9svnu/course_on_machine_learning_for_artists_at_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9svnu/course_on_machine_learning_for_artists_at_the/,155203,1588099337.0,0,,False,,,"{'higi4z05rlv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89e719c0f7592b6eecfd0ff8eade78b20e11fd5d'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4abca0fa5bd615a09b9b91eab0a3737b8e6a078a'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ed7aed4c1bd5cf128247fe562ffc83f8ab366be'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4fcb641f143ae1997c400305e0dd8d01c97eae1'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b0597ad50f90209e8911adc4d1c76de3425d1f0'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9156b6e65147a52c794230ac2f673d2cba2ae71'}], 's': {'y': 720, 'x': 1280, 'u': 'https://preview.redd.it/higi4z05rlv41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=e1409a3348819e6b72ba557ecdc286216820c880'}, 'id': 'higi4z05rlv41'}}",
,learnmachinelearning,,t2_61xh5qps,False,,0,False,Is there a Python version of Dr. Koller's Probabilistic Graphical Models class (on Coursera) assignments ?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga4yq3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588174021.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga4yq3,True,,ppsrs,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga4yq3/is_there_a_python_version_of_dr_kollers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga4yq3/is_there_a_python_version_of_dr_kollers/,155203,1588145221.0,0,,False,,,,
,learnmachinelearning,"Hey guys!

While sitting at home during the quarantine and thinking about life and what not I figured it would be nice to have a bot to discuss some of those existential questions with. After some research, I found that distil-gpt2 language model is available from huggingface transformers and decided to try it out. The model is fine-tuned using Yahoo Answers data. API and the front-end app are deployed to the Google cloud.

Here's the link to the video: [https://www.youtube.com/watch?v=w1hvAWcGd2g](https://www.youtube.com/watch?v=w1hvAWcGd2g)",t2_5rs2yt94,False,,0,False,Building an AI bot to answer lifelong questions,[],r/learnmachinelearning,False,6,,0,,False,t3_ga4uax,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588173360.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys!&lt;/p&gt;

&lt;p&gt;While sitting at home during the quarantine and thinking about life and what not I figured it would be nice to have a bot to discuss some of those existential questions with. After some research, I found that distil-gpt2 language model is available from huggingface transformers and decided to try it out. The model is fine-tuned using Yahoo Answers data. API and the front-end app are deployed to the Google cloud.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s the link to the video: &lt;a href=""https://www.youtube.com/watch?v=w1hvAWcGd2g""&gt;https://www.youtube.com/watch?v=w1hvAWcGd2g&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UHO7liMO0rVRbm-sDSrBfNzadmTWRiqeZcHi0v2cJkY.jpg?auto=webp&amp;s=3af9dca6fce7d4703a45f95b821dcd703711568e', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/UHO7liMO0rVRbm-sDSrBfNzadmTWRiqeZcHi0v2cJkY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee603e7e5acf95e48a909934f7ca369f3b57cd38', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/UHO7liMO0rVRbm-sDSrBfNzadmTWRiqeZcHi0v2cJkY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7aecbb616b553e7f632ec7d76c08c9a6c9ea0ec', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/UHO7liMO0rVRbm-sDSrBfNzadmTWRiqeZcHi0v2cJkY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4e3f9bbcd5adaf7869aef6feb33611d192d5450', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'xrOc01tpnmhBXmXT3c_-xtaUZLFF0NFO85JP0Rlhjnc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga4uax,True,,adam0ling,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga4uax/building_an_ai_bot_to_answer_lifelong_questions/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga4uax/building_an_ai_bot_to_answer_lifelong_questions/,155203,1588144560.0,0,,False,,,,
,learnmachinelearning,"It doesn't matter if it's something trivial. As long as it's new information about machine learning you didn't know until this week, feel free to share!",t2_6l4z3,False,,0,False,TWIL (This Week I Learned) - Share something new that you have learned this week!,[],r/learnmachinelearning,False,6,,0,,False,t3_ga4pfq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588172677.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It doesn&amp;#39;t matter if it&amp;#39;s something trivial. As long as it&amp;#39;s new information about machine learning you didn&amp;#39;t know until this week, feel free to share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,ga4pfq,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga4pfq/twil_this_week_i_learned_share_something_new_that/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga4pfq/twil_this_week_i_learned_share_something_new_that/,155203,1588143877.0,0,,False,,,,
,learnmachinelearning," Calling all marketing data science experts!

I have customer dataset and a corresponding transaction dataset with a few million purchases of various products. This is not homework, this is real work.

If I wanted to score or rank the customers propensity to buy based on their purchases - for example a score from one to ten or a probability - what would be the best way to approach this? The goal is to end up with a column where I can see that John Doe is highly likely to buy something / generally loves buying a lot / a good opportunity for a marketing email and so on.

At first I thought it's simple, just look at their average transaction amount and how often they buy. Larger values = probably big spenders. But I think that's a bit too simple.

I've also researched RFM methods for ranking a customer based on transaction data. This is a step up but only takes into account three features. I have a feeling I can squeeze more out of the data.

What sort of models should I be looking at if I have customer demographics, and what they're buying, and how much they're spending, how often etc.

And if somebody has a model in mind, can it be applied to single products too? For example, John Doe's propensity to buy toothpase.

I work mainly in Python if that helps.

Thanks",t2_xk5mr,False,,0,False,"How to measure a customer's ""propensity to buy"" or apply a purchasing ""score"" to a set of customers with transactions?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9pf6x,False,dark,0.87,,public,16,0,{},,,False,[],,False,False,,{},Question,False,16,,False,self,False,,[],{},,,True,,1588117028.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Calling all marketing data science experts!&lt;/p&gt;

&lt;p&gt;I have customer dataset and a corresponding transaction dataset with a few million purchases of various products. This is not homework, this is real work.&lt;/p&gt;

&lt;p&gt;If I wanted to score or rank the customers propensity to buy based on their purchases - for example a score from one to ten or a probability - what would be the best way to approach this? The goal is to end up with a column where I can see that John Doe is highly likely to buy something / generally loves buying a lot / a good opportunity for a marketing email and so on.&lt;/p&gt;

&lt;p&gt;At first I thought it&amp;#39;s simple, just look at their average transaction amount and how often they buy. Larger values = probably big spenders. But I think that&amp;#39;s a bit too simple.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve also researched RFM methods for ranking a customer based on transaction data. This is a step up but only takes into account three features. I have a feeling I can squeeze more out of the data.&lt;/p&gt;

&lt;p&gt;What sort of models should I be looking at if I have customer demographics, and what they&amp;#39;re buying, and how much they&amp;#39;re spending, how often etc.&lt;/p&gt;

&lt;p&gt;And if somebody has a model in mind, can it be applied to single products too? For example, John Doe&amp;#39;s propensity to buy toothpase.&lt;/p&gt;

&lt;p&gt;I work mainly in Python if that helps.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9pf6x,True,,Scutterbum,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9pf6x/how_to_measure_a_customers_propensity_to_buy_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9pf6x/how_to_measure_a_customers_propensity_to_buy_or/,155203,1588088228.0,0,,False,,,,
,learnmachinelearning," 

I have created a simple Mancala like board game in python in which the board is just represented by a list with the amount of pieces in each pit. I have also created an algorithm based player that just follows a set of rules to pick the 'Best' move to play against.

My goal is just to train an AI player against the algorithm player to see how good it can get. However, I have absolutely no idea how to go about doing this. I have been watching YouTube videos all day on AI/Machine Learning, but nothing seems to help. What 'type' of machine learning should I choose to do this? Like should I try 'supervised' 'unsupervised', or 'reinforced' learning? And can somebody give me an outline of how I might implement this, or any resources that would be helpful?",t2_69bp939g,False,,0,False,Where do i begin?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga3nps,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588167339.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have created a simple Mancala like board game in python in which the board is just represented by a list with the amount of pieces in each pit. I have also created an algorithm based player that just follows a set of rules to pick the &amp;#39;Best&amp;#39; move to play against.&lt;/p&gt;

&lt;p&gt;My goal is just to train an AI player against the algorithm player to see how good it can get. However, I have absolutely no idea how to go about doing this. I have been watching YouTube videos all day on AI/Machine Learning, but nothing seems to help. What &amp;#39;type&amp;#39; of machine learning should I choose to do this? Like should I try &amp;#39;supervised&amp;#39; &amp;#39;unsupervised&amp;#39;, or &amp;#39;reinforced&amp;#39; learning? And can somebody give me an outline of how I might implement this, or any resources that would be helpful?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga3nps,True,,justmadethisthroawy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga3nps/where_do_i_begin/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga3nps/where_do_i_begin/,155203,1588138539.0,0,,False,,,,
,learnmachinelearning,"Hi. I'm studying the famous Xavier initialization paper (_Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot and Bengio, 2010)_) and had a question.

When they explain the forward pass of neural networks, they sample the weight initialization values from a Uniform distribution of equal variance. The intuition is apparently because homogeneity of variance allows training to flow smoother. However, I'm curious as to why this is the case?

That is, why does HoV allow ""smoother"" training?",t2_m8kccne,False,,0,False,Why is homoscedasticity (homogeneity of variance) important in neural network layers?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga2xxz,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588164022.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;m studying the famous Xavier initialization paper (&lt;em&gt;Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot and Bengio, 2010)&lt;/em&gt;) and had a question.&lt;/p&gt;

&lt;p&gt;When they explain the forward pass of neural networks, they sample the weight initialization values from a Uniform distribution of equal variance. The intuition is apparently because homogeneity of variance allows training to flow smoother. However, I&amp;#39;m curious as to why this is the case?&lt;/p&gt;

&lt;p&gt;That is, why does HoV allow &amp;quot;smoother&amp;quot; training?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ga2xxz,True,,Seankala,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga2xxz/why_is_homoscedasticity_homogeneity_of_variance/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga2xxz/why_is_homoscedasticity_homogeneity_of_variance/,155203,1588135222.0,0,,False,,,,
,learnmachinelearning,"Hi,

I'm new to deep networks and I was playing around with Dlib and OpenCV's dnn, and had a general question. Overall OpenCV seems to work fairly well, but does seem to have issues identifying faces that are tilted.

My question is, in your experiences does working with a CNN like MTCNN outperform the OpenCV implementation?

Sorry if this is a bit confusing, I'm pretty new to DL.",t2_m1q72,False,,0,False,Does training a custom deep network for face detection generally have better results than OpenCV DNN?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga2x5c,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588163916.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m new to deep networks and I was playing around with Dlib and OpenCV&amp;#39;s dnn, and had a general question. Overall OpenCV seems to work fairly well, but does seem to have issues identifying faces that are tilted.&lt;/p&gt;

&lt;p&gt;My question is, in your experiences does working with a CNN like MTCNN outperform the OpenCV implementation?&lt;/p&gt;

&lt;p&gt;Sorry if this is a bit confusing, I&amp;#39;m pretty new to DL.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga2x5c,True,,Diminitiv,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga2x5c/does_training_a_custom_deep_network_for_face/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga2x5c/does_training_a_custom_deep_network_for_face/,155203,1588135116.0,0,,False,,,,
,learnmachinelearning,"I've found guides for Ubuntu, but haven't seen any for Windows yet so I was curious if anyone had managed to get it running on Windows.",t2_m1q72,False,,0,False,Has anyone been able to setup OpenCV 4's DNN Face Detector with NVIDIA GPU enabled on Windows?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ga2q2o,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588163042.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve found guides for Ubuntu, but haven&amp;#39;t seen any for Windows yet so I was curious if anyone had managed to get it running on Windows.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ga2q2o,True,,Diminitiv,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga2q2o/has_anyone_been_able_to_setup_opencv_4s_dnn_face/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga2q2o/has_anyone_been_able_to_setup_opencv_4s_dnn_face/,155203,1588134242.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,"Question answering dataset specifically designed for COVID-19, from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge!",[],r/learnmachinelearning,False,6,,0,57.0,False,t3_ga1nyg,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/l8dCpCJXlTYTgo29c75z15-NYS2SzzlFEOzn7kh9HJc.jpg,False,,[],{},,,False,,1588158432.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga1nyg,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga1nyg/question_answering_dataset_specifically_designed/,all_ads,False,/r/LatestInML/comments/ga1m4o/question_answering_dataset_specifically_designed/,155203,1588129632.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""Question answering dataset specifically designed for COVID-19, from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge!\n\nFor project and code/requests: [click here](https://www.catalyzex.com/paper/arxiv:2004.11339)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7a9jiu8naov41.png?width=2220&amp;format=png&amp;auto=webp&amp;s=f010377c2eab56c7e748901ab326c78a213ae143"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""Question answering dataset specifically designed for COVID-19, from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge!"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'hide_score': False, 'media_metadata': {'7a9jiu8naov41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 44, 'x': 108, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3831cc756c80605dc4af61cd56a9c2859f452f55'}, {'y': 88, 'x': 216, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51b5162a78265702a26af476c7c9118f01ac639c'}, {'y': 131, 'x': 320, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=28af964a4d7c1f35bd8ec3869621f896c257a978'}, {'y': 262, 'x': 640, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fe1d628c8226d5f699bb2bd9d89b6b3c2c37783'}, {'y': 393, 'x': 960, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=74f7487a2dfe87b3978c06051b05e3f7e79c5e5b'}, {'y': 442, 'x': 1080, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b0132eda3375088775e8ac1008c3b3a73e793436'}], 's': {'y': 910, 'x': 2220, 'u': 'https://preview.redd.it/7a9jiu8naov41.png?width=2220&amp;format=png&amp;auto=webp&amp;s=f010377c2eab56c7e748901ab326c78a213ae143'}, 'id': '7a9jiu8naov41'}}, 'name': 't3_ga1m4o', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.99, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 11, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/l8dCpCJXlTYTgo29c75z15-NYS2SzzlFEOzn7kh9HJc.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588158214.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Question answering dataset specifically designed for COVID-19, from knowledge gathered from Kaggle&amp;#39;s COVID-19 Open Research Dataset Challenge!&lt;/p&gt;\n\n&lt;p&gt;For project and code/requests: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.11339""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/7a9jiu8naov41.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f010377c2eab56c7e748901ab326c78a213ae143""&gt;https://preview.redd.it/7a9jiu8naov41.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f010377c2eab56c7e748901ab326c78a213ae143&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ga1m4o', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/ga1m4o/question_answering_dataset_specifically_designed/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/ga1m4o/question_answering_dataset_specifically_designed/', 'subreddit_subscribers': 3386, 'created_utc': 1588129414.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_ga1m4o,,
,learnmachinelearning,"Hey guys! I am not really sure if this a question for here. I am a student and my department has a lot of data from different factories like costs, time, etc. I am new to ML and was wondering what are the different ways i could play with data to apply ML. Any Research Papers, Book recommendations or general ideas would help me.",t2_1yu2cg2i,False,,0,False,Recommendations for playing with Factory Data,[],r/learnmachinelearning,False,6,,0,,False,t3_ga1fcr,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588157451.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys! I am not really sure if this a question for here. I am a student and my department has a lot of data from different factories like costs, time, etc. I am new to ML and was wondering what are the different ways i could play with data to apply ML. Any Research Papers, Book recommendations or general ideas would help me.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga1fcr,True,,shanahmedshaffi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga1fcr/recommendations_for_playing_with_factory_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga1fcr/recommendations_for_playing_with_factory_data/,155203,1588128651.0,0,,False,,,,
,learnmachinelearning,"I have two questions about dense prediction in [YOLOv4 paper](https://arxiv.org/pdf/2004.10934.pdf#page=2)

&amp;#x200B;

1. What does it mean by the  (hard negative, online hard) example mining method is not applicable to one-stage object detector, because this kind of detector belongs to the dense prediction architecture  ?
2. Why dense prediction does not belong to two-stage detector ?

&amp;#x200B;

https://preview.redd.it/rmnq11jw6ov41.png?width=1290&amp;format=png&amp;auto=webp&amp;s=8c37848069a4cd6bd4f769308746645277a45d18",t2_bpftl,False,,0,False,YOLO Dense Prediction,[],r/learnmachinelearning,False,6,,0,72.0,False,t3_ga1ba1,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/LmGwMq-zTINY_Q0Z4YIj7S9WqyVmkN5h-nICiNBawhc.jpg,False,,[],{},,,True,,1588156974.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have two questions about dense prediction in &lt;a href=""https://arxiv.org/pdf/2004.10934.pdf#page=2""&gt;YOLOv4 paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What does it mean by the  (hard negative, online hard) example mining method is not applicable to one-stage object detector, because this kind of detector belongs to the dense prediction architecture  ?&lt;/li&gt;
&lt;li&gt;Why dense prediction does not belong to two-stage detector ?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/rmnq11jw6ov41.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c37848069a4cd6bd4f769308746645277a45d18""&gt;https://preview.redd.it/rmnq11jw6ov41.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c37848069a4cd6bd4f769308746645277a45d18&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga1ba1,True,,promach,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga1ba1/yolo_dense_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga1ba1/yolo_dense_prediction/,155203,1588128174.0,0,,False,,,"{'rmnq11jw6ov41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b2d316517fa1e3d9d43b66b49d2027889ba3c7e'}, {'y': 112, 'x': 216, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a53949348c9f997ee35bfc27c09fff0b01fa30ed'}, {'y': 166, 'x': 320, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01714f10475ce2b1b86af56a5bf9ea970446e3b0'}, {'y': 333, 'x': 640, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac60b947d7381eb6024b2c2bb0d1c9981196096e'}, {'y': 500, 'x': 960, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd7bb92dcd7ced923a9e708794851a43c28f6ba8'}, {'y': 562, 'x': 1080, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1d9b7daf4b06d384252df6b9f18bfb21f33cb928'}], 's': {'y': 672, 'x': 1290, 'u': 'https://preview.redd.it/rmnq11jw6ov41.png?width=1290&amp;format=png&amp;auto=webp&amp;s=8c37848069a4cd6bd4f769308746645277a45d18'}, 'id': 'rmnq11jw6ov41'}}",
,learnmachinelearning,"See picture for context

\*Edit for clarity: c is context vector, h is hidden

https://preview.redd.it/m0gk5ebf6ov41.png?width=1715&amp;format=png&amp;auto=webp&amp;s=b1c9846d099123d7b3e93d3155b49edf21ec88b3",t2_2209vt1n,False,,0,False,Does concatenation in math mean use contiguous tensor in code?,[],r/learnmachinelearning,False,6,,0,71.0,False,t3_ga19tq,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/tphwAy3t0Gks1OlOZJq_Dtjfopv8JYdyEvlv4JgVEVU.jpg,1588128398.0,,[],{},,,True,,1588156812.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;See picture for context&lt;/p&gt;

&lt;p&gt;*Edit for clarity: c is context vector, h is hidden&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/m0gk5ebf6ov41.png?width=1715&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1c9846d099123d7b3e93d3155b49edf21ec88b3""&gt;https://preview.redd.it/m0gk5ebf6ov41.png?width=1715&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1c9846d099123d7b3e93d3155b49edf21ec88b3&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga19tq,True,,murphinate,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga19tq/does_concatenation_in_math_mean_use_contiguous/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga19tq/does_concatenation_in_math_mean_use_contiguous/,155203,1588128012.0,0,,False,,,"{'m0gk5ebf6ov41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87e3b26bb6def309a382dd1b96a6af29007f364d'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6b120243ba7f4d36f4cbf940a369a77d9779d2d'}, {'y': 164, 'x': 320, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=addc3067dc7e0e3355bf1fee03ad3d0057005d99'}, {'y': 328, 'x': 640, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=95fe0610dcdc27af3cf91fb498e08b7ae473dbaf'}, {'y': 493, 'x': 960, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c03f8d1edfbfc553b254748c1b24dd6925e3a91'}, {'y': 554, 'x': 1080, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a99f875f878479d7cb46af04bc7d7b9b5b977c0'}], 's': {'y': 881, 'x': 1715, 'u': 'https://preview.redd.it/m0gk5ebf6ov41.png?width=1715&amp;format=png&amp;auto=webp&amp;s=b1c9846d099123d7b3e93d3155b49edf21ec88b3'}, 'id': 'm0gk5ebf6ov41'}}",
,learnmachinelearning,"I read it like ten times and conceptually I understand what it does. But ffs I can't visualize how it happens. When we are running regression splines how we impose constrain by setting first derivatives equal to each other (or constant at the knots, as some texts say). Please ELI5 or explain mathematically doesn't matter.

&amp;#x200B;

thanks!",t2_4c0gz9q3,False,,0,False,How does setting first derivatives equal make regression splines continuous?,[],r/learnmachinelearning,False,6,,0,,False,t3_ga0tqy,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588155023.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read it like ten times and conceptually I understand what it does. But ffs I can&amp;#39;t visualize how it happens. When we are running regression splines how we impose constrain by setting first derivatives equal to each other (or constant at the knots, as some texts say). Please ELI5 or explain mathematically doesn&amp;#39;t matter.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga0tqy,True,,Hellr0x,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga0tqy/how_does_setting_first_derivatives_equal_make/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga0tqy/how_does_setting_first_derivatives_equal_make/,155203,1588126223.0,0,,False,,,,
,learnmachinelearning,I have managed to classify between images of cats and dogs but I have no idea how to make such a thing. Any help is appreciated. I want to make such a software myself.,t2_5fi9eeym,False,,0,False,"If I try to make a face detection software with CNN, where would I find the dataset?",[],r/learnmachinelearning,False,6,,0,,False,t3_g9u66w,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1588132224.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have managed to classify between images of cats and dogs but I have no idea how to make such a thing. Any help is appreciated. I want to make such a software myself.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9u66w,True,,dhokna,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9u66w/if_i_try_to_make_a_face_detection_software_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9u66w/if_i_try_to_make_a_face_detection_software_with/,155203,1588103424.0,0,,False,,,,
,learnmachinelearning,,t2_2y8hmbgp,False,,0,False,Play snake by moving your head: face mesh detection in the browser with TensorFlow,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,135.0,False,t3_g91j89,False,light,0.98,,public,683,0,{},140.0,,False,[],,True,False,,{},Project,False,683,,False,https://b.thumbs.redditmedia.com/i34OBThoPewuBt3jMY4yNNNIgSxyNACO1uHeQZJKGyw.jpg,False,,[],{},image,,False,,1588025956.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?format=png8&amp;s=fea389f9b169c1b08c997c45c91c3eb62622ff95', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3c793f6878b8d47f6d7c5ca8f11eec0b166cc9d6', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=dd0401bf0fc8f08516bbac35f437c55eedb8a438', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f80f03d550a224ea5457ad92e8809614bb803de9', 'width': 320, 'height': 310}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?s=23af45a4682fc0ec480f4d457391d5db891d1183', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;crop=smart&amp;s=4c872f3f261767f7f252af3edeb1021c2f61b0da', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;crop=smart&amp;s=2dac525218c0234bf1ea7401cbe4bb74dfc9d2be', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;crop=smart&amp;s=ac5e94d1ad50e60d890adb25e0e304a85bd606ca', 'width': 320, 'height': 310}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?format=mp4&amp;s=5542d553ccbcba67312fc0aa8ea221e5c11a3ffa', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;format=mp4&amp;s=ff8a18e99acf59418b6ab8d971aa8a367c70dd03', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;format=mp4&amp;s=ae6327ed94776f99519cfa734337748e0e7fce25', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;format=mp4&amp;s=18f74c0dfcf535d0ba6d2dd7475933fffc11c3a3', 'width': 320, 'height': 310}]}}, 'id': 'tuUe8k4H-hqy2rtFvRDcm4GNR81KV2iGgzIjGPaSOlQ'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g91j89,True,,PaulRubenstein,,31,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g91j89/play_snake_by_moving_your_head_face_mesh/,all_ads,False,https://i.redd.it/cgfobl86scv41.gif,155203,1587997156.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'computervision', 'selftext': '', 'author_fullname': 't2_2y8hmbgp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Play snake by moving your head: face mesh detection in the browser with TensorFlow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/computervision', 'hidden': False, 'pwls': 7, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 135, 'hide_score': False, 'name': 't3_g8zo6b', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 128, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'AI/ML/DL', 'can_mod_post': False, 'score': 128, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/i34OBThoPewuBt3jMY4yNNNIgSxyNACO1uHeQZJKGyw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588018898.0, 'link_flair_type': 'text', 'wls': 7, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': True, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?format=png8&amp;s=fea389f9b169c1b08c997c45c91c3eb62622ff95', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3c793f6878b8d47f6d7c5ca8f11eec0b166cc9d6', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=dd0401bf0fc8f08516bbac35f437c55eedb8a438', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f80f03d550a224ea5457ad92e8809614bb803de9', 'width': 320, 'height': 310}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?s=23af45a4682fc0ec480f4d457391d5db891d1183', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;crop=smart&amp;s=4c872f3f261767f7f252af3edeb1021c2f61b0da', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;crop=smart&amp;s=2dac525218c0234bf1ea7401cbe4bb74dfc9d2be', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;crop=smart&amp;s=ac5e94d1ad50e60d890adb25e0e304a85bd606ca', 'width': 320, 'height': 310}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/cgfobl86scv41.gif?format=mp4&amp;s=5542d553ccbcba67312fc0aa8ea221e5c11a3ffa', 'width': 500, 'height': 485}, 'resolutions': [{'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=108&amp;format=mp4&amp;s=ff8a18e99acf59418b6ab8d971aa8a367c70dd03', 'width': 108, 'height': 104}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=216&amp;format=mp4&amp;s=ae6327ed94776f99519cfa734337748e0e7fce25', 'width': 216, 'height': 209}, {'url': 'https://preview.redd.it/cgfobl86scv41.gif?width=320&amp;format=mp4&amp;s=18f74c0dfcf535d0ba6d2dd7475933fffc11c3a3', 'width': 320, 'height': 310}]}}, 'id': 'tuUe8k4H-hqy2rtFvRDcm4GNR81KV2iGgzIjGPaSOlQ'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'e392e0a4-4f5a-11ea-a646-0ef7ed64aa27', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2rfzn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0dd3bb', 'id': 'g8zo6b', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'PaulRubenstein', 'discussion_type': None, 'num_comments': 27, 'send_replies': True, 'whitelist_status': 'some_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/computervision/comments/g8zo6b/play_snake_by_moving_your_head_face_mesh/', 'parent_whitelist_status': 'some_ads', 'stickied': False, 'url': 'https://i.redd.it/cgfobl86scv41.gif', 'subreddit_subscribers': 35033, 'created_utc': 1587990098.0, 'num_crossposts': 5, 'media': None, 'is_video': False}]",t3_g8zo6b,,
,learnmachinelearning,"I'm looking for a few people I can get on discord with and talk about machine learning and work on some kaggle problems together. I know there is a discord but I think the problem is that there's too many people at different levels working on different things.

So my idea is to make a thread where everyone can post their levels, what they're working on, what kind of study partner they want, their timezone, some background information about themselves. And then people who are interested in working with that person can message them privately.

I'll start:

I used to work in web development for several years but trying to transition to machine learning. So good programming skills but only very basic experience with modeling. I'm looking for someone who wants to do some kaggle problems together. I'm in an Asian timezone but right now I'm working from home so I'm available most hours that I'm not sleeping.",t2_5s6ezb34,False,,0,False,Looking for study partners,[],r/learnmachinelearning,False,6,,0,,False,t3_ga02wj,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588152097.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m looking for a few people I can get on discord with and talk about machine learning and work on some kaggle problems together. I know there is a discord but I think the problem is that there&amp;#39;s too many people at different levels working on different things.&lt;/p&gt;

&lt;p&gt;So my idea is to make a thread where everyone can post their levels, what they&amp;#39;re working on, what kind of study partner they want, their timezone, some background information about themselves. And then people who are interested in working with that person can message them privately.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ll start:&lt;/p&gt;

&lt;p&gt;I used to work in web development for several years but trying to transition to machine learning. So good programming skills but only very basic experience with modeling. I&amp;#39;m looking for someone who wants to do some kaggle problems together. I&amp;#39;m in an Asian timezone but right now I&amp;#39;m working from home so I&amp;#39;m available most hours that I&amp;#39;m not sleeping.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ga02wj,True,,shodanorso,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ga02wj/looking_for_study_partners/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ga02wj/looking_for_study_partners/,155203,1588123297.0,0,,False,,,,
,learnmachinelearning,"Hello

Can you guys suggest some good competitions/datasets (ongoing/completed) for a beginner to practice on kaggle?

Please do not suggest cleaned datasets as that is the part of ML that I want to improve my skills in

EDIT - Doesn't have to be necessarily from kaggle

P.S. I have completed titanic and housing",t2_48v57lad,False,,0,False,Kaggle competitions for beginners,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9gxjl,False,light,0.97,,public,37,0,{},,,False,[],,False,False,,{},Discussion,False,37,,False,self,1588050593.0,,[],{},,,True,,1588078738.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello&lt;/p&gt;

&lt;p&gt;Can you guys suggest some good competitions/datasets (ongoing/completed) for a beginner to practice on kaggle?&lt;/p&gt;

&lt;p&gt;Please do not suggest cleaned datasets as that is the part of ML that I want to improve my skills in&lt;/p&gt;

&lt;p&gt;EDIT - Doesn&amp;#39;t have to be necessarily from kaggle&lt;/p&gt;

&lt;p&gt;P.S. I have completed titanic and housing&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9gxjl,True,,raghhuveer,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9gxjl/kaggle_competitions_for_beginners/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9gxjl/kaggle_competitions_for_beginners/,155203,1588049938.0,0,,False,,,,
,learnmachinelearning,,t2_wu1ebt,False,,0,False,I made a video to explain the basics of CycleGAN in 5 minutes!,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g9rbm7,False,dark,0.67,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-8hfnlxEPn4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'CycleGAN Explained in 5 Minutes!', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-8hfnlxEPn4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Matchue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-8hfnlxEPn4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxBlj282mOVF2pndNPmu71w'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-8hfnlxEPn4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g9rbm7', 'height': 338}",,False,2,,False,https://a.thumbs.redditmedia.com/rkj-ZQqYgDK5ZA91wCvWVFnXCz_tkxfMW79jkg1zz24.jpg,False,,[],{},rich:video,,False,,1588123115.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cdNNQshAeBmyHlsfDE6BxU6r9zbVgext-kp5vq1ZbOw.jpg?auto=webp&amp;s=30fcae98493fdb41d2a32ba82ea05f14fbf31869', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/cdNNQshAeBmyHlsfDE6BxU6r9zbVgext-kp5vq1ZbOw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1817de828aceaf0a690a00993c1734979ff409d5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cdNNQshAeBmyHlsfDE6BxU6r9zbVgext-kp5vq1ZbOw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d0c841eef1bd45281345ec81a3ef71bcc7fef6f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cdNNQshAeBmyHlsfDE6BxU6r9zbVgext-kp5vq1ZbOw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f13c2eae90c1e8e4524bb13bde9b98ab06937a0e', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'daGpkuoJqgvCICYAS-ambUdjyZwBBjHVJQxnaD5Nruk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9rbm7,True,,manicman1999,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9rbm7/i_made_a_video_to_explain_the_basics_of_cyclegan/,all_ads,False,https://youtu.be/-8hfnlxEPn4,155203,1588094315.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'CycleGAN Explained in 5 Minutes!', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-8hfnlxEPn4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Matchue', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-8hfnlxEPn4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCxBlj282mOVF2pndNPmu71w'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hi, I made a bot which can chat with you. The chats are context less and won't depend on previous messages for now. I would love to get your opinion and feedback. That being said, I am planning to make the source code and dataset publicly available after I complete the features I have in mind (currently my work is in progress).

link: https://t.me/testing_roebot


Can be found via search @testing_roebot

Thanks.

edit: It will be slow because I am using heroku's free tire to host it.",t2_607po04r,False,,0,False,NLP implementation in telegram BOT!,[],r/learnmachinelearning,False,6,,0,,False,t3_g9w60o,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588138460.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I made a bot which can chat with you. The chats are context less and won&amp;#39;t depend on previous messages for now. I would love to get your opinion and feedback. That being said, I am planning to make the source code and dataset publicly available after I complete the features I have in mind (currently my work is in progress).&lt;/p&gt;

&lt;p&gt;link: &lt;a href=""https://t.me/testing_roebot""&gt;https://t.me/testing_roebot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Can be found via search @testing_roebot&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;edit: It will be slow because I am using heroku&amp;#39;s free tire to host it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FSHkW51Tu246QT7cM7keSwbIy-4OhH9fiEYNampX8k8.jpg?auto=webp&amp;s=1ae236308fee8bcfcb2b89827e50b16e08f4088d', 'width': 320, 'height': 320}, 'resolutions': [{'url': 'https://external-preview.redd.it/FSHkW51Tu246QT7cM7keSwbIy-4OhH9fiEYNampX8k8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3894046f99578f30a357c5e8219d1cae621a28ca', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/FSHkW51Tu246QT7cM7keSwbIy-4OhH9fiEYNampX8k8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0796c683f7dd1ae0b8f4708d94fbca1fbe3b85f8', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/FSHkW51Tu246QT7cM7keSwbIy-4OhH9fiEYNampX8k8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f14497bebbf774b836e35512feb213339d19fc70', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'AaZxNG8iRY1BdNofYTD2dOZ7vy3UTq0jByng0BsuGoY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9w60o,True,,usernamechecksout69_,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9w60o/nlp_implementation_in_telegram_bot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9w60o/nlp_implementation_in_telegram_bot/,155203,1588109660.0,0,,False,,,,
,learnmachinelearning,,t2_hg2gr,False,,0,False,Motion detection for realtime dog data,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_g9vk8q,False,light,0.67,,public,1,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/h4n6m830imv41/DASH_1080?source=fallback', 'height': 1080, 'width': 1080, 'scrubber_media_url': 'https://v.redd.it/h4n6m830imv41/DASH_96', 'dash_url': 'https://v.redd.it/h4n6m830imv41/DASHPlaylist.mpd', 'duration': 5, 'hls_url': 'https://v.redd.it/h4n6m830imv41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},Project,False,1,,False,https://b.thumbs.redditmedia.com/R_r8_giBEEzFC7jqYfZsrlr3vfgBw1xt1enFOyx5swk.jpg,False,,[],{},hosted:video,,False,,1588136556.0,richtext,6,,,text,v.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?format=pjpg&amp;auto=webp&amp;s=5a5cfddaaecf47e541507ad2b0807c98f2152684', 'width': 1080, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a6221ad344a4b97befc3fb58fa5ae9d207a0f1c9', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=558d73fb39087fdde8767ef833be54959e70da9e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4dcdf10db8d8308aacb3be255935838521ba3b63', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8778d940f560d29fb3e6deb3c877365efb5900c9', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1bf0db820ba24e49f66c33ff3dcdb60e5ac8bc3b', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/MKXlkxDYtVO90YCaeuIHOeg0X7jY6xsoxRGjT2yByQ8.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=91bddd8cac2d7b1c1ae888b196e3007338fe76d0', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'yVZZ_8gXS7WoUu4L8yrbDkF7bfDU78ldyrB14c7TEnU'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g9vk8q,True,,archie_swif,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9vk8q/motion_detection_for_realtime_dog_data/,all_ads,False,https://v.redd.it/h4n6m830imv41,155203,1588107756.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/h4n6m830imv41/DASH_1080?source=fallback', 'height': 1080, 'width': 1080, 'scrubber_media_url': 'https://v.redd.it/h4n6m830imv41/DASH_96', 'dash_url': 'https://v.redd.it/h4n6m830imv41/DASHPlaylist.mpd', 'duration': 5, 'hls_url': 'https://v.redd.it/h4n6m830imv41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"How would you go about solving the classic game Connect Four?

There is no hidden information, so it would take in the board as input every round right? But I can't just flatten it out because I would lose important row, column and diagonal information? I was thinking maybe process it as a matrix similar to image object recognition. Any thoughts, is that overkill is there a better approach?

Ideally I would like to take advantage of deep reinforcement learning.",t2_4we1a,False,,0,False,Approaches for solving Connect Four,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9ot20,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588114991.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How would you go about solving the classic game Connect Four?&lt;/p&gt;

&lt;p&gt;There is no hidden information, so it would take in the board as input every round right? But I can&amp;#39;t just flatten it out because I would lose important row, column and diagonal information? I was thinking maybe process it as a matrix similar to image object recognition. Any thoughts, is that overkill is there a better approach?&lt;/p&gt;

&lt;p&gt;Ideally I would like to take advantage of deep reinforcement learning.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9ot20,True,,Stewie977,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ot20/approaches_for_solving_connect_four/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ot20/approaches_for_solving_connect_four/,155203,1588086191.0,0,,False,,,,
,learnmachinelearning,Hello guys. I'm new here. Kindly direct me to where to find datasets on Call Detail Records in telecom industry. Thanks.,t2_6bwy8l5b,False,,0,False,CALL DETAIL RECORDS,[],r/learnmachinelearning,False,6,,0,,False,t3_g9v1d0,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588134927.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello guys. I&amp;#39;m new here. Kindly direct me to where to find datasets on Call Detail Records in telecom industry. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9v1d0,True,,ToluTaiwo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9v1d0/call_detail_records/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9v1d0/call_detail_records/,155203,1588106127.0,0,,False,,,,
,learnmachinelearning,"Hello

Can you guys suggest some good competitions (ongoing/completed) for a beginner to practice on kaggle?

Any datasets which might be good is also appreciated

P.S. I have done titanic and housing",t2_48v57lad,False,,0,False,Kaggle competitions for beginners,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9gxjk,False,light,0.86,,public,16,0,{},,,False,[],,False,False,,{},Discussion,False,16,,False,self,False,,[],{},,,True,,1588078738.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello&lt;/p&gt;

&lt;p&gt;Can you guys suggest some good competitions (ongoing/completed) for a beginner to practice on kaggle?&lt;/p&gt;

&lt;p&gt;Any datasets which might be good is also appreciated&lt;/p&gt;

&lt;p&gt;P.S. I have done titanic and housing&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9gxjk,True,,raghhuveer,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9gxjk/kaggle_competitions_for_beginners/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9gxjk/kaggle_competitions_for_beginners/,155203,1588049938.0,0,,False,,,,
,learnmachinelearning,"Hiya, 

So I am creating a Dash application and have created my first page and getting to grips with it. 

On the 2nd page, I want to be able to demonstrate various different ML models for my project but have a few questions I hope some people with more Dash Plotly experience can answer.

How I imagine it is that the user can select the dataset (by typing in a Stock), selecting a ML model i.e. LSTM, Xgboost, SVR and then it displays the test vs predicted graph and the models metrics i.e. mse, mae. 

My worries are firstly is it possible to do this. The training time for these models can be up to 30/40 minutes so that's a problem. And how the models can actually be implemented, for example the xgboost model is completely different to the lstm.

When I first thought of the idea it seemed simple but now I am worried I cannot produce this, I need to somehow demonstrate these models and the only way I could think was to let the user select the model, produce a graph and metrics. 

If someone could give me some pointers or possible solutions I'd be really grateful.",t2_xutt9v2,False,,0,False,"Dash - Plotly, Machine Learning Advice",[],r/learnmachinelearning,False,6,,0,,False,t3_g9u9mw,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588132522.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hiya, &lt;/p&gt;

&lt;p&gt;So I am creating a Dash application and have created my first page and getting to grips with it. &lt;/p&gt;

&lt;p&gt;On the 2nd page, I want to be able to demonstrate various different ML models for my project but have a few questions I hope some people with more Dash Plotly experience can answer.&lt;/p&gt;

&lt;p&gt;How I imagine it is that the user can select the dataset (by typing in a Stock), selecting a ML model i.e. LSTM, Xgboost, SVR and then it displays the test vs predicted graph and the models metrics i.e. mse, mae. &lt;/p&gt;

&lt;p&gt;My worries are firstly is it possible to do this. The training time for these models can be up to 30/40 minutes so that&amp;#39;s a problem. And how the models can actually be implemented, for example the xgboost model is completely different to the lstm.&lt;/p&gt;

&lt;p&gt;When I first thought of the idea it seemed simple but now I am worried I cannot produce this, I need to somehow demonstrate these models and the only way I could think was to let the user select the model, produce a graph and metrics. &lt;/p&gt;

&lt;p&gt;If someone could give me some pointers or possible solutions I&amp;#39;d be really grateful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9u9mw,True,,rhhh12,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9u9mw/dash_plotly_machine_learning_advice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9u9mw/dash_plotly_machine_learning_advice/,155203,1588103722.0,0,,False,,,,
,learnmachinelearning,"    input_shape=(X.shape[1], 1)
    input_A = Input(shape=input_shape)
    
    cl1 = Conv1D(filters=n_filters1, kernel_size=ks1)(input_A)
    mp1 = MaxPooling1D(pool_size=ps1,strides=s1)(cl1)
    cl2 = Conv1D(filters=n_filters2, kernel_size=ks2)(input_A)
    mp2 = MaxPooling1D(pool_size=ps2,strides=s2)(cl2)
    cl3 = Conv1D(filters=n_filters3, kernel_size=ks3)(input_A)
    mp3 = MaxPooling1D(pool_size=ps3,strides=s3)(cl3)
    flatten1 = Flatten()(mp1)
    flatten2 = Flatten()(mp2)
    flatten3 = Flatten()(mp3)
    concatenate1 = concatenate([flatten1,flatten2,flatten3])
    
    LSTM(64, return_sequences=True)(concatenate1)
    ---------------------------------------------------------
    ValueError: Input 0 is incompatible with layer lstm_16: expected ndim=3, found ndim=2

Hi guys,

Want to input parallel CNN layers into LSTM, what might be the problem?

Cheers",t2_14w7k8,False,,0,False,Conv1d layers into LSTM layesr?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9u02e,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588131680.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;pre&gt;&lt;code&gt;input_shape=(X.shape[1], 1)
input_A = Input(shape=input_shape)

cl1 = Conv1D(filters=n_filters1, kernel_size=ks1)(input_A)
mp1 = MaxPooling1D(pool_size=ps1,strides=s1)(cl1)
cl2 = Conv1D(filters=n_filters2, kernel_size=ks2)(input_A)
mp2 = MaxPooling1D(pool_size=ps2,strides=s2)(cl2)
cl3 = Conv1D(filters=n_filters3, kernel_size=ks3)(input_A)
mp3 = MaxPooling1D(pool_size=ps3,strides=s3)(cl3)
flatten1 = Flatten()(mp1)
flatten2 = Flatten()(mp2)
flatten3 = Flatten()(mp3)
concatenate1 = concatenate([flatten1,flatten2,flatten3])

LSTM(64, return_sequences=True)(concatenate1)
---------------------------------------------------------
ValueError: Input 0 is incompatible with layer lstm_16: expected ndim=3, found ndim=2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;Want to input parallel CNN layers into LSTM, what might be the problem?&lt;/p&gt;

&lt;p&gt;Cheers&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g9u02e,True,,dflash88,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9u02e/conv1d_layers_into_lstm_layesr/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9u02e/conv1d_layers_into_lstm_layesr/,155203,1588102880.0,0,,False,,,,
,learnmachinelearning,"Pandas doesnâ€™t have multiprocessing support and it is slow with bigger datasets. There is a better tool that puts those CPU cores to work!

[https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a](https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a)

TLDR;

\- meet Dask

\- Dask natively scales Python

\- Dask can process larger than memory datasets

\- usage of Dask is recommended only for datasets that donâ€™t fit in the main memory",t2_k9eg8,False,,0,False,Are you still using Pandas for big data?,[],r/learnmachinelearning,False,6,,0,,False,t3_g9n3xm,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588109034.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Pandas doesnâ€™t have multiprocessing support and it is slow with bigger datasets. There is a better tool that puts those CPU cores to work!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a""&gt;https://towardsdatascience.com/are-you-still-using-pandas-for-big-data-12788018ba1a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TLDR;&lt;/p&gt;

&lt;p&gt;- meet Dask&lt;/p&gt;

&lt;p&gt;- Dask natively scales Python&lt;/p&gt;

&lt;p&gt;- Dask can process larger than memory datasets&lt;/p&gt;

&lt;p&gt;- usage of Dask is recommended only for datasets that donâ€™t fit in the main memory&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?auto=webp&amp;s=39a03f643f19415f332a7cd7fb6fd553ccc85636', 'width': 1200, 'height': 1500}, 'resolutions': [{'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c28eda41ca07ca9db591805d0c52386b8dbbfe64', 'width': 108, 'height': 135}, {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44ee9d0c3e388a03be6cf319ac9e3848957c5f11', 'width': 216, 'height': 270}, {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb1ff8d31bdfa9e8f5fef3b2d515905fa5b72b87', 'width': 320, 'height': 400}, {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdf71143018d6cb45b5a675188f89a0c58e071ad', 'width': 640, 'height': 800}, {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=99f9732092eede19169123e5b70a05f0ab6308bc', 'width': 960, 'height': 1200}, {'url': 'https://external-preview.redd.it/FC51AD5dzfeqKLjAflb0yax3pthn9Ig9j4vRpHDPVtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8c7157aadade19419ee99a854a0122647dded82', 'width': 1080, 'height': 1350}], 'variants': {}, 'id': 'ZQI1pSP7k0UxPGpy3W1ulqI_Uf1xQNitH-Vlvm1RXbM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9n3xm,True,,hiphop1987,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9n3xm/are_you_still_using_pandas_for_big_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9n3xm/are_you_still_using_pandas_for_big_data/,155203,1588080234.0,0,,False,,,,
,learnmachinelearning,"Hi all,

At the risk of sounding like an idiot to experienced coders etc. I shall proceed with this post!

I am a very young visual artist/video editor/3D animator so have little to no experience of coding apart from in Houdini. Iâ€˜be been looking to train a GAN with faces for a while so I can create videos akin to this that reminded me about it today - https://www.instagram.com/p/B_iKy_OhW6E/?igshid=nqrr01mq2jye and perhaps experiment after I understand the basics.

I donâ€™t even understand how to operate this sort of thing. Is there an easy software infrastructure that is easy to do this or is it from the ground up coding through python? 

Ideally there is a tutorial or someone here could give me an easy walkthrough as to learning from the ground up about GANS and how to implement this sort of thing!",t2_4xuvrg49,False,,0,False,Learning about GANs,[],r/learnmachinelearning,False,6,,0,,False,t3_g9sy44,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588128364.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;At the risk of sounding like an idiot to experienced coders etc. I shall proceed with this post!&lt;/p&gt;

&lt;p&gt;I am a very young visual artist/video editor/3D animator so have little to no experience of coding apart from in Houdini. Iâ€˜be been looking to train a GAN with faces for a while so I can create videos akin to this that reminded me about it today - &lt;a href=""https://www.instagram.com/p/B_iKy_OhW6E/?igshid=nqrr01mq2jye""&gt;https://www.instagram.com/p/B_iKy_OhW6E/?igshid=nqrr01mq2jye&lt;/a&gt; and perhaps experiment after I understand the basics.&lt;/p&gt;

&lt;p&gt;I donâ€™t even understand how to operate this sort of thing. Is there an easy software infrastructure that is easy to do this or is it from the ground up coding through python? &lt;/p&gt;

&lt;p&gt;Ideally there is a tutorial or someone here could give me an easy walkthrough as to learning from the ground up about GANS and how to implement this sort of thing!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9sy44,True,,reedmat1,,9,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9sy44/learning_about_gans/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9sy44/learning_about_gans/,155203,1588099564.0,0,,False,,,,
,learnmachinelearning,"I  am using the KNeighborsClassifier from sklearn in order to predict the  probabilities data belonging to a class. When I try to use  predict\_proba() to predict a single class I get the following list of  predictions:

    [[0.55555556 0.44444444]  [0.57777778 0.42222222]  [0.55555556 0.44444444]  [0.62222222 0.37777778]  [0.46666667 0.53333333]  [0.44444444 0.55555556]  [0.42222222 0.57777778]  [0.44444444 0.55555556]  [0.35555556 0.64444444]  [0.51111111 0.48888889]  [0.57777778 0.42222222]  [0.71111111 0.28888889]] 

I am not sure how to  interpret the output but I see that each row adds up to 1. Is it true  that the first element of each list is the probability that the data  from each row is in the class and the second element is the probability  that the data is not in the class?",t2_95x2c,False,,0,False,How do I interpret the output of sklearn's predict_proba() function?,[],r/learnmachinelearning,False,6,,0,,False,t3_g9stqj,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588127962.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  am using the KNeighborsClassifier from sklearn in order to predict the  probabilities data belonging to a class. When I try to use  predict_proba() to predict a single class I get the following list of  predictions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[0.55555556 0.44444444]  [0.57777778 0.42222222]  [0.55555556 0.44444444]  [0.62222222 0.37777778]  [0.46666667 0.53333333]  [0.44444444 0.55555556]  [0.42222222 0.57777778]  [0.44444444 0.55555556]  [0.35555556 0.64444444]  [0.51111111 0.48888889]  [0.57777778 0.42222222]  [0.71111111 0.28888889]] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am not sure how to  interpret the output but I see that each row adds up to 1. Is it true  that the first element of each list is the probability that the data  from each row is in the class and the second element is the probability  that the data is not in the class?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9stqj,True,,cheeseisakindof,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9stqj/how_do_i_interpret_the_output_of_sklearns_predict/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9stqj/how_do_i_interpret_the_output_of_sklearns_predict/,155203,1588099162.0,0,,False,,,,
,learnmachinelearning,"I was trying to visualize how the hidden layer weights of a multi layer perceptron look, but it is very confusing and without much perceptible pattern.

The input layer is 784,

hidden\_1 :121 ,

hidden\_2 = 64 and

[hidden layer](https://preview.redd.it/zfb4chxznlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=fe0cbb2574d01917738c9053714ca86f6c7facd4)

output = 10.

while the weights of first to hidden\_1 have very recognizable pattern, the hidden\_1 to hidden\_2 and last layer weights look very random.

I used the method as in :[https://scikit-learn.org/stable/auto\_examples/neural\_networks/plot\_mnist\_filters.html](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html) and my weights in first layer look as in the link.

for the second layer  with weights of shape ( 64,  121), I plot 64 small plots , each of 11x11.

I understnd weights have much more meaning in conv nets and features are more perceptible as we go deeper, but this randomness in MLP after the first layer is making me doubt if I am plotting right? PLease explain.

[input layer](https://preview.redd.it/wv9f31ewnlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=972f73adb7d8e24ce41efe6549a898fe31279170)",t2_65rfnaet,False,,0,False,"multi layer perceptron , weights of hidden layer seem random",[],r/learnmachinelearning,False,6,,0,97.0,False,t3_g9scsw,False,dark,0.66,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/xcynotz1M4bOAAz3QHo4xUnZegdrugcTti0dDWFqcKg.jpg,False,,[],{},,,True,,1588126449.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was trying to visualize how the hidden layer weights of a multi layer perceptron look, but it is very confusing and without much perceptible pattern.&lt;/p&gt;

&lt;p&gt;The input layer is 784,&lt;/p&gt;

&lt;p&gt;hidden_1 :121 ,&lt;/p&gt;

&lt;p&gt;hidden_2 = 64 and&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/zfb4chxznlv41.png?width=292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe0cbb2574d01917738c9053714ca86f6c7facd4""&gt;hidden layer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;output = 10.&lt;/p&gt;

&lt;p&gt;while the weights of first to hidden_1 have very recognizable pattern, the hidden_1 to hidden_2 and last layer weights look very random.&lt;/p&gt;

&lt;p&gt;I used the method as in :&lt;a href=""https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html""&gt;https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html&lt;/a&gt; and my weights in first layer look as in the link.&lt;/p&gt;

&lt;p&gt;for the second layer  with weights of shape ( 64,  121), I plot 64 small plots , each of 11x11.&lt;/p&gt;

&lt;p&gt;I understnd weights have much more meaning in conv nets and features are more perceptible as we go deeper, but this randomness in MLP after the first layer is making me doubt if I am plotting right? PLease explain.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/wv9f31ewnlv41.png?width=292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972f73adb7d8e24ce41efe6549a898fe31279170""&gt;input layer&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9scsw,True,,VeryCuriousLearner,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9scsw/multi_layer_perceptron_weights_of_hidden_layer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9scsw/multi_layer_perceptron_weights_of_hidden_layer/,155203,1588097649.0,0,,False,,,"{'zfb4chxznlv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 75, 'x': 108, 'u': 'https://preview.redd.it/zfb4chxznlv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c504509769e6a52ae7ce0f6868858ff6776c2ea4'}, {'y': 150, 'x': 216, 'u': 'https://preview.redd.it/zfb4chxznlv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=168113fb2f4741d01e0b27d958e364b6cd21df86'}], 's': {'y': 204, 'x': 292, 'u': 'https://preview.redd.it/zfb4chxznlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=fe0cbb2574d01917738c9053714ca86f6c7facd4'}, 'id': 'zfb4chxznlv41'}, 'wv9f31ewnlv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 75, 'x': 108, 'u': 'https://preview.redd.it/wv9f31ewnlv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdae43d2e658adede969d15057005eb4f0244b89'}, {'y': 150, 'x': 216, 'u': 'https://preview.redd.it/wv9f31ewnlv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87925bd4c281720f13df4045be091715b482ab06'}], 's': {'y': 204, 'x': 292, 'u': 'https://preview.redd.it/wv9f31ewnlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=972f73adb7d8e24ce41efe6549a898fe31279170'}, 'id': 'wv9f31ewnlv41'}}",
,learnmachinelearning,,t2_cu8vpox,False,,0,False,statsmodels.api library related errors and their solutions,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9rrh2,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Project,False,0,,False,default,False,,[],{},,,False,,1588124527.0,richtext,6,,,text,self.machineLearning101,False,,,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g9rrh2,True,,shyamcody,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9rrh2/statsmodelsapi_library_related_errors_and_their/,all_ads,False,/r/machineLearning101/comments/g9rqfx/statsmodelsapi_library_related_errors_and_their/,155203,1588095727.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'machineLearning101', 'selftext': 'I have been working on and off with statsmodel for the better part of last year and remembering all the errors once again after starting to use it after using it last time 3 months ago. This time, I am compiling all of the issues I am going to face and how to resolve them. The blog where I am writing all of these is [here](https://shyambhu20.blogspot.com/2020/04/statsmodels-errors-and-solutions.html). \n\nIf you are facing a statsmodel related issue, or some neat trick you play every time working with some functions in it, please comment in the blog and I will add it properly with your name as the contributor. I know GitHub issues and stack overflow is there. But somehow I feel, one descriptive summary blog can do a better job in this than them.', 'author_fullname': 't2_cu8vpox', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'statsmodels.api library related errors and their solutions', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/machineLearning101', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g9rqfx', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588124435.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.machineLearning101', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working on and off with statsmodel for the better part of last year and remembering all the errors once again after starting to use it after using it last time 3 months ago. This time, I am compiling all of the issues I am going to face and how to resolve them. The blog where I am writing all of these is &lt;a href=""https://shyambhu20.blogspot.com/2020/04/statsmodels-errors-and-solutions.html""&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;If you are facing a statsmodel related issue, or some neat trick you play every time working with some functions in it, please comment in the blog and I will add it properly with your name as the contributor. I know GitHub issues and stack overflow is there. But somehow I feel, one descriptive summary blog can do a better job in this than them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_13ki96', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g9rqfx', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'shyamcody', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/machineLearning101/comments/g9rqfx/statsmodelsapi_library_related_errors_and_their/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/machineLearning101/comments/g9rqfx/statsmodelsapi_library_related_errors_and_their/', 'subreddit_subscribers': 5, 'created_utc': 1588095635.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_g9rqfx,,
,learnmachinelearning,"So I'm studying this book with 100% focus to understand every concept without skipping anything.
I'm having a hard time visualising how to estimate B0 and B1 coefficients while plotting a line. How do I learn the formula for the coefficients without having to mug it up?
Along with this, I am also stuck at understanding RSE, f-value and R2(R Square) for model accuracy.",t2_68jk6qqd,False,,0,False,Doubt from Book: ISLR by Hastie - Chapter 3: Linear Regression.,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9rhtj,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,1588095252.0,,[],{},,,True,,1588123664.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m studying this book with 100% focus to understand every concept without skipping anything.
I&amp;#39;m having a hard time visualising how to estimate B0 and B1 coefficients while plotting a line. How do I learn the formula for the coefficients without having to mug it up?
Along with this, I am also stuck at understanding RSE, f-value and R2(R Square) for model accuracy.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9rhtj,True,,Th3L30pard,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9rhtj/doubt_from_book_islr_by_hastie_chapter_3_linear/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9rhtj/doubt_from_book_islr_by_hastie_chapter_3_linear/,155203,1588094864.0,0,,False,,,,
,learnmachinelearning,"**Code with free to use model for non-commercial apps:** [https://github.com/l2-labs/wearmask-model](https://github.com/l2-labs/wearmask-model)

**Use cases:** video steam: [https://www.youtube.com/watch?v=bkrfusEGi3Y](https://www.youtube.com/watch?v=bkrfusEGi3Y), mobile app:

[WearMask Mobile](https://preview.redd.it/lsqvyf7vdlv41.png?width=1600&amp;format=png&amp;auto=webp&amp;s=11eef40ffdeac0f05c0da8a862cb508a25ebe77c)

**Get Android app:** [https://play.google.com/store/apps/details?id=org.l2labs.wearmask](https://play.google.com/store/apps/details?id=org.l2labs.wearmask)

**Data/Model:** private dataset of &gt;1M data samples, TF-lite, pubic model accuracy 94.7%, private model accuracy &gt;99% (free for non-commercial use, contact [admin@l2-labs.com](mailto:admin@l2-labs.com))",t2_60q4alvq,False,,0,False,WearMask is deep learning model to automatically check whether face is protected with some PPE like a mask. Free for none-commercial organizations,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,64.0,False,t3_g9rgfx,False,light,0.57,,public,1,0,{},140.0,,False,[],,False,False,,{},Project,False,1,,False,https://b.thumbs.redditmedia.com/GqE2Z-pv5Vnsu706tn3UatH4u2o5nKpHGt5idwsZu7k.jpg,1588535313.0,,[],{},self,,True,,1588123544.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;Code with free to use model for non-commercial apps:&lt;/strong&gt; &lt;a href=""https://github.com/l2-labs/wearmask-model""&gt;https://github.com/l2-labs/wearmask-model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Use cases:&lt;/strong&gt; video steam: &lt;a href=""https://www.youtube.com/watch?v=bkrfusEGi3Y""&gt;https://www.youtube.com/watch?v=bkrfusEGi3Y&lt;/a&gt;, mobile app:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/lsqvyf7vdlv41.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11eef40ffdeac0f05c0da8a862cb508a25ebe77c""&gt;WearMask Mobile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Get Android app:&lt;/strong&gt; &lt;a href=""https://play.google.com/store/apps/details?id=org.l2labs.wearmask""&gt;https://play.google.com/store/apps/details?id=org.l2labs.wearmask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data/Model:&lt;/strong&gt; private dataset of &amp;gt;1M data samples, TF-lite, pubic model accuracy 94.7%, private model accuracy &amp;gt;99% (free for non-commercial use, contact [&lt;a href=""mailto:admin@l2-labs.com""&gt;admin@l2-labs.com&lt;/a&gt;](mailto:&lt;a href=""mailto:admin@l2-labs.com""&gt;admin@l2-labs.com&lt;/a&gt;))&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?auto=webp&amp;s=600992f5ee6afc3e34501cce95ff375531ca2809', 'width': 1600, 'height': 738}, 'resolutions': [{'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6428dc457c4939ee2ef6783281bf47b5a3e5dfa3', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4efbc4030304e1914a68ae2fd007e65f95178aae', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=160e0ef67f02a339ffb50334e03aee88a6865adc', 'width': 320, 'height': 147}, {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f289ba5135d87173c703343d63c95d6f059d6a2', 'width': 640, 'height': 295}, {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04d3a9c2a1ea1b699cfdd55fbeb4d35ed8de9258', 'width': 960, 'height': 442}, {'url': 'https://external-preview.redd.it/yAcFn6uxzFHRk4BiLV7bpV3Koj0D5rdVYCdGbMKaJA8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8f9ceae839970aacb9973852d2908ef2d24fbf2b', 'width': 1080, 'height': 498}], 'variants': {}, 'id': 'An94Ejq1RFPtxSD7tS81D1nr_gr1dl86v3GAlTt9Twk'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g9rgfx,True,,jedima19,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9rgfx/wearmask_is_deep_learning_model_to_automatically/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9rgfx/wearmask_is_deep_learning_model_to_automatically/,155203,1588094744.0,0,,False,,,"{'lsqvyf7vdlv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 49, 'x': 108, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=70493458de6302b03fe422795be7ccca257b76da'}, {'y': 99, 'x': 216, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3079cbe62e807f854b6d0b978901dd0bf9e38b7f'}, {'y': 147, 'x': 320, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15dcc5bd82a7ebdd28fc32886a103de3da9aa905'}, {'y': 295, 'x': 640, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9f6202c2f832a2ea301cf058b1b15b0d2979b9c'}, {'y': 442, 'x': 960, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c8013f6cf92a23772a290579744a5bc978276551'}, {'y': 498, 'x': 1080, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=349c4404ee7526284ec998536be1fa885dae1f15'}], 's': {'y': 738, 'x': 1600, 'u': 'https://preview.redd.it/lsqvyf7vdlv41.png?width=1600&amp;format=png&amp;auto=webp&amp;s=11eef40ffdeac0f05c0da8a862cb508a25ebe77c'}, 'id': 'lsqvyf7vdlv41'}}",
,learnmachinelearning,"Howdy!

Would it be possible for there to be a pinned thread with the most common and accepted answers to the often asked question ""How do I get started in ML/Where can I learn about ML?""

I mention this because \*I\* was going to ask, and, being a denizen of many other programming subreddits, I know how repetitive that can be. I'm sure  I can search for previous posts, but (from my perspective) it seems like some of this moves quickly and I know that somtimes that what are the best answers/resources today are not tomorrow.

Having a dedicated thread that occasionally people update (or deprecate resources that are no longer relevant) would probably cut down on these kinds of questions (well, maybe; people don't always read the rules/pinned threads).

Anyway, I'm off to search the archives.",t2_z73nd,False,,0,False,[meta] Pinned thread of learning/getting started resources?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9qkuw,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1588120715.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Howdy!&lt;/p&gt;

&lt;p&gt;Would it be possible for there to be a pinned thread with the most common and accepted answers to the often asked question &amp;quot;How do I get started in ML/Where can I learn about ML?&amp;quot;&lt;/p&gt;

&lt;p&gt;I mention this because *I* was going to ask, and, being a denizen of many other programming subreddits, I know how repetitive that can be. I&amp;#39;m sure  I can search for previous posts, but (from my perspective) it seems like some of this moves quickly and I know that somtimes that what are the best answers/resources today are not tomorrow.&lt;/p&gt;

&lt;p&gt;Having a dedicated thread that occasionally people update (or deprecate resources that are no longer relevant) would probably cut down on these kinds of questions (well, maybe; people don&amp;#39;t always read the rules/pinned threads).&lt;/p&gt;

&lt;p&gt;Anyway, I&amp;#39;m off to search the archives.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9qkuw,True,,thememorableusername,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9qkuw/meta_pinned_thread_of_learninggetting_started/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9qkuw/meta_pinned_thread_of_learninggetting_started/,155203,1588091915.0,0,,False,,,,
,learnmachinelearning,"The authors mention some kind of geometric intuition about Linear Regression and how the intercept/constant in regression can be modelled by adding a column of 1s (A feature with just 1 as values) and then doing regression without the constant in the eqn: 2.2.

I don't understand the geometric detail about the hyperplane stuff. Can some explain taking a case with 2 variables (x,y) and then using z co-ordinate for added feature of 1s?

&amp;#x200B;

[Geometric intuition of LR](https://preview.redd.it/armg5ts6xkv41.png?width=1730&amp;format=png&amp;auto=webp&amp;s=5ee2ad45944834ab44324067235fcf86fb3f7ba8)

[Link to book](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)",t2_owuza,False,,0,False,Doubt from book: ESL 2nd ed (Elements of Statistical learning) Chapter 2 (Pg 12),[],r/learnmachinelearning,False,6,,0,30.0,False,t3_g9pjiq,False,dark,0.4,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/XFgdRRNydCmccfLkPF2Mc6k_D8weuxgQGW4uj22vTpk.jpg,False,,[],{},,,True,,1588117421.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The authors mention some kind of geometric intuition about Linear Regression and how the intercept/constant in regression can be modelled by adding a column of 1s (A feature with just 1 as values) and then doing regression without the constant in the eqn: 2.2.&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t understand the geometric detail about the hyperplane stuff. Can some explain taking a case with 2 variables (x,y) and then using z co-ordinate for added feature of 1s?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/armg5ts6xkv41.png?width=1730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5ee2ad45944834ab44324067235fcf86fb3f7ba8""&gt;Geometric intuition of LR&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf""&gt;Link to book&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9pjiq,True,,_faizan_,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9pjiq/doubt_from_book_esl_2nd_ed_elements_of/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9pjiq/doubt_from_book_esl_2nd_ed_elements_of/,155203,1588088621.0,0,,False,,,"{'armg5ts6xkv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 23, 'x': 108, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38c68d9a81befeaeeddf97fd47365c134197e1a3'}, {'y': 47, 'x': 216, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=051bac9a8d445fef2381633a12a800667dc98e6f'}, {'y': 70, 'x': 320, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=350341e365b5d0ed535dbc462dc4ddeabdf122d9'}, {'y': 141, 'x': 640, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf038697fccb24080e5a1d4671e8d3b652d82234'}, {'y': 211, 'x': 960, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ea5dbfe20d8e4c4ba9a8808a9c3beafe6d87c3a'}, {'y': 238, 'x': 1080, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=209dd8bec43da7ab6529cb5302156dfa56a20ccb'}], 's': {'y': 382, 'x': 1730, 'u': 'https://preview.redd.it/armg5ts6xkv41.png?width=1730&amp;format=png&amp;auto=webp&amp;s=5ee2ad45944834ab44324067235fcf86fb3f7ba8'}, 'id': 'armg5ts6xkv41'}}",
,learnmachinelearning,I just need to add some fonts to enhance their recognition. I tried but then may be the old eng.traineddata got replaced and it worked for my fonts but not others. I read the docs but they are all very confusing. I want to add font onto the tesseract not replace them. Can anyone help me out with this? Please.,t2_j6gprpb,False,,0,False,"Guys I need some help with Tesseract Fine Tuning, Can anyone help me how to fine tune tesseract on windows?",[],r/learnmachinelearning,False,6,,0,,False,t3_g9lsgl,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588103455.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just need to add some fonts to enhance their recognition. I tried but then may be the old eng.traineddata got replaced and it worked for my fonts but not others. I read the docs but they are all very confusing. I want to add font onto the tesseract not replace them. Can anyone help me out with this? Please.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9lsgl,True,,Sygmus1897,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9lsgl/guys_i_need_some_help_with_tesseract_fine_tuning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9lsgl/guys_i_need_some_help_with_tesseract_fine_tuning/,155203,1588074655.0,0,,False,,,,
,learnmachinelearning,"I'm trying to create a text recognition project using cnn, I need help regarding the text detection task. I have the training images and bounding box details for them. But I'm unable to figure out how to create the loss function. Can anyone help me by telling how to take the output from the CNN model and compare it to the bounding box labels?",t2_5o7kd0ho,False,,0,False,I need help regarding multi object detection using CNN.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9optn,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588114710.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to create a text recognition project using cnn, I need help regarding the text detection task. I have the training images and bounding box details for them. But I&amp;#39;m unable to figure out how to create the loss function. Can anyone help me by telling how to take the output from the CNN model and compare it to the bounding box labels?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g9optn,True,,slayerfx132,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9optn/i_need_help_regarding_multi_object_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9optn/i_need_help_regarding_multi_object_detection/,155203,1588085910.0,0,,False,,,,
,learnmachinelearning,"As said in the title I have to train a model on a dataset of genes to detect (binary decision) whether or not a specific gene is prone to cancer and I would like to maximize the BCR (*balanced classification rate)* of that model as well as my predicted BCR should be as close as possible to the real one. The training set is about twice as large as the test set and both have exactly the same columns (which is about 23.000). The model to use is free of choice and so I was wondering which to choose ? I am very new to ML and was considering svm's, is it a good idea ?",t2_5bpo50nk,False,,0,False,Best model to get highest performance metric as in the International Performance Prediction Challenge WCCI 2006,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9jrld,False,light,0.8,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588093115.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As said in the title I have to train a model on a dataset of genes to detect (binary decision) whether or not a specific gene is prone to cancer and I would like to maximize the BCR (&lt;em&gt;balanced classification rate)&lt;/em&gt; of that model as well as my predicted BCR should be as close as possible to the real one. The training set is about twice as large as the test set and both have exactly the same columns (which is about 23.000). The model to use is free of choice and so I was wondering which to choose ? I am very new to ML and was considering svm&amp;#39;s, is it a good idea ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g9jrld,True,,thothore,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9jrld/best_model_to_get_highest_performance_metric_as/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9jrld/best_model_to_get_highest_performance_metric_as/,155203,1588064315.0,0,,False,,,,
,learnmachinelearning," Hi

I have an encoder decoder network with : 3 BLSTMs in the encoder and 2 vanilla LSTMs in the decoder connected with a mutli head attention with 4 nodes. Latent dimension is 32 and my total sample looks like (10000,400,128). The encoder network has a dropout of 0.2 and the decoder has a dropout of 0.3. I'm using an adam optimizer with a learning rate of 0.001 and Mean Squared error loss. Finally I have a validation split of 0.3. I rented an Nvidia Titan V (with Coreâ„¢ i9-9820X, 5.0/20 cores and 16/64 GB total effective shared RAM, at least that is what it says and it's cheap) on Vast.ai and it takes \~6 minutes for each epoch when I train it all together (7000 train and 3000 validation samples).

I was hoping to find ways of reducing the total train timing. Any suggestions would be great!  Even for other cheap and good ML cloud GPUs :)

TIA!",t2_17c27e,False,,0,False,Strategies to speed up LSTM training,[],r/learnmachinelearning,False,6,,0,,False,t3_g9jom7,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588092693.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi&lt;/p&gt;

&lt;p&gt;I have an encoder decoder network with : 3 BLSTMs in the encoder and 2 vanilla LSTMs in the decoder connected with a mutli head attention with 4 nodes. Latent dimension is 32 and my total sample looks like (10000,400,128). The encoder network has a dropout of 0.2 and the decoder has a dropout of 0.3. I&amp;#39;m using an adam optimizer with a learning rate of 0.001 and Mean Squared error loss. Finally I have a validation split of 0.3. I rented an Nvidia Titan V (with Coreâ„¢ i9-9820X, 5.0/20 cores and 16/64 GB total effective shared RAM, at least that is what it says and it&amp;#39;s cheap) on Vast.ai and it takes ~6 minutes for each epoch when I train it all together (7000 train and 3000 validation samples).&lt;/p&gt;

&lt;p&gt;I was hoping to find ways of reducing the total train timing. Any suggestions would be great!  Even for other cheap and good ML cloud GPUs :)&lt;/p&gt;

&lt;p&gt;TIA!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9jom7,True,,dinnerdaddy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9jom7/strategies_to_speed_up_lstm_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9jom7/strategies_to_speed_up_lstm_training/,155203,1588063893.0,1,,False,,,,
,learnmachinelearning,"Hi yâ€™all!

As a 21st century data engineer or data scientist, Pandas is the most favorite framework to operate on a dataset. But there are better alternatives to Pandas data frames like Median, Dask, Ray and PySpark. Hereâ€™s quick info, usage and installation instructions to use these powerful frameworks while dealing with huge amounts of data. Check out my medium article on â€˜Boost up Pandas Dataframesâ€™

https://towardsdatascience.com/boost-up-pandas-dataframes-46944a93d33e",t2_10xi6ns5,False,,0,False,Boost up Pandas Dataframes,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9hanh,False,light,0.87,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,self,False,,[],{},self,,True,,1588080500.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi yâ€™all!&lt;/p&gt;

&lt;p&gt;As a 21st century data engineer or data scientist, Pandas is the most favorite framework to operate on a dataset. But there are better alternatives to Pandas data frames like Median, Dask, Ray and PySpark. Hereâ€™s quick info, usage and installation instructions to use these powerful frameworks while dealing with huge amounts of data. Check out my medium article on â€˜Boost up Pandas Dataframesâ€™&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/boost-up-pandas-dataframes-46944a93d33e""&gt;https://towardsdatascience.com/boost-up-pandas-dataframes-46944a93d33e&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?auto=webp&amp;s=20044f1478f7607ead843c3f286c4fa58c9db3d4', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c94315058f1b9b75c22557f3c6cab8a59083ed87', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e54422f65a5fea22045c2ad41cd1100392da1f4', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6308e0d0e6a0695021afbd996b0d8de049d147ec', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4e6ff5b26f51ce83ff001fe15b9b12cb13e7db9', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e692ec695ac888c868b0579efe4873e781048840', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/eWjbZkdlg8322RI6IQHn4wRxiwmdH4FAOOL5tromxxo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74dd3cfc3c7d8ad943c63f44ec4c5c75b08808f', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'Ix4qujv59Ry1-B-w8AeOu9T6KN3Q3BoKfYLcFmUhfFM'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9hanh,True,,ChinmayW,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9hanh/boost_up_pandas_dataframes/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9hanh/boost_up_pandas_dataframes/,155203,1588051700.0,0,,False,,,,
,learnmachinelearning,"Hey everyone, I'm working on building a car that can self steer on main roads (basically lane keep assist) like in [this](https://devblogs.nvidia.com/deep-learning-self-driving-cars/) Nvidia paper from 2016.

I'm planning to take it on a 2500 mile road trip this summer to collect data that I'll open source. It should be around 70 hours of steering angle + image + speed data.

I've been documenting this process and have released a few YouTube videos. I would love to get some feedback on how ""in depth"" I should go with the building / explaining of the model and system.

I know the videos aren't great right now, but hopefully with some suggestions I can make the next ones more watchable.

Self Steering Car Playlist:

[https://www.youtube.com/watch?v=gL4-Od9mui4&amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p](https://www.youtube.com/watch?v=gL4-Od9mui4&amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p)",t2_2r4501gh,False,,0,False,"Building a ""Self Steering"" Car Based on Nvidia's Paper; Would Love Feedback on How to Improve the Videos",[],r/learnmachinelearning,False,6,,0,,False,t3_g9nlzb,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588110884.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I&amp;#39;m working on building a car that can self steer on main roads (basically lane keep assist) like in &lt;a href=""https://devblogs.nvidia.com/deep-learning-self-driving-cars/""&gt;this&lt;/a&gt; Nvidia paper from 2016.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m planning to take it on a 2500 mile road trip this summer to collect data that I&amp;#39;ll open source. It should be around 70 hours of steering angle + image + speed data.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been documenting this process and have released a few YouTube videos. I would love to get some feedback on how &amp;quot;in depth&amp;quot; I should go with the building / explaining of the model and system.&lt;/p&gt;

&lt;p&gt;I know the videos aren&amp;#39;t great right now, but hopefully with some suggestions I can make the next ones more watchable.&lt;/p&gt;

&lt;p&gt;Self Steering Car Playlist:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=gL4-Od9mui4&amp;amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p""&gt;https://www.youtube.com/watch?v=gL4-Od9mui4&amp;amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?auto=webp&amp;s=6334333ec52a023091ac65f7317f0a83d8c5861e', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe5138dd84b4033a4b015063623378ae1a06f911', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be27d7cb6c5ee52c6a8440b4517cea3b662c6f70', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=61ea026c7ac2a2a3126845c1eff1ec18a3a4ad6b', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=be1c471d4dd965c9c2e9a707b9fdbc510d9fced9', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=772bddf874cf2eef578aca7d9577de3d7b94941d', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/MSjGlUE0iSCWx83O_TuztUIVcqh8VwT-s3No9b2VE2U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=92b73343d93de183fe961a2f3694056aaf139563', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '2PBBz_5xIvNaKJnlADysVJqVsrS3DIIqtfZTjp47b0k'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9nlzb,True,,redditpolldancer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9nlzb/building_a_self_steering_car_based_on_nvidias/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9nlzb/building_a_self_steering_car_based_on_nvidias/,155203,1588082084.0,0,,False,,,,
,learnmachinelearning,"Two new intersting datasets and challenges from Kaggle to learn how to work with images and associated metadata.

[Alaska2 Image StegAnalysis](https://www.kaggle.com/c/alaska2-image-steganalysis/overview) 

[Photo Slideshow Optimization](https://www.kaggle.com/c/hashcode-photo-slideshow/overview)

Have fun!",t2_46whgk4x,False,,0,False,Two fresh Kaggle competitions on Image Analysis,[],r/learnmachinelearning,False,6,,0,,False,t3_g9hx8a,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1588083670.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Two new intersting datasets and challenges from Kaggle to learn how to work with images and associated metadata.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/c/alaska2-image-steganalysis/overview""&gt;Alaska2 Image StegAnalysis&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/c/hashcode-photo-slideshow/overview""&gt;Photo Slideshow Optimization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Have fun!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FakpKkDughNTNJRQo60TbzlP0M5FpNwZnv8ispdplTU.jpg?auto=webp&amp;s=e1288edf072224ee7cbf4ebe655f041c0719fd4c', 'width': 240, 'height': 240}, 'resolutions': [{'url': 'https://external-preview.redd.it/FakpKkDughNTNJRQo60TbzlP0M5FpNwZnv8ispdplTU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e41162697643bd63a354c20c97b1a8f4e2cfc0a5', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/FakpKkDughNTNJRQo60TbzlP0M5FpNwZnv8ispdplTU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6aed51a62c27629534b0a8a0b4791ca764696943', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'upSwpkO0t3LrNACRaFk23l1JycTAmyl1bF4RZy7XhFo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9hx8a,True,,-john--doe-,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9hx8a/two_fresh_kaggle_competitions_on_image_analysis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9hx8a/two_fresh_kaggle_competitions_on_image_analysis/,155203,1588054870.0,0,,False,,,,
,learnmachinelearning,"First time poster &amp; not native to English, I apologize for any mistakes.

I'm about to achieve a university degree in computer science. I took classes in ML and image processing and I found it fascinating. I would like to start a career in the field of image processing and neural network.
I guess my questions are: How do I get really good at it? In school they always give us the architecture for the network and I believe the biggest challenge is to build that architecture. Should I just look at as many projects as possible? Should I experiment with my own? Where would I get datasets? Can I create meaningful work on my humble laptop?
Thanks in advance and any other tips would be greatly appreciated!",t2_dqzrp,False,,0,False,How to get into neural network,[],r/learnmachinelearning,False,6,,0,,False,t3_g9ic31,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1588085817.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;First time poster &amp;amp; not native to English, I apologize for any mistakes.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m about to achieve a university degree in computer science. I took classes in ML and image processing and I found it fascinating. I would like to start a career in the field of image processing and neural network.
I guess my questions are: How do I get really good at it? In school they always give us the architecture for the network and I believe the biggest challenge is to build that architecture. Should I just look at as many projects as possible? Should I experiment with my own? Where would I get datasets? Can I create meaningful work on my humble laptop?
Thanks in advance and any other tips would be greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9ic31,True,,PixelPixell,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ic31/how_to_get_into_neural_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ic31/how_to_get_into_neural_network/,155203,1588057017.0,0,,False,,,,
,learnmachinelearning,I might be wrong but is regularised logistic regression the same as Bayesian logistic regression?,t2_zmqho4m,False,,0,False,Regularised logistic regression vs. Bayesian logistic regression,[],r/learnmachinelearning,False,6,,0,,False,t3_g9mnc4,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588107256.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I might be wrong but is regularised logistic regression the same as Bayesian logistic regression?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9mnc4,True,,leockl,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9mnc4/regularised_logistic_regression_vs_bayesian/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9mnc4/regularised_logistic_regression_vs_bayesian/,155203,1588078456.0,0,,False,,,,
,learnmachinelearning,"Example: https://github.com/Horea94/Fruit-Images-Dataset/blob/master/Training/Eggplant/14_100.jpg

Full dataset: https://github.com/Horea94/Fruit-Images-Dataset",t2_33f7596h,False,,0,False,Is it possible to do object detection training with cropped images instead of bounding boxed images (see details for dataset example)?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9mezs,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588106326.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Example: &lt;a href=""https://github.com/Horea94/Fruit-Images-Dataset/blob/master/Training/Eggplant/14_100.jpg""&gt;https://github.com/Horea94/Fruit-Images-Dataset/blob/master/Training/Eggplant/14_100.jpg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Full dataset: &lt;a href=""https://github.com/Horea94/Fruit-Images-Dataset""&gt;https://github.com/Horea94/Fruit-Images-Dataset&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pkmJIeeIsfl2DIKBiUgqFXOpzAXB5hEjEDrOfZUZdRg.jpg?auto=webp&amp;s=b0eb1066c40ffdb688486b2700ebab1cc1fc4566', 'width': 188, 'height': 188}, 'resolutions': [{'url': 'https://external-preview.redd.it/pkmJIeeIsfl2DIKBiUgqFXOpzAXB5hEjEDrOfZUZdRg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d2356a7537b22a260f453ba834cde6ee3b64b4', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'MSZ4yIdoTKMcd3N8aaHYSRjQrrFKpRK9PQsG81VCPFo'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9mezs,True,,whereistimbo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9mezs/is_it_possible_to_do_object_detection_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9mezs/is_it_possible_to_do_object_detection_training/,155203,1588077526.0,0,,False,,,,
,learnmachinelearning,"Crafted by industry practitioners, an innovative, hands-on platform to help you become a data scientist, all set to handle real business problems.

Register for free here â€” &gt;[ https://leaps.analyttica.com/](https://leaps.analyttica.com/)",t2_5l9xnmp0,False,,0,False,ATH Leaps - Hands-on platform to learn Data Science,[],r/learnmachinelearning,False,6,,0,,False,t3_g9m4b9,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1588105020.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Crafted by industry practitioners, an innovative, hands-on platform to help you become a data scientist, all set to handle real business problems.&lt;/p&gt;

&lt;p&gt;Register for free here â€” &amp;gt;&lt;a href=""https://leaps.analyttica.com/""&gt; https://leaps.analyttica.com/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?auto=webp&amp;s=0c8da730ed478536e879c195d422e5cbd829278f', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b4efdf98320a366d283e66a4782a3483ac329f1', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fee41bf463395c9f6bd34f58fde9855ee7f996e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa6603f4abe391b63b8c020a808991d958b736f8', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f76f36f3fd1acdefb48fdc71a40c2fdd51ba31be', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7fbfdd87e359398609df4771c28e79a18f171220', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/JNlISlhw5v80zdZE-6PbocmC2YiMmCSc7vgCLtfbR10.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a4e5e302ed47fa67da289518d78129bced2ea464', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'GwclbU7U1PrQ4hsIlQWRQYoIYTpUK7b2ArbgWulwZa4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9m4b9,True,,SanjanaSharma96,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9m4b9/ath_leaps_handson_platform_to_learn_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9m4b9/ath_leaps_handson_platform_to_learn_data_science/,155203,1588076220.0,0,,False,,,,
,learnmachinelearning,,t2_3jy7u0ah,False,,0,False,"How can I perform dimensionality reduction on this dataset for visualization purposes? I tried PCA and t-SNE but I got no good results. CNN classifier has 98% test accuracy on this dataset, does this mean anything from dimensionality reduction point of view?",[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g9ly4e,False,dark,0.33,,public,0,0,{},140.0,,False,[],,True,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/vkHKCM0E6_z8_T63dMBEq1WH_3A8ZQcVOW4c56ErFDo.jpg,False,,[],{},image,,False,,1588104203.0,text,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/nwe37gdotjv41.png?auto=webp&amp;s=b71162494488077386105d7b4f8805c53be9ab92', 'width': 585, 'height': 588}, 'resolutions': [{'url': 'https://preview.redd.it/nwe37gdotjv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=59ee0aa496eb696fae090f4394afac4e07eaaa53', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/nwe37gdotjv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c34eec8e81a0e37c85d76222140689b7b8216571', 'width': 216, 'height': 217}, {'url': 'https://preview.redd.it/nwe37gdotjv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c10afd3be7338d0da202b20b6192e60ba3059d9', 'width': 320, 'height': 321}], 'variants': {}, 'id': 'nC5LbpPzcypbQQfvLZrRUJkdgfeiTipcftP-mGJfCqg'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9ly4e,True,,abdeljalil73,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ly4e/how_can_i_perform_dimensionality_reduction_on/,all_ads,False,https://i.redd.it/nwe37gdotjv41.png,155203,1588075403.0,0,,False,,,,
,learnmachinelearning,"I am working on custom dataset , I have scraped images from [bing.com](https://bing.com) image search results.

I want to publish processed images after shaping , removing duplicate images and croping these images.

Need advice under which license can I open source these datasets if I can so that it can others too.",t2_13emyx,False,,0,False,Need help regarding license for open sourcing image dataset,[],r/learnmachinelearning,False,6,,0,,False,t3_g9lt1x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588103524.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working on custom dataset , I have scraped images from &lt;a href=""https://bing.com""&gt;bing.com&lt;/a&gt; image search results.&lt;/p&gt;

&lt;p&gt;I want to publish processed images after shaping , removing duplicate images and croping these images.&lt;/p&gt;

&lt;p&gt;Need advice under which license can I open source these datasets if I can so that it can others too.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?auto=webp&amp;s=187abb1a75a4aa97db0f17966be233d4d93cec93', 'width': 1366, 'height': 768}, 'resolutions': [{'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5460c25bd94fcc15e5d4bb5c11982ed6345e037', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2289bf62527a4094888e8485f9a3a890173c952', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ab02881268474c7e839d6186ef0d1c3e065a6eb', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=903654450845a68175dcde7ea0aab020618a753c', 'width': 640, 'height': 359}, {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=940487d8f6d7c650051142ba3d775ae78430a1b8', 'width': 960, 'height': 539}, {'url': 'https://external-preview.redd.it/E3BdejR16ICbom6LCkYqdyBL__wKHTKvkrxtLYrSCTs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e2d4cd72fbf25fc0ad821dd10bf166f07231018', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '9grH3bkLuC-s9M3mWrKKfLiW9KwtDnAJcOIS69H15e0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9lt1x,True,,ItachiUchiha8045,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9lt1x/need_help_regarding_license_for_open_sourcing/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9lt1x/need_help_regarding_license_for_open_sourcing/,155203,1588074724.0,0,,False,,,,
,learnmachinelearning,"Hi guys, 

I've been already working in ML for a few years, but what I sometimes lack is mathematics behind some ML algorithms. Therefore I wanted to ask you if you could recommend a comprehensive book / a MOOC or a (free) university course where one can not only read about algorithms, but also where a number of mathematical exercises are included? I mean exercises that let you go deeper into the foundations of the ML. If such a position would have also some programming tasks to fortify the knowledge it would be a golden bullet! As I said - I am looking for a read at intermediate level, as I already know the algorithms - I just want to understand them better.

&amp;#x200B;

Cheers!",t2_4iga3hey,False,,0,False,"[Recommendation] Book / course for the intermediate with maths, exercises and programming tasks",[],r/learnmachinelearning,False,6,,0,,False,t3_g9lk81,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588102335.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been already working in ML for a few years, but what I sometimes lack is mathematics behind some ML algorithms. Therefore I wanted to ask you if you could recommend a comprehensive book / a MOOC or a (free) university course where one can not only read about algorithms, but also where a number of mathematical exercises are included? I mean exercises that let you go deeper into the foundations of the ML. If such a position would have also some programming tasks to fortify the knowledge it would be a golden bullet! As I said - I am looking for a read at intermediate level, as I already know the algorithms - I just want to understand them better.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9lk81,True,,Garrus990,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9lk81/recommendation_book_course_for_the_intermediate/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9lk81/recommendation_book_course_for_the_intermediate/,155203,1588073535.0,0,,False,,,,
,learnmachinelearning,"Machine Learning is a very interesting concept to me, I am 17 yearsold and pretty much have never coded before though I have been watching different videos and tutorials about the concept of machine learning, and trying to understand algorithms, different structures, neural networks, etc.

My question is, do any of you have any tips or personal experiences about how you've come to understand/learn machine learning? What are some books, YouTube channels, and basics have you looked into? 

&amp;#x200B;

Basically, where the hell should I start?",t2_5omzm7w,False,,0,False,New and Totally Lost,[],r/learnmachinelearning,False,6,,0,,False,t3_g9ier2,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588086203.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Machine Learning is a very interesting concept to me, I am 17 yearsold and pretty much have never coded before though I have been watching different videos and tutorials about the concept of machine learning, and trying to understand algorithms, different structures, neural networks, etc.&lt;/p&gt;

&lt;p&gt;My question is, do any of you have any tips or personal experiences about how you&amp;#39;ve come to understand/learn machine learning? What are some books, YouTube channels, and basics have you looked into? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Basically, where the hell should I start?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9ier2,True,,Aziath,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ier2/new_and_totally_lost/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ier2/new_and_totally_lost/,155203,1588057403.0,0,,False,,,,
,learnmachinelearning,What is normalized frequency encoding ? . Do we have to normalise the data after applying frequency encoding ?,t2_59797uro,False,,0,False,Normalised Frequency Encoding,[],r/learnmachinelearning,False,6,,0,,False,t3_g9iee1,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588086151.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is normalized frequency encoding ? . Do we have to normalise the data after applying frequency encoding ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9iee1,True,,FoolishlyPainful,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9iee1/normalised_frequency_encoding/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9iee1/normalised_frequency_encoding/,155203,1588057351.0,0,,False,,,,
,learnmachinelearning,"Have some time on my hands and I wanted to try some NLP stuff. Jumped onto:

https://paperswithcode.com/area/natural-language-processing

and I don't know where to start. Can you give me a hand?",t2_nqb8q,False,,0,False,Need help with finding a pre-made NLP project to try out,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9ks6l,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Request,False,1,,False,self,False,,[],{},self,,True,,1588098305.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Have some time on my hands and I wanted to try some NLP stuff. Jumped onto:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://paperswithcode.com/area/natural-language-processing""&gt;https://paperswithcode.com/area/natural-language-processing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and I don&amp;#39;t know where to start. Can you give me a hand?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?auto=webp&amp;s=5d5d24793507b827247cd45e66bc47f54a22d034', 'width': 2085, 'height': 1384}, 'resolutions': [{'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c150d9b89ce91147f49d6ecbd3de46090cb470e', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83ac04e346844fc7d77ff2e9364b97ecbc599a38', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2d0b0f61aa03227372b14a6dcd17da99caa6f6bb', 'width': 320, 'height': 212}, {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5fdfded5c1e0c4f2aec18e101772ba51c675b03c', 'width': 640, 'height': 424}, {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cba86436c117de71dd322be91011d99f5fbabf03', 'width': 960, 'height': 637}, {'url': 'https://external-preview.redd.it/hEe_HFBdIyPs5lxJYDNjk-GTh3N_aCPVjH2l0kuib4c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8add335ac57f6ace93c56fd2e44e8c84f20fcdc', 'width': 1080, 'height': 716}], 'variants': {}, 'id': 'Lw9pzKuqdrN1koHc14K0MtStwGBJxGHij0f0raO-cyo'}], 'enabled': False}",[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,g9ks6l,True,,inspectred,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ks6l/need_help_with_finding_a_premade_nlp_project_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ks6l/need_help_with_finding_a_premade_nlp_project_to/,155203,1588069505.0,0,,False,,,,
,learnmachinelearning,,t2_5a24gggr,False,,0,False,Machine Learning - StarCraft 2 Python AI part 4 - Attack,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g9kjyk,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/90m-itTP_Zo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning - StarCraft 2 Python AI part 4 - Attack', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/90m-itTP_Zo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/90m-itTP_Zo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/90m-itTP_Zo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g9kjyk', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/aVd7mHoWHW-zC0YMyfEvwc0mV3IeuVK2xN3A2DLfrNY.jpg,False,,[],{},rich:video,,False,,1588097102.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/H3J5ntZ48EiJ_aqUu-aK8Z0AasnPDqCDqq1QmcFiLW4.jpg?auto=webp&amp;s=a84de3a59077ecd5e45dc9a0b2659ea1b7932a1a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/H3J5ntZ48EiJ_aqUu-aK8Z0AasnPDqCDqq1QmcFiLW4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8096363717f06534dc0fb0242f4a6a85741a34a4', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/H3J5ntZ48EiJ_aqUu-aK8Z0AasnPDqCDqq1QmcFiLW4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f8325a93f0c8e402b10fa5ce5ab6a1a23bfab7', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/H3J5ntZ48EiJ_aqUu-aK8Z0AasnPDqCDqq1QmcFiLW4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=edc82e7ee499cdda896317105c6706b410010934', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'PcR52a0L8cE5k9Kb2Tv27jrD2bO7kIoS7xPXePw49iU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9kjyk,True,,leooister,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9kjyk/machine_learning_starcraft_2_python_ai_part_4/,all_ads,False,https://youtu.be/90m-itTP_Zo,155203,1588068302.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning - StarCraft 2 Python AI part 4 - Attack', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/90m-itTP_Zo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/90m-itTP_Zo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_x8ze3l8,False,,0,False,"Due to popular demand, I've published an Illustrated Guide to Transformer Neural Networks. Happy machine learning!",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g9dwzx,False,dark,0.71,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/4Bdc55j80l8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Illustrated Guide to Transformers Neural Network', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/4Bdc55j80l8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Mikael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/4Bdc55j80l8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/4Bdc55j80l8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g9dwzx', 'height': 338}",,False,4,,False,https://a.thumbs.redditmedia.com/0B8UNzQli_K_IWvtkw4rmcfkbrvT4JTiYPSIBvGeK20.jpg,False,,[],{},rich:video,,False,,1588066163.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RrMxfyF80YG91cf-_UJfjmcttSloe-XIZZYpBCIm_4E.jpg?auto=webp&amp;s=da6ce8db57c7901a6c82766f41ee630b9c318ca2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/RrMxfyF80YG91cf-_UJfjmcttSloe-XIZZYpBCIm_4E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65f91e7ce44abb05e8d75f7b1800835605db28db', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/RrMxfyF80YG91cf-_UJfjmcttSloe-XIZZYpBCIm_4E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d391ef2941175d27ae34299ca484e9b046b2824', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/RrMxfyF80YG91cf-_UJfjmcttSloe-XIZZYpBCIm_4E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=169736c11e204d139f930129630d8551e2d519ed', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'oTyP9jdG1kKKlTQGA0fYv700TTuOVXKxu9RUkObvh-E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9dwzx,True,,LearnedVector,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9dwzx/due_to_popular_demand_ive_published_an/,all_ads,False,https://youtu.be/4Bdc55j80l8,155203,1588037363.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Illustrated Guide to Transformers Neural Network', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/4Bdc55j80l8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Mikael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/4Bdc55j80l8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"In conditional GANs ( if I understood correctly from the paper ), at some point we create shared representation of image and the label by merging them together both in generator and discriminator at the deeper hidden layers. While I understand that by this we learn a joint (?) distribution of image and a label, I don't quite get how it works from the mathematical view point. Does this idea leverages something like a+b = c + b for probability distributions - adding a label distribution to an image distribution both for generator and discriminator ?",t2_57jerxgq,False,,0,False,Mathematics behind conditional GANs,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9ijjm,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588086892.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In conditional GANs ( if I understood correctly from the paper ), at some point we create shared representation of image and the label by merging them together both in generator and discriminator at the deeper hidden layers. While I understand that by this we learn a joint (?) distribution of image and a label, I don&amp;#39;t quite get how it works from the mathematical view point. Does this idea leverages something like a+b = c + b for probability distributions - adding a label distribution to an image distribution both for generator and discriminator ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9ijjm,True,,kiryangol,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ijjm/mathematics_behind_conditional_gans/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ijjm/mathematics_behind_conditional_gans/,155203,1588058092.0,0,,False,,,,
,learnmachinelearning,"Hi,   
I'm comparing some outlier detection methods for my project, I'm having hard time with One Class SVM, specifically with it's plot and scale invariant property.

Data points are from 2D normal distribution, this is my code:  
 

clusteringSVMÂ =Â OneClassSVM(nu=0.05,kernel=""rbf"").fit(points\_origin)  
outlier\_SVMÂ =Â points\_origin\[clusteringSVM.predict(points\_origin)==-1\]  
plt.plot(points\_origin\[:,0\],Â points\_origin\[:,1\],Â 'o')  
plt.plot(outlier\_SVM\[:,0\],Â outlier\_SVM\[:,1\],Â 'ro')  
For some reason, plot looks like this:

&amp;#x200B;

[OneClassSVM, nu = 0.05, points from normal dist.](https://preview.redd.it/gdkfdkrr9iv41.png?width=495&amp;format=png&amp;auto=webp&amp;s=22a2f28aa119c57f8eb46a320ca3c83c82dc9724)

I dont know, why there is that inner ellipse of red outlier points. 

Furthermore, when I try to show, if is this method scale invariant, I multiply y-axis 10 or 100 times. The result is even more weird:

&amp;#x200B;

[y = 10\*y](https://preview.redd.it/35fsa2bkaiv41.png?width=504&amp;format=png&amp;auto=webp&amp;s=befff7a63e100e45d8dbf5948e261c8715e058b8)

&amp;#x200B;

[y=100\*y](https://preview.redd.it/tr9b7tlnaiv41.png?width=512&amp;format=png&amp;auto=webp&amp;s=f8f8839e352960aac924c4f6d88bd5e4403aedb4)

Method fails to detect any outliers whatsoever.

&amp;#x200B;

Thank you very very much for any help, it would be extremly helpful.",t2_4p5medod,False,,0,False,python One Class SVM outlier detection,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_g9ibm9,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/lq_peOuVaXDJ-EpZOlwjRnFUOfTPkIqnz1CHlr3VqpM.jpg,False,,[],{},,,True,,1588085751.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;br/&gt;
I&amp;#39;m comparing some outlier detection methods for my project, I&amp;#39;m having hard time with One Class SVM, specifically with it&amp;#39;s plot and scale invariant property.&lt;/p&gt;

&lt;p&gt;Data points are from 2D normal distribution, this is my code:  &lt;/p&gt;

&lt;p&gt;clusteringSVMÂ =Â OneClassSVM(nu=0.05,kernel=&amp;quot;rbf&amp;quot;).fit(points_origin)&lt;br/&gt;
outlier_SVMÂ =Â points_origin[clusteringSVM.predict(points_origin)==-1]&lt;br/&gt;
plt.plot(points_origin[:,0],Â points_origin[:,1],Â &amp;#39;o&amp;#39;)&lt;br/&gt;
plt.plot(outlier_SVM[:,0],Â outlier_SVM[:,1],Â &amp;#39;ro&amp;#39;)&lt;br/&gt;
For some reason, plot looks like this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/gdkfdkrr9iv41.png?width=495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22a2f28aa119c57f8eb46a320ca3c83c82dc9724""&gt;OneClassSVM, nu = 0.05, points from normal dist.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I dont know, why there is that inner ellipse of red outlier points. &lt;/p&gt;

&lt;p&gt;Furthermore, when I try to show, if is this method scale invariant, I multiply y-axis 10 or 100 times. The result is even more weird:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/35fsa2bkaiv41.png?width=504&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=befff7a63e100e45d8dbf5948e261c8715e058b8""&gt;y = 10*y&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/tr9b7tlnaiv41.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8f8839e352960aac924c4f6d88bd5e4403aedb4""&gt;y=100*y&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Method fails to detect any outliers whatsoever.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you very very much for any help, it would be extremly helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9ibm9,True,,matusba093,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ibm9/python_one_class_svm_outlier_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ibm9/python_one_class_svm_outlier_detection/,155203,1588056951.0,0,,False,,,"{'gdkfdkrr9iv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/gdkfdkrr9iv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d86d7cac310b32cf3d003351ef6a526d7695d56'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/gdkfdkrr9iv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b574d6ee3a35b9b7cd0030432dd188333c2ea030'}, {'y': 213, 'x': 320, 'u': 'https://preview.redd.it/gdkfdkrr9iv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1af645a7be692dfb189e1a8c9b7c27ded7d6288b'}], 's': {'y': 331, 'x': 495, 'u': 'https://preview.redd.it/gdkfdkrr9iv41.png?width=495&amp;format=png&amp;auto=webp&amp;s=22a2f28aa119c57f8eb46a320ca3c83c82dc9724'}, 'id': 'gdkfdkrr9iv41'}, 'tr9b7tlnaiv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 69, 'x': 108, 'u': 'https://preview.redd.it/tr9b7tlnaiv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39f4f0ff06bd0201edd7a1bc0c4db3099ad59cd'}, {'y': 139, 'x': 216, 'u': 'https://preview.redd.it/tr9b7tlnaiv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=befe83285c1c5e928917d8254ae39cdb5df0cded'}, {'y': 206, 'x': 320, 'u': 'https://preview.redd.it/tr9b7tlnaiv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c14437270e983e7ace222d42690b0c8861b13160'}], 's': {'y': 331, 'x': 512, 'u': 'https://preview.redd.it/tr9b7tlnaiv41.png?width=512&amp;format=png&amp;auto=webp&amp;s=f8f8839e352960aac924c4f6d88bd5e4403aedb4'}, 'id': 'tr9b7tlnaiv41'}, '35fsa2bkaiv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 70, 'x': 108, 'u': 'https://preview.redd.it/35fsa2bkaiv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ba96f6c3fa4a69dc87827992c97be3eaa65a25e'}, {'y': 141, 'x': 216, 'u': 'https://preview.redd.it/35fsa2bkaiv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d44ea7280b72af0cfe84557e5038007d3df00df'}, {'y': 210, 'x': 320, 'u': 'https://preview.redd.it/35fsa2bkaiv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d0dedfec5c88217e7d7ebb1edbc4372b81ddaaa'}], 's': {'y': 331, 'x': 504, 'u': 'https://preview.redd.it/35fsa2bkaiv41.png?width=504&amp;format=png&amp;auto=webp&amp;s=befff7a63e100e45d8dbf5948e261c8715e058b8'}, 'id': '35fsa2bkaiv41'}}",
,learnmachinelearning,"I was doing some research on image recognition so that I can use it for my project. Now I am quite confused which algorithm to follow, should I proceed with open cv or tensor flow and to use which algorithm like SFIT Etc. Can you suggest me something",t2_4w0m75rm,False,,0,False,Can you suggest me some topics on image recognition,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9hhbe,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588081441.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was doing some research on image recognition so that I can use it for my project. Now I am quite confused which algorithm to follow, should I proceed with open cv or tensor flow and to use which algorithm like SFIT Etc. Can you suggest me something&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9hhbe,True,,absurd234,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9hhbe/can_you_suggest_me_some_topics_on_image/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9hhbe/can_you_suggest_me_some_topics_on_image/,155203,1588052641.0,0,,False,,,,
,learnmachinelearning,,t2_2o3yzexd,False,,0,False,How Is Artificial Intelligence Used For Defense And Security ? Can We Count On AI to Protect Us? ( with Infographics ),"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_g9jisi,False,light,0.33,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/Qkl-y-lt3Rq7hb5bUJpyUZGsOhv7MGjbaQLYAQR3Z8M.jpg,False,,[],{},link,,False,,1588091880.0,richtext,6,,,text,upsidedownblogger.space,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?auto=webp&amp;s=c966ca3737a8a51acedc5c3b752a6f45cb2bfc49', 'width': 1080, 'height': 567}, 'resolutions': [{'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cdbfd63006018c50f2858229fb469a80155bbb0', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=849d285a03111814cdb559ee21a453a9c3664820', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=81493cbcd4d06d5efbe1ab703e4822792dad46d0', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dc5d3d801516acb64d855359bcac81c0d7adaf9c', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fb623c833a412c2d922c7017481c0d955497e3c8', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/57S5YHywRF8zyUNboZAQZCOe7QTUxSNJEF_0vfA4Vdo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ad270c187d68ed2b4351e3521d518da63ae4a8e', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'oI_mZdmgCPv8yfv33KxjzPrMX7dJsxUmiJew0xryPnU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g9jisi,True,,updownvizzii,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9jisi/how_is_artificial_intelligence_used_for_defense/,all_ads,False,https://www.upsidedownblogger.space/2019/03/how-is-artificial-intelligence-used-for.html,155203,1588063080.0,0,,False,,,,
,learnmachinelearning,"I'm trying to get my head around what's legal and what isn't. I came across the apache license 2.0 for one of Facebook's repos which allowed it to be used for commercial purposes, but I wonder what would be the avenue if they didn't allow use of their code. As the title asks, would it be legal to compose the code myself from reading the research paper and use that in a commercial product?

Thanks!",t2_5pdj75o1,False,,0,False,"Is it legal to read a ML research paper, compose the code without using their git repo and use this in a commercial application?",[],r/learnmachinelearning,False,6,,0,,False,t3_g8xboo,False,dark,0.92,,public,39,0,{},,,False,[],,False,False,,{},,False,39,,False,self,False,,[],{},,,True,,1588007528.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to get my head around what&amp;#39;s legal and what isn&amp;#39;t. I came across the apache license 2.0 for one of Facebook&amp;#39;s repos which allowed it to be used for commercial purposes, but I wonder what would be the avenue if they didn&amp;#39;t allow use of their code. As the title asks, would it be legal to compose the code myself from reading the research paper and use that in a commercial product?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8xboo,True,,Graphene8911,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8xboo/is_it_legal_to_read_a_ml_research_paper_compose/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8xboo/is_it_legal_to_read_a_ml_research_paper_compose/,155203,1587978728.0,0,,False,,,,
,learnmachinelearning,"I'm studying ML on Hands on Machine Learning, a really well done book, however doing the excercices i've noticed that sometime when i try to use colab gpu i have worse performance than when i use cpu.

&amp;#x200B;

 [https://github.com/ageron/handson-ml2/blob/master/16\_nlp\_with\_rnns\_and\_attention.ipynb](https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb) 

&amp;#x200B;

As a reference there is the link of the notebook regarding the chapter 16, if i try to train the first NN (the one regarding fake Shakespear texts) i get 5 hours per epoch using gpu and 3 hours per epoch using cpu. Why is this happening? furthermore my training times are very different from the ones that appears in the notebook, in his example he has only 7000 seconds per epoch...",t2_xmzlc,False,,0,False,Google colab GPU slower than CPU,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8ygrq,False,dark,0.86,,public,27,0,{},,,False,[],,False,False,,{},Question,False,27,,False,self,False,,[],{},self,,True,,1588013127.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m studying ML on Hands on Machine Learning, a really well done book, however doing the excercices i&amp;#39;ve noticed that sometime when i try to use colab gpu i have worse performance than when i use cpu.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb""&gt;https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As a reference there is the link of the notebook regarding the chapter 16, if i try to train the first NN (the one regarding fake Shakespear texts) i get 5 hours per epoch using gpu and 3 hours per epoch using cpu. Why is this happening? furthermore my training times are very different from the ones that appears in the notebook, in his example he has only 7000 seconds per epoch...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/u2jHTJzGSDo3zni-NQt02JwcpdHPXK2TH0SoeAjfABI.jpg?auto=webp&amp;s=f7d28f925151e049f211082f2463aa9327872ce2', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/u2jHTJzGSDo3zni-NQt02JwcpdHPXK2TH0SoeAjfABI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c573fe518c6e378f5262012b35563426bb137513', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/u2jHTJzGSDo3zni-NQt02JwcpdHPXK2TH0SoeAjfABI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=577d3ebb7f1e9a0d4a990d84ddd1a2b3f839fccf', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/u2jHTJzGSDo3zni-NQt02JwcpdHPXK2TH0SoeAjfABI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d30a31ec47eec841ff096907263f291a5e1cde3', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'vIgVwSgolw3h9ko0iHlEmLDDscBiD2GW88mG1Axhkjs'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8ygrq,True,,Ste29ebasta,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8ygrq/google_colab_gpu_slower_than_cpu/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8ygrq/google_colab_gpu_slower_than_cpu/,155203,1587984327.0,0,,False,,,,
,learnmachinelearning,,t2_ybp6n,False,,0,False,First Machine Learning Project successfully completed.,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g9gmqv,False,dark,0.6,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uGNiUTQ0rE4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'CS 570 Data Mining Final Project Presentation | Emory University | Spring 2020', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uGNiUTQ0rE4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'VISHWANATH SESHAGIRI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uGNiUTQ0rE4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/13sevenup'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uGNiUTQ0rE4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g9gmqv', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/e6M1WiajRNdZKjilqkWJ8tsc-AZEmifWF9qCRM9Gudo.jpg,False,,[],{},rich:video,,False,,1588077317.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hKEPrt94I2r8Swvdd097xBHkg5oVOkgbVJHJ-EHqnV4.jpg?auto=webp&amp;s=e653203e209e67b749e146c7f56b23935513b410', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/hKEPrt94I2r8Swvdd097xBHkg5oVOkgbVJHJ-EHqnV4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b50d1a3d6bd0fe3cf1cbfd59f46bb74f963e1a0b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/hKEPrt94I2r8Swvdd097xBHkg5oVOkgbVJHJ-EHqnV4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd4e073ff3d0b1f62baa88729e597c7c64b0e0e0', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/hKEPrt94I2r8Swvdd097xBHkg5oVOkgbVJHJ-EHqnV4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db2fe88f710b5f12b467d82e93f90d1ec4ee1308', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fITt9_aHuJW-pcWGXyb10a238Uz_o8mN-aplw-67R6Q'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9gmqv,True,,ibrahmin13,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9gmqv/first_machine_learning_project_successfully/,all_ads,False,https://youtu.be/uGNiUTQ0rE4,155203,1588048517.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'CS 570 Data Mining Final Project Presentation | Emory University | Spring 2020', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/uGNiUTQ0rE4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'VISHWANATH SESHAGIRI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/uGNiUTQ0rE4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/13sevenup'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_3ps7w65l,False,,0,False,How does k nearest neighbors work? | Machine Learning Basics,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g8kovq,False,dark,0.97,,public,395,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/0p0o5cmgLdE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How does k nearest neighbors work? | Machine Learning Basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/0p0o5cmgLdE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Intuitive Machine Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/0p0o5cmgLdE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiuhuf2Xq0d05_4sHG0xmQA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/0p0o5cmgLdE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8kovq', 'height': 338}",,False,395,,True,https://a.thumbs.redditmedia.com/aPtUxCNbg3IWOjDRPpZ9TeyRBx_yw6RWCDlp6aoMRy4.jpg,False,,[],{},rich:video,,False,,1587958067.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MIfVGWHfF8WzHuGgQ_lnc93qUNEjGz5TKolF16PLw0U.jpg?auto=webp&amp;s=727261d8f35b77b40f37bfa6938a2ed860aacb5a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/MIfVGWHfF8WzHuGgQ_lnc93qUNEjGz5TKolF16PLw0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9322bfebf8ca67cf398b6d37772085016849f7fd', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/MIfVGWHfF8WzHuGgQ_lnc93qUNEjGz5TKolF16PLw0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff9f7e9aa6fd6b1a09c8dca2c27c0febec5da541', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/MIfVGWHfF8WzHuGgQ_lnc93qUNEjGz5TKolF16PLw0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cb23fa9b01b81011e5cc5003b07e1d9fb4a9559', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Dxlu6632m4igeckYY4_xgXeGm67Jbiqw20VAdU1imqs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8kovq,True,,wstcpyt1988,,28,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8kovq/how_does_k_nearest_neighbors_work_machine/,all_ads,False,https://youtu.be/0p0o5cmgLdE,155203,1587929267.0,3,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How does k nearest neighbors work? | Machine Learning Basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/0p0o5cmgLdE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Intuitive Machine Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/0p0o5cmgLdE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiuhuf2Xq0d05_4sHG0xmQA'}}",False,,,,
,learnmachinelearning,,t2_12p7ag,False,,0,False,Neural Networks Overview - Everything we need to know about,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_g9iryv,False,dark,0.25,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/64M2qLY8c75KkeMd8zvjEhb82Qj6Y0GX0yS_C_O9QKk.jpg,False,,[],{},link,,False,,1588088074.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?auto=webp&amp;s=d270e88870f8a0fd75d821def3f9d157f3f208c1', 'width': 1200, 'height': 608}, 'resolutions': [{'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=30b0bf60fde0b9e9a5703eaaf1bd3b12ad9db6bf', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=512f88cafca5296cdc06a919dc452c79697a6b6e', 'width': 216, 'height': 109}, {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71aa49562fb73ebbef16e73b3b3d9c36091c72d9', 'width': 320, 'height': 162}, {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9ff308e09b66e2efb6e73b3db75b3b55d4a8ff5b', 'width': 640, 'height': 324}, {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ffc64096614fa96f5efda08ce7b3c3dba3f2efd6', 'width': 960, 'height': 486}, {'url': 'https://external-preview.redd.it/zlvWSXqAyz-F0KaXhKcdaYggSHI59Vrqq59Fbc-xny4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02e2d60495cd628f055826085cbb9d2b04835c59', 'width': 1080, 'height': 547}], 'variants': {}, 'id': 'awbJwCF9UFLo1nFgtOMrsZgCImbckWVuw-RbQFrLzuw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9iryv,True,,CorderMenqui,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9iryv/neural_networks_overview_everything_we_need_to/,all_ads,False,https://towardsdatascience.com/neural-networks-overview-c3e6ab3e366b,155203,1588059274.0,0,,False,,,,
,learnmachinelearning,"Iâ€™m planning to make a video series around everything to know about hyperparameter optimization (or at least the more common and widely useful aspects). Off of the top of my head Iâ€™m thinking of random search, grid search, genetic algorithm based search, and bayesian based optimization. Also hoping to digest some papers (weâ€™ll see how that goes...) about marginal gain of tuning certain parameters. Are there other subfields of this topic that I completely missed out? (I feel like thereâ€™s probably a lot more interesting research going on around this topic)

This is the intro video and motivation to why this is crucial if you are interested: https://youtu.be/snXUilHbvxw",t2_522c8qe4,False,,0,False,Video series on everything to know about hyperparameter optimization,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9ehyz,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Project,False,0,,False,self,False,,[],{},self,,True,,1588068375.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m planning to make a video series around everything to know about hyperparameter optimization (or at least the more common and widely useful aspects). Off of the top of my head Iâ€™m thinking of random search, grid search, genetic algorithm based search, and bayesian based optimization. Also hoping to digest some papers (weâ€™ll see how that goes...) about marginal gain of tuning certain parameters. Are there other subfields of this topic that I completely missed out? (I feel like thereâ€™s probably a lot more interesting research going on around this topic)&lt;/p&gt;

&lt;p&gt;This is the intro video and motivation to why this is crucial if you are interested: &lt;a href=""https://youtu.be/snXUilHbvxw""&gt;https://youtu.be/snXUilHbvxw&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fEa5TpNo-d75I0OAJycw1ZI_2OJmJLekYROLqMtXsQA.jpg?auto=webp&amp;s=10160b8fd486279d1fbc4f6daff8cfb924ffb260', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/fEa5TpNo-d75I0OAJycw1ZI_2OJmJLekYROLqMtXsQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ad05812edd511234c78dc49d266a6799bd3b12f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fEa5TpNo-d75I0OAJycw1ZI_2OJmJLekYROLqMtXsQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=694d57258541d67099ef104699ce8993808dd60d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fEa5TpNo-d75I0OAJycw1ZI_2OJmJLekYROLqMtXsQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4b1fa49d9cfaa7fd4da11755d24c5fb98edef2d', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KW9oM2SiMfxKN6FRj_L5AIHHXvXRCdAr_p6aBXR1mNg'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g9ehyz,True,,sweetpotatowedge9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9ehyz/video_series_on_everything_to_know_about/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9ehyz/video_series_on_everything_to_know_about/,155203,1588039575.0,0,,False,,,,
,learnmachinelearning,[How to Learn Machine Learning and Deep Learning: a guide for Software Engineers](https://renanmf.com/machine-learning-and-deep-learning-software-engineers/),t2_42uenc14,False,,0,False,"I learned modern Machine Learning using only online resources, here is what I used","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8t3nj,False,light,0.9,,public,58,0,{},,,False,[],,False,False,,{},Discussion,False,58,,False,self,False,,[],{},self,,True,,1587988305.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://renanmf.com/machine-learning-and-deep-learning-software-engineers/""&gt;How to Learn Machine Learning and Deep Learning: a guide for Software Engineers&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?auto=webp&amp;s=a9b6cd00c14a935ae01bb65faa926ffe6208e91b', 'width': 1024, 'height': 768}, 'resolutions': [{'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8d2d11d641363b312716a37fe5c382aba3f98af', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bb7676dd41dc2950a463a45eddbc40f2488dcda', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9651af50aea2d28a76406f184254c19d10fe6d0', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=147f9a7700849d8f5f28c4e2e6ba65dc416b96d2', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/doN5O-3__Mbf3GaFrYsy4Q5dakKtx09IIg5bAZn1t3E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b875a608bb457f8166222f21484d49cebed8b44', 'width': 960, 'height': 720}], 'variants': {}, 'id': 'BG5NR0xUTPavM4R1ehi9YnpDWzSqGbO1PYgIm9mchRI'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8t3nj,True,,renanmoura,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8t3nj/i_learned_modern_machine_learning_using_only/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8t3nj/i_learned_modern_machine_learning_using_only/,155203,1587959505.0,0,,False,,,,
,learnmachinelearning,"Hi everyone!

So I want to create surveys (quantitative) and ask users about their experience on various websites and then use the answers as a feature (usability score, e.g. 1 to 5) for my training dataset. I also want to store the screenshot for every website on the survey. I wonder if I can predict that usability score based on a website screenshot using for example a CNN model (or any other)? What's the most suitable algorithm for this problem? Or do I have to use other features to base predictions on rather than just website screenshots? Thanks!",t2_4p7a3prw,False,,0,False,Algorithm to predict the usability score for a webpage,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g9d94j,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588063563.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;So I want to create surveys (quantitative) and ask users about their experience on various websites and then use the answers as a feature (usability score, e.g. 1 to 5) for my training dataset. I also want to store the screenshot for every website on the survey. I wonder if I can predict that usability score based on a website screenshot using for example a CNN model (or any other)? What&amp;#39;s the most suitable algorithm for this problem? Or do I have to use other features to base predictions on rather than just website screenshots? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g9d94j,True,,obrazvomgle,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9d94j/algorithm_to_predict_the_usability_score_for_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9d94j/algorithm_to_predict_the_usability_score_for_a/,155203,1588034763.0,0,,False,,,,
,learnmachinelearning,"Sorry for the vague title, but i'm a bit inexperienced to form a good question.

I am trying to use [RRIN](https://github.com/HopLee6/RRIN) to interpolate images, which works for images resized to fit the demo images dimensions. However, using images of other dimensions, i get errors like this: (For reference input images are 400 x 249)

    invalid argument 0: Sizes of tensors must match except in dimension 1. Got 249 and 240 in dimension 2

I understand that it has to do with the shapes of the layers, but i am at a loss on how to avoid it.

In this case, the error happens at this [line](https://github.com/HopLee6/RRIN/blob/cbf66f1282c2627a3d850e6af5cfa01480085ca1/model.py#L37), however, the differently sized tensors are a result of the Flow [UNet](https://github.com/HopLee6/RRIN/blob/master/unet.py), and this is where i get way over my head, as i don't really understand much of whats happening inside of that. Which makes it hard to find a solution.

If anyone has an idea on how i can fix it, (or if making it work with any images sizes is just impossible) then i'd love to know, or in the bad case any alternative ways i could go about it.

# EDIT:

I should read fully down pages, the most probable cause it pretty clearly stated in the UNet documentation. I need input sizes that are  `depth - 1` times divisible by 2. If anyone know a simple way to pad input tensors and then crop away the output, i'd love to know. Because i still don't quite get how to fix this, even if i pad images to follow the given rule.

# EDIT 2: 

Solved! Simply found the next divisible per the rule, and padded the image accordingly, then cropped the model output again. ",t2_avg4b,False,,0,False,Trying to interpolate images,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g99sva,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,1588027406.0,,[],{},self,,True,,1588051489.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry for the vague title, but i&amp;#39;m a bit inexperienced to form a good question.&lt;/p&gt;

&lt;p&gt;I am trying to use &lt;a href=""https://github.com/HopLee6/RRIN""&gt;RRIN&lt;/a&gt; to interpolate images, which works for images resized to fit the demo images dimensions. However, using images of other dimensions, i get errors like this: (For reference input images are 400 x 249)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;invalid argument 0: Sizes of tensors must match except in dimension 1. Got 249 and 240 in dimension 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I understand that it has to do with the shapes of the layers, but i am at a loss on how to avoid it.&lt;/p&gt;

&lt;p&gt;In this case, the error happens at this &lt;a href=""https://github.com/HopLee6/RRIN/blob/cbf66f1282c2627a3d850e6af5cfa01480085ca1/model.py#L37""&gt;line&lt;/a&gt;, however, the differently sized tensors are a result of the Flow &lt;a href=""https://github.com/HopLee6/RRIN/blob/master/unet.py""&gt;UNet&lt;/a&gt;, and this is where i get way over my head, as i don&amp;#39;t really understand much of whats happening inside of that. Which makes it hard to find a solution.&lt;/p&gt;

&lt;p&gt;If anyone has an idea on how i can fix it, (or if making it work with any images sizes is just impossible) then i&amp;#39;d love to know, or in the bad case any alternative ways i could go about it.&lt;/p&gt;

&lt;h1&gt;EDIT:&lt;/h1&gt;

&lt;p&gt;I should read fully down pages, the most probable cause it pretty clearly stated in the UNet documentation. I need input sizes that are  &lt;code&gt;depth - 1&lt;/code&gt; times divisible by 2. If anyone know a simple way to pad input tensors and then crop away the output, i&amp;#39;d love to know. Because i still don&amp;#39;t quite get how to fix this, even if i pad images to follow the given rule.&lt;/p&gt;

&lt;h1&gt;EDIT 2:&lt;/h1&gt;

&lt;p&gt;Solved! Simply found the next divisible per the rule, and padded the image accordingly, then cropped the model output again. &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZG85hidbOV_KoMj28mFGQrx6yKjKeGDaeOIYYUvm_iA.jpg?auto=webp&amp;s=fb8f3ca6af84b81e14159012a3bbd7bc62d04f37', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZG85hidbOV_KoMj28mFGQrx6yKjKeGDaeOIYYUvm_iA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=157c1b9efa4dc2359f464208df58796d76916d3d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ZG85hidbOV_KoMj28mFGQrx6yKjKeGDaeOIYYUvm_iA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a36e8d2d44e569a39288e4c7463201c5a9e72d99', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ZG85hidbOV_KoMj28mFGQrx6yKjKeGDaeOIYYUvm_iA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd37db1e422985e66712021fc070cc781dce5bf8', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_N3f1Tqen1iVA_oL-qSI9b92CRe9XvAW0D9Ys3zVY9s'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g99sva,True,,Thomasedv,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g99sva/trying_to_interpolate_images/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g99sva/trying_to_interpolate_images/,155203,1588022689.0,0,,False,,,,
,learnmachinelearning,"Hi! I'm a Management student and I'm currently writing my Master's thesis on AI in project management.

As an introductory part, I would like to collect some information about people's perspective on AI, both in their daily lives and in the workplace.

If you work/worked in a company, could you please fill in this short questionnaire? Thank you so much in advance, feel free to contact me for any question!

The link is the following: [https://docs.google.com/forms/d/e/1FAIpQLSfT-5VWT-fiZJ4Z3c73eHKJhbfBs3Jk5Qs0FL2zjtOLaA4Igg/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSfT-5VWT-fiZJ4Z3c73eHKJhbfBs3Jk5Qs0FL2zjtOLaA4Igg/viewform?usp=sf_link)",t2_4vz0vxul,False,,0,False,AI in project management,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_g99i4n,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Request,False,0,,False,self,False,,[],{},self,,True,,1588050560.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! I&amp;#39;m a Management student and I&amp;#39;m currently writing my Master&amp;#39;s thesis on AI in project management.&lt;/p&gt;

&lt;p&gt;As an introductory part, I would like to collect some information about people&amp;#39;s perspective on AI, both in their daily lives and in the workplace.&lt;/p&gt;

&lt;p&gt;If you work/worked in a company, could you please fill in this short questionnaire? Thank you so much in advance, feel free to contact me for any question!&lt;/p&gt;

&lt;p&gt;The link is the following: &lt;a href=""https://docs.google.com/forms/d/e/1FAIpQLSfT-5VWT-fiZJ4Z3c73eHKJhbfBs3Jk5Qs0FL2zjtOLaA4Igg/viewform?usp=sf_link""&gt;https://docs.google.com/forms/d/e/1FAIpQLSfT-5VWT-fiZJ4Z3c73eHKJhbfBs3Jk5Qs0FL2zjtOLaA4Igg/viewform?usp=sf_link&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?auto=webp&amp;s=8c2e875c26e4b24fde4907a455a6a6c49a3935a5', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=271a12bc7eb27ff03a90a7c1bdb1d758cc810a26', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e42d1c69f9d1f29f43ebe8a747a3acee807e39c', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51262b8ae9b66f78cfcb4e7bae03e66cc15f084c', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b79a539584af1aecd320a12afd3b2b21d73c94b3', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e38d9345050777414f6236a593c3d3dd0d71f1eb', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/5JSlkWLPeTv-PEmnEezplxqD--4BZLltzZhcElE3v4g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c6a840e1e15b97307aadd0581af8d1eac87f5e0', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '_dtLRjO1kfItsyPle6BAMDDi1GaQ-2I1l1eg7y6BwFc'}], 'enabled': False}",[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,g99i4n,True,,pmst_su,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g99i4n/ai_in_project_management/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g99i4n/ai_in_project_management/,155203,1588021760.0,0,,False,,,,
,learnmachinelearning,Could anyone please post some free courses during lockdown period for our community?,t2_2ic7kgpw,False,,0,False,Covid 19 free Courses,[],r/learnmachinelearning,False,6,,0,,False,t3_g97zi3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588045912.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could anyone please post some free courses during lockdown period for our community?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g97zi3,True,,gauravlogical,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g97zi3/covid_19_free_courses/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g97zi3/covid_19_free_courses/,155203,1588017112.0,0,,False,,,,
,learnmachinelearning,"[Link to GitHub repository](https://www.github.com/neuron-whisperer/basicbert/)

Earlier this year, I began experimenting with the Google Bidirectional Encoder Representation from Transformers (BERT) model for text classification.

While BERT is a powerful and capable library, the codebase is unduly - and unnecessarily - difficult to learn and adapt for everyday use in simple text classification tasks. I spent quite some time delving into the quirks and messy details of the codebase, and ended up developing a wrapper class for the Google BERT codebase that can be invoked in a very simple manner (but with a range of configurable options), like this:

    from basicbert import *
    bert = BERT()
    bert.prepare_data()         # prepare .tsvs from data.csv
    bert.train(5)               # train for five epochs
    metrics = bert.eval()       # evaluate training accuracy
    bert.export()               # export model from checkpoint
    bert.predict(input_text)    # predict class of input

`basicbert.py` is a single module that provides the `BERT` wrapper class. It can be configured by initializing it with a dictionary, by reading a `config.txt` file in the same folder as the script, or by using default values. It can operate on a standard set of data files (train.tsv, dev.tsv, test.tsv) as used by the Google BERT codebase, or it can generate these files from a master .csv file with any desired train/dev/test split.

I hope that this wrapper class enables other developers to use BERT-based text classification in their projects without having to study the BERT codebase at length.",t2_66satgmm,False,,0,False,basicbert: A wrapper class and usage guide for the Google BERT text classification model,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g93f83,False,light,0.67,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},self,,True,,1588031988.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://www.github.com/neuron-whisperer/basicbert/""&gt;Link to GitHub repository&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Earlier this year, I began experimenting with the Google Bidirectional Encoder Representation from Transformers (BERT) model for text classification.&lt;/p&gt;

&lt;p&gt;While BERT is a powerful and capable library, the codebase is unduly - and unnecessarily - difficult to learn and adapt for everyday use in simple text classification tasks. I spent quite some time delving into the quirks and messy details of the codebase, and ended up developing a wrapper class for the Google BERT codebase that can be invoked in a very simple manner (but with a range of configurable options), like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from basicbert import *
bert = BERT()
bert.prepare_data()         # prepare .tsvs from data.csv
bert.train(5)               # train for five epochs
metrics = bert.eval()       # evaluate training accuracy
bert.export()               # export model from checkpoint
bert.predict(input_text)    # predict class of input
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;basicbert.py&lt;/code&gt; is a single module that provides the &lt;code&gt;BERT&lt;/code&gt; wrapper class. It can be configured by initializing it with a dictionary, by reading a &lt;code&gt;config.txt&lt;/code&gt; file in the same folder as the script, or by using default values. It can operate on a standard set of data files (train.tsv, dev.tsv, test.tsv) as used by the Google BERT codebase, or it can generate these files from a master .csv file with any desired train/dev/test split.&lt;/p&gt;

&lt;p&gt;I hope that this wrapper class enables other developers to use BERT-based text classification in their projects without having to study the BERT codebase at length.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wKSLEQTyQnHEBLXG-kTKx2PpmIA67r6Mf8hBDJ1WkW0.jpg?auto=webp&amp;s=2af4f13eb13662acdc0e8d13b156ae89af454cf5', 'width': 200, 'height': 200}, 'resolutions': [{'url': 'https://external-preview.redd.it/wKSLEQTyQnHEBLXG-kTKx2PpmIA67r6Mf8hBDJ1WkW0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1c1a3bcbc7c106eb844e238d9868f519919d1510', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'f4rbOjFOa1OKRb9fXC79I6jmHaqv2XBPalkbvJK2BNU'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g93f83,True,,neuron_whisperer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g93f83/basicbert_a_wrapper_class_and_usage_guide_for_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g93f83/basicbert_a_wrapper_class_and_usage_guide_for_the/,155203,1588003188.0,0,,False,,,,
,learnmachinelearning,,t2_45portqj,False,,0,False,"Can anyone explain how the pseudocode for this minmax algorithm works in detail, I dont quite understand the Wikipedia explanation.",[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g937es,False,dark,0.67,,public,2,0,{},140.0,,False,[],,True,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/JY5i9ZJJIKGrjWvuArN6To2atJPULDkaDHNFhZ5th-I.jpg,False,,[],{},image,,False,,1588031311.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?auto=webp&amp;s=d9b3f32341184c9594b650e28214623001290106', 'width': 1080, 'height': 1920}, 'resolutions': [{'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c074dad62a46bb77627eedf9cd8d0ab3ddbe2a0c', 'width': 108, 'height': 192}, {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83786588b663920a36b1c5f5860f5582c75e48b6', 'width': 216, 'height': 384}, {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=610176b237f04cadc45f317a3e6687259912645a', 'width': 320, 'height': 568}, {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a3fe6182e676ac343327dcdee2f534933c13460', 'width': 640, 'height': 1137}, {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4e352ce0d47eb8a19bfa63141c82f36b2959780', 'width': 960, 'height': 1706}, {'url': 'https://preview.redd.it/q11wyk29tdv41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=897a8577e9b7cc648ad36f309c2e30d394aa5550', 'width': 1080, 'height': 1920}], 'variants': {}, 'id': '8qc2q3XCq0yaumEu3xDeJsOQLovGdg2YsEHjJFhYxII'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g937es,True,,Vertex_SouthAfrica,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g937es/can_anyone_explain_how_the_pseudocode_for_this/,all_ads,False,https://i.redd.it/q11wyk29tdv41.jpg,155203,1588002511.0,0,,False,,,,
,learnmachinelearning," 

Hi there,

Unfortunately, I didn't find any particular opinions on this notebook regarding machine learning so I thought that I'd post the question here:

*Do you think the* ***Dell Precision 5540*** *is a good pick for performing various machine learning tasks and training some models locally on the laptop?*

These are its specs:

* IntelÂ® Coreâ„¢ Prozessor i7-9850H, 6-Core
* Nvidia Quadro T1000 w/4GB
* 16 GB RAM",t2_60fvtl81,False,,0,False,[D] Dell Precision 5540 good for machine learning?,[],r/learnmachinelearning,False,6,,0,,False,t3_g9789a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588043547.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;Unfortunately, I didn&amp;#39;t find any particular opinions on this notebook regarding machine learning so I thought that I&amp;#39;d post the question here:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Do you think the&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Dell Precision 5540&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;is a good pick for performing various machine learning tasks and training some models locally on the laptop?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These are its specs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IntelÂ® Coreâ„¢ Prozessor i7-9850H, 6-Core&lt;/li&gt;
&lt;li&gt;Nvidia Quadro T1000 w/4GB&lt;/li&gt;
&lt;li&gt;16 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g9789a,True,,liondungl,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g9789a/d_dell_precision_5540_good_for_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g9789a/d_dell_precision_5540_good_for_machine_learning/,155203,1588014747.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

At the Vrije Universiteit Amsterdam, we are conducting a survey on team dynamics in the software industry.

If you are working in the software industry, could you help us out and fill out our survey? It takes around 10 minutes to complete. Here the link: https://docs.google.com/forms/d/e/1FAIpQLSc3NQarMS3Vs07Czl965X_NadwFZjMrpW-rZXkG06hucoG5Kw/viewform?usp=sf_link

Thank you, stay safe!",t2_5q6mvqze,False,,0,False,Survey on team dynamics in the software industry.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g96klg,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1588041542.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;At the Vrije Universiteit Amsterdam, we are conducting a survey on team dynamics in the software industry.&lt;/p&gt;

&lt;p&gt;If you are working in the software industry, could you help us out and fill out our survey? It takes around 10 minutes to complete. Here the link: &lt;a href=""https://docs.google.com/forms/d/e/1FAIpQLSc3NQarMS3Vs07Czl965X_NadwFZjMrpW-rZXkG06hucoG5Kw/viewform?usp=sf_link""&gt;https://docs.google.com/forms/d/e/1FAIpQLSc3NQarMS3Vs07Czl965X_NadwFZjMrpW-rZXkG06hucoG5Kw/viewform?usp=sf_link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you, stay safe!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?auto=webp&amp;s=8c6dbf5c62d9d73ccd4f77d406bfc26bd87ac53f', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a52a5a6a0661c0f974062ab99fc292f72825e710', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41c732159be23415a46c7c4a2ce859948e9ca457', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5468fc7cc0a4b734b227af4c96faa685b529f541', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1824ea9b14981390b63cffc70a83c2b7a3c858e9', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=98d41035f4fdb99d5b1e34c4d8e1dd99d35ed228', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/EVY27kaAn8pV6fuuN2ZOlKiIZikbMBGryAzY38UtZdI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d24f985d22eddbf25a1d1575d231ba39f318c6e', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'L9A4xNA6yr8-VXNV_m0W5j9UgKRT8HiUVfs4tmnhOk4'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g96klg,True,,Blitz_2512,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g96klg/survey_on_team_dynamics_in_the_software_industry/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g96klg/survey_on_team_dynamics_in_the_software_industry/,155203,1588012742.0,0,,False,,,,
,learnmachinelearning,"Hello, I am a computer science student and I need some help with finding a topic for a project.

&amp;#x200B;

I need to write a paper / implementation of an optimization technique for a machine learning algorithm.

&amp;#x200B;

To give more context, the optimization technique should be applicable to any of these algorithms: K-Nearest Neighbors, K-Means Clustering, Decision Trees, Linear Regression or Support Vector Machines. By optimization technique I believe it's when you try to find the minimum of a function.

Gradient Descent is one such technique, but I need to find another one.

&amp;#x200B;

I have been searching the web for materials, but I can't understand anything from most papers out there :(. Sadly, the professor couldn't hold the lectures because of the quarantine and we only have very shallow course materials. My only basic understanding of machine learning doesn't help either.

&amp;#x200B;

That is why I need something simple that I can implement and understand.

&amp;#x200B;

Does anyone have any ideas for a simple machine learning optimization technique and where I can find it clearly explained? I would be very grateful.

&amp;#x200B;

Thanks a lot!

&amp;#x200B;

Note: I have also asked this in /r/AskComputerScience and /r/ComputerScience, I hope it's ok",t2_hhbrr,False,,0,False,I am looking for an easy ML optimization techniquie,[],r/learnmachinelearning,False,6,,0,,False,t3_g96bfx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588040776.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am a computer science student and I need some help with finding a topic for a project.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I need to write a paper / implementation of an optimization technique for a machine learning algorithm.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;To give more context, the optimization technique should be applicable to any of these algorithms: K-Nearest Neighbors, K-Means Clustering, Decision Trees, Linear Regression or Support Vector Machines. By optimization technique I believe it&amp;#39;s when you try to find the minimum of a function.&lt;/p&gt;

&lt;p&gt;Gradient Descent is one such technique, but I need to find another one.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I have been searching the web for materials, but I can&amp;#39;t understand anything from most papers out there :(. Sadly, the professor couldn&amp;#39;t hold the lectures because of the quarantine and we only have very shallow course materials. My only basic understanding of machine learning doesn&amp;#39;t help either.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;That is why I need something simple that I can implement and understand.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Does anyone have any ideas for a simple machine learning optimization technique and where I can find it clearly explained? I would be very grateful.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Note: I have also asked this in &lt;a href=""/r/AskComputerScience""&gt;/r/AskComputerScience&lt;/a&gt; and &lt;a href=""/r/ComputerScience""&gt;/r/ComputerScience&lt;/a&gt;, I hope it&amp;#39;s ok&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g96bfx,True,,daverave1212,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g96bfx/i_am_looking_for_an_easy_ml_optimization/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g96bfx/i_am_looking_for_an_easy_ml_optimization/,155203,1588011976.0,0,,False,,,,
,learnmachinelearning,,t2_3n5irqjg,False,,0,False,"What is anomaly detection, and why you need it?",[],r/learnmachinelearning,False,6,,0,93.0,False,t3_g95fcd,False,dark,0.6,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/ULzNOnInN7PV_obHbf1jflQAWDWhLP4TOu6fnjIP758.jpg,False,,[],{},link,,False,,1588038035.0,text,6,,,text,thedatascientist.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?auto=webp&amp;s=de5278eee6e2e800304677371a972e84fbb0ea07', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9bbd9bc75f9432f3efb1ec029b2e0539565dcd3', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0764d7f67278ea10ed913159458d399b01d64154', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13136916fcc3ddf6be154dee3e50ae678f2eb6d2', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf71ef889ca90cc821af861a973d4a5374fc7ccf', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93c9a4cce0fbfc44d399e7e293e8cb16f3a7518e', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/5tmlmr3Ac-Z_iOLsuQbtbnYVCh3RHa2blIAD_wOTBm4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=70f15d85cbf0d781bf36d1248adfafee72273368', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'vswUqR7kZpOKSn7d8hTLDfOEY7Xk8RLbzo-cNgYyUrM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g95fcd,True,,TheTesseractAcademy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g95fcd/what_is_anomaly_detection_and_why_you_need_it/,all_ads,False,https://thedatascientist.com/anomaly-detection-why-you-need-it/,155203,1588009235.0,0,,False,,,,
,learnmachinelearning,I was reading about Generalized Additive Models (GAMs) few days back. I wanted to know how can we utilize it for tasks like binary classification.,t2_6apatrrn,False,,0,False,Using Generalized Linear Models for Classification,[],r/learnmachinelearning,False,6,,0,,False,t3_g956qq,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588037316.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was reading about Generalized Additive Models (GAMs) few days back. I wanted to know how can we utilize it for tasks like binary classification.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g956qq,True,,noob_level,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g956qq/using_generalized_linear_models_for_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g956qq/using_generalized_linear_models_for_classification/,155203,1588008516.0,0,,False,,,,
,learnmachinelearning,,t2_53t39z3t,False,,0,False,Introduction to Linear Regression in Sklearn,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g951xi,False,light,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FN9goRxR9fA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Linear Regression in Sklearn | Practical machine learning tutorial with python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FN9goRxR9fA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'datageekrj', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FN9goRxR9fA/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCeEsbCOjbSXao5dwqXLlJmQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FN9goRxR9fA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g951xi', 'height': 338}",Project,False,0,,False,https://b.thumbs.redditmedia.com/Szp4AqC7H_UWkKExB4Hwb8-0Qv4Adi6_UYSzMWEeVVA.jpg,False,,[],{},rich:video,,False,,1588036926.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ttCsHeCKH-gwPclTa1hBsySFsyhm81NKkPUr2a08H6k.jpg?auto=webp&amp;s=07959607760f2bbb8cbe487c05ecc280d9900877', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ttCsHeCKH-gwPclTa1hBsySFsyhm81NKkPUr2a08H6k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=873769a102450d600ba7a661954669360c54c9c9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ttCsHeCKH-gwPclTa1hBsySFsyhm81NKkPUr2a08H6k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c87f908c6e66a5ba84a6b5b50d3d0d9cbe49e6e3', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ttCsHeCKH-gwPclTa1hBsySFsyhm81NKkPUr2a08H6k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1227d363d9355fb901b4bc603a7067013f1165a', 'width': 320, 'height': 240}], 'variants': {}, 'id': '1xHHEj4DexhUMID9-uojp1Hv9T0BRYfONOHP3a2LeHo'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g951xi,True,,datageekrj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g951xi/introduction_to_linear_regression_in_sklearn/,all_ads,False,https://youtu.be/FN9goRxR9fA,155203,1588008126.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Linear Regression in Sklearn | Practical machine learning tutorial with python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/FN9goRxR9fA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'datageekrj', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/FN9goRxR9fA/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCeEsbCOjbSXao5dwqXLlJmQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hi guys,

In recent times, I read a couple of papers where they use neural networks, parallel layers CNNs in particular, for extracting features from regular time series and using these features to feed them in other models. I could not find a practical implementation of this, so started to implement them my self.

In NNs like these, do we just create parallel layers CNNs and just return the output of one of the layers to see how the network extracted some features? If you can share some good resources it would be of great help!

Cheers.",t2_14w7k8,False,,0,False,Extracting features using CNNs?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g94wtb,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,1588008172.0,,[],{},,,True,,1588036492.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;In recent times, I read a couple of papers where they use neural networks, parallel layers CNNs in particular, for extracting features from regular time series and using these features to feed them in other models. I could not find a practical implementation of this, so started to implement them my self.&lt;/p&gt;

&lt;p&gt;In NNs like these, do we just create parallel layers CNNs and just return the output of one of the layers to see how the network extracted some features? If you can share some good resources it would be of great help!&lt;/p&gt;

&lt;p&gt;Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g94wtb,True,,dflash88,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g94wtb/extracting_features_using_cnns/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g94wtb/extracting_features_using_cnns/,155203,1588007692.0,0,,False,,,,
,learnmachinelearning,,t2_3sxzs6ia,False,,0,False,Data Scientist vs Data Analyst,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g94j4u,False,light,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/juZ_Bb9Bpxs?start=313&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Data Scientist vs Data Analyst | Difference Between Data Scientist And Data Analyst | Simplilearn', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/juZ_Bb9Bpxs?start=313&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Simplilearn', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/juZ_Bb9Bpxs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/Simplilearn'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/juZ_Bb9Bpxs?start=313&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g94j4u', 'height': 338}",Discussion,False,0,,False,https://b.thumbs.redditmedia.com/LWKaNLbglGuKGE1wreLxDTBICm-1DW0JMwaZ77JH0lQ.jpg,False,,[],{},rich:video,,False,,1588035345.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mK2GJGfyEbbg6T_gmugEG9flUuWOLyKIiefSKHYxX9U.jpg?auto=webp&amp;s=11999c9374885cbf790d4ae2040f1db373746aa1', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/mK2GJGfyEbbg6T_gmugEG9flUuWOLyKIiefSKHYxX9U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b00e8654e734739fef9d4e555167ae7c5ff53e4e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/mK2GJGfyEbbg6T_gmugEG9flUuWOLyKIiefSKHYxX9U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=33e7f65c583a7e74ac2fdfbe4dff6ac4efa171b5', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/mK2GJGfyEbbg6T_gmugEG9flUuWOLyKIiefSKHYxX9U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d60950152df9b90fcd442b4f406ca74e81069811', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'foiap8AeUoJd3xHxWw2oTIe1ol6KBd8vGUfJoOEodNE'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g94j4u,True,,Public_Conflict,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g94j4u/data_scientist_vs_data_analyst/,all_ads,False,https://www.youtube.com/watch?v=juZ_Bb9Bpxs&amp;t=313s,155203,1588006545.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Data Scientist vs Data Analyst | Difference Between Data Scientist And Data Analyst | Simplilearn', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/juZ_Bb9Bpxs?start=313&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Simplilearn', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/juZ_Bb9Bpxs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/Simplilearn'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Good afternoon,

I'm working on a project where, using the information from multiple sensors, I estimate the GPS coordinates of a bike. To solve this problem I'm implementing a regressor using neural networks.

I've noticed that in all the projects I've found so far MSE or MAE are used as loss functions for regressors, while the Euclidean distance or RMSE are used as metrics. However,in my opinion, it seems that by using MSE or MAE, that are computed individually for each of the outputs (X and Y coordinates, in this case), the regressor could lose information about the relationship between X and Y while by using euclidean distance as loss function this relationship would be preserved.

So, to resume, my question is: does it make sense to use euclidean distance as a loss function? Could somebody point me some book or paper that address the development of custom loss functions from a mathematical point of view?

Thank you very much",t2_1kpdnprn,False,,0,False,Loss functions and regressors; Euclidean distance as loss for GPS regressor,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g945xk,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,1588005770.0,,[],{},,,True,,1588034223.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Good afternoon,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on a project where, using the information from multiple sensors, I estimate the GPS coordinates of a bike. To solve this problem I&amp;#39;m implementing a regressor using neural networks.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve noticed that in all the projects I&amp;#39;ve found so far MSE or MAE are used as loss functions for regressors, while the Euclidean distance or RMSE are used as metrics. However,in my opinion, it seems that by using MSE or MAE, that are computed individually for each of the outputs (X and Y coordinates, in this case), the regressor could lose information about the relationship between X and Y while by using euclidean distance as loss function this relationship would be preserved.&lt;/p&gt;

&lt;p&gt;So, to resume, my question is: does it make sense to use euclidean distance as a loss function? Could somebody point me some book or paper that address the development of custom loss functions from a mathematical point of view?&lt;/p&gt;

&lt;p&gt;Thank you very much&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g945xk,True,,Nikrusa,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g945xk/loss_functions_and_regressors_euclidean_distance/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g945xk/loss_functions_and_regressors_euclidean_distance/,155203,1588005423.0,0,,False,,,,
,learnmachinelearning,"And for dataset, I'm planning to use videos.

Lets say I have 100.000 videos. Each video is consisting of 1 game-round. (two characters fighting.)

In these videos, sometimes characters attack, sometimes they block moves  and punish it, sometimes move hits, sometimes not, etc.... We, as the players of this game can see the mistakes these characters do. But how my computer going to know if its good or bad?

And even then how will this process go? There are like millions of factors. I don't even know how to start. Can you guys tell me how can I approach this project?

Also do you think using ""videos"" as datasets is best way to go?

\-----

The game is called TEKKEN btw, for the curious people.

Thank you",t2_js1xl,False,,0,False,I want to create an AI that can play a fighting game,[],r/learnmachinelearning,False,6,,0,,False,t3_g94080,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588033742.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;And for dataset, I&amp;#39;m planning to use videos.&lt;/p&gt;

&lt;p&gt;Lets say I have 100.000 videos. Each video is consisting of 1 game-round. (two characters fighting.)&lt;/p&gt;

&lt;p&gt;In these videos, sometimes characters attack, sometimes they block moves  and punish it, sometimes move hits, sometimes not, etc.... We, as the players of this game can see the mistakes these characters do. But how my computer going to know if its good or bad?&lt;/p&gt;

&lt;p&gt;And even then how will this process go? There are like millions of factors. I don&amp;#39;t even know how to start. Can you guys tell me how can I approach this project?&lt;/p&gt;

&lt;p&gt;Also do you think using &amp;quot;videos&amp;quot; as datasets is best way to go?&lt;/p&gt;

&lt;p&gt;-----&lt;/p&gt;

&lt;p&gt;The game is called TEKKEN btw, for the curious people.&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g94080,True,,Blackwater_7,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g94080/i_want_to_create_an_ai_that_can_play_a_fighting/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g94080/i_want_to_create_an_ai_that_can_play_a_fighting/,155203,1588004942.0,0,,False,,,,
,learnmachinelearning,,t2_52t69px,False,,0,False,"I am starting my new series on Neural Networks in C#, Unity. This first episode covers a basic backpropagation perceptron and a brief introduction to hyperparameters such as learning create and weight decay!",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g93tg4,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-WjKICvAOsY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Neural Network implementation in C# using backpropagation!', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-WjKICvAOsY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'uNicoDev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-WjKICvAOsY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiKZ0GfG4AMCDhdm4J99fjQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-WjKICvAOsY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g93tg4', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/yfoLfd_jwEVZSvG-PE1Gr2HPdiNiNRbZl013fwTNNeM.jpg,False,,[],{},rich:video,,False,,1588033174.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/e-VvjAXHaakuwwaUEnaOWXcU7J5HVyl2EOPufjHADKc.jpg?auto=webp&amp;s=55418e8ea9c920777f70c6e7101e154f34b82ac6', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/e-VvjAXHaakuwwaUEnaOWXcU7J5HVyl2EOPufjHADKc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0a1332498c52494c722de709a6ccabdd44855bd', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/e-VvjAXHaakuwwaUEnaOWXcU7J5HVyl2EOPufjHADKc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bd9b07199886bec64bd2e4409338af0a32396a77', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/e-VvjAXHaakuwwaUEnaOWXcU7J5HVyl2EOPufjHADKc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c05fac19e058e36ae5acf3ba9e4a8394d0a28612', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'lKsJLYLJ4G6_mEiYJsSRMp3crOj6I3dMYK8G-6i5I_A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g93tg4,True,,NICO_THE_PRO,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g93tg4/i_am_starting_my_new_series_on_neural_networks_in/,all_ads,False,https://www.youtube.com/watch?v=-WjKICvAOsY,155203,1588004374.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Neural Network implementation in C# using backpropagation!', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-WjKICvAOsY?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'uNicoDev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-WjKICvAOsY/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCiKZ0GfG4AMCDhdm4J99fjQ'}}",False,,,,
,learnmachinelearning,,t2_2o4hfubx,False,,0,False,Simple AI Tutorial with NEAT-python,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g8y09z,False,dark,0.83,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/2o-jMhXmmxA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Simple AI Tutorial with NEAT-python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/2o-jMhXmmxA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Cheesy AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/2o-jMhXmmxA/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCrFtlRPsSJOpXYqlEc6yqJg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/2o-jMhXmmxA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8y09z', 'height': 338}",,False,4,,False,https://b.thumbs.redditmedia.com/awwRsAn3m5owrIu9fByYTsKlU8DB_qe-QgHnS4W2ELs.jpg,False,,[],{},rich:video,,False,,1588010820.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/HXEJ9tBOlACob0eGZJxkEGIgV2Y8ovnCtbRfUbAZluY.jpg?auto=webp&amp;s=127acdd70259a4ab72388369f93cc2e5a5023928', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/HXEJ9tBOlACob0eGZJxkEGIgV2Y8ovnCtbRfUbAZluY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e3dd8f21cc67c553d152ddabf337e381f144b7a', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/HXEJ9tBOlACob0eGZJxkEGIgV2Y8ovnCtbRfUbAZluY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee8f0ba3e75be6945c29196fc288b0cf8b8d69b1', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/HXEJ9tBOlACob0eGZJxkEGIgV2Y8ovnCtbRfUbAZluY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa9987bf1f2dfad65061d1ace2214c8583e45877', 'width': 320, 'height': 240}], 'variants': {}, 'id': '3NENh2eLGbhbd9giN5EbAJ_iprZtWXsTx6qLfSMbb-s'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8y09z,True,,Mingoooose,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8y09z/simple_ai_tutorial_with_neatpython/,all_ads,False,https://youtu.be/2o-jMhXmmxA,155203,1587982020.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Simple AI Tutorial with NEAT-python', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/2o-jMhXmmxA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Cheesy AI', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/2o-jMhXmmxA/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCrFtlRPsSJOpXYqlEc6yqJg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_3srn066n,False,,0,False,"Hey everyone! Wrote an introduction post to Markov Decision Processes! This is my first technical blog so any feedback, comments and suggestions would be highly appreciated!",[],r/learnmachinelearning,False,6,,0,60.0,False,t3_g93iep,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/5mI277t1H5lwPwnKwR1EKzCkf3NXx65z0b8_p9c_q2s.jpg,False,,[],{},link,,False,,1588032245.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?auto=webp&amp;s=df2e853185e1418824b72b597b9d88682ee9d58b', 'width': 1200, 'height': 520}, 'resolutions': [{'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c11efd2c24d355d19f76f7f41029a998d5b24f67', 'width': 108, 'height': 46}, {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8c0c35d71189e076c254f1ff998bdf907aadd6', 'width': 216, 'height': 93}, {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1c445589d8d09737f7461cd6dc9a9d8a8eb01f4e', 'width': 320, 'height': 138}, {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31f813b088f974968cf5d1a1c3b3ee65cb993d48', 'width': 640, 'height': 277}, {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e6df6fadca1f5bbe653912aab3e921c0184e6e8', 'width': 960, 'height': 416}, {'url': 'https://external-preview.redd.it/tUIYehwsGM_sX4MFCp0isDByHEGE_3neWceAlNQWRjU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c075255cd99bf448f37af63ca7a31f4a8d0690f', 'width': 1080, 'height': 468}], 'variants': {}, 'id': '67SxLBuu07kqm1Acqz9v3Ixvcn5e_bIUju3bi7OXKJA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g93iep,True,,mitesh1612,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g93iep/hey_everyone_wrote_an_introduction_post_to_markov/,all_ads,False,https://medium.com/@mitesh_shah/introduction-to-markov-decision-processes-c966874d4b5,155203,1588003445.0,0,,False,,,,
,learnmachinelearning,"What I'm trying to do is, give an input (a double), train a population of networks to simulate (produce the same output) of a nth degree polynomial.

I've decided to try this little project because, as far as I know, it wold be one of the easiest/simplest possible simulations for a network, but I haven't had much success this far. After a thousand generations of training a population with 1000 individuals, the top fitness network still produces output that differs a lot from the given polynomial

I'm using C# with no framework or fancy libraries, with relu as activation function.

Is this achievable? If so, what's the best way to go about this or how would you do it? 

Thanks.",t2_426zrg3o,False,,0,False,Neuroevolution for polynomial simulation,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g93dx1,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588031874.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What I&amp;#39;m trying to do is, give an input (a double), train a population of networks to simulate (produce the same output) of a nth degree polynomial.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve decided to try this little project because, as far as I know, it wold be one of the easiest/simplest possible simulations for a network, but I haven&amp;#39;t had much success this far. After a thousand generations of training a population with 1000 individuals, the top fitness network still produces output that differs a lot from the given polynomial&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using C# with no framework or fancy libraries, with relu as activation function.&lt;/p&gt;

&lt;p&gt;Is this achievable? If so, what&amp;#39;s the best way to go about this or how would you do it? &lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g93dx1,True,,GCCSheepReddit,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g93dx1/neuroevolution_for_polynomial_simulation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g93dx1/neuroevolution_for_polynomial_simulation/,155203,1588003074.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Pothole detection on roads,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g8awjf,False,light,0.96,,public,300,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/pHYCxOzPVEc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Pothole detection on roads', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/pHYCxOzPVEc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/pHYCxOzPVEc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/pHYCxOzPVEc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8awjf', 'height': 344}",Discussion,False,300,,False,https://b.thumbs.redditmedia.com/NzRzuHziGMmirmqnSTQc2fO58fd6iJnSEP0zFVaRo9s.jpg,False,,[],{},rich:video,,False,,1587918710.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/egHoh-zLym3URhaFeMe7T1dTzPcefwcelmoBTMlk24w.jpg?auto=webp&amp;s=58feabeb185f8cf0e07557dc5291262355ded7d3', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/egHoh-zLym3URhaFeMe7T1dTzPcefwcelmoBTMlk24w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ace1171d87c9def7182f81351c8bb4231b714', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/egHoh-zLym3URhaFeMe7T1dTzPcefwcelmoBTMlk24w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2160d72499b87517a56d2a476a0b06340c9cd154', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/egHoh-zLym3URhaFeMe7T1dTzPcefwcelmoBTMlk24w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf8b329dcb65d1dbc7d3000af59bb467a20fac48', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'wx9G5qoW4k5LbXSYIfcHf6iKwcQebZ7Z65ozTv-D64s'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8awjf,True,,cmillionaire9,,35,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8awjf/pothole_detection_on_roads/,all_ads,False,https://youtu.be/pHYCxOzPVEc,155203,1587889910.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Pothole detection on roads', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/pHYCxOzPVEc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/pHYCxOzPVEc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"tI am working on a project to separate three signals from one dataset (signal 'S' = A + B +C). This is recorded by a magnetometer. To clean up the data, I want separate the three series. A is the signal we want to keep and is a function of position; 'B' is a function of time (of day); and 'C' is a function of sensor orientation, acceleration, etc.  I also have many inputs (position, speed, orientation, time of day) to help separate the three sources along with what 'B' should be. All parameters will vary over time as I traverse over a survey area.

S(x,y,z,v,t,yaw,pitch,roll,...) = A(x,y,z) + B(t) + C(v, yaw, pitch, roll)

I have been reading up on Audio Source Separation (such as Multichannel NMF) as a starting point for this but I'm uncertain that it is my best approach. My understanding is that the additional inputs I have (position, orientation, etc) wouldn't be used, or I'm unsure on how to properly use them? I thought M-NMF would be good since it seems to work for all cases (N&lt;M, N=M, and N&gt;M - where N is # of sources, and M is # of sensors). I figured for my case there are three sources (N=3 from A, B, and C) and 1 sensor (M=1 primary recording device - magnetometer). I don't know how to include other sensor information or if it's possible (position (GPS), etc).

Does anyone has a suggestion of how to approach this? The primary goal is to remove C from the signal. I thought I could use B as a way to supervise the learning (semi-supervised?). Is there a technique to do semi-supervision for signal separation? Maybe there is a better option than M-NMF out there?

I am new to ML so pardon my lack of knowledge in some things. I've just thrown myself into the deep end to figure it out. 

Thank you in advance for the help!",t2_gzba6,False,,0,False,"Question - signal separation separate three functions dependent on different inputs from a signal (S = A(x,y,z) + B(t) + C(v,o))",[],r/learnmachinelearning,False,6,,0,,False,t3_g92im9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588029148.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;tI am working on a project to separate three signals from one dataset (signal &amp;#39;S&amp;#39; = A + B +C). This is recorded by a magnetometer. To clean up the data, I want separate the three series. A is the signal we want to keep and is a function of position; &amp;#39;B&amp;#39; is a function of time (of day); and &amp;#39;C&amp;#39; is a function of sensor orientation, acceleration, etc.  I also have many inputs (position, speed, orientation, time of day) to help separate the three sources along with what &amp;#39;B&amp;#39; should be. All parameters will vary over time as I traverse over a survey area.&lt;/p&gt;

&lt;p&gt;S(x,y,z,v,t,yaw,pitch,roll,...) = A(x,y,z) + B(t) + C(v, yaw, pitch, roll)&lt;/p&gt;

&lt;p&gt;I have been reading up on Audio Source Separation (such as Multichannel NMF) as a starting point for this but I&amp;#39;m uncertain that it is my best approach. My understanding is that the additional inputs I have (position, orientation, etc) wouldn&amp;#39;t be used, or I&amp;#39;m unsure on how to properly use them? I thought M-NMF would be good since it seems to work for all cases (N&amp;lt;M, N=M, and N&amp;gt;M - where N is # of sources, and M is # of sensors). I figured for my case there are three sources (N=3 from A, B, and C) and 1 sensor (M=1 primary recording device - magnetometer). I don&amp;#39;t know how to include other sensor information or if it&amp;#39;s possible (position (GPS), etc).&lt;/p&gt;

&lt;p&gt;Does anyone has a suggestion of how to approach this? The primary goal is to remove C from the signal. I thought I could use B as a way to supervise the learning (semi-supervised?). Is there a technique to do semi-supervision for signal separation? Maybe there is a better option than M-NMF out there?&lt;/p&gt;

&lt;p&gt;I am new to ML so pardon my lack of knowledge in some things. I&amp;#39;ve just thrown myself into the deep end to figure it out. &lt;/p&gt;

&lt;p&gt;Thank you in advance for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g92im9,True,,DinnerTimeBandit,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g92im9/question_signal_separation_separate_three/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g92im9/question_signal_separation_separate_three/,155203,1588000348.0,0,,False,,,,
,learnmachinelearning,Hiii I am using some x feature selection for text classification. I have number of datasets for text classification. How to fix  the number of features in feature selection algorithm for different datasets. Any help would be appreciated?,t2_4xpoa8ie,False,,0,False,[D ]: How to determine the number of features in feature selection algorithm for text classification?,[],r/learnmachinelearning,False,6,,0,,False,t3_g927jw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588028178.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hiii I am using some x feature selection for text classification. I have number of datasets for text classification. How to fix  the number of features in feature selection algorithm for different datasets. Any help would be appreciated?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g927jw,True,,sreenuroyal568,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g927jw/d_how_to_determine_the_number_of_features_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g927jw/d_how_to_determine_the_number_of_features_in/,155203,1587999378.0,0,,False,,,,
,learnmachinelearning,Hiii I am using some x feature selection for text classification. I have number of datasets for text classification. How to fix  the number of features in feature selection algorithm for different datasets. Any help would be appreciated?,t2_4xpoa8ie,False,,0,False,[D]: How to determines the number of features in feature selection algorithm for text classification?,[],r/learnmachinelearning,False,6,,0,,False,t3_g925gc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588027982.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hiii I am using some x feature selection for text classification. I have number of datasets for text classification. How to fix  the number of features in feature selection algorithm for different datasets. Any help would be appreciated?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g925gc,True,,sreenuroyal568,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g925gc/d_how_to_determines_the_number_of_features_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g925gc/d_how_to_determines_the_number_of_features_in/,155203,1587999182.0,0,,False,,,,
,learnmachinelearning,,t2_7dttr,False,,0,False,"Google Drive with 64 Books from Springer about Data Science, Machine Learning, and other Analysis",[],r/learnmachinelearning,False,6,,0,,False,t3_g91xz9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588027301.0,text,6,,,text,self.opendirectories,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g91xz9,True,,dtaivp,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g91xz9/google_drive_with_64_books_from_springer_about/,all_ads,False,/r/opendirectories/comments/g91u12/google_drive_with_64_books_from_springer_about/,155203,1587998501.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'opendirectories', 'selftext': 'This is my drive. I will keep it up until 5/27/2020. DM me if you read this after and want access. All the books are PDF\n\n[https://drive.google.com/drive/folders/1rDJvZsz8EEuVVgZ43pwSvFRRKUo2TIIY?usp=sharing](https://drive.google.com/drive/folders/1rDJvZsz8EEuVVgZ43pwSvFRRKUo2TIIY?usp=sharing)', 'author_fullname': 't2_7dttr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Google Drive with 64 Books from Springer about Data Science, Machine Learning, and other Analysis', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/opendirectories', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'ebook-link', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g91u12', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 98, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'EBooks', 'can_mod_post': False, 'score': 98, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588026942.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.opendirectories', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is my drive. I will keep it up until 5/27/2020. DM me if you read this after and want access. All the books are PDF&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://drive.google.com/drive/folders/1rDJvZsz8EEuVVgZ43pwSvFRRKUo2TIIY?usp=sharing""&gt;https://drive.google.com/drive/folders/1rDJvZsz8EEuVVgZ43pwSvFRRKUo2TIIY?usp=sharing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '128ba476-d6a6-11e1-bc89-12313d28169d', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r1e4', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g91u12', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'dtaivp', 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/opendirectories/comments/g91u12/google_drive_with_64_books_from_springer_about/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/opendirectories/comments/g91u12/google_drive_with_64_books_from_springer_about/', 'subreddit_subscribers': 149503, 'created_utc': 1587998142.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_g91u12,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Diverse Image Synthesis for Multiple Domains,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g91kyl,False,light,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/oCTZJBOJ3cw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Diverse Image Synthesis for Multiple Domains', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/oCTZJBOJ3cw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oCTZJBOJ3cw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/oCTZJBOJ3cw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g91kyl', 'height': 338}",Discussion,False,0,,False,https://b.thumbs.redditmedia.com/IRJmAuivwvrrlcEgw8fAUy4lpEP1De3vI-CdD58De6M.jpg,False,,[],{},rich:video,,False,,1588026116.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Ep3mfdfuAlaVWlgpp9SxiPSWxYNUoe2P7jYOD6Jd678.jpg?auto=webp&amp;s=84a59092f6b16148c9911367159a72cf0cbc791a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Ep3mfdfuAlaVWlgpp9SxiPSWxYNUoe2P7jYOD6Jd678.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=61ea02c72ee398b580cffd9868fcec64f3783020', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Ep3mfdfuAlaVWlgpp9SxiPSWxYNUoe2P7jYOD6Jd678.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3797b6aecf75739eb9e5f69b761bc02d1a9810ef', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Ep3mfdfuAlaVWlgpp9SxiPSWxYNUoe2P7jYOD6Jd678.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a987013eeacfff68b858fa45875560e37d69ffc', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'sKQqGvoDU7QQExBylRlmZ4r76mXZtUtUcThN7seuJpA'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g91kyl,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g91kyl/diverse_image_synthesis_for_multiple_domains/,all_ads,False,https://youtu.be/oCTZJBOJ3cw,155203,1587997316.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Diverse Image Synthesis for Multiple Domains', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/oCTZJBOJ3cw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/oCTZJBOJ3cw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Configuring deep learning frameworks to train on any GPU is hard, given the number of versions of software you need to orchestrate to get it right.

Trying to configure a Docker container to run on top of that is even more difficult! 

I recently had luck with [NVIDIA's docker container toolkit](https://github.com/NVIDIA/nvidia-docker)

&amp;#x200B;

https://preview.redd.it/w2y6q9axcdv41.png?width=612&amp;format=png&amp;auto=webp&amp;s=3077453683310b94d12b471546e14cb9e7a2487d

It is a base docker container that allows you to expose GPUs to the container that have been configured on the local machine. So if nvidia-smi works on the local machine, it will work in the docker container. 

Has anyone else used this? And are there other, more proper ways to configure GPUs on Docker?",t2_64v8pu7s,False,,0,False,NVIDIA Docker Container Toolkit,[],r/learnmachinelearning,False,6,,0,91.0,False,t3_g91j3w,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Jw6B3AwwDROdan6MIgwSbWCOa9CvIoS3FK4uy1ugdCk.jpg,False,,[],{},self,,True,,1588025944.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Configuring deep learning frameworks to train on any GPU is hard, given the number of versions of software you need to orchestrate to get it right.&lt;/p&gt;

&lt;p&gt;Trying to configure a Docker container to run on top of that is even more difficult! &lt;/p&gt;

&lt;p&gt;I recently had luck with &lt;a href=""https://github.com/NVIDIA/nvidia-docker""&gt;NVIDIA&amp;#39;s docker container toolkit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/w2y6q9axcdv41.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3077453683310b94d12b471546e14cb9e7a2487d""&gt;https://preview.redd.it/w2y6q9axcdv41.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3077453683310b94d12b471546e14cb9e7a2487d&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is a base docker container that allows you to expose GPUs to the container that have been configured on the local machine. So if nvidia-smi works on the local machine, it will work in the docker container. &lt;/p&gt;

&lt;p&gt;Has anyone else used this? And are there other, more proper ways to configure GPUs on Docker?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wMIrpOFgZZI3fD58iMnrGavaQ34PRlx4SyJMIlvzf8c.jpg?auto=webp&amp;s=3745ef8897d4ed666af18eeb8291a21e8cc06924', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/wMIrpOFgZZI3fD58iMnrGavaQ34PRlx4SyJMIlvzf8c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=94cee894e18b5951dd477fe35c0c795571f1cbfe', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/wMIrpOFgZZI3fD58iMnrGavaQ34PRlx4SyJMIlvzf8c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=686c58e2ed7bd62c1e58f4eeb56d86f42dc209ef', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/wMIrpOFgZZI3fD58iMnrGavaQ34PRlx4SyJMIlvzf8c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=665ebe1e7327c8c8a20695487ed7ca0a53863d70', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'T7Iz9XNfot2kLaFy8vduiuWaQnAglvUCjiAwrn7lNQQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g91j3w,True,,jacobsolawetz,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g91j3w/nvidia_docker_container_toolkit/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g91j3w/nvidia_docker_container_toolkit/,155203,1587997144.0,0,,False,,,"{'w2y6q9axcdv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 70, 'x': 108, 'u': 'https://preview.redd.it/w2y6q9axcdv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d06c0858c95643db7d7fd9a13359c1801184fd5'}, {'y': 140, 'x': 216, 'u': 'https://preview.redd.it/w2y6q9axcdv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd8a16c94e5dd75a073fe05ac80dfb70a86ffcb5'}, {'y': 208, 'x': 320, 'u': 'https://preview.redd.it/w2y6q9axcdv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd89ae2c61c9c14f6906c49c5754de69a1d68c07'}], 's': {'y': 399, 'x': 612, 'u': 'https://preview.redd.it/w2y6q9axcdv41.png?width=612&amp;format=png&amp;auto=webp&amp;s=3077453683310b94d12b471546e14cb9e7a2487d'}, 'id': 'w2y6q9axcdv41'}}",
,learnmachinelearning,"I'm working on a project which has sequential data set like below.

X\['Event1', 'Event2', 'Event3'\] ---&gt; Y\[C1\]

X\['Event2', 'Event1', 'Event5', 'Event2', 'Event1', 'Event5'\] ---&gt; Y\[C2\]

C1, C2,C3...Cn  are separate families in  the data set. There are no hidden states in each family.

According I have read, it needs train separate HMM model for each family(C1,C2, ...) . An HMM model that is trained on a given family should generate higher scores when tested on event sequence of the same family. On the other hand, event sequence from a different family should yield lower scores.What are the available libraries for train these models?",t2_55v3wni1,False,,0,False,Hidden Markov Model libraries for classification,[],r/learnmachinelearning,False,6,,0,,False,t3_g919gl,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1587996426.0,,[],{},,,True,,1588025010.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project which has sequential data set like below.&lt;/p&gt;

&lt;p&gt;X[&amp;#39;Event1&amp;#39;, &amp;#39;Event2&amp;#39;, &amp;#39;Event3&amp;#39;] ---&amp;gt; Y[C1]&lt;/p&gt;

&lt;p&gt;X[&amp;#39;Event2&amp;#39;, &amp;#39;Event1&amp;#39;, &amp;#39;Event5&amp;#39;, &amp;#39;Event2&amp;#39;, &amp;#39;Event1&amp;#39;, &amp;#39;Event5&amp;#39;] ---&amp;gt; Y[C2]&lt;/p&gt;

&lt;p&gt;C1, C2,C3...Cn  are separate families in  the data set. There are no hidden states in each family.&lt;/p&gt;

&lt;p&gt;According I have read, it needs train separate HMM model for each family(C1,C2, ...) . An HMM model that is trained on a given family should generate higher scores when tested on event sequence of the same family. On the other hand, event sequence from a different family should yield lower scores.What are the available libraries for train these models?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g919gl,True,,icpicanto,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g919gl/hidden_markov_model_libraries_for_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g919gl/hidden_markov_model_libraries_for_classification/,155203,1587996210.0,0,,False,,,,
,learnmachinelearning,"So I trained a YOLOv3 algorithm to detect a custom invoice dataset - dont mind the accuracy - and I want to move on to text recognition of the text within the bounding box however Im not too sure how to go about doing it.

Any help would be great ! 

https://preview.redd.it/kqj1h12c8dv41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=461d45fc2291827960d06f516051f2f2e40a534f",t2_5qphimtw,False,,0,False,How do i extract text from bounding boxes after running the image on YOLOv3?,[],r/learnmachinelearning,False,6,,0,110.0,False,t3_g913dl,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/i7p5UQHFNp4RvCbhz0IH-oDHquQSsqCn3u0QPGEJ3AQ.jpg,False,,[],{},,,True,,1588024420.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I trained a YOLOv3 algorithm to detect a custom invoice dataset - dont mind the accuracy - and I want to move on to text recognition of the text within the bounding box however Im not too sure how to go about doing it.&lt;/p&gt;

&lt;p&gt;Any help would be great ! &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/kqj1h12c8dv41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=461d45fc2291827960d06f516051f2f2e40a534f""&gt;https://preview.redd.it/kqj1h12c8dv41.jpg?width=800&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=461d45fc2291827960d06f516051f2f2e40a534f&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g913dl,True,,dumboNernie,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g913dl/how_do_i_extract_text_from_bounding_boxes_after/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g913dl/how_do_i_extract_text_from_bounding_boxes_after/,155203,1587995620.0,0,,False,,,"{'kqj1h12c8dv41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 85, 'x': 108, 'u': 'https://preview.redd.it/kqj1h12c8dv41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6da36a92d664fdbf5aa91bef0474c70576f896d'}, {'y': 170, 'x': 216, 'u': 'https://preview.redd.it/kqj1h12c8dv41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84c53d2df6764035e7a5cb97ce854d017caa81ec'}, {'y': 252, 'x': 320, 'u': 'https://preview.redd.it/kqj1h12c8dv41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d74b65358631218b28ec95fb58eab0eac8ea584a'}, {'y': 504, 'x': 640, 'u': 'https://preview.redd.it/kqj1h12c8dv41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d86155e76042ee70f42538e3cabaf58d0f7592a4'}], 's': {'y': 630, 'x': 800, 'u': 'https://preview.redd.it/kqj1h12c8dv41.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=461d45fc2291827960d06f516051f2f2e40a534f'}, 'id': 'kqj1h12c8dv41'}}",
,learnmachinelearning,,t2_1wrbw240,False,,0,False,"Books on Data Science, ML and Statistics",[],r/learnmachinelearning,False,6,,0,86.0,False,t3_g8xbyq,False,dark,0.57,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Kh1FjU6Es2dZefSZbnui1W8HCgvdRqKa7hHURhrYb_E.jpg,False,,[],{},link,,False,,1588007562.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rwPhkzvyLJMq-0-_a77PfrIdHC-I721IDVnFa7wUApc.jpg?auto=webp&amp;s=23b0b75217ac80a539b118a812c90c78bdae2aac', 'width': 825, 'height': 510}, 'resolutions': [{'url': 'https://external-preview.redd.it/rwPhkzvyLJMq-0-_a77PfrIdHC-I721IDVnFa7wUApc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53470bc17172b931ef3cd467dd248e8acfe0c30e', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/rwPhkzvyLJMq-0-_a77PfrIdHC-I721IDVnFa7wUApc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0520379b99f2b6e7b0cfd5b1a76617df35066002', 'width': 216, 'height': 133}, {'url': 'https://external-preview.redd.it/rwPhkzvyLJMq-0-_a77PfrIdHC-I721IDVnFa7wUApc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ecce8cf88a315ef5f96bab12be0b95026eea92d', 'width': 320, 'height': 197}, {'url': 'https://external-preview.redd.it/rwPhkzvyLJMq-0-_a77PfrIdHC-I721IDVnFa7wUApc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5efc39763f86e0d75ab19c0b20203eb3a0f00fb5', 'width': 640, 'height': 395}], 'variants': {}, 'id': 'gHsyYLuw69gmUnaLk2AT9scom0rsdMjkOBJt7C0NiiY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8xbyq,True,,lordvader_31,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8xbyq/books_on_data_science_ml_and_statistics/,all_ads,False,https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189,155203,1587978762.0,0,,False,,,,
,learnmachinelearning,"Hey everyone, I completed Andrew Ng Machine Learning Course, and now looking for some practical machine learning courses, taught in Python. Thanks.",t2_47jpmh5m,False,,0,False,Practical Machine Learning Courses Suggestions,[],r/learnmachinelearning,False,6,,0,,False,t3_g901x2,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588020485.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I completed Andrew Ng Machine Learning Course, and now looking for some practical machine learning courses, taught in Python. Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g901x2,True,,ItisAhmad,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g901x2/practical_machine_learning_courses_suggestions/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g901x2/practical_machine_learning_courses_suggestions/,155203,1587991685.0,0,,False,,,,
,learnmachinelearning,,t2_jh4n9,False,,0,False,7 Beautiful Visualizations using a single line of Python code ðŸ”¥| Data Visualization Cheatsheet,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g901tt,False,dark,0.66,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/mEYCCE2YXsRhvqx5br24dEHdQjL4QfAaj1W1Zm4FY5A.jpg,False,,[],{},link,,False,,1588020475.0,text,6,,,text,dropbox.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?auto=webp&amp;s=8844a2f59ff2ffe318ad0e2b7846e1e37aed5128', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=20970d5f7f7c343396aff132ae5ffcee0ce03cb8', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'KMbn3CCPvk6_ukzEK1zzIufQuyyC0aay4raAkv3Slng'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g901tt,True,,vishalpathikgupta,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g901tt/7_beautiful_visualizations_using_a_single_line_of/,all_ads,False,https://www.dropbox.com/s/4hq6sdjgd9wx09p/Data%20Visualization%20Cheatsheet%20by%20Jovian.ml.pdf,155203,1587991675.0,0,,False,,,,
,learnmachinelearning,"Both are on Youtube for free, was wondering which one is better. I have a fairly strong math background and can use python.",t2_16yc3l,False,,0,False,Stanford CS231n vs Cornell CS4780?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8zyeh,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588020073.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Both are on Youtube for free, was wondering which one is better. I have a fairly strong math background and can use python.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8zyeh,True,,aerobic_respiration,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8zyeh/stanford_cs231n_vs_cornell_cs4780/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8zyeh/stanford_cs231n_vs_cornell_cs4780/,155203,1587991273.0,0,,False,,,,
,learnmachinelearning,,t2_2uuc7994,False,,0,False,"Machine Learning with Python 10.5 hours full free course - These are good in case you haven't come across this channel yet. They post entire classes on AI, ML, DL, Data Science, etc. Pretty cool and all for free thankfully.",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g8pke9,False,dark,0.93,,public,12,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RnFGwxJwx-0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning with Python | Machine Learning Tutorial for Beginners | Machine Learning Tutorial', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RnFGwxJwx-0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Great Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/RnFGwxJwx-0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/beaconelearning'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RnFGwxJwx-0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8pke9', 'height': 338}",,False,12,,False,https://b.thumbs.redditmedia.com/WBAb3elPMbONc8HIRSZn4ODG-MbB3LsXeR2dC8BFwkU.jpg,False,,[],{},rich:video,,False,,1587974443.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ud200d7x7OxrlzM0UjKncAlJCG7Gd8VV29xTmjfVcaw.jpg?auto=webp&amp;s=9a61dfdd750d55ca32c3e5b136fc6892a2d27157', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ud200d7x7OxrlzM0UjKncAlJCG7Gd8VV29xTmjfVcaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f55ec3367e8981c2a055c85dbcfaba06f0359d9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ud200d7x7OxrlzM0UjKncAlJCG7Gd8VV29xTmjfVcaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cb9cfdee6240866c66e1f52d4116af427aef4aec', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ud200d7x7OxrlzM0UjKncAlJCG7Gd8VV29xTmjfVcaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cad721f05e4b9a249c79d13de0fcee5dc7e34bc1', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'rUAgaFbUQGH7ELQMKn2KDb8l46y30lQs6K22LcfflHs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8pke9,True,,IngaaP,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8pke9/machine_learning_with_python_105_hours_full_free/,all_ads,False,https://www.youtube.com/watch?v=RnFGwxJwx-0,155203,1587945643.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Machine Learning with Python | Machine Learning Tutorial for Beginners | Machine Learning Tutorial', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/RnFGwxJwx-0?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Great Learning', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/RnFGwxJwx-0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/beaconelearning'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I have a bimodal distribution for the range \[-0.1, 0.1\] which can be viewed here:

&amp;#x200B;

https://preview.redd.it/fdkmcbjincv41.png?width=614&amp;format=png&amp;auto=webp&amp;s=d3eae3a650e992f3fd9f207b8673a0ec9a01a0b1

&amp;#x200B;

I want to train/fit a Kernel Density Estimation (KDE) on the bimodal distribution as shown in the picture and then, given any other distribution say a uniform distribution such as:

&amp;#x200B;

&gt;\# a uniform distribution between the same range \[-0.1, 0.1\]-  
&gt;  
&gt;u\_data = np.random.uniform(low = -0.1, high = 0.1, size = (1782,))

I want to be able to use the trained KDE to 'predict' how many of the data points from the given data distribution (say, 'u\_data') belong to the target bimodal distribution.

&amp;#x200B;

I tried the following code but it doesn't work out:

&amp;#x200B;

&gt;\# Here 'a' is the numpy array containing target bimodal distribution.  
&gt;  
&gt;\# Generate random samples-  
&gt;  
&gt;kde\_samples = {}  
&gt;  
&gt;for kernel in \['tophat', 'gaussian'\]:  
&gt;  
&gt;\# Train a kernel on bimodal data distribution 'a'-  
&gt;  
&gt;kde = KernelDensity(kernel=kernel, bandwidth=0.2).fit(a.reshape(-1, 1))  
&gt;  
&gt;  
&gt;  
&gt;\# Try and generate 300 random samples from trained model-  
&gt;  
&gt;kde\_samples\[kernel\] = np.exp(kde.sample(300))  
&gt;  
&gt;\# Visualize data distribution using histograms-  
&gt;  
&gt;plt.hist(a, bins=20, label = 'original distribution')  
&gt;  
&gt;\# sns.distplot(a, kde = True, bins = 20, label = 'original distribution')  
&gt;  
&gt;plt.hist(kde\_samples\['gaussian'\], bins = 20, label = 'KDE: Gaussian')  
&gt;  
&gt;plt.hist(kde\_samples\['tophat'\], bins = 20, label = 'KDE: tophat')  
&gt;  
&gt;plt.title(""KDE: Data distribution"")  
&gt;  
&gt;plt.xlabel(""weights"")  
&gt;  
&gt;plt.ylabel(""frequency"")  
&gt;  
&gt;plt.legend(loc = 'best')  
&gt;  
&gt;[plt.show](https://plt.show)()

This gives the following visualization:

&amp;#x200B;

https://preview.redd.it/kq3htgtgncv41.png?width=640&amp;format=png&amp;auto=webp&amp;s=593e407885be4a82255e2dadb89df915fe20b6e8

&amp;#x200B;

Two things are wrong:

&amp;#x200B;

1. The range of the generated samples are wrong!
2. The distribution of generated data is NOT bimodal

&amp;#x200B;

How can I therefore: train/fit a Kernel Density Estimation (KDE) on the bimodal distribution and then, given any other distribution (say a uniform or normal distribution) be able to use the trained KDE to 'predict' how many of the data points from the given data distribution belong to the target bimodal distribution.

&amp;#x200B;

I am using Python 3.8 and sklearn 0.22.

&amp;#x200B;

Thanks!",t2_2mmql89p,False,,0,False,Kernel Density Estimation for bimodal distribution with Python,[],r/learnmachinelearning,False,6,,0,109.0,False,t3_g8zawm,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/_5EMfGd3GszyQcBRJNjmRGqKbhwyOEC94osCyfHui3I.jpg,False,,[],{},,,True,,1588017223.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a bimodal distribution for the range [-0.1, 0.1] which can be viewed here:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/fdkmcbjincv41.png?width=614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d3eae3a650e992f3fd9f207b8673a0ec9a01a0b1""&gt;https://preview.redd.it/fdkmcbjincv41.png?width=614&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d3eae3a650e992f3fd9f207b8673a0ec9a01a0b1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I want to train/fit a Kernel Density Estimation (KDE) on the bimodal distribution as shown in the picture and then, given any other distribution say a uniform distribution such as:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;# a uniform distribution between the same range [-0.1, 0.1]-  &lt;/p&gt;

&lt;p&gt;u_data = np.random.uniform(low = -0.1, high = 0.1, size = (1782,))&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I want to be able to use the trained KDE to &amp;#39;predict&amp;#39; how many of the data points from the given data distribution (say, &amp;#39;u_data&amp;#39;) belong to the target bimodal distribution.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I tried the following code but it doesn&amp;#39;t work out:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;# Here &amp;#39;a&amp;#39; is the numpy array containing target bimodal distribution.  &lt;/p&gt;

&lt;p&gt;# Generate random samples-  &lt;/p&gt;

&lt;p&gt;kde_samples = {}  &lt;/p&gt;

&lt;p&gt;for kernel in [&amp;#39;tophat&amp;#39;, &amp;#39;gaussian&amp;#39;]:  &lt;/p&gt;

&lt;p&gt;# Train a kernel on bimodal data distribution &amp;#39;a&amp;#39;-  &lt;/p&gt;

&lt;p&gt;kde = KernelDensity(kernel=kernel, bandwidth=0.2).fit(a.reshape(-1, 1))  &lt;/p&gt;

&lt;p&gt;# Try and generate 300 random samples from trained model-  &lt;/p&gt;

&lt;p&gt;kde_samples[kernel] = np.exp(kde.sample(300))  &lt;/p&gt;

&lt;p&gt;# Visualize data distribution using histograms-  &lt;/p&gt;

&lt;p&gt;plt.hist(a, bins=20, label = &amp;#39;original distribution&amp;#39;)  &lt;/p&gt;

&lt;p&gt;# sns.distplot(a, kde = True, bins = 20, label = &amp;#39;original distribution&amp;#39;)  &lt;/p&gt;

&lt;p&gt;plt.hist(kde_samples[&amp;#39;gaussian&amp;#39;], bins = 20, label = &amp;#39;KDE: Gaussian&amp;#39;)  &lt;/p&gt;

&lt;p&gt;plt.hist(kde_samples[&amp;#39;tophat&amp;#39;], bins = 20, label = &amp;#39;KDE: tophat&amp;#39;)  &lt;/p&gt;

&lt;p&gt;plt.title(&amp;quot;KDE: Data distribution&amp;quot;)  &lt;/p&gt;

&lt;p&gt;plt.xlabel(&amp;quot;weights&amp;quot;)  &lt;/p&gt;

&lt;p&gt;plt.ylabel(&amp;quot;frequency&amp;quot;)  &lt;/p&gt;

&lt;p&gt;plt.legend(loc = &amp;#39;best&amp;#39;)  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://plt.show""&gt;plt.show&lt;/a&gt;()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This gives the following visualization:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/kq3htgtgncv41.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593e407885be4a82255e2dadb89df915fe20b6e8""&gt;https://preview.redd.it/kq3htgtgncv41.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593e407885be4a82255e2dadb89df915fe20b6e8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Two things are wrong:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The range of the generated samples are wrong!&lt;/li&gt;
&lt;li&gt;The distribution of generated data is NOT bimodal&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;How can I therefore: train/fit a Kernel Density Estimation (KDE) on the bimodal distribution and then, given any other distribution (say a uniform or normal distribution) be able to use the trained KDE to &amp;#39;predict&amp;#39; how many of the data points from the given data distribution belong to the target bimodal distribution.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am using Python 3.8 and sklearn 0.22.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8zawm,True,,grid_world,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8zawm/kernel_density_estimation_for_bimodal/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8zawm/kernel_density_estimation_for_bimodal/,155203,1587988423.0,0,,False,,,"{'kq3htgtgncv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 81, 'x': 108, 'u': 'https://preview.redd.it/kq3htgtgncv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=759a9c0435263f51a50af18da86b2ce7f0ced6f3'}, {'y': 162, 'x': 216, 'u': 'https://preview.redd.it/kq3htgtgncv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aadcac28d0423e556ff523fd44fa756aa740f794'}, {'y': 240, 'x': 320, 'u': 'https://preview.redd.it/kq3htgtgncv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=becec2823896385c80657a61579807617c83df07'}, {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/kq3htgtgncv41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed61026a06332e5908ad54c5c7d06b83761c2d00'}], 's': {'y': 480, 'x': 640, 'u': 'https://preview.redd.it/kq3htgtgncv41.png?width=640&amp;format=png&amp;auto=webp&amp;s=593e407885be4a82255e2dadb89df915fe20b6e8'}, 'id': 'kq3htgtgncv41'}, 'fdkmcbjincv41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 84, 'x': 108, 'u': 'https://preview.redd.it/fdkmcbjincv41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=15a4c0e8bf6b3326a736719959079dc7876eca2d'}, {'y': 168, 'x': 216, 'u': 'https://preview.redd.it/fdkmcbjincv41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a2ba42e240f808f9107c1f8125da7b6aab46fca0'}, {'y': 250, 'x': 320, 'u': 'https://preview.redd.it/fdkmcbjincv41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a8c2fd39aed9c74dc95a672e96f55b7b847b358'}], 's': {'y': 480, 'x': 614, 'u': 'https://preview.redd.it/fdkmcbjincv41.png?width=614&amp;format=png&amp;auto=webp&amp;s=d3eae3a650e992f3fd9f207b8673a0ec9a01a0b1'}, 'id': 'fdkmcbjincv41'}}",
,learnmachinelearning,What do you recommend?,t2_axav08j,False,,0,False,How to scale values for LSTM networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8y8bm,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588011921.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What do you recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8y8bm,True,,everek123,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8y8bm/how_to_scale_values_for_lstm_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8y8bm/how_to_scale_values_for_lstm_networks/,155203,1587983121.0,0,,False,,,,
,learnmachinelearning,"So I'm in my final year of computer science at uni and I've had a good career in web development so far having made software for startups, an elite football club and a government. However, I can't say that my degree has helped me in that industry in terms of my practical skills. I feel like it would benefit network engineers, embedded systems developers etc. a lot more.

So my question to you is this, has your formal education for machine learning helped you in a practical scenario such as a job or coding a ML model. Just for clarification, I'm not asking whether your education in ML helped you land a job, rather I am asking you whether your education in ML has helped you on the job. 

I would prefer engineers that have had a formal education in ML to answer this one since they've had the experience with the education system. For self taught people, feel free to contribute your learning materials here if you truly found them valuable. The reason I ask is because I'm debating whether I should slowly self learn or  take a master's in machine learning at some point before I'm 25.",t2_52tw4wtc,False,,0,False,Professional AI/ML engineers... Was your education in machine learning/artificial intelligence useful?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8s67t,False,light,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,self,False,,[],{},,,True,,1587984525.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I&amp;#39;m in my final year of computer science at uni and I&amp;#39;ve had a good career in web development so far having made software for startups, an elite football club and a government. However, I can&amp;#39;t say that my degree has helped me in that industry in terms of my practical skills. I feel like it would benefit network engineers, embedded systems developers etc. a lot more.&lt;/p&gt;

&lt;p&gt;So my question to you is this, has your formal education for machine learning helped you in a practical scenario such as a job or coding a ML model. Just for clarification, I&amp;#39;m not asking whether your education in ML helped you land a job, rather I am asking you whether your education in ML has helped you on the job. &lt;/p&gt;

&lt;p&gt;I would prefer engineers that have had a formal education in ML to answer this one since they&amp;#39;ve had the experience with the education system. For self taught people, feel free to contribute your learning materials here if you truly found them valuable. The reason I ask is because I&amp;#39;m debating whether I should slowly self learn or  take a master&amp;#39;s in machine learning at some point before I&amp;#39;m 25.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8s67t,True,,sonjpaul,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8s67t/professional_aiml_engineers_was_your_education_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8s67t/professional_aiml_engineers_was_your_education_in/,155203,1587955725.0,0,,False,,,,
,learnmachinelearning,"There are many different ways to use Keras.

Some people define their models using Sequential, others use Model and Input. Some people write the whole thing as a single script, others use functions to define their models, and others still use classes.

**What are your best practice recommendations for building a model in keras?**",t2_lt2bnkg,False,,0,False,Keras: Best Practice discussion,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8xvaq,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588010165.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There are many different ways to use Keras.&lt;/p&gt;

&lt;p&gt;Some people define their models using Sequential, others use Model and Input. Some people write the whole thing as a single script, others use functions to define their models, and others still use classes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are your best practice recommendations for building a model in keras?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8xvaq,True,,DeadliestToast,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8xvaq/keras_best_practice_discussion/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8xvaq/keras_best_practice_discussion/,155203,1587981365.0,0,,False,,,,
,learnmachinelearning,,t2_xdyq1,False,,0,False,An introduction to Swift for TensorFlow,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g8xi4e,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/cMLy2p5IImCJqZpwvr5GytspH2GNgEvXzbY_V3JKK2Y.jpg,False,,[],{},link,,False,,1588008374.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?auto=webp&amp;s=29924f166f504ec9108d2f27319ff29f76572a6f', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e904540081e7161ddfc2ef89a145e514f2843bb5', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6731c08bdee041a02870cad975201e21959dcade', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e6e0d951b9f9f9df0b00d436bbefffd9a3ce9d3', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47af63c8663fb3b3dba6e28377636210cccea2eb', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=318bcf11e1a8aae2917f4ea1ea28a64d31d5a528', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/zb4R899AeCwJIKzswULWjAhcTZSMYpPC6JR1utwSKGc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de7a0a61df6237b45f477db11ed2d7b93fcc8acd', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '-zj0is7PZekUrv6O29g8ful58kqnu3mXBMXtiUF-OKU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8xi4e,True,,dev_esh,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8xi4e/an_introduction_to_swift_for_tensorflow/,all_ads,False,https://youtu.be/3ORBRESf0Og,155203,1587979574.0,0,,False,,,,
,learnmachinelearning,"Hello, 

Hope you all are doing good!

Could you please suggest/recommend to guide me through improving my coding skills in python for ML.",t2_3v6pyvi2,False,,0,False,Any recommendations for improving coding skills for ML,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8wqhz,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1588004787.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, &lt;/p&gt;

&lt;p&gt;Hope you all are doing good!&lt;/p&gt;

&lt;p&gt;Could you please suggest/recommend to guide me through improving my coding skills in python for ML.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8wqhz,True,,bubdi,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8wqhz/any_recommendations_for_improving_coding_skills/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8wqhz/any_recommendations_for_improving_coding_skills/,155203,1587975987.0,0,,False,,,,
,learnmachinelearning,"I am trying to use UNET to do multiclass image segmentation, I have 4 classes, the data I have is not annotated exhaustively. What I mean is, the background class may actually belong to either of the classes, ie the background class is strictly not background, but the rest of the classes are strict. Is it possible to avoid backpropagating through background class but only backprop for the rest? Will it work?",t2_wnq7j,False,,0,False,"Multiclass image segmentation, backprop on selected classes?",[],r/learnmachinelearning,False,6,,0,,False,t3_g8weaa,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588003264.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to use UNET to do multiclass image segmentation, I have 4 classes, the data I have is not annotated exhaustively. What I mean is, the background class may actually belong to either of the classes, ie the background class is strictly not background, but the rest of the classes are strict. Is it possible to avoid backpropagating through background class but only backprop for the rest? Will it work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8weaa,True,,shay6992,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8weaa/multiclass_image_segmentation_backprop_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8weaa/multiclass_image_segmentation_backprop_on/,155203,1587974464.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,Collaborative Filtering with Machine Learning and Python,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g8vzj0,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/WWb5KSgzYPyjvfaVufeD0CztyGjhLdh8aY9MICz4NzQ.jpg,False,,[],{},link,,False,,1588001314.0,text,6,,,text,rubikscode.net,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?auto=webp&amp;s=60fa81ddfa6292cb4cf0f8e398c5cdf882589cf1', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71b7bc2162b502a339326374771909b6189170a9', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1109ee182bf4d5b87d21e1939c819a4773772b9f', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=05bc3043f1a9b673cd48da164462d9ef3d5e5abf', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9626857438f407ae5f69b16dfc908a7d679c8560', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=145b34f9c6b92955bb5cabcd2e6b2e427ab61738', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/zRlpMFJPaOGK7JDvQRDhbYuZ_eAQcrIQ8RGQH_XDT7M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5307bceac2e81f5d9c0083ff06fcddcf4517ef25', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'FsuvUUV649yhmLxkeliMRrWOhWm_FC-rnLHy0DC6nZA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8vzj0,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8vzj0/collaborative_filtering_with_machine_learning_and/,all_ads,False,https://rubikscode.net/2020/04/27/collaborative-filtering-with-machine-learning-and-python/,155203,1587972514.0,0,,False,,,,
,learnmachinelearning,"Hi Guys. I'm new to machine learning, but I am going to be required to learn it for a project at school. I  am going to use a  [dataset](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/bot_iot.php) in CSV format which has 46 attributes. Right now I am supposed to rank them, which are the best attributes. 

I'm not sure how to do this manually, but I know that Weka (version 3.8.4) has a ranking system. But the InfoGainAttributeEval is greyed out for me. I tried to use python to rank the attributes ([with this code](https://pastebin.com/LAnNfeyY)), but the error was that there is not enough memory. Are there any online cloud services that I could use for this? 

`pandas.errors.ParserError: Error tokenizing data. C error: out of memory`

I'm a bit stuck on moving forward, I never did a ML class in school. Hope someone is able to give me some pointer on where to go.",t2_f866i,False,,0,False,"Trying to rank attributes using Weka,","[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8vaay,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},self,,True,,1587998072.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Guys. I&amp;#39;m new to machine learning, but I am going to be required to learn it for a project at school. I  am going to use a  &lt;a href=""https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/bot_iot.php""&gt;dataset&lt;/a&gt; in CSV format which has 46 attributes. Right now I am supposed to rank them, which are the best attributes. &lt;/p&gt;

&lt;p&gt;I&amp;#39;m not sure how to do this manually, but I know that Weka (version 3.8.4) has a ranking system. But the InfoGainAttributeEval is greyed out for me. I tried to use python to rank the attributes (&lt;a href=""https://pastebin.com/LAnNfeyY""&gt;with this code&lt;/a&gt;), but the error was that there is not enough memory. Are there any online cloud services that I could use for this? &lt;/p&gt;

&lt;p&gt;&lt;code&gt;pandas.errors.ParserError: Error tokenizing data. C error: out of memory&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m a bit stuck on moving forward, I never did a ML class in school. Hope someone is able to give me some pointer on where to go.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?auto=webp&amp;s=91de23ca1c5b14e5cd8d2bc84e5bf80e376d123e', 'width': 1343, 'height': 810}, 'resolutions': [{'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d202ff7da38c07ac05489a6a899274225409b194', 'width': 108, 'height': 65}, {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=480fef3a75c531b4eb11da716313c4d59575d72d', 'width': 216, 'height': 130}, {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e74fcd7f580dc7ea315224abb58a240694602d9', 'width': 320, 'height': 193}, {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1fbb87bd11acc453d8b53a66a58ef27e661fe0c', 'width': 640, 'height': 386}, {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e159e17bb712c5308832a2ca7437c349af66130', 'width': 960, 'height': 579}, {'url': 'https://external-preview.redd.it/ym_f1rmd50GgjYMrvucpWiDCkQilA1JShONFtR7LSzA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ade93c0fa5af42b743447645cf0072e53c06a60', 'width': 1080, 'height': 651}], 'variants': {}, 'id': 'Vlun1xoZ7o_oP-wSSG-X294lP6SkMgCVMWiTMDacJVM'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g8vaay,True,,I-Made-You-Read-This,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8vaay/trying_to_rank_attributes_using_weka/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8vaay/trying_to_rank_attributes_using_weka/,155203,1587969272.0,0,,False,,,,
,learnmachinelearning,"I have an elementary background in Java (made a couple games using Processing) and MATLAB (did well in a college course), however I could teach myself whatever language(s) is necessary to do the most effective work. I'm interested in first learning about the various models (if that's the right term) like neural networks, then about frameworks and concepts. I see that Humble Bundle has a set of books as part of a book bundle right now, would this be a good place to start?

[https://www.humblebundle.com/books/artificial-intelligence-machine-learning-morgan-claypool-books?hmb\_source=navbar&amp;hmb\_medium=product\_tile&amp;hmb\_campaign=tile\_index\_8](https://www.humblebundle.com/books/artificial-intelligence-machine-learning-morgan-claypool-books?hmb_source=navbar&amp;hmb_medium=product_tile&amp;hmb_campaign=tile_index_8)",t2_xteas,False,,0,False,"I'm a bioprocess engineer / cell biologist interested in learning / creating machine learning algorithms, where do I start?",[],r/learnmachinelearning,False,6,,0,,False,t3_g8uols,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587995276.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an elementary background in Java (made a couple games using Processing) and MATLAB (did well in a college course), however I could teach myself whatever language(s) is necessary to do the most effective work. I&amp;#39;m interested in first learning about the various models (if that&amp;#39;s the right term) like neural networks, then about frameworks and concepts. I see that Humble Bundle has a set of books as part of a book bundle right now, would this be a good place to start?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.humblebundle.com/books/artificial-intelligence-machine-learning-morgan-claypool-books?hmb_source=navbar&amp;amp;hmb_medium=product_tile&amp;amp;hmb_campaign=tile_index_8""&gt;https://www.humblebundle.com/books/artificial-intelligence-machine-learning-morgan-claypool-books?hmb_source=navbar&amp;amp;hmb_medium=product_tile&amp;amp;hmb_campaign=tile_index_8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?auto=webp&amp;s=4dc3f23996637726e8acb1491bdbf1b93672343d', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eca58b2cebda44c2b49a266fd4b46054e996fba6', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fa5ed03d1c94bf5b842dd8d3feb9d2856f992c8', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5dc5472923c5035e1e2d6a731bb302d76c229ae', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6789fd503f499a53b1375466fde5858c50020c21', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c65b2e182c3b1e9b71539cc993b3204f8b50a3a', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/KiUnyNi4N1DEMB-RrZ6heZF-4ZFmZEjYyeKRTiA461U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57749c2487572d457109f953ba5f2c8cc4b8fd26', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'yYm4XetO8TqH_t9ze3N6LBb-uJGNJql353hPhc4u8Jc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8uols,True,,Saamic,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8uols/im_a_bioprocess_engineer_cell_biologist/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8uols/im_a_bioprocess_engineer_cell_biologist/,155203,1587966476.0,0,,False,,,,True
,learnmachinelearning," 

Hey Reddit! I'm a current high school junior student, who's both really interested in the fields of Machine/Deep Learning and Economics. Economics was my long favorite subject since I was a freshmen, but from this winter I've been learning Machine Learning and getting more fascinated about it day by day.

I have built some regression and classification models using ANNs and CNNs, and now I'm reading some of the more recent models and techniques that are being developed. Of course they aren't so easy, but I have been truly impressed of the speed how this technology is improving.

My question is, that I would like to use Machine Learning to do something related with the economy in general (besides californian housing prices). Are there any articles or books that I should read on? Some potential research topics?

Thanks!",t2_gyyjrs0,False,,0,False,ML and Economics,[],r/learnmachinelearning,False,6,,0,,False,t3_g8um9u,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587994949.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Reddit! I&amp;#39;m a current high school junior student, who&amp;#39;s both really interested in the fields of Machine/Deep Learning and Economics. Economics was my long favorite subject since I was a freshmen, but from this winter I&amp;#39;ve been learning Machine Learning and getting more fascinated about it day by day.&lt;/p&gt;

&lt;p&gt;I have built some regression and classification models using ANNs and CNNs, and now I&amp;#39;m reading some of the more recent models and techniques that are being developed. Of course they aren&amp;#39;t so easy, but I have been truly impressed of the speed how this technology is improving.&lt;/p&gt;

&lt;p&gt;My question is, that I would like to use Machine Learning to do something related with the economy in general (besides californian housing prices). Are there any articles or books that I should read on? Some potential research topics?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8um9u,True,,sunpsa77,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8um9u/ml_and_economics/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8um9u/ml_and_economics/,155203,1587966149.0,0,,False,,,,
,learnmachinelearning,"So I recently discovered this udacity course on TF 2.0 which I thought was very neatly designed, well placed and very informative, the way they ask the audience questions and in very short under 10 min videos they cover some pretty huge topics, for the sake of simplicity. It a free course.  


If you have finished this course what did you guys think about it and what other courses would you recommend to newbies(I help college students like me who are new to machine learning get a hands-on)",t2_1mgycdg5,False,,0,False,Udacity Course on Tensorflow,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8lzjc,False,light,1.0,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,self,False,,[],{},,,True,,1587962399.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I recently discovered this udacity course on TF 2.0 which I thought was very neatly designed, well placed and very informative, the way they ask the audience questions and in very short under 10 min videos they cover some pretty huge topics, for the sake of simplicity. It a free course.  &lt;/p&gt;

&lt;p&gt;If you have finished this course what did you guys think about it and what other courses would you recommend to newbies(I help college students like me who are new to machine learning get a hands-on)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8lzjc,True,,Quantamphysx,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8lzjc/udacity_course_on_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8lzjc/udacity_course_on_tensorflow/,155203,1587933599.0,1,,False,,,,
,learnmachinelearning," Just started to learn about machine learning. I built my own Neural Network in C# from scratch, and built a project where cars ""learn"" to drive around a circuit (with Unity).

It works well now, but I am looking for a new challenge, if possible more interesting and applicable in real life :)

What project ideas could you recommend ? something for UNSUPERVISED learning.

Cheers guys",t2_4r6xja6v,False,,0,False,Project ideas for unsupervised learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g8o40l,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587969428.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Just started to learn about machine learning. I built my own Neural Network in C# from scratch, and built a project where cars &amp;quot;learn&amp;quot; to drive around a circuit (with Unity).&lt;/p&gt;

&lt;p&gt;It works well now, but I am looking for a new challenge, if possible more interesting and applicable in real life :)&lt;/p&gt;

&lt;p&gt;What project ideas could you recommend ? something for UNSUPERVISED learning.&lt;/p&gt;

&lt;p&gt;Cheers guys&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8o40l,True,,carndacier,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8o40l/project_ideas_for_unsupervised_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8o40l/project_ideas_for_unsupervised_learning/,155203,1587940628.0,0,,False,,,,
,learnmachinelearning,"Iâ€™m a complete beginner who has never learned about ML. The only thing I really know about machine learning is that it requires data for categorisation... I think?
My end goal is to be able to build recommendation systems such as the one on YouTube.
I know a decent amount of JavaScript, though Iâ€™m aware JavaScript isnâ€™t suitable for ML.

Do you have any resources and/or advice?

Many thanks",t2_20ufw8os,False,,0,False,Learning ML for the very first time,[],r/learnmachinelearning,False,6,,0,,False,t3_g8pgqf,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587974080.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m a complete beginner who has never learned about ML. The only thing I really know about machine learning is that it requires data for categorisation... I think?
My end goal is to be able to build recommendation systems such as the one on YouTube.
I know a decent amount of JavaScript, though Iâ€™m aware JavaScript isnâ€™t suitable for ML.&lt;/p&gt;

&lt;p&gt;Do you have any resources and/or advice?&lt;/p&gt;

&lt;p&gt;Many thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8pgqf,True,,7zQwerty,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8pgqf/learning_ml_for_the_very_first_time/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8pgqf/learning_ml_for_the_very_first_time/,155203,1587945280.0,0,,False,,,,
,learnmachinelearning,,t2_3q1ag9u6,False,,0,False,Online Learning for Latent Dirichlet Allocation - ppt download,[],r/learnmachinelearning,False,6,,0,,False,t3_g8sm5j,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1587986342.0,text,6,,,text,slideplayer.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8sm5j,True,,ducvu2203,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8sm5j/online_learning_for_latent_dirichlet_allocation/,all_ads,False,https://slideplayer.com/slide/7478588/#.XqZPDGpD518.reddit,155203,1587957542.0,0,,False,,,,
,learnmachinelearning,"Hi, I'm training an LSTM network to classify values on a trigonometric curve as either turning point or not. I trained my network through 100 epochs  (each takes 400 seconds). I know that this model can still be improved (it didn't perform too well) but I have no idea how to tweak the parameters (I don't want to do it randomly and wait a day for the net to train just to see a drop in accuracy). Do you have any suggestions?

Thanks :)",t2_axav08j,False,,0,False,I trained my net. It didn't perform too well. How do I know which hyperparameters to tune?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8jlph,False,light,0.86,,public,5,0,{},,,False,[],,False,False,,{},HELP,False,5,,False,self,False,,[],{},,,True,,1587954681.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m training an LSTM network to classify values on a trigonometric curve as either turning point or not. I trained my network through 100 epochs  (each takes 400 seconds). I know that this model can still be improved (it didn&amp;#39;t perform too well) but I have no idea how to tweak the parameters (I don&amp;#39;t want to do it randomly and wait a day for the net to train just to see a drop in accuracy). Do you have any suggestions?&lt;/p&gt;

&lt;p&gt;Thanks :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g8jlph,True,,everek123,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8jlph/i_trained_my_net_it_didnt_perform_too_well_how_do/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8jlph/i_trained_my_net_it_didnt_perform_too_well_how_do/,155203,1587925881.0,0,,False,,,,
,learnmachinelearning,"I've been struggling with this problem for a while, one of the models I'm training is learning to replace a PID loop to control the steering of a car based on a few things: `[desired_angle, desired_rate, current_angle, current_rate, speed]` and its output is the torque to the steering wheel to reach the desired angle from the current angle. Desired angle is the angle from another model to keep me in my lane which works with a PID or LQR controller, desired rate is the desired angle rate per seconds, current angle, rate and speed should be self explanatory.

The problem I'm facing is that if a model is learning to control something in the physical world, and its output directly influences one of the inputs, it falls into a feedback loop and its predictions are horrible. For example, say it initializes on a curve with a straight steering wheel. It knows it must apply some torque to take the curve since the desired angle is raising slowly. It outputs some torque value, then the current angle rate changes, then it knows from its training when the current rate is high, it was always applying torque in training, so it continues to apply even more torque and sends me off the road if I don't take over.

So I guess my question is, how do you give a model additional information about the state of the object its controlling when the output directly affects one of the inputs without it learning to fall into feedback loops all the time? Loss is actually low, since it's just learning to change torque based on current rate instead of desired angle or desired rate. Should I just remove angle rate so it only knows the angle which contains no information about the past? Or is more data to overcome this false learning the fix?",t2_zlmpx,False,,0,False,How do you train a model where its output directly affects one of the inputs?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8m5g1,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587962955.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been struggling with this problem for a while, one of the models I&amp;#39;m training is learning to replace a PID loop to control the steering of a car based on a few things: &lt;code&gt;[desired_angle, desired_rate, current_angle, current_rate, speed]&lt;/code&gt; and its output is the torque to the steering wheel to reach the desired angle from the current angle. Desired angle is the angle from another model to keep me in my lane which works with a PID or LQR controller, desired rate is the desired angle rate per seconds, current angle, rate and speed should be self explanatory.&lt;/p&gt;

&lt;p&gt;The problem I&amp;#39;m facing is that if a model is learning to control something in the physical world, and its output directly influences one of the inputs, it falls into a feedback loop and its predictions are horrible. For example, say it initializes on a curve with a straight steering wheel. It knows it must apply some torque to take the curve since the desired angle is raising slowly. It outputs some torque value, then the current angle rate changes, then it knows from its training when the current rate is high, it was always applying torque in training, so it continues to apply even more torque and sends me off the road if I don&amp;#39;t take over.&lt;/p&gt;

&lt;p&gt;So I guess my question is, how do you give a model additional information about the state of the object its controlling when the output directly affects one of the inputs without it learning to fall into feedback loops all the time? Loss is actually low, since it&amp;#39;s just learning to change torque based on current rate instead of desired angle or desired rate. Should I just remove angle rate so it only knows the angle which contains no information about the past? Or is more data to overcome this false learning the fix?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8m5g1,True,,ShaneSmiskol,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8m5g1/how_do_you_train_a_model_where_its_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8m5g1/how_do_you_train_a_model_where_its_output/,155203,1587934155.0,1,,False,,,,
,learnmachinelearning,"seed size is about 2mm, is it very challenging?  or is there a good model to complete the task?
I have 3664 * 2748 good quality images(10MP)",t2_z39i9,False,,0,False,how to do small seed classification?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8reiz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1587959420.0,,[],{},,,True,,1587981452.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;seed size is about 2mm, is it very challenging?  or is there a good model to complete the task?
I have 3664 * 2748 good quality images(10MP)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8reiz,True,,googcheng,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8reiz/how_to_do_small_seed_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8reiz/how_to_do_small_seed_classification/,155203,1587952652.0,0,,False,,,,
,learnmachinelearning,"
I have a trained ml model (random forest) but I want to go the other way around. I want to find a set of features that correspond to a given goal.
What type of machine learning model is fit best for such task? I was thinking of some sort of reinforcement learning (q-learning) to find all combinations.",t2_mue41,False,,0,False,Generate features from a given goal and a trained ml model,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8r7cy,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587980660.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a trained ml model (random forest) but I want to go the other way around. I want to find a set of features that correspond to a given goal.
What type of machine learning model is fit best for such task? I was thinking of some sort of reinforcement learning (q-learning) to find all combinations.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8r7cy,True,,Darxploit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8r7cy/generate_features_from_a_given_goal_and_a_trained/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8r7cy/generate_features_from_a_given_goal_and_a_trained/,155203,1587951860.0,0,,False,,,,
,learnmachinelearning,"X^(T)y - X^(T)XB = 0

Wouldn't the X^(T) cancel out?

https://preview.redd.it/yyv5imw6c9v41.png?width=1160&amp;format=png&amp;auto=webp&amp;s=4ab7523d5ba2ad5be420f7bf0560c144e23c42a0",t2_tc4ow,False,,0,False,Learning Least Squares in the EOML book. Why don't the X transpose matrices cancel each other out?,[],r/learnmachinelearning,False,6,,0,73.0,False,t3_g8q9l5,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/2HgJkFHMHqDooRf168gZlCmZnqkozbM2WMCSqnZwrT8.jpg,False,,[],{},,,True,,1587976978.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;X&lt;sup&gt;T&lt;/sup&gt;y - X&lt;sup&gt;T&lt;/sup&gt;XB = 0&lt;/p&gt;

&lt;p&gt;Wouldn&amp;#39;t the X&lt;sup&gt;T&lt;/sup&gt; cancel out?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/yyv5imw6c9v41.png?width=1160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ab7523d5ba2ad5be420f7bf0560c144e23c42a0""&gt;https://preview.redd.it/yyv5imw6c9v41.png?width=1160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ab7523d5ba2ad5be420f7bf0560c144e23c42a0&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8q9l5,True,,whatif2187,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8q9l5/learning_least_squares_in_the_eoml_book_why_dont/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8q9l5/learning_least_squares_in_the_eoml_book_why_dont/,155203,1587948178.0,0,,False,,,"{'yyv5imw6c9v41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 56, 'x': 108, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a1767de013e92e89ae9d471331d147b9f1aade2'}, {'y': 113, 'x': 216, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=edaa1aa1568a9bef55eff1ae2d7c24e61dbb254f'}, {'y': 168, 'x': 320, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ad8cd6711efc1dec0e71568920184d0cb3ad5cc'}, {'y': 336, 'x': 640, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fce55f73fff2c7e08ea621ff0a3af1879db87132'}, {'y': 504, 'x': 960, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c0e741374e8293e36fa7dabc12b212dbabe0f82'}, {'y': 567, 'x': 1080, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7dc6186e90c1f0c0497f05e911016bdf539e9677'}], 's': {'y': 610, 'x': 1160, 'u': 'https://preview.redd.it/yyv5imw6c9v41.png?width=1160&amp;format=png&amp;auto=webp&amp;s=4ab7523d5ba2ad5be420f7bf0560c144e23c42a0'}, 'id': 'yyv5imw6c9v41'}}",
,learnmachinelearning,"  
Recently i've been reading some papers on paperswithcode and i quickly realised that i have a critically low level of knowledge in machine learning field except some basic working principles  


Where can i get more knowledge on machine learning-related topics, like math behind it, different techniques, for a person who is just finishing his school education?",t2_2ixta5gv,False,,0,False,Source of knowledge for learning machine learning,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8iq0e,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1587951889.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recently i&amp;#39;ve been reading some papers on paperswithcode and i quickly realised that i have a critically low level of knowledge in machine learning field except some basic working principles  &lt;/p&gt;

&lt;p&gt;Where can i get more knowledge on machine learning-related topics, like math behind it, different techniques, for a person who is just finishing his school education?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8iq0e,True,,Grubzer,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8iq0e/source_of_knowledge_for_learning_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8iq0e/source_of_knowledge_for_learning_machine_learning/,155203,1587923089.0,0,,False,,,,
,learnmachinelearning,My style transfer implementation is creating [almost static](https://imgur.com/a/Ue4PRxS). Any ideas on what the problem could be?,t2_4ru495iw,False,,0,False,Style Transfer generating static,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8olra,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1587971124.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My style transfer implementation is creating &lt;a href=""https://imgur.com/a/Ue4PRxS""&gt;almost static&lt;/a&gt;. Any ideas on what the problem could be?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Cpu8DHzsHZwY3xqSRMYFIaPcpkz_2I_k_NI4wpTWFok.jpg?auto=webp&amp;s=62efdba5b02a11069fb76b4cbe97ba12545796ab', 'width': 165, 'height': 240}, 'resolutions': [{'url': 'https://external-preview.redd.it/Cpu8DHzsHZwY3xqSRMYFIaPcpkz_2I_k_NI4wpTWFok.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8f9d1757d8c46b2d4d0a034a2452d99a96a817c', 'width': 108, 'height': 157}], 'variants': {}, 'id': '7SjORvDlyLfiiA61ZkCWNP3F6BzDUEo4Qg30Reeb6ZE'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8olra,True,,cereal_final,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8olra/style_transfer_generating_static/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8olra/style_transfer_generating_static/,155203,1587942324.0,0,,False,,,,
,learnmachinelearning,"There are many use cases where identifying variable importance in predicting or classifying is more impactful than creating an accurate, precise, and scalable model. Which techniques do you recommend are best? Bonus points for ease of explaining the results to non-technical audiences.

Example question: Which variables predict a user to purchase on a website? In which direction do those variables increase/decrease likelihood?",t2_4mimz4r9,False,,0,False,"Which techniques are best to understand the importance of variables for prediction/classification, where interpretability is more important than model accuracy?",[],r/learnmachinelearning,False,6,,0,,False,t3_g8oj3c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587970869.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;There are many use cases where identifying variable importance in predicting or classifying is more impactful than creating an accurate, precise, and scalable model. Which techniques do you recommend are best? Bonus points for ease of explaining the results to non-technical audiences.&lt;/p&gt;

&lt;p&gt;Example question: Which variables predict a user to purchase on a website? In which direction do those variables increase/decrease likelihood?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8oj3c,True,,oishiiokonomiyaki,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8oj3c/which_techniques_are_best_to_understand_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8oj3c/which_techniques_are_best_to_understand_the/,155203,1587942069.0,0,,False,,,,
,learnmachinelearning," I have a question regarding publishing a paper. If anyone is working on some models for a specific dataset using transfer learning. What if the person gets highest accuracy on SOTA dataset such as Imagenet or Stanford Cars etc., then this result can be published or not? Or it can be just posted as a post on website such as Medium etc? Because the person has not done anything novel rather than experimenting with different models and getting highest accuracy as compared to all other works.",t2_2sjg338q,False,,0,False,A newbie question regarding paper publishing based on highest SOTA accuracy,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8hgkj,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1587947794.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a question regarding publishing a paper. If anyone is working on some models for a specific dataset using transfer learning. What if the person gets highest accuracy on SOTA dataset such as Imagenet or Stanford Cars etc., then this result can be published or not? Or it can be just posted as a post on website such as Medium etc? Because the person has not done anything novel rather than experimenting with different models and getting highest accuracy as compared to all other works.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8hgkj,True,,awaiss113,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8hgkj/a_newbie_question_regarding_paper_publishing/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8hgkj/a_newbie_question_regarding_paper_publishing/,155203,1587918994.0,0,,False,,,,
,learnmachinelearning,"Hey there,

I've been using AWS SageMaker a lot as a dev machine for ML work and it's very annoying how it removes conda environments after the machine is turned off.

I've created this short guide (3min really) to save conda environments between restarts! [https://biasandvariance.com/sagemaker-save-your-conda-environments/](https://biasandvariance.com/sagemaker-save-your-conda-environments/)

I hope you'll like it.",t2_2zijr2cb,False,,0,False,AWS SageMaker: do not lose conda environments on machine shutdown,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8jsga,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},,,True,,1587955251.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been using AWS SageMaker a lot as a dev machine for ML work and it&amp;#39;s very annoying how it removes conda environments after the machine is turned off.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve created this short guide (3min really) to save conda environments between restarts! &lt;a href=""https://biasandvariance.com/sagemaker-save-your-conda-environments/""&gt;https://biasandvariance.com/sagemaker-save-your-conda-environments/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope you&amp;#39;ll like it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g8jsga,True,,derivablefunc,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8jsga/aws_sagemaker_do_not_lose_conda_environments_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8jsga/aws_sagemaker_do_not_lose_conda_environments_on/,155203,1587926451.0,0,,False,,,,
,learnmachinelearning,,t2_xf2t5,False,,0,False,Data Science and Machine Learning,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_g8jr4r,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/Y74cIB-jZ1Z2JxHcsVPhHT86lPYuIfjEUOIu9pzrNHg.jpg,False,,[],{},link,,False,,1587955136.0,text,6,,,text,luminousmen.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?auto=webp&amp;s=89a13ac781985865840510c0d9879d94ff96ad4e', 'width': 1400, 'height': 1049}, 'resolutions': [{'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=58252a18a1593f7fb545b9382eb5608b0bb1458b', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=365d7c16d46a4c084a59151c5298e57329ca4ef9', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5855bdfdd8d6ff5112c08c17fbd4a5232955bcf8', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74bcbc13f4bf2e9eb073bf018cfdb59967c2d462', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac193f1a7548807dbe10f936d424e38dad83818d', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/K1AHz2OzJr-sZQjltPutmtNhd4RbIcAF2t9NUCiLx7I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c14b6d6213fb8c2d75f2d153ab2dab84847f344', 'width': 1080, 'height': 809}], 'variants': {}, 'id': 'XchPegrlLOJRJQT5oEMGz4llIFmlZdNZ-uqU9NhlWB0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8jr4r,True,,luminoumen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8jr4r/data_science_and_machine_learning/,all_ads,False,https://luminousmen.com/post/data-science-and-machine-learning,155203,1587926336.0,0,,False,,,,
,learnmachinelearning,,t2_13h9rzxq,False,,0,False,Dem fools think it's about some sexy girl,[],r/learnmachinelearning,False,6,,0,87.0,False,t3_g7rmp3,False,dark,0.96,,public,665,0,{},140.0,,False,[],,True,False,,{},,False,665,,False,https://a.thumbs.redditmedia.com/0MglT7XEntPfoQfbOAJ4XGs26P3ap2SO0RfoyzLa9P8.jpg,False,,[],{},image,,False,,1587840712.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?auto=webp&amp;s=73cbf54261eed051efc61c4ab04130b295e5208b', 'width': 700, 'height': 436}, 'resolutions': [{'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11180fc1f57099a47feecb449568bfedb07cf299', 'width': 108, 'height': 67}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91317c6bdad65ef0bf973985f6a0b79fdbeacbfc', 'width': 216, 'height': 134}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5fd6bab3c057d147752c339e3e064964af4ad3cf', 'width': 320, 'height': 199}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6be467e8dde5d13105256db4dfa13e6b7cb64d9e', 'width': 640, 'height': 398}], 'variants': {}, 'id': 'vw3Q785hx6ySfY1k80YNC-Zr43gsnNp-GnPtFsJ2vwk'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7rmp3,True,,Yoohao,,10,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7rmp3/dem_fools_think_its_about_some_sexy_girl/,all_ads,False,https://i.redd.it/j586af7nxvu41.jpg,155203,1587811912.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'memes', 'selftext': '', 'author_fullname': 't2_3x05yr14', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Watching a train model', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/memes', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 87, 'hide_score': False, 'name': 't3_g7mtog', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13216, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 13216, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/0MglT7XEntPfoQfbOAJ4XGs26P3ap2SO0RfoyzLa9P8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1587814863.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': True, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?auto=webp&amp;s=73cbf54261eed051efc61c4ab04130b295e5208b', 'width': 700, 'height': 436}, 'resolutions': [{'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11180fc1f57099a47feecb449568bfedb07cf299', 'width': 108, 'height': 67}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=91317c6bdad65ef0bf973985f6a0b79fdbeacbfc', 'width': 216, 'height': 134}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5fd6bab3c057d147752c339e3e064964af4ad3cf', 'width': 320, 'height': 199}, {'url': 'https://preview.redd.it/j586af7nxvu41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6be467e8dde5d13105256db4dfa13e6b7cb64d9e', 'width': 640, 'height': 398}], 'variants': {}, 'id': 'vw3Q785hx6ySfY1k80YNC-Zr43gsnNp-GnPtFsJ2vwk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qjpg', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g7mtog', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'sejin_mb', 'discussion_type': None, 'num_comments': 104, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/memes/comments/g7mtog/watching_a_train_model/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://i.redd.it/j586af7nxvu41.jpg', 'subreddit_subscribers': 10009486, 'created_utc': 1587786063.0, 'num_crossposts': 4, 'media': None, 'is_video': False}]",t3_g7mtog,,
,learnmachinelearning,,t2_42dg2rb1,False,,0,False,An Introduction to Deep Reinforcement Learning: Meta-Learning,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g8j9we,False,dark,1.0,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/UmY1dBq6Lgs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Deep Reinforcement Learning: Meta-Learning | Analytics Club at ETH | Data Science Meetup', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/UmY1dBq6Lgs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/UmY1dBq6Lgs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/UmY1dBq6Lgs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8j9we', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/Ce_htEKmqphSNqpsg7KksEVimly1ZGsDrk_NMl2F9vY.jpg,False,,[],{},rich:video,,False,,1587953647.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/LubOyyWN98r5u0RQ3F2mLwNElCDnl9BmTHfsBNw3UN4.jpg?auto=webp&amp;s=b71c95f2dacab173c0f5c14c5d7247e6b0207652', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/LubOyyWN98r5u0RQ3F2mLwNElCDnl9BmTHfsBNw3UN4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd5dd797ec257ad9a79f0de56a639b0ddcf79bf', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/LubOyyWN98r5u0RQ3F2mLwNElCDnl9BmTHfsBNw3UN4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97fd2909373204aedeef16670ac9d8ed9f7e8c6a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/LubOyyWN98r5u0RQ3F2mLwNElCDnl9BmTHfsBNw3UN4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb6c1fbc66351372ab716fe22da5779c1587ed07', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Gs_gy69zkDsIEraAGgrfTahDU8lhkqSueNFxRF722tg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8j9we,True,,reddit_data_guy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8j9we/an_introduction_to_deep_reinforcement_learning/,all_ads,False,https://youtu.be/UmY1dBq6Lgs,155203,1587924847.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Deep Reinforcement Learning: Meta-Learning | Analytics Club at ETH | Data Science Meetup', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/UmY1dBq6Lgs?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/UmY1dBq6Lgs/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,I have a network that classifies galaxies and I want to build a new network from scratch. But I also want to understand how to analyse it from output of each layer. Could someone help?,t2_2wg8xj15,False,,0,False,How to analyse a convolutional network layer by layer?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8mhiw,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587964026.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a network that classifies galaxies and I want to build a new network from scratch. But I also want to understand how to analyse it from output of each layer. Could someone help?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g8mhiw,True,,ninpou_kuchiyosei,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8mhiw/how_to_analyse_a_convolutional_network_layer_by/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8mhiw/how_to_analyse_a_convolutional_network_layer_by/,155203,1587935226.0,0,,False,,,,
,learnmachinelearning,"Hey guys, I hope I can explain this well enough.

I have some geo-spatial data where each row has a latitude and longitude. 

I also have a file that gives a latitude and longitude for each county in the US.

I want to map me data to these counties but the problem is that my data points will be located at random places in the county and not exactly at the point that the mapping file includes.

You see the problem? I was thinking I could use some sort of cluster analysis but I don't know how to do that with predefined centroids (the county data). 

Does anyone know how to do this? (I'm okay with a decent amount of error, this isn't an important project)

Thanks!!",t2_r09ik,False,,0,False,Can I use some kind of cluster analysis to map latitude/longitude data to county?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8lne2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1587961243.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I hope I can explain this well enough.&lt;/p&gt;

&lt;p&gt;I have some geo-spatial data where each row has a latitude and longitude. &lt;/p&gt;

&lt;p&gt;I also have a file that gives a latitude and longitude for each county in the US.&lt;/p&gt;

&lt;p&gt;I want to map me data to these counties but the problem is that my data points will be located at random places in the county and not exactly at the point that the mapping file includes.&lt;/p&gt;

&lt;p&gt;You see the problem? I was thinking I could use some sort of cluster analysis but I don&amp;#39;t know how to do that with predefined centroids (the county data). &lt;/p&gt;

&lt;p&gt;Does anyone know how to do this? (I&amp;#39;m okay with a decent amount of error, this isn&amp;#39;t an important project)&lt;/p&gt;

&lt;p&gt;Thanks!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8lne2,True,,PM_ME_GOOD_NEWS_,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8lne2/can_i_use_some_kind_of_cluster_analysis_to_map/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8lne2/can_i_use_some_kind_of_cluster_analysis_to_map/,155203,1587932443.0,0,,False,,,,
,learnmachinelearning,"Hi guys, I'm about to create a classifier for a dataset with 1500 features and 3000 instances. What algorithms would you recommend for a dataset with such a high degree of dimensionality? Should I consider feature reduction, e.g. PCA?

Initial analysis shows clear patterns and correlation between features.",t2_26n50qtm,False,,0,False,"Algorithm for (3000, 1500) dataset",[],r/learnmachinelearning,False,6,,0,,False,t3_g8exbp,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587938949.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys, I&amp;#39;m about to create a classifier for a dataset with 1500 features and 3000 instances. What algorithms would you recommend for a dataset with such a high degree of dimensionality? Should I consider feature reduction, e.g. PCA?&lt;/p&gt;

&lt;p&gt;Initial analysis shows clear patterns and correlation between features.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8exbp,True,,Academy-,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8exbp/algorithm_for_3000_1500_dataset/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8exbp/algorithm_for_3000_1500_dataset/,155203,1587910149.0,0,,False,,,,
,learnmachinelearning,,t2_2zcmkda3,False,,0,False,Social distances using deep learning anyone interested I am planning to write a blog on this,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,70.0,False,t3_g7n9uh,False,light,0.97,,public,1537,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/1gmrk6qg3wu41/DASH_240?source=fallback', 'height': 212, 'width': 426, 'scrubber_media_url': 'https://v.redd.it/1gmrk6qg3wu41/DASH_96', 'dash_url': 'https://v.redd.it/1gmrk6qg3wu41/DASHPlaylist.mpd', 'duration': 34, 'hls_url': 'https://v.redd.it/1gmrk6qg3wu41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},Project,False,1537,,False,https://b.thumbs.redditmedia.com/e8PDt88r1NWswcqATorD9B_qMetfcF5DQBlp_9-dz8Y.jpg,False,,[],{},hosted:video,,False,,1587816849.0,richtext,6,,,text,v.redd.it,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/9ky5gk37s1VF7761y1fJHan8T-us07wj5HpCLH6XAyE.png?format=pjpg&amp;auto=webp&amp;s=7ef78c371492251b21aa6b0cb5f7ff181c24084b', 'width': 640, 'height': 320}, 'resolutions': [{'url': 'https://external-preview.redd.it/9ky5gk37s1VF7761y1fJHan8T-us07wj5HpCLH6XAyE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d5af257c3564eec7eb82305f896006c7c7a4508f', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/9ky5gk37s1VF7761y1fJHan8T-us07wj5HpCLH6XAyE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=33ab0b0e730035e0a61cdeb026bdfea150ffd996', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/9ky5gk37s1VF7761y1fJHan8T-us07wj5HpCLH6XAyE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2ce4c59399f713fb14a2b55f94cbf8873e140613', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/9ky5gk37s1VF7761y1fJHan8T-us07wj5HpCLH6XAyE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=494be644a43bcbdf0f093e15e839c0f01ec0ba11', 'width': 640, 'height': 320}], 'variants': {}, 'id': 'Z8svLP9cF0FNal4ALVEomsqx2GO8fryp82KXGAFCAMc'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g7n9uh,True,,cudanexus,,113,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7n9uh/social_distances_using_deep_learning_anyone/,all_ads,False,https://v.redd.it/1gmrk6qg3wu41,155203,1587788049.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/1gmrk6qg3wu41/DASH_240?source=fallback', 'height': 212, 'width': 426, 'scrubber_media_url': 'https://v.redd.it/1gmrk6qg3wu41/DASH_96', 'dash_url': 'https://v.redd.it/1gmrk6qg3wu41/DASHPlaylist.mpd', 'duration': 34, 'hls_url': 'https://v.redd.it/1gmrk6qg3wu41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"I am planning to create a Network that classifies if a word is a curse word (I know, completley overkill for a simple censoring :p)  


My idea is that I have a dataset of bad words, and some good words. Each word gets tokenized into a array of the letter IDs (and I just realized the array length wont always be the same..)

and then the network classifies if a word is a curse word based of the ID array. What type of network would be good for that? A simple feed forward? Considering the array wont be the same length all the time since every word has a different length...",t2_3a0uwyjx,False,,0,False,Whats a good Network for simple number classification?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8kurm,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,True,self,False,,[],{},,,True,,1587958618.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am planning to create a Network that classifies if a word is a curse word (I know, completley overkill for a simple censoring :p)  &lt;/p&gt;

&lt;p&gt;My idea is that I have a dataset of bad words, and some good words. Each word gets tokenized into a array of the letter IDs (and I just realized the array length wont always be the same..)&lt;/p&gt;

&lt;p&gt;and then the network classifies if a word is a curse word based of the ID array. What type of network would be good for that? A simple feed forward? Considering the array wont be the same length all the time since every word has a different length...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8kurm,True,,TrackLabs,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8kurm/whats_a_good_network_for_simple_number/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8kurm/whats_a_good_network_for_simple_number/,155203,1587929818.0,0,,False,,,,
,learnmachinelearning,"Hi. I've created a predictive model which classifies if a person is wearing a helmet or not and trying to do this on a video stream.

Right now I'm having a speed of 0.5\~0.6s per inference on my CPU. I'd like to speed this up quite a bit to around 0.1s but I'm not sure where to start.

Model: mobileNetv2  - Trained with FastAi - Deployed to a pure pytorch prediction

CPU: Intel(R) Core(TM) i7-4710HQ CPU 2.50Ghz

&amp;#x200B;

Is that a reasonable inference speed for a CPU or am I doing something wrong? I've tried using Resnet18 as well which gives me a slightly worse speed of around 0.6\~0.5s.",t2_xoak9,False,,0,False,What's a reasonable Inference Speed on a Laptop CPU?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8kmf9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587957841.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I&amp;#39;ve created a predictive model which classifies if a person is wearing a helmet or not and trying to do this on a video stream.&lt;/p&gt;

&lt;p&gt;Right now I&amp;#39;m having a speed of 0.5~0.6s per inference on my CPU. I&amp;#39;d like to speed this up quite a bit to around 0.1s but I&amp;#39;m not sure where to start.&lt;/p&gt;

&lt;p&gt;Model: mobileNetv2  - Trained with FastAi - Deployed to a pure pytorch prediction&lt;/p&gt;

&lt;p&gt;CPU: Intel(R) Core(TM) i7-4710HQ CPU 2.50Ghz&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is that a reasonable inference speed for a CPU or am I doing something wrong? I&amp;#39;ve tried using Resnet18 as well which gives me a slightly worse speed of around 0.6~0.5s.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8kmf9,True,,Tomas1337,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8kmf9/whats_a_reasonable_inference_speed_on_a_laptop_cpu/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8kmf9/whats_a_reasonable_inference_speed_on_a_laptop_cpu/,155203,1587929041.0,0,,False,,,,
,learnmachinelearning,What are some good modeling techniques.,t2_689s8lfz,False,,0,False,Best modelling techniques for very high dimensional data sets where rows less than features. Example text data.,[],r/learnmachinelearning,False,6,,0,,False,t3_g8ki0v,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587957455.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What are some good modeling techniques.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8ki0v,True,,prkohli1992,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8ki0v/best_modelling_techniques_for_very_high/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8ki0v/best_modelling_techniques_for_very_high/,155203,1587928655.0,0,,False,,,,
,learnmachinelearning,"I'm a senior software developer (8+ years of mostly web/cloud/SASS) looking to switch positions within about a year. I can move laterally to another developer position but I'm also interested in ML, have some basic skills (though not enough to be employable at this point) and was wondering what the job market is like for ML?

Are there tons of jobs available (COVID aside) or is it a fairly niche and kinda limited job market? 

Can I expect to see 90-100k at least at the mid level? 

What (and I know this is a vague question) is the skills threshold for entry to mid-level employment? Please feel free to interpret that question however you see fit.",t2_2jf7lps,False,,0,False,What's the job market like?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8jmys,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587954786.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a senior software developer (8+ years of mostly web/cloud/SASS) looking to switch positions within about a year. I can move laterally to another developer position but I&amp;#39;m also interested in ML, have some basic skills (though not enough to be employable at this point) and was wondering what the job market is like for ML?&lt;/p&gt;

&lt;p&gt;Are there tons of jobs available (COVID aside) or is it a fairly niche and kinda limited job market? &lt;/p&gt;

&lt;p&gt;Can I expect to see 90-100k at least at the mid level? &lt;/p&gt;

&lt;p&gt;What (and I know this is a vague question) is the skills threshold for entry to mid-level employment? Please feel free to interpret that question however you see fit.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8jmys,True,,gusmeowmeow,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8jmys/whats_the_job_market_like/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8jmys/whats_the_job_market_like/,155203,1587925986.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,You can manipulate faces in high resolution,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g8j0ty,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/t4Zx5pbG278?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'You can manipulate faces in high resolution', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/t4Zx5pbG278?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/t4Zx5pbG278/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/t4Zx5pbG278?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8j0ty', 'height': 344}",Discussion,False,1,,False,https://a.thumbs.redditmedia.com/AkQFfj6yA4XvNQ49hcdVkSf_6ULbW5y0gP_uT41n348.jpg,False,,[],{},rich:video,,False,,1587952863.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/tUBJMfK1AOVqLXMn6X1rDWoPz4XIyhfVR3f2dWBR00U.jpg?auto=webp&amp;s=0e93a912e561d34a89c267be7e80728c6b17c095', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/tUBJMfK1AOVqLXMn6X1rDWoPz4XIyhfVR3f2dWBR00U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb44f22b3ab6a360d96abf58f05a335a34bbb283', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/tUBJMfK1AOVqLXMn6X1rDWoPz4XIyhfVR3f2dWBR00U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e46ca2128f54f15194b5cd5bfc3c53eb475fc1', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/tUBJMfK1AOVqLXMn6X1rDWoPz4XIyhfVR3f2dWBR00U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec5e8e4a25fe76c8a35989b3547238785c9405da', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'LXKpI7FrgH-oOr83tzOeubo55AE3tJrd08UUD2uU9ys'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8j0ty,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8j0ty/you_can_manipulate_faces_in_high_resolution/,all_ads,False,https://youtu.be/t4Zx5pbG278,155203,1587924063.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'You can manipulate faces in high resolution', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/t4Zx5pbG278?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/t4Zx5pbG278/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,PerceptionRNN component of Waymo's ChauffeurNet predicts the trajectory of other cars,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g8iop3,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/VL8ILndLymU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""PerceptionRNN component of Waymo's ChauffeurNet predicts the trajectory of other cars"", 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/VL8ILndLymU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VL8ILndLymU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/VL8ILndLymU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g8iop3', 'height': 344}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/jFKISu-6K1vPjglIekvAZXhBvp6H3FE1-CvII6d1n8U.jpg,False,,[],{},rich:video,,False,,1587951769.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/k4x8Qf2QfcIuR4rSDNw6QNF0Y7NSJ2RBjs4PvQhWIWg.jpg?auto=webp&amp;s=d082be72118bf4eb82955a25d8645a5b442a4b60', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/k4x8Qf2QfcIuR4rSDNw6QNF0Y7NSJ2RBjs4PvQhWIWg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=08fca2f2146bf2be356057bab051f15c6e2a8ccf', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/k4x8Qf2QfcIuR4rSDNw6QNF0Y7NSJ2RBjs4PvQhWIWg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=31675228950e9074ebcd6286925ad54757d7cbc3', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/k4x8Qf2QfcIuR4rSDNw6QNF0Y7NSJ2RBjs4PvQhWIWg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=580a8bd6abbd6bd51cf816cec7bbb25cc3520170', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'tCVh215uXux41YCZRkGzaSbRbW5fFNLGKsGYiBYeB_I'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g8iop3,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8iop3/perceptionrnn_component_of_waymos_chauffeurnet/,all_ads,False,https://youtu.be/VL8ILndLymU,155203,1587922969.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""PerceptionRNN component of Waymo's ChauffeurNet predicts the trajectory of other cars"", 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/VL8ILndLymU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/VL8ILndLymU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,I'd like to use 10-20 features which are in the form of percentile rankings. The output will also be a percentile ranking. I'd like to consider the interaction of the features as well if possible. Is there a machine learning model that would lend itself to this kind of learning?,t2_3l9ay3tj,False,,0,False,Which machine learning model should I be using?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8i5z9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587950075.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;d like to use 10-20 features which are in the form of percentile rankings. The output will also be a percentile ranking. I&amp;#39;d like to consider the interaction of the features as well if possible. Is there a machine learning model that would lend itself to this kind of learning?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8i5z9,True,,rachelwilliams20,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8i5z9/which_machine_learning_model_should_i_be_using/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8i5z9/which_machine_learning_model_should_i_be_using/,155203,1587921275.0,0,,False,,,,
,learnmachinelearning,"I have posted [this](https://datascience.stackexchange.com/questions/72974/reason-for-capping-learning-rate-alpha-up-to-1-for-gradient-descent) question on StackOverflow, as well. I wanted to extend the discussion and hence, posting it here.

I am curious as to why do I generally see posts about setting the limit of the learning rate to 1 because in my case I reached the minimum cost in lesser number iterations when the learning rate was greater than 1 (`alpha = 1.2`)

Do algorithms like sklearn's `LinearRegression()` set the limit to 1, by default, and do not check for learning rates above that?",t2_4k2xxp8y,False,,0,False,Setting Gradient Descent learning rate (alpha) to a maximum of 1,[],r/learnmachinelearning,False,6,,0,,False,t3_g8aqu6,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},self,,True,,1587917778.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have posted &lt;a href=""https://datascience.stackexchange.com/questions/72974/reason-for-capping-learning-rate-alpha-up-to-1-for-gradient-descent""&gt;this&lt;/a&gt; question on StackOverflow, as well. I wanted to extend the discussion and hence, posting it here.&lt;/p&gt;

&lt;p&gt;I am curious as to why do I generally see posts about setting the limit of the learning rate to 1 because in my case I reached the minimum cost in lesser number iterations when the learning rate was greater than 1 (&lt;code&gt;alpha = 1.2&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Do algorithms like sklearn&amp;#39;s &lt;code&gt;LinearRegression()&lt;/code&gt; set the limit to 1, by default, and do not check for learning rates above that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?auto=webp&amp;s=dfe139c3d980684c3a0a719fe3c8cc430f520d0c', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d16981948e2864492cbef38c818d98a0fede03c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e77ea1c37c5848bd68de53e901736c593b63cd', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'hEAu3ZrBsaXLg2OKLZfA6MTEbSMf2Y6jPgvtzg3KwlE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8aqu6,True,,SufferDieoxide,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8aqu6/setting_gradient_descent_learning_rate_alpha_to_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8aqu6/setting_gradient_descent_learning_rate_alpha_to_a/,155203,1587888978.0,0,,False,,,,
,learnmachinelearning,,t2_205ygpnb,False,,0,False,"Machine Learning with Python : Part 2 : K Nearest Neighbours :: How to make K Nearest Model, Initiate K Nearest Model and Fitting Data into K Nearest Model and importing Logistic Regression Model . more on : www.facebook.com/seevecoding",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g8d781,False,dark,1.0,,public,2,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/3ysx9ot4k5v41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/3ysx9ot4k5v41/DASH_96', 'dash_url': 'https://v.redd.it/3ysx9ot4k5v41/DASHPlaylist.mpd', 'duration': 35, 'hls_url': 'https://v.redd.it/3ysx9ot4k5v41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/yOm_mvh1yRnqXRbB-c2nkHhrG38_CiCHMm7v3JxxsK0.jpg,False,,[],{},hosted:video,,False,,1587931416.0,text,6,,,text,v.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?format=pjpg&amp;auto=webp&amp;s=0834012a610ea7c25eae98b324283bc497fc0760', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fe17423c647a95f28763ea3a296d6857c32dc3ed', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=db89b40e62c9fdad3a2b4e21890e7bed03a81e4b', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e982b1ef964c0fc9eae8f28164439a0dd765c17a', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2bf07c70317a4712d13d3ce9d5ef1fa0a08cec8c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=df68d82eda152cb75d622d1d7bf15fa3add720b9', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/BqDGS6SzcUqmJ08eNTnd3B2Wb2OObXf10CidXOphJL4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=45170f3ec57d1a36628c93c737ac9e41ba67b80e', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '5en38LLCtIQUG45IaXc2cAo0wn3u3PiIq1a6EuadGgQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8d781,True,,iamrealadvait,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8d781/machine_learning_with_python_part_2_k_nearest/,all_ads,False,https://v.redd.it/3ysx9ot4k5v41,155203,1587902616.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/3ysx9ot4k5v41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/3ysx9ot4k5v41/DASH_96', 'dash_url': 'https://v.redd.it/3ysx9ot4k5v41/DASHPlaylist.mpd', 'duration': 35, 'hls_url': 'https://v.redd.it/3ysx9ot4k5v41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"Exactly as the title suggests. We know that NN's can be thought of as graphs, with neurons as nodes and weights as links, so what makes Graph neural networks so special to warrant their own class?",t2_jgca7,False,,0,False,"Given that Neural Networks are Graphs, what's a Graph Neural Network",[],r/learnmachinelearning,False,6,,0,,False,t3_g8g624,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587943468.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Exactly as the title suggests. We know that NN&amp;#39;s can be thought of as graphs, with neurons as nodes and weights as links, so what makes Graph neural networks so special to warrant their own class?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8g624,True,,dxjustice,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8g624/given_that_neural_networks_are_graphs_whats_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8g624/given_that_neural_networks_are_graphs_whats_a/,155203,1587914668.0,0,,False,,,,
,learnmachinelearning,,t2_6g3z9,False,,0,False,Inconsistency on A. Graves' original Connectionist Temporal Classification (CTC) paper?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_g8g5cx,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},HELP,False,1,,False,https://b.thumbs.redditmedia.com/0KXHy7XRYet_MyjvBO2sYOMqStyOZWCet9_jIJMvJkA.jpg,False,,[],{},link,,False,,1587943395.0,richtext,6,,,text,datascience.stackexchange.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?auto=webp&amp;s=dfe139c3d980684c3a0a719fe3c8cc430f520d0c', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d16981948e2864492cbef38c818d98a0fede03c', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/m8jHwzE904jCjUr0Gb6715a2PHv3HXlVTX5gFvTZoXw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e77ea1c37c5848bd68de53e901736c593b63cd', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'hEAu3ZrBsaXLg2OKLZfA6MTEbSMf2Y6jPgvtzg3KwlE'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g8g5cx,True,,Asierro,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8g5cx/inconsistency_on_a_graves_original_connectionist/,all_ads,False,https://datascience.stackexchange.com/questions/73023/inconsistency-on-a-graves-original-connectionist-temporal-classification-ctc,155203,1587914595.0,0,,False,,,,
,learnmachinelearning,"Hi guys,

I have data where the index is every hour starting from 01.01.2016 to 01.03.2020 (for US readers 3.1.2020).

First, I split everything before 2020 for training and everything after 2020 for validation. For testing, I use the first week of March.

The data is dependent on the weather, this is a time series problem. So is this approach good to train on all 4 years and then validate just on 3 months and just test on one week? Or is it smarter to use another approach, train on the 3 first years (2016,2017,2018), validate on 2019 and test on everything from 2020?

Cheers.",t2_14w7k8,False,,0,False,How to split continuous data of 4 years for training/validation/testing?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g89gdv,False,dark,0.74,,public,5,0,{},,,False,[],,False,False,,{},Question,False,5,,False,self,1587885966.0,,[],{},,,True,,1587910743.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I have data where the index is every hour starting from 01.01.2016 to 01.03.2020 (for US readers 3.1.2020).&lt;/p&gt;

&lt;p&gt;First, I split everything before 2020 for training and everything after 2020 for validation. For testing, I use the first week of March.&lt;/p&gt;

&lt;p&gt;The data is dependent on the weather, this is a time series problem. So is this approach good to train on all 4 years and then validate just on 3 months and just test on one week? Or is it smarter to use another approach, train on the 3 first years (2016,2017,2018), validate on 2019 and test on everything from 2020?&lt;/p&gt;

&lt;p&gt;Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g89gdv,True,,dflash88,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g89gdv/how_to_split_continuous_data_of_4_years_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g89gdv/how_to_split_continuous_data_of_4_years_for/,155203,1587881943.0,0,,False,,,,
,learnmachinelearning,All the help I can find online seems to be for sequential layers. Currently working with tf 2.0.0,t2_2u07axoa,False,,0,False,"adding gaussian noise layer for a model built with subclassing. I mean seriously, how?",[],r/learnmachinelearning,False,6,,0,,False,t3_g8f41f,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587939680.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;All the help I can find online seems to be for sequential layers. Currently working with tf 2.0.0&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8f41f,True,,poisoned-piper,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8f41f/adding_gaussian_noise_layer_for_a_model_built/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8f41f/adding_gaussian_noise_layer_for_a_model_built/,155203,1587910880.0,0,,False,,,,
,learnmachinelearning,"Hello, I am currently a student and I am interested in the usages of machine learning in stock market prediction. 

I am wondering if I could get some suggestions regarding state of art papers (the names) summarizing techniques and methodologies to grasp some of the ideas used. From my own search, I have found different categories of methods, but nothing clearly as unique measurements across different papers to decide which on average is better and such. As an example, for the 'classic' object detection and such, metrics and contests were done, being able to find almost the best solutions a lot easier.

Any help is welcomed (papers names, links etc). Thank you.",t2_12hn2dgi,False,,0,False,Stock Market Prediction,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8end2,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587937869.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I am currently a student and I am interested in the usages of machine learning in stock market prediction. &lt;/p&gt;

&lt;p&gt;I am wondering if I could get some suggestions regarding state of art papers (the names) summarizing techniques and methodologies to grasp some of the ideas used. From my own search, I have found different categories of methods, but nothing clearly as unique measurements across different papers to decide which on average is better and such. As an example, for the &amp;#39;classic&amp;#39; object detection and such, metrics and contests were done, being able to find almost the best solutions a lot easier.&lt;/p&gt;

&lt;p&gt;Any help is welcomed (papers names, links etc). Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8end2,True,,jurjstyle,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8end2/stock_market_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8end2/stock_market_prediction/,155203,1587909069.0,0,,False,,,,
,learnmachinelearning,"Hi everyone,

I am new to machine learning, taking a few classes on Udemy and going through ML/Data Science books. I find it all extremely fascinating. I do, however, have a question regarding the issue with beginner classes. How often do machine learning professionals/Data Scientists actually use linear regression, logistic regression, and k nearest neighbors, etc. (basically the models taught to beginners)? I know this really depends on their particular expertise, so I guess I'm asking if the beginner models are commonly used in the real world of Data Science?",t2_3pnizflv,False,,0,False,ML Models Taught to Beginners,[],r/learnmachinelearning,False,6,,0,,False,t3_g88ijs,False,dark,0.7,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,1587881173.0,,[],{},,,True,,1587906143.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I am new to machine learning, taking a few classes on Udemy and going through ML/Data Science books. I find it all extremely fascinating. I do, however, have a question regarding the issue with beginner classes. How often do machine learning professionals/Data Scientists actually use linear regression, logistic regression, and k nearest neighbors, etc. (basically the models taught to beginners)? I know this really depends on their particular expertise, so I guess I&amp;#39;m asking if the beginner models are commonly used in the real world of Data Science?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g88ijs,True,,Tyron_Slothrop,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g88ijs/ml_models_taught_to_beginners/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g88ijs/ml_models_taught_to_beginners/,155203,1587877343.0,0,,False,,,,
,learnmachinelearning,,t2_1hc0c5p4,False,,0,False,[Free Coursera Course] Machine Learning Course offered by Stanford University (with Professional Certificate),[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g7zkvw,False,dark,0.84,,public,26,0,{},140.0,,False,[],,False,False,,{},,False,26,,False,https://b.thumbs.redditmedia.com/sP_tl53gMGSWKL9B3KgGdR56t5ZRwfB2-bwfwVBSRCo.jpg,False,,[],{},link,,False,,1587871391.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7FaKevHK4YdwuXPpY7HWS7DmTHH4Hskc-M4g4MaJsOY.jpg?auto=webp&amp;s=25768c6240bbca2fcbc149dc2cc2df3f29f60faa', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/7FaKevHK4YdwuXPpY7HWS7DmTHH4Hskc-M4g4MaJsOY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=55b17c89affd5a3a0c7fa1304ddc245a074d2c55', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/7FaKevHK4YdwuXPpY7HWS7DmTHH4Hskc-M4g4MaJsOY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=98ce695eaaa57375f2e0776f77431af8f3113966', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'KN4I_491J_dcCISp2_ugw0TnbJUYYOXgsB1ktY61ROU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7zkvw,True,,ewan_m,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7zkvw/free_coursera_course_machine_learning_course/,all_ads,False,https://medium.com/@naasifn/best-python-machine-learning-courses-from-coursera-for-professionals-3036e50631dc,155203,1587842591.0,0,,False,,,,
,learnmachinelearning,"
Hello, Iâ€™m a civil engineer and I donâ€™t code for my profession, however I am interested to know if it is possible to develop a model with machine learning which can place polygons side by side without overlapping.
Can you please tell me if there are existing examples and what type of neural network such that I have something to start with? I can learn python. Thank you",t2_2flog1ru,False,,0,False,Placing polygons side by side,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g8dsvk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587934211.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, Iâ€™m a civil engineer and I donâ€™t code for my profession, however I am interested to know if it is possible to develop a model with machine learning which can place polygons side by side without overlapping.
Can you please tell me if there are existing examples and what type of neural network such that I have something to start with? I can learn python. Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g8dsvk,True,,gluchi,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8dsvk/placing_polygons_side_by_side/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8dsvk/placing_polygons_side_by_side/,155203,1587905411.0,0,,False,,,,
,learnmachinelearning,"# Start here! Predict survival on the Titanic and get familiar with ML basics

Have you ever created a [**#Kaggle**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6659953616952131584&amp;keywords=%23Kaggle&amp;originTrackingId=wZ%2BNc6SOI%2FIIhCi%2BVW0dKA%3D%3D) Kernel? or want to create one? Kaggle kernel is one easy way to showcase your data science skills.

[Create Your First Kaggle Kernel Using PyCaret](https://reddit.com/link/g88myt/video/jvlrfolvi3v41/player)

In this 3-minute video tutorial, we go end-to-end on how to create a Kaggle Kernel using PyCaret.

[https://www.youtube.com/watch?v=nqMM6rngNCA](https://www.youtube.com/watch?v=nqMM6rngNCA)

Official Website : [https://www.pycaret.org](https://www.pycaret.org)

Git : [https://www.github.com/pycaret/pycaret](https://www.github.com/pycaret/pycaret)",t2_6b23z3qw,False,,0,False,Create your First Kaggle Kernel using PyCaret,[],r/learnmachinelearning,False,6,,0,64.0,False,t3_g88myt,False,dark,0.75,,public,4,0,{},64.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/JzrcZreUH3LWsTfM2cjyEK5WLXHiPmqBNJxE18MPDSA.jpg,False,,[],{},self,,True,,1587906693.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;h1&gt;Start here! Predict survival on the Titanic and get familiar with ML basics&lt;/h1&gt;

&lt;p&gt;Have you ever created a &lt;a href=""https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6659953616952131584&amp;amp;keywords=%23Kaggle&amp;amp;originTrackingId=wZ%2BNc6SOI%2FIIhCi%2BVW0dKA%3D%3D""&gt;&lt;strong&gt;#Kaggle&lt;/strong&gt;&lt;/a&gt; Kernel? or want to create one? Kaggle kernel is one easy way to showcase your data science skills.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/g88myt/video/jvlrfolvi3v41/player""&gt;Create Your First Kaggle Kernel Using PyCaret&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this 3-minute video tutorial, we go end-to-end on how to create a Kaggle Kernel using PyCaret.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=nqMM6rngNCA""&gt;https://www.youtube.com/watch?v=nqMM6rngNCA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Official Website : &lt;a href=""https://www.pycaret.org""&gt;https://www.pycaret.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Git : &lt;a href=""https://www.github.com/pycaret/pycaret""&gt;https://www.github.com/pycaret/pycaret&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;s=52cc36e047bdca039326e84d3b7ce7aabaf12be6', 'width': 64, 'height': 64}, 'resolutions': [], 'variants': {}, 'id': 'QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g88myt,True,,moezali,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g88myt/create_your_first_kaggle_kernel_using_pycaret/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g88myt/create_your_first_kaggle_kernel_using_pycaret/,155203,1587877893.0,0,,False,,,"{'jvlrfolvi3v41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/g88myt/asset/jvlrfolvi3v41/DASHPlaylist.mpd', 'x': 1920, 'y': 1080, 'hlsUrl': 'https://v.redd.it/link/g88myt/asset/jvlrfolvi3v41/HLSPlaylist.m3u8', 'id': 'jvlrfolvi3v41', 'isGif': False}}",
,learnmachinelearning,,t2_2vj4dxir,False,,0,False,Learn Complete Machine Learning from Scratch,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g8cqz7,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/XGubGvVGHzC6quU8LxJoT9VyeVKb3n_1BlWQNtqDUO0.jpg,False,,[],{},link,,False,,1587929047.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XagfTn8wWiixTxJ5vJ_PqeAnVKH54X-mGYWwe9vwDTI.jpg?auto=webp&amp;s=b783a6f49bc817548d9f06ffc21c1b72e8fb8cce', 'width': 448, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/XagfTn8wWiixTxJ5vJ_PqeAnVKH54X-mGYWwe9vwDTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f684e340f956c0e2b9afc483080a40d8c68b5421', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/XagfTn8wWiixTxJ5vJ_PqeAnVKH54X-mGYWwe9vwDTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad6579a9a642afec010a7f4c017e5e37925c2775', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/XagfTn8wWiixTxJ5vJ_PqeAnVKH54X-mGYWwe9vwDTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ad56773cd40a8bd84402437946038bd39781037', 'width': 320, 'height': 180}], 'variants': {}, 'id': 'dklRCdntzE9A8Fbo8aQZw2bX65A6VgkJALBlNzZkU2E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8cqz7,True,,cimmba,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8cqz7/learn_complete_machine_learning_from_scratch/,all_ads,False,https://medium.com/@devexpert/learn-complete-machine-learning-from-scratch-6d50c14eef45,155203,1587900247.0,0,,False,,,,
,learnmachinelearning,,t2_57i0bime,False,,0,False,Help Nasa classify terrain types to aid the Mars Rover!,[],r/learnmachinelearning,False,6,,0,,False,t3_g85l69,False,dark,0.73,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,default,False,,[],{},,,False,,1587893461.0,text,6,,,text,zooniverse.org,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g85l69,True,,tylersuard,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g85l69/help_nasa_classify_terrain_types_to_aid_the_mars/,all_ads,False,https://www.zooniverse.org/projects/hiro-ono/ai4mars/classify,155203,1587864661.0,0,,False,,,,
,learnmachinelearning,"I just wanted to remind you that you have to cancel it before 1 month period if you don't want to be charged. My wife almost forgot and they would charge her like $300.   


There's also another useful information. You can't cancel it in a standard way. You have to write to Udacity and they will try to convince you to stay. Eventually, they give you a discount, so in case you're going to extend, you can use it for your advantage.   


I'm just writing to let you know.",t2_5yd6bcu6,False,,0,False,Is there anyone who enrolled in free Udacity courses recently?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8bqch,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587923535.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just wanted to remind you that you have to cancel it before 1 month period if you don&amp;#39;t want to be charged. My wife almost forgot and they would charge her like $300.   &lt;/p&gt;

&lt;p&gt;There&amp;#39;s also another useful information. You can&amp;#39;t cancel it in a standard way. You have to write to Udacity and they will try to convince you to stay. Eventually, they give you a discount, so in case you&amp;#39;re going to extend, you can use it for your advantage.   &lt;/p&gt;

&lt;p&gt;I&amp;#39;m just writing to let you know.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8bqch,True,,JK_Bielan,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8bqch/is_there_anyone_who_enrolled_in_free_udacity/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8bqch/is_there_anyone_who_enrolled_in_free_udacity/,155203,1587894735.0,0,,False,,,,
,learnmachinelearning,,t2_xf2t5,False,,0,False,Data Science. Probability,[],r/learnmachinelearning,False,6,,0,,False,t3_g86b98,False,dark,0.8,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,default,False,,[],{},,,False,,1587896420.0,text,6,,,text,luminousmen.com,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g86b98,True,,luminoumen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g86b98/data_science_probability/,all_ads,False,https://luminousmen.com/post/data-science-probability,155203,1587867620.0,0,,False,,,,
,learnmachinelearning,"is there any pre-trained ML/DL model or dataset (opens source or commercial) for driver behaviour analysis/recognition (harsh acceleration/braking/high speed turning etc) from mobile phone sensor data (accelerometer, gyroscope, magnetometer etc)",t2_pwz8s,False,,0,False,pre-trained model or dataset for driver behaviour analysis from mobile phone sensor data,[],r/learnmachinelearning,False,6,,0,,False,t3_g8af4c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587915929.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;is there any pre-trained ML/DL model or dataset (opens source or commercial) for driver behaviour analysis/recognition (harsh acceleration/braking/high speed turning etc) from mobile phone sensor data (accelerometer, gyroscope, magnetometer etc)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8af4c,True,,wiama,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8af4c/pretrained_model_or_dataset_for_driver_behaviour/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8af4c/pretrained_model_or_dataset_for_driver_behaviour/,155203,1587887129.0,0,,False,,,,
,learnmachinelearning,"I have been working on a company project to find duplicate accounts in our database. I have ran a script using TF-IDF and Cosine Similarity and the results came out really well.

I really want to understand this topic better and was wondering if anybody had good pointers for me.

FYI - I am using Python and Pandas.",t2_2a11come,False,,0,False,Getting Started on TF-IDF Cosine Similarity,[],r/learnmachinelearning,False,6,,0,,False,t3_g87lbk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587901953.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working on a company project to find duplicate accounts in our database. I have ran a script using TF-IDF and Cosine Similarity and the results came out really well.&lt;/p&gt;

&lt;p&gt;I really want to understand this topic better and was wondering if anybody had good pointers for me.&lt;/p&gt;

&lt;p&gt;FYI - I am using Python and Pandas.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g87lbk,True,,Mmetr,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g87lbk/getting_started_on_tfidf_cosine_similarity/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g87lbk/getting_started_on_tfidf_cosine_similarity/,155203,1587873153.0,0,,False,,,,
,learnmachinelearning,"Alright, so I just went through some ANN codes I developed on Python and R; when I got my results for my stock market prediction, I do wonder if I can rely on these results to make decisions:

1. I just used the most fundamental feature which is ***% of*** ***ROI*** for each day from several years; but from my POV, even if I do put in more data (e.g. sentiment features, etc.), how could an ML model predict the stock market correctly when there are just ***so many things that can change*** in its environment?
2. Suppose, even if it has 70-80% accuracy and used K-fold testing or whatnot, this is based on ***the data it's trained on - isn't this an artificial and risky conclusion?***
3. And if we really want to believe the result, the investor ***really need to believe*** ***in the theory*** of ""efficient market hypothesis"" where it states past data can tell future data's performance? - as I progress through these ML methods, I am certainly more inclined that the market is based on the random walk theory, thus no correct pattern can be predicted in future? (because it (i.e. the ML results and the mechanism that gave the results) just doesn't make sense to me)

I understand that ANN (regardless of its variance), uses ""weights"" to detect pattern from the dataset (the dataset can be anything from pictures, financial data, etc.); thus this is how ANN learns i.e. it goes through some iterations to update the weights until the error reaches some acceptable value. Once we understand what  weights randomisation, local minima, gradient descent, the need to have very large data for training, etc., the trader may see the limits of MLs such as ANN, and I think the model alone is not enough (and expensive) to make the call for buying/ selling stocks?

Or am I missing something here?",t2_3z6gqvrh,False,,0,False,Should MLs in Stock Market prediction be taken seriously?,[],r/learnmachinelearning,False,6,,0,,False,t3_g818c5,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,1587849133.0,,[],{},,,True,,1587877173.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Alright, so I just went through some ANN codes I developed on Python and R; when I got my results for my stock market prediction, I do wonder if I can rely on these results to make decisions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I just used the most fundamental feature which is &lt;strong&gt;&lt;em&gt;% of&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;ROI&lt;/em&gt;&lt;/strong&gt; for each day from several years; but from my POV, even if I do put in more data (e.g. sentiment features, etc.), how could an ML model predict the stock market correctly when there are just &lt;strong&gt;&lt;em&gt;so many things that can change&lt;/em&gt;&lt;/strong&gt; in its environment?&lt;/li&gt;
&lt;li&gt;Suppose, even if it has 70-80% accuracy and used K-fold testing or whatnot, this is based on &lt;strong&gt;&lt;em&gt;the data it&amp;#39;s trained on - isn&amp;#39;t this an artificial and risky conclusion?&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;And if we really want to believe the result, the investor &lt;strong&gt;&lt;em&gt;really need to believe&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;in the theory&lt;/em&gt;&lt;/strong&gt; of &amp;quot;efficient market hypothesis&amp;quot; where it states past data can tell future data&amp;#39;s performance? - as I progress through these ML methods, I am certainly more inclined that the market is based on the random walk theory, thus no correct pattern can be predicted in future? (because it (i.e. the ML results and the mechanism that gave the results) just doesn&amp;#39;t make sense to me)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I understand that ANN (regardless of its variance), uses &amp;quot;weights&amp;quot; to detect pattern from the dataset (the dataset can be anything from pictures, financial data, etc.); thus this is how ANN learns i.e. it goes through some iterations to update the weights until the error reaches some acceptable value. Once we understand what  weights randomisation, local minima, gradient descent, the need to have very large data for training, etc., the trader may see the limits of MLs such as ANN, and I think the model alone is not enough (and expensive) to make the call for buying/ selling stocks?&lt;/p&gt;

&lt;p&gt;Or am I missing something here?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g818c5,True,,runnersgo,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g818c5/should_mls_in_stock_market_prediction_be_taken/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g818c5/should_mls_in_stock_market_prediction_be_taken/,155203,1587848373.0,0,,False,,,,
,learnmachinelearning,"Hey guys, sorry if this seems like a very noob-like question but I just never learned how to deploy a neural network model into a public game like Minecraft or rocket league.

Iâ€™ve made some neural nets for image identification, and Iâ€™ve even made some that run in my own, self-programmed games, but never a public game like Minecraft. 

Can someone please point me in the right direction? A tutorial or something that explains it well would be preferred. Thanks!",t2_5c2uqup1,False,,0,False,Deploy into outside game,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g87ac4,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587900635.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, sorry if this seems like a very noob-like question but I just never learned how to deploy a neural network model into a public game like Minecraft or rocket league.&lt;/p&gt;

&lt;p&gt;Iâ€™ve made some neural nets for image identification, and Iâ€™ve even made some that run in my own, self-programmed games, but never a public game like Minecraft. &lt;/p&gt;

&lt;p&gt;Can someone please point me in the right direction? A tutorial or something that explains it well would be preferred. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g87ac4,True,,Real_MichaelCera,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g87ac4/deploy_into_outside_game/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g87ac4/deploy_into_outside_game/,155203,1587871835.0,0,,False,,,,
,learnmachinelearning,,t2_4amqi1oj,False,,0,False,Bayes Theorem - Drone Spotting using Machine Learning,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g7z4pm,False,dark,1.0,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/pfu0sUdnePc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Bayes Theorem - Drone Spotting using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/pfu0sUdnePc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/pfu0sUdnePc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/pfu0sUdnePc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g7z4pm', 'height': 338}",,False,4,,True,https://b.thumbs.redditmedia.com/eNgA4J7xeycBIWTUJ1V27oNorgxkQYXU8co5nsCoJjE.jpg,False,,[],{},rich:video,,False,,1587869941.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2rwGwIlfy2UQFLwiT95gp43Y-s8NEIDtoU83wwQPP90.jpg?auto=webp&amp;s=3b798849fe624ef9f7cbb130c5d813a1c0bd4c52', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/2rwGwIlfy2UQFLwiT95gp43Y-s8NEIDtoU83wwQPP90.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3ed508c3b20067c9187ff358f04e7df4697f697', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/2rwGwIlfy2UQFLwiT95gp43Y-s8NEIDtoU83wwQPP90.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8cb387e134cc69e37f9ebe8971d51de95aaf4594', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/2rwGwIlfy2UQFLwiT95gp43Y-s8NEIDtoU83wwQPP90.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b324ffaac3e27e2e10b1e4e1e7f1b93d8bf39797', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'uox6uktQvsI5oZ8z6k3zZveZSmp33GtzSJyGxlDqN4E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7z4pm,True,,burdin271,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7z4pm/bayes_theorem_drone_spotting_using_machine/,all_ads,False,https://youtu.be/pfu0sUdnePc,155203,1587841141.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Bayes Theorem - Drone Spotting using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/pfu0sUdnePc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/pfu0sUdnePc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I would love to start work on a personal, Ai assistant. For organising and automation. Is it possible for me to build a google asssistant/Alexa style system that I can run from my home server and can ideally learn from me and improve? Where the heck do I start, and what would the steps be to give it full functionality?",t2_alkduh,False,,0,False,J.A.R.V.I.S | yep I said it. As someone with some coding knowledge and computer knowledge... where do you start?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8624r,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587895371.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I would love to start work on a personal, Ai assistant. For organising and automation. Is it possible for me to build a google asssistant/Alexa style system that I can run from my home server and can ideally learn from me and improve? Where the heck do I start, and what would the steps be to give it full functionality?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8624r,True,,Nickgregoryyoutube,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8624r/jarvis_yep_i_said_it_as_someone_with_some_coding/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8624r/jarvis_yep_i_said_it_as_someone_with_some_coding/,155203,1587866571.0,0,,False,,,,
,learnmachinelearning," ""This could be especially useful in elderly populations, where viral infections have higher rates of morbidity and mortality,"" **blogged chief executive Sundar Pichai**.

Google's life-science research arm, Verily is developing a small body-worn temperature patch that transmits data to a phone app. We can say this is AI but what are the chances that it will give a accurate measure, and what are the chances that people even know about this application.

YouTube, meanwhile, is using its homepage to direct users to the World Health Organization and other groups, for education and information, while working to remove videos suggesting alternative cures as soon as they go live. How this is even AI ??

What are your suggestion about deployment of AI in COVID-19 battle??",t2_69s564sx,False,,0,False,"What?? Deployment of AI in battle of AI, is that even possible?",[],r/learnmachinelearning,False,6,,0,,False,t3_g88uz8,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587907770.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;quot;This could be especially useful in elderly populations, where viral infections have higher rates of morbidity and mortality,&amp;quot; &lt;strong&gt;blogged chief executive Sundar Pichai&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Google&amp;#39;s life-science research arm, Verily is developing a small body-worn temperature patch that transmits data to a phone app. We can say this is AI but what are the chances that it will give a accurate measure, and what are the chances that people even know about this application.&lt;/p&gt;

&lt;p&gt;YouTube, meanwhile, is using its homepage to direct users to the World Health Organization and other groups, for education and information, while working to remove videos suggesting alternative cures as soon as they go live. How this is even AI ??&lt;/p&gt;

&lt;p&gt;What are your suggestion about deployment of AI in COVID-19 battle??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g88uz8,True,,ImZee819,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g88uz8/what_deployment_of_ai_in_battle_of_ai_is_that/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g88uz8/what_deployment_of_ai_in_battle_of_ai_is_that/,155203,1587878970.0,0,,False,,,,
,learnmachinelearning,"Hey r/learnmachinelearning! I run a youtube channel applying machine learning concepts to online games like Runescape and Neopets. I posted here awhile back and had an amazing reception (thank you!). 

Anyway, Iâ€™ve had a few students reach out to me wishing theyâ€™d have the time of day to work on projects like these but they are worried that the projects will not be taken as seriously as other projects would. Itâ€™s especially worrying during this pandemic with jobs being scarce and their future being uncertain which I can definitely empathize with.

That said, I wanted to ask professionals/students alike in this community a couple questions regarding projects that involve games:

1. In the context of hiring, are DS/ML projects that involve games as valid as projects that donâ€™t involve games?
2. For side projects in general, how long would a typically good project take? Does the time frame really matter?
3. What do recruiters/managers look for to measure good understanding in these DS/ML projects?
4. If youâ€™re a student, what would be your concerns with projects like these? 

I personally think that properly applying DS/ML principles in an end-to-end project would be a valid talking point in any context. However, Iâ€™m a little biased so I definitely wanted to see what people thought. Please let me know and thank you for your time!",t2_3tqetf9o,False,,0,False,Should Students Put DS/ML Projects That Involve Games On Their Resume? Will They Be Taken Seriously?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7u71s,False,dark,0.9,,public,8,0,{},,,False,[],,False,False,,{},Question,False,8,,True,self,False,,[],{},,,True,,1587852959.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey &lt;a href=""/r/learnmachinelearning""&gt;r/learnmachinelearning&lt;/a&gt;! I run a youtube channel applying machine learning concepts to online games like Runescape and Neopets. I posted here awhile back and had an amazing reception (thank you!). &lt;/p&gt;

&lt;p&gt;Anyway, Iâ€™ve had a few students reach out to me wishing theyâ€™d have the time of day to work on projects like these but they are worried that the projects will not be taken as seriously as other projects would. Itâ€™s especially worrying during this pandemic with jobs being scarce and their future being uncertain which I can definitely empathize with.&lt;/p&gt;

&lt;p&gt;That said, I wanted to ask professionals/students alike in this community a couple questions regarding projects that involve games:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In the context of hiring, are DS/ML projects that involve games as valid as projects that donâ€™t involve games?&lt;/li&gt;
&lt;li&gt;For side projects in general, how long would a typically good project take? Does the time frame really matter?&lt;/li&gt;
&lt;li&gt;What do recruiters/managers look for to measure good understanding in these DS/ML projects?&lt;/li&gt;
&lt;li&gt;If youâ€™re a student, what would be your concerns with projects like these? &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I personally think that properly applying DS/ML principles in an end-to-end project would be a valid talking point in any context. However, Iâ€™m a little biased so I definitely wanted to see what people thought. Please let me know and thank you for your time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7u71s,True,,chriskok1337,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7u71s/should_students_put_dsml_projects_that_involve/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7u71s/should_students_put_dsml_projects_that_involve/,155203,1587824159.0,0,,False,,,,
,learnmachinelearning,"Hello everyone

I am trying to make an anomaly detection algorithm that looks at a time series data set that takes place over 20 years. I want to just analyze that data. The problem is that I want to be able to 20 different variables. Sometimes the variables will be related to each other and cause the anomaly in both, while other times it will have nothing to do with each other. Also, would it be possible to find a pattern when those anomalies occur and find when that ""anomaly pattern"" occurs in the rest of the data set?",t2_yd07i,False,,0,False,Question about anomaly detection,[],r/learnmachinelearning,False,6,,0,,False,t3_g85m3p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587893556.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone&lt;/p&gt;

&lt;p&gt;I am trying to make an anomaly detection algorithm that looks at a time series data set that takes place over 20 years. I want to just analyze that data. The problem is that I want to be able to 20 different variables. Sometimes the variables will be related to each other and cause the anomaly in both, while other times it will have nothing to do with each other. Also, would it be possible to find a pattern when those anomalies occur and find when that &amp;quot;anomaly pattern&amp;quot; occurs in the rest of the data set?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g85m3p,True,,Seth219,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g85m3p/question_about_anomaly_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g85m3p/question_about_anomaly_detection/,155203,1587864756.0,0,,False,,,,
,learnmachinelearning,,t2_4cohyf78,False,,0,False,Is there still being researched on NAS-nets or are they just dead?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7z5zd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587870056.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7z5zd,True,,mariusjohan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7z5zd/is_there_still_being_researched_on_nasnets_or_are/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7z5zd/is_there_still_being_researched_on_nasnets_or_are/,155203,1587841256.0,0,,False,,,,
,learnmachinelearning,"How are the algorithms developed? What are the models used? What about factors like lighting, different angles, etc?",t2_1p0afsx0,False,,0,False,Absolute beginner here. Can someone explain to me the math behind facial recognition please?,[],r/learnmachinelearning,False,6,,0,,False,t3_g84zob,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587891038.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How are the algorithms developed? What are the models used? What about factors like lighting, different angles, etc?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g84zob,True,,citizenmushroom,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g84zob/absolute_beginner_here_can_someone_explain_to_me/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g84zob/absolute_beginner_here_can_someone_explain_to_me/,155203,1587862238.0,0,,False,,,,
,learnmachinelearning,,t2_xtuyc,False,,0,False,Has anyone ever computed the macro roc and micro roc for their data? Did you find it to be more informative/useful than the standard roc? Thanks!,[],r/learnmachinelearning,False,6,,0,108.0,False,t3_g83n5y,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/LHvZEDKEtTLmvxUKT94_56EzqnEIHCe_5d-PZPaOhDk.jpg,False,,[],{},image,,False,,1587885901.0,text,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?auto=webp&amp;s=31bed6802b149f67119ae8b6dfde7f11090c3e62', 'width': 1080, 'height': 839}, 'resolutions': [{'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=29be18cdb250198094892c863e5bbb0f8e7c993e', 'width': 108, 'height': 83}, {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bfd8a79b192dcfce6ecda2045eda5f081c4db9d', 'width': 216, 'height': 167}, {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f599e57af90badbc840306ded7eb2f81e4270063', 'width': 320, 'height': 248}, {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d01290cd1dab21ddf7ad385e8bcea6389cccfe9', 'width': 640, 'height': 497}, {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=99ea763234d7d8c344aacbc901847bb4eeb1e1c5', 'width': 960, 'height': 745}, {'url': 'https://preview.redd.it/kai4utmvs1v41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b52edf32d2107c3415415cb0e276733fbe343139', 'width': 1080, 'height': 839}], 'variants': {}, 'id': 'akPFz-Ncz8xKmcYvtWSUESe0LUY71x0H9SKNZiNZ_nk'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g83n5y,True,,ottawalanguages,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g83n5y/has_anyone_ever_computed_the_macro_roc_and_micro/,all_ads,False,https://i.redd.it/kai4utmvs1v41.jpg,155203,1587857101.0,0,,False,,,,True
,learnmachinelearning,"I got bored using Siri and Google Assistant with only text and voice. There should be a level up in the user experience of chatbots. Therefore I thought of writing a post on giving emotional intelligence to a chatbot using emoticons.

What do you think? Read and share your thoughts.

[https://medium.com/ai-in-plain-english/how-to-give-emotional-intelligence-to-a-chatbot-72a0500095b8](https://medium.com/ai-in-plain-english/how-to-give-emotional-intelligence-to-a-chatbot-72a0500095b8)",t2_65r9al4m,False,,0,False,Give emotional intelligence to a chatbot,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g815a1,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Project,False,0,,False,self,False,,[],{},self,,True,,1587876878.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I got bored using Siri and Google Assistant with only text and voice. There should be a level up in the user experience of chatbots. Therefore I thought of writing a post on giving emotional intelligence to a chatbot using emoticons.&lt;/p&gt;

&lt;p&gt;What do you think? Read and share your thoughts.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/ai-in-plain-english/how-to-give-emotional-intelligence-to-a-chatbot-72a0500095b8""&gt;https://medium.com/ai-in-plain-english/how-to-give-emotional-intelligence-to-a-chatbot-72a0500095b8&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?auto=webp&amp;s=1ead1e4b0c0540ab40261c8062009787eda0eaf9', 'width': 1200, 'height': 756}, 'resolutions': [{'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e51b9420017d26048cc4c37c49ad4c0bc59a518', 'width': 108, 'height': 68}, {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1259ae89b869ebbd85d25fa48f2d0a73a7b1c541', 'width': 216, 'height': 136}, {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=20341ff4cc0d56a34c799961f8a6089fd9b38f61', 'width': 320, 'height': 201}, {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6e10c77d92fbaf3746b6d23d0f0ac7b5931d3b4', 'width': 640, 'height': 403}, {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=848c264713ace035eca02670e6a28dd7f73dba3e', 'width': 960, 'height': 604}, {'url': 'https://external-preview.redd.it/clf-ze-WcX8iy4zSwX5YbAk2TIxjKzlW3ZF-UEFiilE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b23751f8e6c99c1d4a15f7fedd194e17275ec35', 'width': 1080, 'height': 680}], 'variants': {}, 'id': 'bOo4x5f1qo6cwBne9kLZWjLJx556qvwIzMUj_iVpdN8'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g815a1,True,,kartikeyarana10,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g815a1/give_emotional_intelligence_to_a_chatbot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g815a1/give_emotional_intelligence_to_a_chatbot/,155203,1587848078.0,0,,False,,,,
,learnmachinelearning,"I'm trying to forecast a time series using Encoder/Decoder model, from what I understood the input should be (batch, input\_sequence\_length, features) and the target sequence (1, prediction\_length, features)

For example if I input the model \[0,1,2,4,5,6\] I expect the output to be \[7,8,9\].

Is that correct?",t2_h2akc,False,,0,False,"In an Encoder Decoder architecture, what should be the target sequence shape?",[],r/learnmachinelearning,False,6,,0,,False,t3_g80qsd,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587875454.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to forecast a time series using Encoder/Decoder model, from what I understood the input should be (batch, input_sequence_length, features) and the target sequence (1, prediction_length, features)&lt;/p&gt;

&lt;p&gt;For example if I input the model [0,1,2,4,5,6] I expect the output to be [7,8,9].&lt;/p&gt;

&lt;p&gt;Is that correct?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g80qsd,True,,maroxtn,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g80qsd/in_an_encoder_decoder_architecture_what_should_be/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g80qsd/in_an_encoder_decoder_architecture_what_should_be/,155203,1587846654.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is a Genetic Algorithm ? | How algorithms evolve ? | Introduction for everyone,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g80bg1,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/tEUBWmKGDlg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is a Genetic Algorithm ? | How algorithms evolve ? | Introduction for everyone 26', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/tEUBWmKGDlg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/tEUBWmKGDlg/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/tEUBWmKGDlg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g80bg1', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/KbTLT_m_-S7tB6qkywwFh_ym5Dn0IPe4EXD2P8mhnpI.jpg,False,,[],{},rich:video,,False,,1587873970.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7WuBXOFetIvI4yoxA0ALnFeTCDRJqcvaPmWvfnxuIw4.jpg?auto=webp&amp;s=e96e39bfd35c3607138a633d82cf52467fb33226', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/7WuBXOFetIvI4yoxA0ALnFeTCDRJqcvaPmWvfnxuIw4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0148dd5ac9c8e5b7124dcc90aa84e3578ef75b68', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/7WuBXOFetIvI4yoxA0ALnFeTCDRJqcvaPmWvfnxuIw4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f8bcf5a6714b00f00b7194da26ae84d7ebdd0e1', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/7WuBXOFetIvI4yoxA0ALnFeTCDRJqcvaPmWvfnxuIw4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dbef51501e5924a123a914e9423c4e961fd48a79', 'width': 320, 'height': 240}], 'variants': {}, 'id': '8qd49JhWVqRiZNYTPbdTG7Yp2tMoSISBXnsQy5fFv8A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g80bg1,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g80bg1/what_is_a_genetic_algorithm_how_algorithms_evolve/,all_ads,False,https://www.youtube.com/watch?v=tEUBWmKGDlg,155203,1587845170.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is a Genetic Algorithm ? | How algorithms evolve ? | Introduction for everyone 26', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/tEUBWmKGDlg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/tEUBWmKGDlg/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I had begun with this book as an initial contact with neural networks. I'm a software engineer with basic AI background, when I studied it back in the days when I was at university. Back then, I hated it because of the awful teacher I had, and I just wanted to get rid of this topic. Now, I want to get back into it, being specially interested on neural networks, and so far I'm thrilled. The fact of liking math (although I'm rusty, it's a lot of years since I took calculus) also helps.

I'm currently on the last chapter of the book where he talks about deep learning and convolutional neural networks, and I want to read it thoroughly, but I would like to have an upfront outlook of my possible learning paths. My achievement so far has been to develop a NN from scratch, without having any actual code as reference, with an accuracy of 97,47% for the MNIST data. I know it can be improvable, but I think it's a fair start.

So basically I'm wondering of what can be next to keep learning on this topic. It can be everything: books, papers, blogs, reddit posts, etc. Something important I want to point out is that I'd like to implement everything I'm going to work with, i.e. not jumping to Keras or TF right away and start doing stuff without understanding what's going on behind the scenes.

Thank you beforehand!

PD: in case someone doesn't know about this book, here it is: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) I found it awesome, completely accessible for beginners on the topic.",t2_qv4ta45,False,,0,False,"What comes after Michael Nielsen's ""Neural Networks and Deep Learning"" book?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7w7h3,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},self,,True,,1587860400.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had begun with this book as an initial contact with neural networks. I&amp;#39;m a software engineer with basic AI background, when I studied it back in the days when I was at university. Back then, I hated it because of the awful teacher I had, and I just wanted to get rid of this topic. Now, I want to get back into it, being specially interested on neural networks, and so far I&amp;#39;m thrilled. The fact of liking math (although I&amp;#39;m rusty, it&amp;#39;s a lot of years since I took calculus) also helps.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m currently on the last chapter of the book where he talks about deep learning and convolutional neural networks, and I want to read it thoroughly, but I would like to have an upfront outlook of my possible learning paths. My achievement so far has been to develop a NN from scratch, without having any actual code as reference, with an accuracy of 97,47% for the MNIST data. I know it can be improvable, but I think it&amp;#39;s a fair start.&lt;/p&gt;

&lt;p&gt;So basically I&amp;#39;m wondering of what can be next to keep learning on this topic. It can be everything: books, papers, blogs, reddit posts, etc. Something important I want to point out is that I&amp;#39;d like to implement everything I&amp;#39;m going to work with, i.e. not jumping to Keras or TF right away and start doing stuff without understanding what&amp;#39;s going on behind the scenes.&lt;/p&gt;

&lt;p&gt;Thank you beforehand!&lt;/p&gt;

&lt;p&gt;PD: in case someone doesn&amp;#39;t know about this book, here it is: &lt;a href=""http://neuralnetworksanddeeplearning.com/""&gt;http://neuralnetworksanddeeplearning.com/&lt;/a&gt; I found it awesome, completely accessible for beginners on the topic.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?auto=webp&amp;s=a8cffd3e23c129ddb2dd6c593216a062fb8f1380', 'width': 200, 'height': 214}, 'resolutions': [{'url': 'https://external-preview.redd.it/EjfpIanDwagwM-0h1NRBfRUwdEhvQbrGy1nVto8ZieU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=06a541c2dd540bb4b74af834f56661b7512cc87c', 'width': 108, 'height': 115}], 'variants': {}, 'id': '_BeH_HLgJJJyvbJXuBeembjdUClSovixHNQF4PeqIeE'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7w7h3,True,,el_juli,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7w7h3/what_comes_after_michael_nielsens_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7w7h3/what_comes_after_michael_nielsens_neural_networks/,155203,1587831600.0,0,,False,,,,
,learnmachinelearning,"Right now, I am taking Andrew Ng's ML course on coursera and I am at week 5 and I cannot understand anything about Neural Networks. Is there any other website or course or a video where I can understand the concepts in better detail? Even though I am unclear about it, NN seems like an interesting topic to study about.",t2_4soo1cf0,False,,0,False,Where can I learn neural network from scratch?,[],r/learnmachinelearning,False,6,,0,,False,t3_g8039u,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587873164.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Right now, I am taking Andrew Ng&amp;#39;s ML course on coursera and I am at week 5 and I cannot understand anything about Neural Networks. Is there any other website or course or a video where I can understand the concepts in better detail? Even though I am unclear about it, NN seems like an interesting topic to study about.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g8039u,True,,gxrlxxn,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g8039u/where_can_i_learn_neural_network_from_scratch/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g8039u/where_can_i_learn_neural_network_from_scratch/,155203,1587844364.0,0,,False,,,,
,learnmachinelearning,,t2_5dk1rkk2,False,,0,False,Are you satisfied with your job as a Data scientist / machine learning engineer?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7bbxl,False,dark,0.99,,public,144,0,{},,,False,[],,False,False,,{},,False,144,,False,self,False,,[],{},,,True,,1587774096.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7bbxl,True,,shawn2james,,63,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7bbxl/are_you_satisfied_with_your_job_as_a_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7bbxl/are_you_satisfied_with_your_job_as_a_data/,155203,1587745296.0,0,,False,,,,
,learnmachinelearning," So I have been reading a lot of articles and papers on abstractive text summarization models and I have followed some tutorials to make my own. My question is now that I have built this model and trained it on a dataset how can I use it with any text I want,

Any help appreciated, Thank you!",t2_1o1u69zq,False,,0,False,Abstractive text summarization,[],r/learnmachinelearning,False,6,,0,,False,t3_g7z12j,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587869609.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I have been reading a lot of articles and papers on abstractive text summarization models and I have followed some tutorials to make my own. My question is now that I have built this model and trained it on a dataset how can I use it with any text I want,&lt;/p&gt;

&lt;p&gt;Any help appreciated, Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7z12j,True,,NotAPowVirgin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7z12j/abstractive_text_summarization/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7z12j/abstractive_text_summarization/,155203,1587840809.0,0,,False,,,,
,learnmachinelearning,"Hi /r/learnmachinelearning,    
I am currently trying to understand which parts of my GAN-based model are important for the results I am getting. I have tuned the hyperparameters with bayesian optimization and now would like to change/omit some parts of the model, e.g. perform an ablation study. However I am not sure whether I should perform again hyperparameter tuning after changing the model or keep the same hyperparameters for the non-changing parts of the model. What is the proper way to do the ablation study? Thanks!",t2_r0ub0,False,,0,False,Ablation study &amp; hyperparameter optimization,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7ytbe,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587868917.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi &lt;a href=""/r/learnmachinelearning""&gt;/r/learnmachinelearning&lt;/a&gt;,&lt;br/&gt;
I am currently trying to understand which parts of my GAN-based model are important for the results I am getting. I have tuned the hyperparameters with bayesian optimization and now would like to change/omit some parts of the model, e.g. perform an ablation study. However I am not sure whether I should perform again hyperparameter tuning after changing the model or keep the same hyperparameters for the non-changing parts of the model. What is the proper way to do the ablation study? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7ytbe,True,,th3owner,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7ytbe/ablation_study_hyperparameter_optimization/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7ytbe/ablation_study_hyperparameter_optimization/,155203,1587840117.0,0,,False,,,,
,learnmachinelearning," Hi redditers,

I'm working on the Caltech-UCSD Birds dataset and I'm using a Faster R-CNN with resnet50 pretrained on ImageNet as backbone. I want to test if the use of the iNaturalist dataset helps the model to have a smaller overfitting. Do you think it is a good idea? And is there a place where I can find a resnet pretrained on iNat?",t2_30b4spgy,False,,0,False,Resnet50 pretrained on iNaturalist not from scratch,[],r/learnmachinelearning,False,6,,0,,False,t3_g7ypre,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587868602.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi redditers,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on the Caltech-UCSD Birds dataset and I&amp;#39;m using a Faster R-CNN with resnet50 pretrained on ImageNet as backbone. I want to test if the use of the iNaturalist dataset helps the model to have a smaller overfitting. Do you think it is a good idea? And is there a place where I can find a resnet pretrained on iNat?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7ypre,True,,aleflabo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7ypre/resnet50_pretrained_on_inaturalist_not_from/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7ypre/resnet50_pretrained_on_inaturalist_not_from/,155203,1587839802.0,0,,False,,,,
,learnmachinelearning,"That is, the computer has a large selection of moves it can make but only knows whether it has won or lost if the game is over.",t2_2xz6764h,False,,0,False,How do you teach a computer a game where it is unclear how good a single move was?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7yf65,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587867634.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;That is, the computer has a large selection of moves it can make but only knows whether it has won or lost if the game is over.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7yf65,True,,User1377420,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7yf65/how_do_you_teach_a_computer_a_game_where_it_is/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7yf65/how_do_you_teach_a_computer_a_game_where_it_is/,155203,1587838834.0,0,,False,,,,
,learnmachinelearning," 

Hello,

I would like to collaborate with someone on a machine learning project to get a feedback loop and to learn new things.

Possible areas of interest: soccer matches, basketball matches, stocks, personality profiles. Possibly something with health as well, however my knowledge of the domain is quite limited.

I have skills in Python, namely Pandas. Some of my code, a predictor of football matches is available at: [https://github.com/BorutFlis/predictor](https://github.com/BorutFlis/predictor)",t2_63px4pzu,False,,0,False,Anybody wants to team up for a machine learning project?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7y72y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587866905.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I would like to collaborate with someone on a machine learning project to get a feedback loop and to learn new things.&lt;/p&gt;

&lt;p&gt;Possible areas of interest: soccer matches, basketball matches, stocks, personality profiles. Possibly something with health as well, however my knowledge of the domain is quite limited.&lt;/p&gt;

&lt;p&gt;I have skills in Python, namely Pandas. Some of my code, a predictor of football matches is available at: &lt;a href=""https://github.com/BorutFlis/predictor""&gt;https://github.com/BorutFlis/predictor&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7y72y,True,,BorutFlis,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7y72y/anybody_wants_to_team_up_for_a_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7y72y/anybody_wants_to_team_up_for_a_machine_learning/,155203,1587838105.0,0,,False,,,,
,learnmachinelearning," I have tried normalization, activation functions, costs, different weight initialization methods, and still the network doesn't want to converge. I really don't know what to do. Anyone who has the time and wants to help here is the code: 

    import numpy as np
    import pandas as pd
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split
    from tqdm.notebook import tqdm
    
    class Layer():
    
        #Activation Functions and their derivatives
        @staticmethod
        def sigmoid(x):
            return np.where(x &gt;= 0, 1. / (1. + np.exp(-x)), np.exp(x) / (1. + np.exp(x)))
    
        @staticmethod
        def dsigmoid(x):
            return Layer.sigmoid(x) * (1. - Layer.sigmoid(x))  
    
        @staticmethod
        def softmax(x):
            exp = np.exp(x - np.max(x))
            return exp / np.sum(exp)
    
        @staticmethod
        def dsoftmax(x):
            return Layer.softmax(x) * (1. - Layer.softmax(x))
    
        @staticmethod
        def relu(x):
            return np.where(x&gt;=0, x, 0)
    
        @staticmethod
        def drelu(x):
            return np.where(x&gt;=0, 1, 0)
    
        @staticmethod
        def leaky_relu(x):
            return np.where(x&gt;=0, x, 0.01*x)
    
        @staticmethod
        def dleaky_relu(x):
            return np.where(x&gt;=0, 1, 0.01)
    
        functions = {
            'sigmoid': sigmoid.__func__,
            'softmax': softmax.__func__,
            'relu': relu.__func__,
            'leaky_relu': leaky_relu.__func__,
            'input': np.nan
        }
    
        dfunctions = {
            'sigmoid': dsigmoid.__func__,
            'softmax': dsoftmax.__func__,
            'relu': drelu.__func__,
            'leaky_relu': dleaky_relu.__func__,
            'input': np.nan
        }
    
        #Layar init, weights and other ??? are assigned later
        def __init__(self, n_nodes, activation='sigmoid'):
            self.n_nodes = n_nodes
            self.activation = Layer.functions[activation]
            self.dactivation = Layer.dfunctions[activation]
    
    class NeuralNetwork():
    
        #Cost functions and their derivatives
        @staticmethod
        def quadratic(y, pred):
            return np.sum((y - pred) ** 2) * 0.5
    
        @staticmethod
        def dquadratic(y, pred):
            return pred - y
    
        @staticmethod
        def cross_entropy(y, pred):
            return -1. * np.dot(y, np.log(pred))
    
        @staticmethod
        def dcross_entropy(y, pred):
            return pred - y
    
        costs = {
            'quadratic': quadratic.__func__,
            'log_loss': cross_entropy.__func__,
            'cross_entropy': cross_entropy.__func__
        }
    
        dcosts = {
            'quadratic': dquadratic.__func__,
            'log_loss': dcross_entropy.__func__,
            'cross_entropy': dcross_entropy.__func__
        }
    
        def __init__(self, cost, lr=0.03, epochs=10, verbose=True):
            self.lr = lr
            self.epochs = epochs
            self.verbose = verbose
            self.cost = NeuralNetwork.costs[cost]
            self.dcost = NeuralNetwork.dcosts[cost]
    
        def fit(self, X, y, layers):
            self.X = X
            self.y = y
    
            #Input layer
            self.layers = []
            self.layers.append(Layer(self.X[0].shape[0], 'input'))
    
            #Initialize layers and weights
            for index, layer in enumerate(layers):                                               
                layer.n_prev = self.layers[index].n_nodes
    
                layer.weights = np.random.rand(layer.n_nodes, layer.n_prev) * np.sqrt(2. / layer.n_nodes+layer.n_prev)
    
                self.layers.append(layer)
    
    
            for epoch in range(self.epochs):
                if self.verbose==True:
                    print(f'Epoch: {epoch+1}')
                epoch_cost = 0
    
                for X, y in tqdm(zip(self.X, self.y), total=self.X.shape[0]):
                    self.layers[0].a = X
    
                    #Propagation
                    for layer in self.layers[1:]:
                        layer.z = np.dot(layer.weights, self.layers[self.layers.index(layer)-1].a)
                        try:
                            layer.a = layer.activation(layer.z)
                        except RuntimeWarning:
                            print(layer.z)
    
                    #Errors
                    self.layers[-1].a[self.layers[-1].a==0] = 10 ** -10
                    epoch_cost += self.cost(y, self.layers[-1].a)
                    self.layers[-1].error = np.multiply(self.dcost(y, self.layers[-1].a),
                                            self.layers[-1].dactivation(self.layers[-1].z))
                    #print(self.layers[-1].error)
    
                    for layer in reversed(self.layers[1:-1]):
                        layer.error = (np.dot(np.transpose(self.layers[self.layers.index(layer)+1].weights),
                                self.layers[self.layers.index(layer)+1].error)) * layer.dactivation(layer.z)
    
    
                    #Gradients and update weights
                    for layer in self.layers[1:]:
                        layer.gradients = np.dot(layer.error.reshape(-1, 1),
                                          self.layers[self.layers.index(layer)-1].a.reshape(1, -1))
    
                        layer.weights -= self.lr * layer.gradients
    
    
                #ADD METRICS
                if self.verbose==True:
                    print(f'Cost: {(epoch_cost / self.y.shape[0]).round(4)}')
    
    
        def predict(self, X):
            self.X_test = X
    
            predictions = []
            for X in self.X_test:
                #Input
                self.layers[0].a = X
    
                #Propagation
                for layer in self.layers[1:]:
                    layer.z = np.dot(layer.weights, self.layers[self.layers.index(layer)-1].a)
                    layer.a = layer.activation(layer.z)
    
                predictions.append(np.where(self.layers[-1].a&gt;0.5, 1, 0))
    
            return predictions
    
    
    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
    y_classes = np.copy(y)
    y = np.array(pd.get_dummies(y))
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    #Normalize
    X_train, X_test = X_train / 255., X_test / 255.
    
    
    nn = NeuralNetwork(lr=0.3, epochs=10, cost='cross_entropy')
    layers = [
        Layer(200, 'leaky_relu'),
        Layer(100, 'leaky_relu'),
        Layer(50, 'leaky_relu'),
        Layer(10, 'softmax')
    ]
    nn.fit(X_train, y_train, layers)

 Note that this code works when applied to a binary classification problem with simple sigmoid activation functions.",t2_b3i55f5,False,,0,False,I can't get my Neural Network to work,[],r/learnmachinelearning,False,6,,0,,False,t3_g7xrgg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587865515.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have tried normalization, activation functions, costs, different weight initialization methods, and still the network doesn&amp;#39;t want to converge. I really don&amp;#39;t know what to do. Anyone who has the time and wants to help here is the code: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm

class Layer():

    #Activation Functions and their derivatives
    @staticmethod
    def sigmoid(x):
        return np.where(x &amp;gt;= 0, 1. / (1. + np.exp(-x)), np.exp(x) / (1. + np.exp(x)))

    @staticmethod
    def dsigmoid(x):
        return Layer.sigmoid(x) * (1. - Layer.sigmoid(x))  

    @staticmethod
    def softmax(x):
        exp = np.exp(x - np.max(x))
        return exp / np.sum(exp)

    @staticmethod
    def dsoftmax(x):
        return Layer.softmax(x) * (1. - Layer.softmax(x))

    @staticmethod
    def relu(x):
        return np.where(x&amp;gt;=0, x, 0)

    @staticmethod
    def drelu(x):
        return np.where(x&amp;gt;=0, 1, 0)

    @staticmethod
    def leaky_relu(x):
        return np.where(x&amp;gt;=0, x, 0.01*x)

    @staticmethod
    def dleaky_relu(x):
        return np.where(x&amp;gt;=0, 1, 0.01)

    functions = {
        &amp;#39;sigmoid&amp;#39;: sigmoid.__func__,
        &amp;#39;softmax&amp;#39;: softmax.__func__,
        &amp;#39;relu&amp;#39;: relu.__func__,
        &amp;#39;leaky_relu&amp;#39;: leaky_relu.__func__,
        &amp;#39;input&amp;#39;: np.nan
    }

    dfunctions = {
        &amp;#39;sigmoid&amp;#39;: dsigmoid.__func__,
        &amp;#39;softmax&amp;#39;: dsoftmax.__func__,
        &amp;#39;relu&amp;#39;: drelu.__func__,
        &amp;#39;leaky_relu&amp;#39;: dleaky_relu.__func__,
        &amp;#39;input&amp;#39;: np.nan
    }

    #Layar init, weights and other ??? are assigned later
    def __init__(self, n_nodes, activation=&amp;#39;sigmoid&amp;#39;):
        self.n_nodes = n_nodes
        self.activation = Layer.functions[activation]
        self.dactivation = Layer.dfunctions[activation]

class NeuralNetwork():

    #Cost functions and their derivatives
    @staticmethod
    def quadratic(y, pred):
        return np.sum((y - pred) ** 2) * 0.5

    @staticmethod
    def dquadratic(y, pred):
        return pred - y

    @staticmethod
    def cross_entropy(y, pred):
        return -1. * np.dot(y, np.log(pred))

    @staticmethod
    def dcross_entropy(y, pred):
        return pred - y

    costs = {
        &amp;#39;quadratic&amp;#39;: quadratic.__func__,
        &amp;#39;log_loss&amp;#39;: cross_entropy.__func__,
        &amp;#39;cross_entropy&amp;#39;: cross_entropy.__func__
    }

    dcosts = {
        &amp;#39;quadratic&amp;#39;: dquadratic.__func__,
        &amp;#39;log_loss&amp;#39;: dcross_entropy.__func__,
        &amp;#39;cross_entropy&amp;#39;: dcross_entropy.__func__
    }

    def __init__(self, cost, lr=0.03, epochs=10, verbose=True):
        self.lr = lr
        self.epochs = epochs
        self.verbose = verbose
        self.cost = NeuralNetwork.costs[cost]
        self.dcost = NeuralNetwork.dcosts[cost]

    def fit(self, X, y, layers):
        self.X = X
        self.y = y

        #Input layer
        self.layers = []
        self.layers.append(Layer(self.X[0].shape[0], &amp;#39;input&amp;#39;))

        #Initialize layers and weights
        for index, layer in enumerate(layers):                                               
            layer.n_prev = self.layers[index].n_nodes

            layer.weights = np.random.rand(layer.n_nodes, layer.n_prev) * np.sqrt(2. / layer.n_nodes+layer.n_prev)

            self.layers.append(layer)


        for epoch in range(self.epochs):
            if self.verbose==True:
                print(f&amp;#39;Epoch: {epoch+1}&amp;#39;)
            epoch_cost = 0

            for X, y in tqdm(zip(self.X, self.y), total=self.X.shape[0]):
                self.layers[0].a = X

                #Propagation
                for layer in self.layers[1:]:
                    layer.z = np.dot(layer.weights, self.layers[self.layers.index(layer)-1].a)
                    try:
                        layer.a = layer.activation(layer.z)
                    except RuntimeWarning:
                        print(layer.z)

                #Errors
                self.layers[-1].a[self.layers[-1].a==0] = 10 ** -10
                epoch_cost += self.cost(y, self.layers[-1].a)
                self.layers[-1].error = np.multiply(self.dcost(y, self.layers[-1].a),
                                        self.layers[-1].dactivation(self.layers[-1].z))
                #print(self.layers[-1].error)

                for layer in reversed(self.layers[1:-1]):
                    layer.error = (np.dot(np.transpose(self.layers[self.layers.index(layer)+1].weights),
                            self.layers[self.layers.index(layer)+1].error)) * layer.dactivation(layer.z)


                #Gradients and update weights
                for layer in self.layers[1:]:
                    layer.gradients = np.dot(layer.error.reshape(-1, 1),
                                      self.layers[self.layers.index(layer)-1].a.reshape(1, -1))

                    layer.weights -= self.lr * layer.gradients


            #ADD METRICS
            if self.verbose==True:
                print(f&amp;#39;Cost: {(epoch_cost / self.y.shape[0]).round(4)}&amp;#39;)


    def predict(self, X):
        self.X_test = X

        predictions = []
        for X in self.X_test:
            #Input
            self.layers[0].a = X

            #Propagation
            for layer in self.layers[1:]:
                layer.z = np.dot(layer.weights, self.layers[self.layers.index(layer)-1].a)
                layer.a = layer.activation(layer.z)

            predictions.append(np.where(self.layers[-1].a&amp;gt;0.5, 1, 0))

        return predictions


X, y = fetch_openml(&amp;#39;mnist_784&amp;#39;, version=1, return_X_y=True)
y_classes = np.copy(y)
y = np.array(pd.get_dummies(y))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#Normalize
X_train, X_test = X_train / 255., X_test / 255.


nn = NeuralNetwork(lr=0.3, epochs=10, cost=&amp;#39;cross_entropy&amp;#39;)
layers = [
    Layer(200, &amp;#39;leaky_relu&amp;#39;),
    Layer(100, &amp;#39;leaky_relu&amp;#39;),
    Layer(50, &amp;#39;leaky_relu&amp;#39;),
    Layer(10, &amp;#39;softmax&amp;#39;)
]
nn.fit(X_train, y_train, layers)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this code works when applied to a binary classification problem with simple sigmoid activation functions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7xrgg,True,,rLoper,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7xrgg/i_cant_get_my_neural_network_to_work/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7xrgg/i_cant_get_my_neural_network_to_work/,155203,1587836715.0,0,,False,,,,
,learnmachinelearning,"Hey guys,  


I wrote an [article](https://medium.com/@diegoagher/neuroscience-and-ai-a-beautiful-entanglement-6093cbbce382) on the relationship of Artifical Intelligence and Neuroscience. I think it's quite understandable and I feel it's a very interesting approach to think about ML and Deep Learning in general. Explore some key strategies to make algorithms able to learn.  


I do not have a neuroscience background, I have an Applied Mathematics Bachelors and currently doing a Masters in Artificial Intelligence but I find Neuro just fascinating so I'm getting my toes wet.   
And one way about really understanding concepts and research is by creating dissemination articles.  
So give it a go if you are interested and let me know your thoughts.",t2_4wzhupt2,False,,0,False,Learn about Neuroscience and Machine Learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_g7jcle,False,dark,0.93,,public,26,0,{},,,False,[],,False,False,,{},,False,26,,False,self,False,,[],{},self,,True,,1587800686.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,  &lt;/p&gt;

&lt;p&gt;I wrote an &lt;a href=""https://medium.com/@diegoagher/neuroscience-and-ai-a-beautiful-entanglement-6093cbbce382""&gt;article&lt;/a&gt; on the relationship of Artifical Intelligence and Neuroscience. I think it&amp;#39;s quite understandable and I feel it&amp;#39;s a very interesting approach to think about ML and Deep Learning in general. Explore some key strategies to make algorithms able to learn.  &lt;/p&gt;

&lt;p&gt;I do not have a neuroscience background, I have an Applied Mathematics Bachelors and currently doing a Masters in Artificial Intelligence but I find Neuro just fascinating so I&amp;#39;m getting my toes wet.&lt;br/&gt;
And one way about really understanding concepts and research is by creating dissemination articles.&lt;br/&gt;
So give it a go if you are interested and let me know your thoughts.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RUZyv3zTvcrjRKIc2z_nZpbVis8PgxNj2tMcL6LYzhY.jpg?auto=webp&amp;s=e6c1e19a5f9dd1f41961e212cb1987eb709c1864', 'width': 744, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/RUZyv3zTvcrjRKIc2z_nZpbVis8PgxNj2tMcL6LYzhY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93ee0f93a35ca3e99d4020e90d6e6a0ac44269a0', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/RUZyv3zTvcrjRKIc2z_nZpbVis8PgxNj2tMcL6LYzhY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9068f93f74f096e8b1f28611aec623b690f48016', 'width': 216, 'height': 87}, {'url': 'https://external-preview.redd.it/RUZyv3zTvcrjRKIc2z_nZpbVis8PgxNj2tMcL6LYzhY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bdfcd4861fcb4a985d478bde0f89473be1e7827', 'width': 320, 'height': 129}, {'url': 'https://external-preview.redd.it/RUZyv3zTvcrjRKIc2z_nZpbVis8PgxNj2tMcL6LYzhY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4ebc139365487e367a10b83e3a7071a52a9da5c', 'width': 640, 'height': 258}], 'variants': {}, 'id': '_cdbdsWpCldDslQuzKy6DcI8rmpJncf5IEKB6vD5Vjs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7jcle,True,,diegoagher,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7jcle/learn_about_neuroscience_and_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7jcle/learn_about_neuroscience_and_machine_learning/,155203,1587771886.0,0,,False,,,,
,learnmachinelearning,"Hi. I was following many tutorials about implementing a neural network from scratch and it is said to use a sigmoid activation function, but why that? I know there are many so which criteria makes you choose a different one like RELU o ArcTan?",t2_1ltbqoi6,False,,0,False,Which activation function should I use?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7xapw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587863996.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. I was following many tutorials about implementing a neural network from scratch and it is said to use a sigmoid activation function, but why that? I know there are many so which criteria makes you choose a different one like RELU o ArcTan?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7xapw,True,,rockcamus,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7xapw/which_activation_function_should_i_use/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7xapw/which_activation_function_should_i_use/,155203,1587835196.0,0,,False,,,,
,learnmachinelearning,"i want to start machine learning , i already know linear algebra , calculus , probability from high school.  does reading the mml book is alone enough or is there any collection of courses for this?",t2_1sc4qlc2,False,,0,False,is mml book good enough?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7wwu1,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587862773.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;i want to start machine learning , i already know linear algebra , calculus , probability from high school.  does reading the mml book is alone enough or is there any collection of courses for this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7wwu1,True,,zirten,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7wwu1/is_mml_book_good_enough/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7wwu1/is_mml_book_good_enough/,155203,1587833973.0,0,,False,,,,
,learnmachinelearning,"Hi,

I just want to process some part of image. (for instance, 200 to 400 on horizontal and 300 to 500 on vertical). Model which I've been working, processes images as a whole. How can I process only a specified region?

    while cap.isOpened():
          # Capture frame-by-frame
          ret, frame = cap.read()
          if not ret:
            break
          raw_frames = [np.array(frame)]
          detections_bs = driver.serve_images(raw_frames)
          new_frame = driver.visualize(raw_frames[0], detections_bs[0], **kwargs)",t2_4aky7agd,False,,0,False,How to set region of interest in video frames?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7ww8g,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587862718.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I just want to process some part of image. (for instance, 200 to 400 on horizontal and 300 to 500 on vertical). Model which I&amp;#39;ve been working, processes images as a whole. How can I process only a specified region?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while cap.isOpened():
      # Capture frame-by-frame
      ret, frame = cap.read()
      if not ret:
        break
      raw_frames = [np.array(frame)]
      detections_bs = driver.serve_images(raw_frames)
      new_frame = driver.visualize(raw_frames[0], detections_bs[0], **kwargs)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7ww8g,True,,hernancrespo89,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7ww8g/how_to_set_region_of_interest_in_video_frames/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7ww8g/how_to_set_region_of_interest_in_video_frames/,155203,1587833918.0,0,,False,,,,
,learnmachinelearning,"I'm working on a school project in which one of the questions is to choose between the following algorithms:
- xgboost
- GradientBoostClassifier
- neural_network.MLPClassifier

Before getting into the metrics of it all, are there any guidelines to follow when choosing an algorithm (i.e. such algorithm is good for such type of data/problem)? Or is it a brute force problem and should I just keep the one for which I happen to find the best model?",t2_3szn66lj,False,,0,False,How to choose between different algorithms?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7wfmg,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587861155.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a school project in which one of the questions is to choose between the following algorithms:
- xgboost
- GradientBoostClassifier
- neural_network.MLPClassifier&lt;/p&gt;

&lt;p&gt;Before getting into the metrics of it all, are there any guidelines to follow when choosing an algorithm (i.e. such algorithm is good for such type of data/problem)? Or is it a brute force problem and should I just keep the one for which I happen to find the best model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7wfmg,True,,ChefCiscoRZ,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7wfmg/how_to_choose_between_different_algorithms/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7wfmg/how_to_choose_between_different_algorithms/,155203,1587832355.0,0,,False,,,,
,learnmachinelearning,"I'm in the process of learning about ML and neural networks for a uni project. I'm seeing the terms ""activation"" and ""normalization"" applied to what seems to be the same things, like the sigmoid function, tanh, etc. For the life of me, I can't tell if people are just using the terms interchangeably, or if they serve somehow different purposes. Are the same group of functions being used for two different purposes? I was under the impression that a neuron takes an input, activates it (which is to say, applies some kind of function), adds a bias, and finally normalizes it (maps it to a handy interval like (0,1) ) before sending it on its way. Thanks in advance!",t2_xtduo,False,,0,False,Neural Network activation vs normalization,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7vry3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1587858953.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m in the process of learning about ML and neural networks for a uni project. I&amp;#39;m seeing the terms &amp;quot;activation&amp;quot; and &amp;quot;normalization&amp;quot; applied to what seems to be the same things, like the sigmoid function, tanh, etc. For the life of me, I can&amp;#39;t tell if people are just using the terms interchangeably, or if they serve somehow different purposes. Are the same group of functions being used for two different purposes? I was under the impression that a neuron takes an input, activates it (which is to say, applies some kind of function), adds a bias, and finally normalizes it (maps it to a handy interval like (0,1) ) before sending it on its way. Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7vry3,True,,tedgemon,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7vry3/neural_network_activation_vs_normalization/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7vry3/neural_network_activation_vs_normalization/,155203,1587830153.0,0,,False,,,,True
,learnmachinelearning,"Hello All,

I have a question regarding sample alignment for doing survival analysis. How do we do that? Is there any R or python package for doing that? How we should align the patient based on their failure event?

Any kind of help would be appreciated in advance.

Best",t2_61511ugu,False,,0,False,Sample alignment for doing survival analysis,[],r/learnmachinelearning,False,6,,0,,False,t3_g7syx8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587847658.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello All,&lt;/p&gt;

&lt;p&gt;I have a question regarding sample alignment for doing survival analysis. How do we do that? Is there any R or python package for doing that? How we should align the patient based on their failure event?&lt;/p&gt;

&lt;p&gt;Any kind of help would be appreciated in advance.&lt;/p&gt;

&lt;p&gt;Best&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7syx8,True,,SepGol,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7syx8/sample_alignment_for_doing_survival_analysis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7syx8/sample_alignment_for_doing_survival_analysis/,155203,1587818858.0,0,,False,,,,
,learnmachinelearning,"In one of the most interesting papers I've read (it offers a kmeans alternative that improves clustering significantly if I'm understanding correctly):

https://arxiv.org/abs/1807.06653

They do their clustering using similarity between paired images. They are looking at correlations between probabilities of classes/clusters and creating what looks like a cross correlation matrix.

For segmentation they use neighboring patches for the correlations.  I have a few questions please.

1) is it looking at the probability of assigning an input pair into a cluster like I think or is it classes?

2) they give the objective function but how exactly is each cluster being updated?  Is it a running average with both images being added to a given cluster?  Are they updating all the clusters based on the probability match or just the best fit one?

3) How does it nudge the classes into the correct clusters when semi-supervised?  I have the same question for the Deep Clustering paper:

https://arxiv.org/abs/1807.05520

4) with the neighboring patches in segmentation, how does it know if 2 neighboring patches are indeed in the same class?

5) in the segmentation how are they adding the relative location exactly?  It seems like they're making th probability matrix be h x w (neighboring windows) long on each side and correlating those instead of transformed images.  Is that correct?

Thanks.",t2_5mz3he12,False,,0,False,Invariant information clustering and segmentation explanation?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7sn3w,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587846130.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In one of the most interesting papers I&amp;#39;ve read (it offers a kmeans alternative that improves clustering significantly if I&amp;#39;m understanding correctly):&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1807.06653""&gt;https://arxiv.org/abs/1807.06653&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;They do their clustering using similarity between paired images. They are looking at correlations between probabilities of classes/clusters and creating what looks like a cross correlation matrix.&lt;/p&gt;

&lt;p&gt;For segmentation they use neighboring patches for the correlations.  I have a few questions please.&lt;/p&gt;

&lt;p&gt;1) is it looking at the probability of assigning an input pair into a cluster like I think or is it classes?&lt;/p&gt;

&lt;p&gt;2) they give the objective function but how exactly is each cluster being updated?  Is it a running average with both images being added to a given cluster?  Are they updating all the clusters based on the probability match or just the best fit one?&lt;/p&gt;

&lt;p&gt;3) How does it nudge the classes into the correct clusters when semi-supervised?  I have the same question for the Deep Clustering paper:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1807.05520""&gt;https://arxiv.org/abs/1807.05520&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;4) with the neighboring patches in segmentation, how does it know if 2 neighboring patches are indeed in the same class?&lt;/p&gt;

&lt;p&gt;5) in the segmentation how are they adding the relative location exactly?  It seems like they&amp;#39;re making th probability matrix be h x w (neighboring windows) long on each side and correlating those instead of transformed images.  Is that correct?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7sn3w,True,,mustgoplay,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7sn3w/invariant_information_clustering_and_segmentation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7sn3w/invariant_information_clustering_and_segmentation/,155203,1587817330.0,0,,False,,,,
,learnmachinelearning,"Does anybody know where I can find good project examples/resources on Github/Bitbucket that display an entire ML pipeline from data-gathering and refinement, through to cloud model deployment with a front-end?

I have been experimenting with some NLP models and that I think could be worth adding to my portfolio for job apps, but without having any experience on the soft-dev side, I really don't know where to start.

Looking for resources around:

\- Best practice around venvs / containers in 2020

\- Best practice use of Git and repo-structure

\- Best practice deployment onto cloud providers (agnostic as to which, I'd like to experiment with GC/MS/AWS/IBM).- Clean front-end interfaces.

\- Automated model-retraining.",t2_fszrf,False,,0,False,End-to-end project examples.,[],r/learnmachinelearning,False,6,,0,,False,t3_g759xq,False,dark,0.97,,public,138,0,{},,,False,[],,False,False,,{},,False,138,,False,self,False,,[],{},,,True,,1587748766.0,text,6,,,text,self.learnmachinelearning,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anybody know where I can find good project examples/resources on Github/Bitbucket that display an entire ML pipeline from data-gathering and refinement, through to cloud model deployment with a front-end?&lt;/p&gt;

&lt;p&gt;I have been experimenting with some NLP models and that I think could be worth adding to my portfolio for job apps, but without having any experience on the soft-dev side, I really don&amp;#39;t know where to start.&lt;/p&gt;

&lt;p&gt;Looking for resources around:&lt;/p&gt;

&lt;p&gt;- Best practice around venvs / containers in 2020&lt;/p&gt;

&lt;p&gt;- Best practice use of Git and repo-structure&lt;/p&gt;

&lt;p&gt;- Best practice deployment onto cloud providers (agnostic as to which, I&amp;#39;d like to experiment with GC/MS/AWS/IBM).- Clean front-end interfaces.&lt;/p&gt;

&lt;p&gt;- Automated model-retraining.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g759xq,True,,Procrastatarian,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g759xq/endtoend_project_examples/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g759xq/endtoend_project_examples/,155203,1587719966.0,0,,False,,,,
,learnmachinelearning,"Mechanically, I understand how cross-validation works. So I don't need an ELI5. But

1. How to I interpret the result? Is this the accuracy rate I should expect irl?
2. My dataset is a little small (which I'm not *too* worried about because my p-values rock). Should I expect a great performance when my training set is pretty skimpy?
3. Also, my dataset has a prime x2 number of rows. I'm reading that you want your kfolds to be a factor of your rows. Is that a big deal?

tl;dr, I'm in denial that my model is shit.",t2_3sqf72hz,False,,0,False,Interpreting Cross-Validation Scores,[],r/learnmachinelearning,False,6,,0,,False,t3_g7sejf,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1587817325.0,,[],{},,,True,,1587844986.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Mechanically, I understand how cross-validation works. So I don&amp;#39;t need an ELI5. But&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to I interpret the result? Is this the accuracy rate I should expect irl?&lt;/li&gt;
&lt;li&gt;My dataset is a little small (which I&amp;#39;m not &lt;em&gt;too&lt;/em&gt; worried about because my p-values rock). Should I expect a great performance when my training set is pretty skimpy?&lt;/li&gt;
&lt;li&gt;Also, my dataset has a prime x2 number of rows. I&amp;#39;m reading that you want your kfolds to be a factor of your rows. Is that a big deal?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;tl;dr, I&amp;#39;m in denial that my model is shit.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7sejf,True,,fransquaoi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7sejf/interpreting_crossvalidation_scores/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7sejf/interpreting_crossvalidation_scores/,155203,1587816186.0,0,,False,,,,
,learnmachinelearning,"Hello everyone, hope you are all well and safe!

I recently got into ML, it's about a month that i started with ML and Python. I studied physics, i have the math part pretty much covered so it's somewhat easier from what it would be if i had no background in math.

I would like to know, with about how much experience one can land a job at last. What is the bare minimum i need to know and apply so that a company (or whoever is in need of ML engineers) would consider hiring me?

Thanks in advance!

&amp;#x200B;

EDIT: Not only i didn't get an answer, i also got downvoted. That's weird. Are this kind of posts not allowed in this subreddit?",t2_46fd1yrr,False,,0,False,I have some questions,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7s1fq,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},Request,False,0,,False,self,1587841093.0,,[],{},,,True,,1587843018.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, hope you are all well and safe!&lt;/p&gt;

&lt;p&gt;I recently got into ML, it&amp;#39;s about a month that i started with ML and Python. I studied physics, i have the math part pretty much covered so it&amp;#39;s somewhat easier from what it would be if i had no background in math.&lt;/p&gt;

&lt;p&gt;I would like to know, with about how much experience one can land a job at last. What is the bare minimum i need to know and apply so that a company (or whoever is in need of ML engineers) would consider hiring me?&lt;/p&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;EDIT: Not only i didn&amp;#39;t get an answer, i also got downvoted. That&amp;#39;s weird. Are this kind of posts not allowed in this subreddit?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,g7s1fq,True,,gvachtan,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7s1fq/i_have_some_questions/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7s1fq/i_have_some_questions/,155203,1587814218.0,0,,False,,,,
,learnmachinelearning,"\#FederatedLearning is known for keeping the user's data private by only training a machine learning model on the user's device and only sharing the trained model to a server.

Just the model is enough to retrieve the user's private data and this post discusses how the privacy of federated learning could be broken.

https://heartbeat.fritz.ai/breaking-privacy-in-federated-learning-77fa08ccac9a

Check the previous post in which federated learning is introduced: https://heartbeat.fritz.ai/introduction-to-federated-learning-40eb122754a2",t2_3vthdjeh,False,,0,False,Breaking Privacy in Federated Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g7rl0w,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587840463.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;#FederatedLearning is known for keeping the user&amp;#39;s data private by only training a machine learning model on the user&amp;#39;s device and only sharing the trained model to a server.&lt;/p&gt;

&lt;p&gt;Just the model is enough to retrieve the user&amp;#39;s private data and this post discusses how the privacy of federated learning could be broken.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/breaking-privacy-in-federated-learning-77fa08ccac9a""&gt;https://heartbeat.fritz.ai/breaking-privacy-in-federated-learning-77fa08ccac9a&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check the previous post in which federated learning is introduced: &lt;a href=""https://heartbeat.fritz.ai/introduction-to-federated-learning-40eb122754a2""&gt;https://heartbeat.fritz.ai/introduction-to-federated-learning-40eb122754a2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?auto=webp&amp;s=5fa741ae9eeef39fe17796be83c5b2272369ca12', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=813d46902fffa6197bd5910bf6ecff53e3cdf79d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5754e846cf461c9fbcba137ddcf917f9ecb2a59a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4f115a66ca9814ce11e0f08e91ea57e1ca7ca32', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=77616f65e0628a0cac75e6cdac6c84eb48adbde1', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d5a310ac858a652ea2ea9f5540f84ed124e793b', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/6B8OxjOrvmrpUooSBb5cUMhVgAy0ZMFd2t4yBHym4kk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=17637176fa971b4e3c4f9201f9c949e28a8c9930', 'width': 1080, 'height': 810}], 'variants': {}, 'id': '6GPdQ_IMWbkqKgraPbHG_uNUt8iq6JuBdMovlsTu4Y8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7rl0w,True,,ahmedfgad,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7rl0w/breaking_privacy_in_federated_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7rl0w/breaking_privacy_in_federated_learning/,155203,1587811663.0,0,,False,,,,
,learnmachinelearning,"For our college's final year megaproject, we have to develop hardware or software that implements Artificial Intelligence in general. For that, we can spend about 6 months or more for the development process. I want to build a model that can forecast natural calamities.

I want to know:

* If the project is feasible in general
* What particular datasets do I need for this pro
* Where will I get the needed data and will I be having a hard time dealing with it?
* In terms of complexity and applicability should I stick with one natural disaster (floods) or forecast many disasters

I am also looking for suggestions on other feasible topics in case this doesn't get approved. (since I already crossed the deadline for proposal and the time extension is about to end soon). I am especially interested to do anything that comes under ""AI for good"". I'm sorry for posting this here cause I didn't find any subreddits which can answer me since the r/MLQuestions is restricted. Thank you. :)",t2_5swmj3d7,False,,0,False,Are projects like Natural Disaster Prediction feasible/doable for college-level students?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7rkc0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587840341.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For our college&amp;#39;s final year megaproject, we have to develop hardware or software that implements Artificial Intelligence in general. For that, we can spend about 6 months or more for the development process. I want to build a model that can forecast natural calamities.&lt;/p&gt;

&lt;p&gt;I want to know:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the project is feasible in general&lt;/li&gt;
&lt;li&gt;What particular datasets do I need for this pro&lt;/li&gt;
&lt;li&gt;Where will I get the needed data and will I be having a hard time dealing with it?&lt;/li&gt;
&lt;li&gt;In terms of complexity and applicability should I stick with one natural disaster (floods) or forecast many disasters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am also looking for suggestions on other feasible topics in case this doesn&amp;#39;t get approved. (since I already crossed the deadline for proposal and the time extension is about to end soon). I am especially interested to do anything that comes under &amp;quot;AI for good&amp;quot;. I&amp;#39;m sorry for posting this here cause I didn&amp;#39;t find any subreddits which can answer me since the &lt;a href=""/r/MLQuestions""&gt;r/MLQuestions&lt;/a&gt; is restricted. Thank you. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7rkc0,True,,494746943,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7rkc0/are_projects_like_natural_disaster_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7rkc0/are_projects_like_natural_disaster_prediction/,155203,1587811541.0,0,,False,,,,
,learnmachinelearning,"Stating that I have huge lacks in theory, the bias is the error between the average model prediction and the ground truth.

I have to calculate the bias between my CNN forecasts and the ground truth.

    #I have two matrices
    actual_value=ground_truth
    predicted=cnn_forecast

Is it correct to compute bias in this way?

    bias=np.average(predicted - actual_value)

I have a lot confusion in mind. I should take the mean only of my prediction but how could I make the difference between a number and a matrix?

I know this is a pretty noob question but I can't find an explanation online.",t2_3wjr3tig,False,,0,False,Help BIAS,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7re4n,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587839336.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Stating that I have huge lacks in theory, the bias is the error between the average model prediction and the ground truth.&lt;/p&gt;

&lt;p&gt;I have to calculate the bias between my CNN forecasts and the ground truth.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#I have two matrices
actual_value=ground_truth
predicted=cnn_forecast
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Is it correct to compute bias in this way?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bias=np.average(predicted - actual_value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have a lot confusion in mind. I should take the mean only of my prediction but how could I make the difference between a number and a matrix?&lt;/p&gt;

&lt;p&gt;I know this is a pretty noob question but I can&amp;#39;t find an explanation online.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7re4n,True,,giacpolish,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7re4n/help_bias/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7re4n/help_bias/,155203,1587810536.0,0,,False,,,,
,learnmachinelearning,"Hi,

So I have this custom NER in spacy. Suppose I tag the entities in two separate documents (text files), how do I compare them and judge their similarity? 

I want to be able to say, Doc A has 80% same entities as Doc B.

Right now I just used manual search. I look T all the entities in Doc A that the NER identified and look for same in Doc B (that the NER identified?
) and give the similarity percent. Is there anyway to automate this using ML?

Sorry if the question seems confusing or silly. 

Thanks in advance.",t2_o9jpm,False,,0,False,Help with customer NER comparison,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7raxk,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587838803.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;So I have this custom NER in spacy. Suppose I tag the entities in two separate documents (text files), how do I compare them and judge their similarity? &lt;/p&gt;

&lt;p&gt;I want to be able to say, Doc A has 80% same entities as Doc B.&lt;/p&gt;

&lt;p&gt;Right now I just used manual search. I look T all the entities in Doc A that the NER identified and look for same in Doc B (that the NER identified?
) and give the similarity percent. Is there anyway to automate this using ML?&lt;/p&gt;

&lt;p&gt;Sorry if the question seems confusing or silly. &lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7raxk,True,,Kryotasin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7raxk/help_with_customer_ner_comparison/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7raxk/help_with_customer_ner_comparison/,155203,1587810003.0,0,,False,,,,
,learnmachinelearning,"I need to know from people that already get hired and work as a data scientists, what are the most needed requirements/ prerequisites for anyone to get a into the field ? Is it master/PhD degree , nice pipelines of codes at github , experience at the field or what ? And thanks in advance",t2_1hnpr9g9,False,,0,False,What is needed to get hired as a Data Scientist,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7lvly,False,light,1.0,,public,4,0,{},,,False,[],,False,False,,{},HELP,False,4,,False,self,False,,[],{},,,True,,1587810773.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to know from people that already get hired and work as a data scientists, what are the most needed requirements/ prerequisites for anyone to get a into the field ? Is it master/PhD degree , nice pipelines of codes at github , experience at the field or what ? And thanks in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7lvly,True,,almeldin,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7lvly/what_is_needed_to_get_hired_as_a_data_scientist/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7lvly/what_is_needed_to_get_hired_as_a_data_scientist/,155203,1587781973.0,0,,False,,,,
,learnmachinelearning,"Iâ€™m working with some graph data (represented as weighted adjacency matrixes). 

Any suggestions (other than GANs) to augment this kind of data? I am not getting good enough results out of the current matrixes Iâ€™m working with right now.",t2_ivwi3,False,,0,False,Data augmenting for graphs,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7qz7h,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587836895.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m working with some graph data (represented as weighted adjacency matrixes). &lt;/p&gt;

&lt;p&gt;Any suggestions (other than GANs) to augment this kind of data? I am not getting good enough results out of the current matrixes Iâ€™m working with right now.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7qz7h,True,,andyboss97,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7qz7h/data_augmenting_for_graphs/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7qz7h/data_augmenting_for_graphs/,155203,1587808095.0,0,,False,,,,
,learnmachinelearning,"The future of Jupyter Notebook editing with features like coding assistance and more.

[https://towardsdatascience.com/jupyterlab-2-0-edd4155ab897](https://towardsdatascience.com/jupyterlab-2-0-edd4155ab897)

**TLDR;**

Improve your JupyterLab experience by using JupyterLab-LSP

\- Code linting and hoovering suggestions

\- Code Completion

\- Function Signature Suggestion

\- Diagnostic panel",t2_k9eg8,False,,0,False,JupyterLab 2.0,[],r/learnmachinelearning,False,6,,0,,False,t3_g7quax,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587836057.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The future of Jupyter Notebook editing with features like coding assistance and more.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/jupyterlab-2-0-edd4155ab897""&gt;https://towardsdatascience.com/jupyterlab-2-0-edd4155ab897&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Improve your JupyterLab experience by using JupyterLab-LSP&lt;/p&gt;

&lt;p&gt;- Code linting and hoovering suggestions&lt;/p&gt;

&lt;p&gt;- Code Completion&lt;/p&gt;

&lt;p&gt;- Function Signature Suggestion&lt;/p&gt;

&lt;p&gt;- Diagnostic panel&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?auto=webp&amp;s=38298b6778d8c837be3595dd31109135cd4fb618', 'width': 1200, 'height': 799}, 'resolutions': [{'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fb3b6fb54eff50991daeaaa68d60c0fba719ab', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c1519c06b08ef2b3aea37a637ed0798ec9ce782', 'width': 216, 'height': 143}, {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b5ae32e54f0862dc11cd831820372542891a356', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3e24ecd598a39abec32a983e0a8cad46b862d6aa', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d157b8cdbe10bece178062a1aade6322355809f', 'width': 960, 'height': 639}, {'url': 'https://external-preview.redd.it/MOuO3HgDiT6L4vNbP4whqZTaiH_Ff_9bFx6jOz-cX74.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53d9a627e327aebe3c71f076cd42240f383d8ca9', 'width': 1080, 'height': 719}], 'variants': {}, 'id': 'ojDgr5APcx67uI1NKCyL30BFb8xthZkkgs7LJgexQLE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7quax,True,,hiphop1987,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7quax/jupyterlab_20/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7quax/jupyterlab_20/,155203,1587807257.0,0,,False,,,,
,learnmachinelearning,"For each competition, a bunch of users publish their kernels for everyone to see. What's stopping someone from making an identical submission and listing their kaggle ranking on their resume? I often suspect that this is the case when you look at a competition leaderboard and see a bunch of users clustered together around an identical score.


A kaggle competition is not much of a competition if someone works hard tuning their model for good performance, only to drop in rank by 100 spots if someone above you publishes a great public kernel that everyone copies.

I used to work as a data scientist at a medium sized tech company, and no one there uses kaggle as a resume building activity. They just use it as a way to gain expertise in something new, like tensorflow or data exploration in pandas.

Also, I don't like how kaggle encourages overly complex models just because they achieve a slightly higher score, as execution speed, simplicity, and explainability are important in building models. No one is going to deploy a complex ensemble in production of xgboost, an LSTM, and a random forest just because it beats the baseline xgboost by 0.1%.",t2_3plcyhxk,False,,0,False,Do data scientists and hiring managers actually take kaggle seriously?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7acmd,False,light,1.0,,public,22,0,{},,,False,[],,False,False,,{},Discussion,False,22,,False,self,1587742259.0,,[],{},,,True,,1587770826.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For each competition, a bunch of users publish their kernels for everyone to see. What&amp;#39;s stopping someone from making an identical submission and listing their kaggle ranking on their resume? I often suspect that this is the case when you look at a competition leaderboard and see a bunch of users clustered together around an identical score.&lt;/p&gt;

&lt;p&gt;A kaggle competition is not much of a competition if someone works hard tuning their model for good performance, only to drop in rank by 100 spots if someone above you publishes a great public kernel that everyone copies.&lt;/p&gt;

&lt;p&gt;I used to work as a data scientist at a medium sized tech company, and no one there uses kaggle as a resume building activity. They just use it as a way to gain expertise in something new, like tensorflow or data exploration in pandas.&lt;/p&gt;

&lt;p&gt;Also, I don&amp;#39;t like how kaggle encourages overly complex models just because they achieve a slightly higher score, as execution speed, simplicity, and explainability are important in building models. No one is going to deploy a complex ensemble in production of xgboost, an LSTM, and a random forest just because it beats the baseline xgboost by 0.1%.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g7acmd,True,,statistical_engineer,,16,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7acmd/do_data_scientists_and_hiring_managers_actually/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7acmd/do_data_scientists_and_hiring_managers_actually/,155203,1587742026.0,0,,False,,,,
,learnmachinelearning,"It is only recently when I started out teaching myself Machine learning and the first problem that I faced was looking for a reliable place to get my data from, I just completed my first project on Coronavirus and I think it would have taken more time if I didn't come across the amazing thing internet has to offer. 

If you guys want to you around with **datasets**: [http://archive.ics.uci.edu/ml/datasets](http://archive.ics.uci.edu/ml/datasets)   
This UCI website is the dataset repository. You can find a lot of datasets here. 

If you are looking for books or papers then check out  
For books and articles :  [https://z-lib.org/](https://z-lib.org/) 

For papers :  [https://sci-hub.tw/](https://sci-hub.tw/)",t2_7d6e41,False,,0,False,If you people are looking for datasets to play around,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7409a,False,light,0.98,,public,99,0,{},,,False,[],,False,False,,{},Discussion,False,99,,False,self,False,,[],{},,,True,,1587741782.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It is only recently when I started out teaching myself Machine learning and the first problem that I faced was looking for a reliable place to get my data from, I just completed my first project on Coronavirus and I think it would have taken more time if I didn&amp;#39;t come across the amazing thing internet has to offer. &lt;/p&gt;

&lt;p&gt;If you guys want to you around with &lt;strong&gt;datasets&lt;/strong&gt;: &lt;a href=""http://archive.ics.uci.edu/ml/datasets""&gt;http://archive.ics.uci.edu/ml/datasets&lt;/a&gt;&lt;br/&gt;
This UCI website is the dataset repository. You can find a lot of datasets here. &lt;/p&gt;

&lt;p&gt;If you are looking for books or papers then check out&lt;br/&gt;
For books and articles :  &lt;a href=""https://z-lib.org/""&gt;https://z-lib.org/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For papers :  &lt;a href=""https://sci-hub.tw/""&gt;https://sci-hub.tw/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g7409a,True,,my_name_jeffff,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7409a/if_you_people_are_looking_for_datasets_to_play/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7409a/if_you_people_are_looking_for_datasets_to_play/,155203,1587712982.0,0,,False,,,,
,learnmachinelearning,"I want to learn Machine Learning and i was confused where to start. After a few reseach i found out Andrew Ng course on coursera or CS229 would be best to start with. After seeing first two lectures i feel like i am in a sea of unknown and don't know where to go. I have totally forgot linear algebra, calculus. If i want to learn and make the most out i=of the course what should i do? Anyone who has taken this course can help me out. Some suggestion how to tackle the problems or go about the course. It would be really helpful.",t2_4mezd8mm,False,,0,False,How to make most out of CS229 - Machine Learning by Andrew Ng,[],r/learnmachinelearning,False,6,,0,,False,t3_g7aaov,False,dark,0.96,,public,18,0,{},,,False,[],,False,False,,{},,False,18,,False,self,False,,[],{},,,True,,1587770638.0,text,6,,,text,self.learnmachinelearning,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to learn Machine Learning and i was confused where to start. After a few reseach i found out Andrew Ng course on coursera or CS229 would be best to start with. After seeing first two lectures i feel like i am in a sea of unknown and don&amp;#39;t know where to go. I have totally forgot linear algebra, calculus. If i want to learn and make the most out i=of the course what should i do? Anyone who has taken this course can help me out. Some suggestion how to tackle the problems or go about the course. It would be really helpful.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7aaov,True,,shuvob4,,16,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7aaov/how_to_make_most_out_of_cs229_machine_learning_by/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7aaov/how_to_make_most_out_of_cs229_machine_learning_by/,155203,1587741838.0,0,,False,,,,
,learnmachinelearning,"Let's have a  meeting!

1. What have you accomplished since last week?
2. What are your goals for next week?
3. Do you have any blockers that need helps from the /r/LearnMachineLearning community?

Don't be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.",t2_6l4z3,False,,0,False,"Weekly Status Check Meeting - Share your progress, your goals, and whatever is stopping you from achieving those goals!",[],r/learnmachinelearning,False,6,,0,,False,t3_g7p9fz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1587827071.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s have a  meeting!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What have you accomplished since last week?&lt;/li&gt;
&lt;li&gt;What are your goals for next week?&lt;/li&gt;
&lt;li&gt;Do you have any blockers that need helps from the &lt;a href=""/r/LearnMachineLearning""&gt;/r/LearnMachineLearning&lt;/a&gt; community?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Don&amp;#39;t be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,g7p9fz,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7p9fz/weekly_status_check_meeting_share_your_progress/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7p9fz/weekly_status_check_meeting_share_your_progress/,155203,1587798271.0,0,,False,,,,
,learnmachinelearning,"Hi,

I have the Python solutions/assignments from Andrew Ngâ€™a ML class, but I also noticed he has several videos within his class that concern how to use Octave to manipulate data and do operations. Are there any python-based alternatives/replacements for these videos?

Thanks for the help!",t2_ocdtv,False,,0,False,Andrew Ng Coursera ML Course - Octave to Python,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g72a87,False,light,0.98,,public,114,0,{},,,False,[],,False,False,,{},HELP,False,114,,False,self,False,,[],{},,,True,,1587733116.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have the Python solutions/assignments from Andrew Ngâ€™a ML class, but I also noticed he has several videos within his class that concern how to use Octave to manipulate data and do operations. Are there any python-based alternatives/replacements for these videos?&lt;/p&gt;

&lt;p&gt;Thanks for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g72a87,True,,jzlee,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g72a87/andrew_ng_coursera_ml_course_octave_to_python/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g72a87/andrew_ng_coursera_ml_course_octave_to_python/,155203,1587704316.0,0,,False,,,,
,learnmachinelearning,"I've always been interested in literature. I got bored of my old teaching job and decided to learn programming. This was in hopes of doing something NLP-related in my off time while also switching careers. I'm not really thinking about ""where"" in NLP I want to end up, just building the skills first and seeing what's out there.

So, I'm going through DataQuest's ""Data Scientist"" path (it's excellent) as well as the NLTK book. I know the NLTK book is pretty outdated, but it's a good foundational resource, and so much is built with NLTK anyway. Question is, what should I do when I'm done with DataQuest and NLTK?

Andrew Ng's ""Intro to Machine Learning"" course is constantly recommended, so I'm thinking of taking that for the more theoretical stuff. Then I plan to get through [fast.ai](https://fast.ai)'s more practical ""Deep Learning"" courses, and work through their (however brief) NLP course.

Is that a sensible plan? There's a lot to learn, so what comes after this foundation is laid down? I definitely have some projects in mind I'd like to build, but how should I be continuing my education? I have a bachelor's in Classical Studies and English Literature, so don't have any background in computer science.

Udacity's NLP nanodegree looks good, and I definitely got a lot out of their Front-End track some time ago. Don't really know what else is out there, however, or if this is where I should be looking.",t2_3aiucj51,False,,0,False,"If I want to ultimately work in NLP, what should I do after completing DataQuest and working through the NLTK book?",[],r/learnmachinelearning,False,6,,0,,False,t3_g7hj3q,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1587794195.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve always been interested in literature. I got bored of my old teaching job and decided to learn programming. This was in hopes of doing something NLP-related in my off time while also switching careers. I&amp;#39;m not really thinking about &amp;quot;where&amp;quot; in NLP I want to end up, just building the skills first and seeing what&amp;#39;s out there.&lt;/p&gt;

&lt;p&gt;So, I&amp;#39;m going through DataQuest&amp;#39;s &amp;quot;Data Scientist&amp;quot; path (it&amp;#39;s excellent) as well as the NLTK book. I know the NLTK book is pretty outdated, but it&amp;#39;s a good foundational resource, and so much is built with NLTK anyway. Question is, what should I do when I&amp;#39;m done with DataQuest and NLTK?&lt;/p&gt;

&lt;p&gt;Andrew Ng&amp;#39;s &amp;quot;Intro to Machine Learning&amp;quot; course is constantly recommended, so I&amp;#39;m thinking of taking that for the more theoretical stuff. Then I plan to get through &lt;a href=""https://fast.ai""&gt;fast.ai&lt;/a&gt;&amp;#39;s more practical &amp;quot;Deep Learning&amp;quot; courses, and work through their (however brief) NLP course.&lt;/p&gt;

&lt;p&gt;Is that a sensible plan? There&amp;#39;s a lot to learn, so what comes after this foundation is laid down? I definitely have some projects in mind I&amp;#39;d like to build, but how should I be continuing my education? I have a bachelor&amp;#39;s in Classical Studies and English Literature, so don&amp;#39;t have any background in computer science.&lt;/p&gt;

&lt;p&gt;Udacity&amp;#39;s NLP nanodegree looks good, and I definitely got a lot out of their Front-End track some time ago. Don&amp;#39;t really know what else is out there, however, or if this is where I should be looking.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7hj3q,True,,trading-statistics,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7hj3q/if_i_want_to_ultimately_work_in_nlp_what_should_i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7hj3q/if_i_want_to_ultimately_work_in_nlp_what_should_i/,155203,1587765395.0,0,,False,,,,
,learnmachinelearning,"During linear regression the algorithm jumbles with the coefficients to create a linear model that closely fits real life data points. How good/crappy models are is tested by measuring the residuals, squaring them and taking the sum. If you graph the sum of squared residuals for a series of linear models (each of them getting progressively better at predicting the real life data points)  it will look like a quadratic function. So ultimately the best model produces the minimized value of the quadratic loss function. 

I also understand that in order to test how good/crappy a linear classification model is, you can measure how many misclassifications there are and assign one point for every misclassification and zero points for every correct classification (0-1 loss function - it looks like a step model, check picture below). Therefore the best linear classification model is selected by determining which model minimizes the number of misclassifications. I have recently learned that the loss function for logistic regression appears to be exponential in shape (look at picture). I have questions regarding this loss function. 

**First Question:** The connection between linear regression and the quadratic loss function is very clear to me. I can  graph  the sum of squared residuals for progressively better  linear models and it will produce a quadratic function (a parabola). However how is logistic regression models and the exponential loss function connected?? Can you prove/tell me how this is possible? I read  that the traditional 0-1 loss function is altered for logistic regression models and  a heavier penalty (not just one point) is applied for misclassification - however I'm still lost in seeing the connection between logistic regression and its corresponding loss function.  

**Second Question:** If the loss function for logistic regression is an exponential function, how do we look to minimize this loss function? If I look at the quadratic loss function i can clearly see the minimum value, moreover I can even take the derivative of the quadratic loss function to find when the slope is zero. If I'm looking at the logistic loss function how would I minimize the function? 

&amp;#x200B;

https://preview.redd.it/2u3scbigmvu41.png?width=699&amp;format=png&amp;auto=webp&amp;s=e5e25bcca5c09ff3ed1fc479963512226ba391d8

&amp;#x200B;

https://preview.redd.it/zb6h8r6imvu41.png?width=683&amp;format=png&amp;auto=webp&amp;s=a6f082d4d834f093259c393f8e82f0d590ff6c92",t2_2zdvka5h,False,,0,False,Loss Functions - Linear Classification - Logistic Regression,[],r/learnmachinelearning,False,6,,0,94.0,False,t3_g7lyj8,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/VRYst42VqIqetTaMO-4KYNVgu_YohFu1QEjF3XDEenY.jpg,False,,[],{},,,True,,1587811126.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;During linear regression the algorithm jumbles with the coefficients to create a linear model that closely fits real life data points. How good/crappy models are is tested by measuring the residuals, squaring them and taking the sum. If you graph the sum of squared residuals for a series of linear models (each of them getting progressively better at predicting the real life data points)  it will look like a quadratic function. So ultimately the best model produces the minimized value of the quadratic loss function. &lt;/p&gt;

&lt;p&gt;I also understand that in order to test how good/crappy a linear classification model is, you can measure how many misclassifications there are and assign one point for every misclassification and zero points for every correct classification (0-1 loss function - it looks like a step model, check picture below). Therefore the best linear classification model is selected by determining which model minimizes the number of misclassifications. I have recently learned that the loss function for logistic regression appears to be exponential in shape (look at picture). I have questions regarding this loss function. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First Question:&lt;/strong&gt; The connection between linear regression and the quadratic loss function is very clear to me. I can  graph  the sum of squared residuals for progressively better  linear models and it will produce a quadratic function (a parabola). However how is logistic regression models and the exponential loss function connected?? Can you prove/tell me how this is possible? I read  that the traditional 0-1 loss function is altered for logistic regression models and  a heavier penalty (not just one point) is applied for misclassification - however I&amp;#39;m still lost in seeing the connection between logistic regression and its corresponding loss function.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second Question:&lt;/strong&gt; If the loss function for logistic regression is an exponential function, how do we look to minimize this loss function? If I look at the quadratic loss function i can clearly see the minimum value, moreover I can even take the derivative of the quadratic loss function to find when the slope is zero. If I&amp;#39;m looking at the logistic loss function how would I minimize the function? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/2u3scbigmvu41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e25bcca5c09ff3ed1fc479963512226ba391d8""&gt;https://preview.redd.it/2u3scbigmvu41.png?width=699&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5e25bcca5c09ff3ed1fc479963512226ba391d8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/zb6h8r6imvu41.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6f082d4d834f093259c393f8e82f0d590ff6c92""&gt;https://preview.redd.it/zb6h8r6imvu41.png?width=683&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6f082d4d834f093259c393f8e82f0d590ff6c92&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7lyj8,True,,Charlie_M_,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7lyj8/loss_functions_linear_classification_logistic/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7lyj8/loss_functions_linear_classification_logistic/,155203,1587782326.0,0,,False,,,"{'2u3scbigmvu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 73, 'x': 108, 'u': 'https://preview.redd.it/2u3scbigmvu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f0604dfe3321ba9cc399e306aeae165b0d593fd3'}, {'y': 146, 'x': 216, 'u': 'https://preview.redd.it/2u3scbigmvu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=36f0efded21f8e1119bd31c957eb05ef23dbd3c5'}, {'y': 216, 'x': 320, 'u': 'https://preview.redd.it/2u3scbigmvu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0302aff71229d673771da5725af2c769b336a356'}, {'y': 433, 'x': 640, 'u': 'https://preview.redd.it/2u3scbigmvu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61309286d1e2cd989effb088a941ada91335c702'}], 's': {'y': 474, 'x': 699, 'u': 'https://preview.redd.it/2u3scbigmvu41.png?width=699&amp;format=png&amp;auto=webp&amp;s=e5e25bcca5c09ff3ed1fc479963512226ba391d8'}, 'id': '2u3scbigmvu41'}, 'zb6h8r6imvu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 72, 'x': 108, 'u': 'https://preview.redd.it/zb6h8r6imvu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44e7d5280991dd2afec1712c33cdb7726320179c'}, {'y': 144, 'x': 216, 'u': 'https://preview.redd.it/zb6h8r6imvu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=02cdf86e28bf5429726668f04ecf0985748b03c4'}, {'y': 214, 'x': 320, 'u': 'https://preview.redd.it/zb6h8r6imvu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3594413b763b0a3ce4d2fe4008abbf1e0ed28043'}, {'y': 429, 'x': 640, 'u': 'https://preview.redd.it/zb6h8r6imvu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d717b46d05106708b464c88ad04bb8d8ed58fcb3'}], 's': {'y': 458, 'x': 683, 'u': 'https://preview.redd.it/zb6h8r6imvu41.png?width=683&amp;format=png&amp;auto=webp&amp;s=a6f082d4d834f093259c393f8e82f0d590ff6c92'}, 'id': 'zb6h8r6imvu41'}}",
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: Like zoomâ€™s virtual background function but better! Deep learning and GANs to enable professional-quality background replacement from your own home,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_g7lrk0,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://a.thumbs.redditmedia.com/skU_7OMC4IYLKRSx1c2rj2sFxiqu_4_DpAfPerEvga4.jpg,False,,[],{},link,,False,,1587810273.0,text,6,,,text,self.LatestInML,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?auto=webp&amp;s=8a3efb7492c76d6d41ef6956629cdf5e114d57e9', 'width': 550, 'height': 412}, 'resolutions': [{'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89eae21f3a30bbb9c4f47a7f0863d2bf0b7811ad', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df1dc06d1b7487d68edf725eeffd6578c81e164c', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6aa30c261bf3a368aad315685a29e3ed898ae721', 'width': 320, 'height': 239}], 'variants': {}, 'id': 'yq7o0bpmpWGb4Y5pv1_RHdZyE3mm31DfKeV-2AUcEEc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7lrk0,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7lrk0/from_cvpr_20_like_zooms_virtual_background/,all_ads,False,/r/LatestInML/comments/g7lge4/from_cvpr_20_like_zooms_virtual_background/,155203,1587781473.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""From CVPR '20: Like zoomâ€™s virtual background function but better! Deep learning and GANs to enable professional-quality background replacement from your own home\n\n[link to research paper](https://www.catalyzex.com/paper/arxiv:2004.00626)  \n\n\nhttps://reddit.com/link/g7lge4/video/q9c43l3wfvu41/player"", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: Like zoomâ€™s virtual background function but better! Deep learning and GANs to enable professional-quality background replacement from your own home"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 104, 'hide_score': False, 'media_metadata': {'q9c43l3wfvu41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/g7lge4/asset/q9c43l3wfvu41/DASHPlaylist.mpd', 'x': 640, 'y': 360, 'hlsUrl': 'https://v.redd.it/link/g7lge4/asset/q9c43l3wfvu41/HLSPlaylist.m3u8', 'id': 'q9c43l3wfvu41', 'isGif': False}}, 'name': 't3_g7lge4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 35, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 35, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/skU_7OMC4IYLKRSx1c2rj2sFxiqu_4_DpAfPerEvga4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587809010.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From CVPR &amp;#39;20: Like zoomâ€™s virtual background function but better! Deep learning and GANs to enable professional-quality background replacement from your own home&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.00626""&gt;link to research paper&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/g7lge4/video/q9c43l3wfvu41/player""&gt;https://reddit.com/link/g7lge4/video/q9c43l3wfvu41/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?auto=webp&amp;s=8a3efb7492c76d6d41ef6956629cdf5e114d57e9', 'width': 550, 'height': 412}, 'resolutions': [{'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89eae21f3a30bbb9c4f47a7f0863d2bf0b7811ad', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df1dc06d1b7487d68edf725eeffd6578c81e164c', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/ONl7d6PIQa70WVOZ0DdzqSLbfpdYQvwXlv7uTzf_SUU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6aa30c261bf3a368aad315685a29e3ed898ae721', 'width': 320, 'height': 239}], 'variants': {}, 'id': 'yq7o0bpmpWGb4Y5pv1_RHdZyE3mm31DfKeV-2AUcEEc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g7lge4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/g7lge4/from_cvpr_20_like_zooms_virtual_background/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/g7lge4/from_cvpr_20_like_zooms_virtual_background/', 'subreddit_subscribers': 3386, 'created_utc': 1587780210.0, 'num_crossposts': 9, 'media': None, 'is_video': False}]",t3_g7lge4,,
,learnmachinelearning,"Hey guys, 

Do you suggest me some Reinforcement Learning/Machine Learning method based control? I am a beginner and I just would like to try things out, applying some method in a simple system. 

If you could also give me some code examples, Matlab or Python makes no difference! 

Thank you so much!",t2_3msq6715,False,,0,False,ML/RL base control for a beginner,[],r/learnmachinelearning,False,6,,0,,False,t3_g7oj24,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587823088.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, &lt;/p&gt;

&lt;p&gt;Do you suggest me some Reinforcement Learning/Machine Learning method based control? I am a beginner and I just would like to try things out, applying some method in a simple system. &lt;/p&gt;

&lt;p&gt;If you could also give me some code examples, Matlab or Python makes no difference! &lt;/p&gt;

&lt;p&gt;Thank you so much!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7oj24,True,,alphack_,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7oj24/mlrl_base_control_for_a_beginner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7oj24/mlrl_base_control_for_a_beginner/,155203,1587794288.0,0,,False,,,,
,learnmachinelearning,"I started machine learning recently, and applied what I have learned yet on Titanic Data Set and got 80% on train set and 76 % of test set in Kaggle, after hardwork and googling of a day Is it a good start",t2_47jpmh5m,False,,0,False,76% accuracy on Titanic,[],r/learnmachinelearning,False,6,,0,,False,t3_g779i1,False,dark,0.82,,public,27,0,{},,,False,[],,False,False,,{},,False,27,,False,self,False,,[],{},,,True,,1587759000.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I started machine learning recently, and applied what I have learned yet on Titanic Data Set and got 80% on train set and 76 % of test set in Kaggle, after hardwork and googling of a day Is it a good start&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g779i1,True,,ItisAhmad,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g779i1/76_accuracy_on_titanic/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g779i1/76_accuracy_on_titanic/,155203,1587730200.0,0,,False,,,,
,learnmachinelearning,"This may be a dumb question, as I am not too knowledgable when it comes to this topic or anything related, but how do people use machine learning in a video game? I've seen videos of machine learning programs learning how to play certain games extremely well after a few hours of trial and error. Are they running their coding or program over the game somehow, or are they making their own similar recreation of the game so they can run there code, as I have seen in the Youtuber, Code Bullet's videos? I personally have googled this, but have not found an answer, although I didn't search for longer than 10 to 15 minutes.",t2_39l603x9,False,,0,False,How do people use machine learning in a video game?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7jggh,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1587801093.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This may be a dumb question, as I am not too knowledgable when it comes to this topic or anything related, but how do people use machine learning in a video game? I&amp;#39;ve seen videos of machine learning programs learning how to play certain games extremely well after a few hours of trial and error. Are they running their coding or program over the game somehow, or are they making their own similar recreation of the game so they can run there code, as I have seen in the Youtuber, Code Bullet&amp;#39;s videos? I personally have googled this, but have not found an answer, although I didn&amp;#39;t search for longer than 10 to 15 minutes.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7jggh,True,,Haxerdous,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7jggh/how_do_people_use_machine_learning_in_a_video_game/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7jggh/how_do_people_use_machine_learning_in_a_video_game/,155203,1587772293.0,0,,False,,,,
,learnmachinelearning,"(I hope it is appropriate to post this here)

If you are interested in the engineering side of Machine Learning, please checkout the **free** weekly series we are running:

[Machine Learning Engineering](https://elephantscale.com/learning-series/machine-learning-engineering/) \- Learn tools and techniques to deploy ML 

* Weekly series on Thursdays
* Hands-on
* Watch recordings of previous sessions

[Register for FREE](https://www.eventbrite.com/e/learning-series-machine-learning-engineering-and-devops-tickets-101966741268?aff=RedditLearnMachineLearning)

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/j3kyb6zbbtu41.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=6d3cc398d7dca4b40fb0364d3fa8ba0b99a02998",t2_2lw7eoml,False,,0,False,"""Machine Learning Engineering"" Learning Series (free)",[],r/learnmachinelearning,False,6,,0,70.0,False,t3_g7e8z1,False,dark,0.86,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/VTPrQaLU2X31z1kWUP0JZwf76msS1RXhdImXcko9eOc.jpg,False,,[],{},,,True,,1587783377.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;(I hope it is appropriate to post this here)&lt;/p&gt;

&lt;p&gt;If you are interested in the engineering side of Machine Learning, please checkout the &lt;strong&gt;free&lt;/strong&gt; weekly series we are running:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://elephantscale.com/learning-series/machine-learning-engineering/""&gt;Machine Learning Engineering&lt;/a&gt; - Learn tools and techniques to deploy ML &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Weekly series on Thursdays&lt;/li&gt;
&lt;li&gt;Hands-on&lt;/li&gt;
&lt;li&gt;Watch recordings of previous sessions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=""https://www.eventbrite.com/e/learning-series-machine-learning-engineering-and-devops-tickets-101966741268?aff=RedditLearnMachineLearning""&gt;Register for FREE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/j3kyb6zbbtu41.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6d3cc398d7dca4b40fb0364d3fa8ba0b99a02998""&gt;https://preview.redd.it/j3kyb6zbbtu41.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6d3cc398d7dca4b40fb0364d3fa8ba0b99a02998&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7e8z1,True,,red_blue_mango,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7e8z1/machine_learning_engineering_learning_series_free/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7e8z1/machine_learning_engineering_learning_series_free/,155203,1587754577.0,0,,False,,,"{'j3kyb6zbbtu41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 54, 'x': 108, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb7aab837d4830cba8c6455e905516178b6f0a20'}, {'y': 108, 'x': 216, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=27116aa151bbc55c433a51665d4974366a2e5b87'}, {'y': 160, 'x': 320, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00e1f659bdca7b32ee32f83308cf28c50fc13460'}, {'y': 320, 'x': 640, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6600f922afda8896561141eb036d59ccaefb6a70'}, {'y': 480, 'x': 960, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=648b69ead7b2c57e54607f7badaf47ced7fb074f'}], 's': {'y': 512, 'x': 1024, 'u': 'https://preview.redd.it/j3kyb6zbbtu41.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=6d3cc398d7dca4b40fb0364d3fa8ba0b99a02998'}, 'id': 'j3kyb6zbbtu41'}}",
,learnmachinelearning,"I am very interested about how data analytics can be applied in sports and the ""Moneyball"" aspect. I have basic programming knowledge and have started learning data science in Python. Anyone in this sub got suggestions about what to learn and how to apply for interning positions in the sports industry?

(I have seen some articles about how it is very hard to get a job of this kind, but I am definitely interested in it. I apologize If I sound arrogant or pretentious)",t2_1netdk23,False,,0,False,How to get into the sports analytics industry?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7nj0h,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1587789537.0,,[],{},,,True,,1587818064.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am very interested about how data analytics can be applied in sports and the &amp;quot;Moneyball&amp;quot; aspect. I have basic programming knowledge and have started learning data science in Python. Anyone in this sub got suggestions about what to learn and how to apply for interning positions in the sports industry?&lt;/p&gt;

&lt;p&gt;(I have seen some articles about how it is very hard to get a job of this kind, but I am definitely interested in it. I apologize If I sound arrogant or pretentious)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7nj0h,True,,serDavosOfSeaworth,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7nj0h/how_to_get_into_the_sports_analytics_industry/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7nj0h/how_to_get_into_the_sports_analytics_industry/,155203,1587789264.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

[Very sophisticated 2 layer NN](https://preview.redd.it/31aonyhuutu41.jpg?width=1699&amp;format=pjpg&amp;auto=webp&amp;s=02701480d07f0b517d5be0df6145226332be4ea3)

Hello, I am trying to do all of the math out for a two layer neural network and have hit a road block with understanding in the back propagation phase. If I understand things correctly I need to find the partials of W\_1 and w\_11 w.r.t. Loss.

Assuming I am only back propagating through the red line :

I know that dL/dW = dl/y\_hat \*dy\_hat/W\_1

But I'm not sure if dL/dw\_11 = dL/dy\_hat\*dy\_hat/W\_1\*dh\_1/dw\_11",t2_7jjem,False,,0,False,Help figuring out chain rule with two layer neural network.,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g7g8vh,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/1qZagOqaowQ2U_fntdMoYYYpzYt_0OF5_8dsL0lAUQo.jpg,1587762572.0,,[],{},,,True,,1587789879.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/31aonyhuutu41.jpg?width=1699&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=02701480d07f0b517d5be0df6145226332be4ea3""&gt;Very sophisticated 2 layer NN&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hello, I am trying to do all of the math out for a two layer neural network and have hit a road block with understanding in the back propagation phase. If I understand things correctly I need to find the partials of W_1 and w_11 w.r.t. Loss.&lt;/p&gt;

&lt;p&gt;Assuming I am only back propagating through the red line :&lt;/p&gt;

&lt;p&gt;I know that dL/dW = dl/y_hat *dy_hat/W_1&lt;/p&gt;

&lt;p&gt;But I&amp;#39;m not sure if dL/dw_11 = dL/dy_hat*dy_hat/W_1*dh_1/dw_11&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7g8vh,True,,raidicy,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7g8vh/help_figuring_out_chain_rule_with_two_layer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7g8vh/help_figuring_out_chain_rule_with_two_layer/,155203,1587761079.0,1,,False,,,"{'31aonyhuutu41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 148, 'x': 108, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6dc3e4db5d9c06fea73296907f2f52226c22bce0'}, {'y': 297, 'x': 216, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ea701f472ad6c274c7c2c8d77155f9cbf9beb152'}, {'y': 440, 'x': 320, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbb84e7fcd7dcf1401f98cb2d3dbc6a1ca5f6c6d'}, {'y': 880, 'x': 640, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d69192125718bfb4b255c0b055f081e6b90d5f68'}, {'y': 1321, 'x': 960, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a044d0d0575de67e64d04223f15ba68e54856324'}, {'y': 1486, 'x': 1080, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0f78d5038a9861123a8dee7206f3e8496ba7245'}], 's': {'y': 2338, 'x': 1699, 'u': 'https://preview.redd.it/31aonyhuutu41.jpg?width=1699&amp;format=pjpg&amp;auto=webp&amp;s=02701480d07f0b517d5be0df6145226332be4ea3'}, 'id': '31aonyhuutu41'}}",
,learnmachinelearning,,t2_5klbqy31,False,,0,False,"Given a positively co-related feature, how come its coeff. comes negative in a multiple Linear Reg. equation?",[],r/learnmachinelearning,False,6,,0,,False,t3_g7n53i,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587816249.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7n53i,True,,mama_oooooo,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7n53i/given_a_positively_corelated_feature_how_come_its/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7n53i/given_a_positively_corelated_feature_how_come_its/,155203,1587787449.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Hand gesture recognition without an optical camera | Radar + AI,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g76224,False,light,0.96,,public,24,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/7zLr6Fn84dc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Hand gesture recognition without an optical camera | Radar + AI', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/7zLr6Fn84dc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7zLr6Fn84dc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/7zLr6Fn84dc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g76224', 'height': 338}",Discussion,False,24,,False,https://b.thumbs.redditmedia.com/5E7JO5eTXR5Z_hb5zvBKd01-ZooGNpDHT8gUrjpW1as.jpg,False,,[],{},rich:video,,False,,1587752911.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/T5ZW0zKQa-wwImwzqnNvx9B1_Ej3tGVB_PYmx9oKhDM.jpg?auto=webp&amp;s=3c2bbadab466579dfbfcdc54c2ca75dd5b14d644', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/T5ZW0zKQa-wwImwzqnNvx9B1_Ej3tGVB_PYmx9oKhDM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=54b54e9069f8c1b94a4fc7cea0295cfce32003f5', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/T5ZW0zKQa-wwImwzqnNvx9B1_Ej3tGVB_PYmx9oKhDM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0701296d878bbee4166d8401da445b3770d4ec9d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/T5ZW0zKQa-wwImwzqnNvx9B1_Ej3tGVB_PYmx9oKhDM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a4ddce7b6ed89d1b902915f472873a776e77929', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'sD8ci1dHKVlBmJhgESVwVtynT3Jke0OWKuLt7-2C53w'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g76224,True,,cmillionaire9,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g76224/hand_gesture_recognition_without_an_optical/,all_ads,False,https://youtu.be/7zLr6Fn84dc,155203,1587724111.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Hand gesture recognition without an optical camera | Radar + AI', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/7zLr6Fn84dc?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/7zLr6Fn84dc/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Basicaly how every week works:

The salesman tells the director what quota he will make, based on the last week. So, last supposed he did 1.0 m last week, now he says he will do 1.3. 

What I was thinking is to get the 1.3, put it on the model (with more features besides it) and predict if he will or not hit quota.

Did I was too confusing?

Thanks!",t2_31p7yre5,False,,0,False,I am thinking about creating a model for predict if a salesman will or will not hit quota every week. Do you guys have some suggestions on how to start?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7mmz5,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587814046.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Basicaly how every week works:&lt;/p&gt;

&lt;p&gt;The salesman tells the director what quota he will make, based on the last week. So, last supposed he did 1.0 m last week, now he says he will do 1.3. &lt;/p&gt;

&lt;p&gt;What I was thinking is to get the 1.3, put it on the model (with more features besides it) and predict if he will or not hit quota.&lt;/p&gt;

&lt;p&gt;Did I was too confusing?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7mmz5,True,,sergiopestana,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7mmz5/i_am_thinking_about_creating_a_model_for_predict/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7mmz5/i_am_thinking_about_creating_a_model_for_predict/,155203,1587785246.0,0,,False,,,,
,learnmachinelearning,"I have extracted features from Resnet50 which amount to 100352 features per image, then when I divide the dataset into training, validation, and testing, I feed them to my own constructed network.

I have 3 classes to classify. The strange thing is that my model starts with around 70% training accuracy, and 90% validation accuracy, and increases little by little until like 95% validation accuracy.

The testing results are similar, the model predicts most of the tests correctly. 

My question is that, is it normal for a model to have a high accuracy from the first epoch ? is that normal ?",t2_14kfio,False,,0,False,Transfer learning - model gets very high accuracy on the first epoch...,[],r/learnmachinelearning,False,6,,0,,False,t3_g7me57,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587812994.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have extracted features from Resnet50 which amount to 100352 features per image, then when I divide the dataset into training, validation, and testing, I feed them to my own constructed network.&lt;/p&gt;

&lt;p&gt;I have 3 classes to classify. The strange thing is that my model starts with around 70% training accuracy, and 90% validation accuracy, and increases little by little until like 95% validation accuracy.&lt;/p&gt;

&lt;p&gt;The testing results are similar, the model predicts most of the tests correctly. &lt;/p&gt;

&lt;p&gt;My question is that, is it normal for a model to have a high accuracy from the first epoch ? is that normal ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7me57,True,,PetterGriffinFriend,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7me57/transfer_learning_model_gets_very_high_accuracy/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7me57/transfer_learning_model_gets_very_high_accuracy/,155203,1587784194.0,0,,False,,,,
,learnmachinelearning,"Hope everybody is doing great!

Recently, I just started linear algebra and statistics for machine learning. However, I am not sure how I can practice Python along side with studying mathematics since I don't want to lose my Python skills. Are there any websites through which I can brush up my skills in Python and at the same time prepare for machine learning?

The question might be confusing for some people but being a Machine Learning Engineer means being strong in both programming and mathematics. 

Any tips/criticisms would be greatly appreciated.",t2_2ufovsg1,False,,0,False,Help needed for a newbie in Machine Learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_g7fw73,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587788708.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hope everybody is doing great!&lt;/p&gt;

&lt;p&gt;Recently, I just started linear algebra and statistics for machine learning. However, I am not sure how I can practice Python along side with studying mathematics since I don&amp;#39;t want to lose my Python skills. Are there any websites through which I can brush up my skills in Python and at the same time prepare for machine learning?&lt;/p&gt;

&lt;p&gt;The question might be confusing for some people but being a Machine Learning Engineer means being strong in both programming and mathematics. &lt;/p&gt;

&lt;p&gt;Any tips/criticisms would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7fw73,True,,codefreak-123,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7fw73/help_needed_for_a_newbie_in_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7fw73/help_needed_for_a_newbie_in_machine_learning/,155203,1587759908.0,0,,False,,,,
,learnmachinelearning,"I am a beginner and learning machine learning and deep learning . I am mostly able to implement majority of algorithms of machine learning on my own .
But I have a lot of difficulty in  implementation  of gradient descent and backpropagation algorithm using numpy on my own from scratch. 

I have seen the numpy videos and 3blue1brown 

Please tell me how to be more effective in implementing algorithms using numpy from scratch and also neural networks",t2_50tvaljh,False,,0,False,Help in implementing Neural networks and numpy,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7llzu,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587809619.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a beginner and learning machine learning and deep learning . I am mostly able to implement majority of algorithms of machine learning on my own .
But I have a lot of difficulty in  implementation  of gradient descent and backpropagation algorithm using numpy on my own from scratch. &lt;/p&gt;

&lt;p&gt;I have seen the numpy videos and 3blue1brown &lt;/p&gt;

&lt;p&gt;Please tell me how to be more effective in implementing algorithms using numpy from scratch and also neural networks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7llzu,True,,drunkeninja42,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7llzu/help_in_implementing_neural_networks_and_numpy/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7llzu/help_in_implementing_neural_networks_and_numpy/,155203,1587780819.0,0,,False,,,,
,learnmachinelearning,,t2_6anfnvtm,False,,0,False,A Simple Machine Learning Project Since Data Collect until deploy with Python and Django,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,79.0,False,t3_g7d15k,False,light,0.69,,public,5,0,{},140.0,,False,[],,True,False,,{},Project,False,5,,False,https://b.thumbs.redditmedia.com/AwlV_Y3PM41eagTc_MtyL8RIjJU0HXTF9gxhKJiHI4Q.jpg,False,,[],{},image,,False,,1587779557.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/uoerxybh0tu41.png?auto=webp&amp;s=13be318f327b06f400c68fee871242a4d90b4c69', 'width': 2180, 'height': 1239}, 'resolutions': [{'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b847e568feccb73a3d162b26a20f5ea6e072f8d', 'width': 108, 'height': 61}, {'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=521885b4141c199432cdd1f10a5028b48b889a0c', 'width': 216, 'height': 122}, {'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=044d7aeddd42d6796a28a2ec8805be526a69d2e6', 'width': 320, 'height': 181}, {'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a60a7f40255c971162730ee08d0ac1b977fd79c1', 'width': 640, 'height': 363}, {'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=77e66fe4aa0f25551e7020a98860eff35e0f8b7e', 'width': 960, 'height': 545}, {'url': 'https://preview.redd.it/uoerxybh0tu41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b76b478fbf693b4dd82ff853f6d406273bb82ee7', 'width': 1080, 'height': 613}], 'variants': {}, 'id': 'YVcI5t_v-e_8dAYvC4KFCqN02pl4NMFQC1tuRUiTRzM'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g7d15k,True,,diegopmayer,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7d15k/a_simple_machine_learning_project_since_data/,all_ads,False,https://i.redd.it/uoerxybh0tu41.png,155203,1587750757.0,1,,False,,,,
,learnmachinelearning,"Hey all, 

I'm very new to this and don't quite know the ins and outs yet (My prof quite literally threw code at us and said good luck). He wants us to develop a model that has an accumulated loss that's less than 0.7 and I've been chasing that all day. Almost all of my models have been at around 1.0 to 0.95 using an assortment of trials. As of right now, I just recently got my 'nicest' loss to epoch curve (shown) using a conv2d, ReLU, and MaxPool2d for the network (also shown). However, the accumulated loss is somewhere around 1 still. What can I do to help reduce the loss? Any tips and tricks would be MUCH appreciated! 

For some more insight, I've quite literally been doing this all day and even with running it on a GPU; it takes a bit of time per trail so hopefully asking some pro's will save some time!

&amp;#x200B;

https://preview.redd.it/a252x4d60vu41.png?width=448&amp;format=png&amp;auto=webp&amp;s=068d880005ea138811e40ae7cfd70a5dbd04cde3

https://preview.redd.it/7sy9o0wpzuu41.png?width=509&amp;format=png&amp;auto=webp&amp;s=88d4b8afc7df76c2c0208c14360e06cf344c11ec",t2_16mrks,False,,0,False,RC car training - loss question,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,43.0,False,t3_g7k5sy,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/N3HEKKFKZFWG60StzRMROWuJYfybZwd0XujpRsxSaVY.jpg,False,,[],{},,,True,,1587803834.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all, &lt;/p&gt;

&lt;p&gt;I&amp;#39;m very new to this and don&amp;#39;t quite know the ins and outs yet (My prof quite literally threw code at us and said good luck). He wants us to develop a model that has an accumulated loss that&amp;#39;s less than 0.7 and I&amp;#39;ve been chasing that all day. Almost all of my models have been at around 1.0 to 0.95 using an assortment of trials. As of right now, I just recently got my &amp;#39;nicest&amp;#39; loss to epoch curve (shown) using a conv2d, ReLU, and MaxPool2d for the network (also shown). However, the accumulated loss is somewhere around 1 still. What can I do to help reduce the loss? Any tips and tricks would be MUCH appreciated! &lt;/p&gt;

&lt;p&gt;For some more insight, I&amp;#39;ve quite literally been doing this all day and even with running it on a GPU; it takes a bit of time per trail so hopefully asking some pro&amp;#39;s will save some time!&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/a252x4d60vu41.png?width=448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=068d880005ea138811e40ae7cfd70a5dbd04cde3""&gt;https://preview.redd.it/a252x4d60vu41.png?width=448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=068d880005ea138811e40ae7cfd70a5dbd04cde3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7sy9o0wpzuu41.png?width=509&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88d4b8afc7df76c2c0208c14360e06cf344c11ec""&gt;https://preview.redd.it/7sy9o0wpzuu41.png?width=509&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=88d4b8afc7df76c2c0208c14360e06cf344c11ec&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7k5sy,True,,DynamicDumbA55,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7k5sy/rc_car_training_loss_question/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7k5sy/rc_car_training_loss_question/,155203,1587775034.0,0,,False,,,"{'7sy9o0wpzuu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 83, 'x': 108, 'u': 'https://preview.redd.it/7sy9o0wpzuu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db1383f607e3a27b20ebcaaaa625c733ccd37815'}, {'y': 167, 'x': 216, 'u': 'https://preview.redd.it/7sy9o0wpzuu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16de2d4b735c3af7459536406f828c134591120a'}, {'y': 247, 'x': 320, 'u': 'https://preview.redd.it/7sy9o0wpzuu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d0e9e6cac9d1512256f8ef95afb40bf43e875c1'}], 's': {'y': 394, 'x': 509, 'u': 'https://preview.redd.it/7sy9o0wpzuu41.png?width=509&amp;format=png&amp;auto=webp&amp;s=88d4b8afc7df76c2c0208c14360e06cf344c11ec'}, 'id': '7sy9o0wpzuu41'}, 'a252x4d60vu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 33, 'x': 108, 'u': 'https://preview.redd.it/a252x4d60vu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8aee2ebf1fd94ec37558b9649d051bc57c447a10'}, {'y': 67, 'x': 216, 'u': 'https://preview.redd.it/a252x4d60vu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4d70e83e125ed489321887658eb49901dc0c5c2'}, {'y': 100, 'x': 320, 'u': 'https://preview.redd.it/a252x4d60vu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2bb3a129805eba1fd234f6f61fb22abc035956d'}], 's': {'y': 140, 'x': 448, 'u': 'https://preview.redd.it/a252x4d60vu41.png?width=448&amp;format=png&amp;auto=webp&amp;s=068d880005ea138811e40ae7cfd70a5dbd04cde3'}, 'id': 'a252x4d60vu41'}}",
,learnmachinelearning," Hello, beginner in data science here. I have created many different models, yet I have no idea when and how to deploy which one and how to imporve the model. My goal is to improve my scores in kaggle competitions. What and how should I learn? Any help is appreciated in advance.",t2_4ec5yiw3,False,,0,False,Kaggle Mastery,[],r/learnmachinelearning,False,6,,0,,False,t3_g7byp0,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1587776170.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, beginner in data science here. I have created many different models, yet I have no idea when and how to deploy which one and how to imporve the model. My goal is to improve my scores in kaggle competitions. What and how should I learn? Any help is appreciated in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7byp0,True,,MedicaLatina,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7byp0/kaggle_mastery/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7byp0/kaggle_mastery/,155203,1587747370.0,0,,False,,,,
,learnmachinelearning,"Hello all

&amp;#x200B;

I am writing my BA thesis on machine learning. Initially, the idea was to conduct an analysis of failed companies based on financial indicators.

As you know, you need to do some research in your BA thesis. Analysis of this data would guarantee just such an analysis.

&amp;#x200B;

Unfortunately, I cannot use the same data that has already been used in another study.

&amp;#x200B;

As I'm a beginner in the subject, I wanted to find some research that I can do using simple, ready-made algorithms using python 3 and the scikit-learn library. I am still working on a chapter on theory, although I have a month to go and I need to find an idea where I could apply these algorithms to pass my research in my BA thesis.

&amp;#x200B;

I know that databases are available on pages like kaggle. If you have any idea where I could use simple classifiers in the form of an examination certain event, I would be very grateful.

&amp;#x200B;

I am talking about classifiers such as: Logistic Regression, Support Vector Machine, Naive Bayes classifier, Decision Tree classifier, Random Forest Classification.

&amp;#x200B;

For all your help THANK YOU!.",t2_13p4ln,False,,0,False,Machine Learning BA thesis.,[],r/learnmachinelearning,False,6,,0,,False,t3_g7jlzf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587801683.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am writing my BA thesis on machine learning. Initially, the idea was to conduct an analysis of failed companies based on financial indicators.&lt;/p&gt;

&lt;p&gt;As you know, you need to do some research in your BA thesis. Analysis of this data would guarantee just such an analysis.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Unfortunately, I cannot use the same data that has already been used in another study.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;As I&amp;#39;m a beginner in the subject, I wanted to find some research that I can do using simple, ready-made algorithms using python 3 and the scikit-learn library. I am still working on a chapter on theory, although I have a month to go and I need to find an idea where I could apply these algorithms to pass my research in my BA thesis.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I know that databases are available on pages like kaggle. If you have any idea where I could use simple classifiers in the form of an examination certain event, I would be very grateful.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am talking about classifiers such as: Logistic Regression, Support Vector Machine, Naive Bayes classifier, Decision Tree classifier, Random Forest Classification.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For all your help THANK YOU!.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7jlzf,True,,informacja,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7jlzf/machine_learning_ba_thesis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7jlzf/machine_learning_ba_thesis/,155203,1587772883.0,0,,False,,,,
,learnmachinelearning,"Another attempt at Kaggle's M5 forecasting competition, this time finally taking a common ML approach. For those without context, the data is given with each (product, store, sales from 1000+ days) as one row, and in order for us to do interesting feature engineering we need for it to be (product, store, day number x, sales) for each day. So this Notebook is about how such time series transformations can be implemented and how to create lag features after the transformation.

There is also a local WRMSSE calculation in this Notebook that matches with the leaderboard score, so we don't have to submit to see the actual model performance.

The notebook where this is implemented: [https://www.kaggle.com/qcw171717/time-series-transformation](https://www.kaggle.com/qcw171717/time-series-transformation)

And I made a video explaining this as well: [https://youtu.be/iU3HhKH325g](https://youtu.be/iU3HhKH325g)",t2_522c8qe4,False,,0,False,"M5 Competition: Time Series Transformation, LightGBM, and a local WRMSSE","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7jl8a,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1587801602.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Another attempt at Kaggle&amp;#39;s M5 forecasting competition, this time finally taking a common ML approach. For those without context, the data is given with each (product, store, sales from 1000+ days) as one row, and in order for us to do interesting feature engineering we need for it to be (product, store, day number x, sales) for each day. So this Notebook is about how such time series transformations can be implemented and how to create lag features after the transformation.&lt;/p&gt;

&lt;p&gt;There is also a local WRMSSE calculation in this Notebook that matches with the leaderboard score, so we don&amp;#39;t have to submit to see the actual model performance.&lt;/p&gt;

&lt;p&gt;The notebook where this is implemented: &lt;a href=""https://www.kaggle.com/qcw171717/time-series-transformation""&gt;https://www.kaggle.com/qcw171717/time-series-transformation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And I made a video explaining this as well: &lt;a href=""https://youtu.be/iU3HhKH325g""&gt;https://youtu.be/iU3HhKH325g&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?auto=webp&amp;s=317a9be4dd095d5a4b95cfdd96ada08acea08513', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/hIZn74UCUzMss02AeqYwTjjr9Q4P0f4_c6SjRwG_mWM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0ef6b067fd0b46d01dc9f262edb560782bc9f2c', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'p15coSqe7L8wApjnVlwASEYE50BcnmvRuPbSVpGUPaM'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g7jl8a,True,,sweetpotatowedge9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7jl8a/m5_competition_time_series_transformation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7jl8a/m5_competition_time_series_transformation/,155203,1587772802.0,0,,False,,,,
,learnmachinelearning,"Now that I have some free time, what would be the best Aws certification course for someone who wants to pursue a career in data science?

Thanks",t2_28nup7z5,False,,0,False,What's the Best AWS certification for a Data analytics Graduate!,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7jdz2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587800844.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Now that I have some free time, what would be the best Aws certification course for someone who wants to pursue a career in data science?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g7jdz2,True,,internet----explorer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7jdz2/whats_the_best_aws_certification_for_a_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7jdz2/whats_the_best_aws_certification_for_a_data/,155203,1587772044.0,0,,False,,,,
,learnmachinelearning,,t2_5yd6bcu6,False,,0,False,How I Got Into Machine Learning - relate to your journey,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g79syd,False,dark,0.84,,public,4,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aqDCcuzDcNM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How I Got Into Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aqDCcuzDcNM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/aqDCcuzDcNM/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aqDCcuzDcNM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g79syd', 'height': 338}",,False,4,,False,https://b.thumbs.redditmedia.com/tSlyGk7Bwoe72p2nPrPe12iG9IFSD6HNS5qwVI2jNiU.jpg,False,,[],{},rich:video,,False,,1587768967.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/boyr4OPxQW2kixPIxRX7eum5zVvQLNCWWGspEBo2s_w.jpg?auto=webp&amp;s=fbd9ffc777a0b7ec6bf581fd31d8eccc3327ec38', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/boyr4OPxQW2kixPIxRX7eum5zVvQLNCWWGspEBo2s_w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=56e27678f89cc83689e37e75d3ba4b1adf25868e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/boyr4OPxQW2kixPIxRX7eum5zVvQLNCWWGspEBo2s_w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6614ef092fb298fbcec25d3e9fa6f8e3321ead9d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/boyr4OPxQW2kixPIxRX7eum5zVvQLNCWWGspEBo2s_w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=717cc34655cb619e8cff387e1b1ab0e4b7186f8c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'gOorXONmbV1E_y09OmYpBfEjtOji4CkuST1reu7w66Y'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g79syd,True,,JK_Bielan,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g79syd/how_i_got_into_machine_learning_relate_to_your/,all_ads,False,https://www.youtube.com/watch?v=aqDCcuzDcNM,155203,1587740167.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How I Got Into Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/aqDCcuzDcNM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/aqDCcuzDcNM/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"What if there were a way to reliably measure inner growth? Then, instead of being based on our formerly-ok pattern of consuming resources that is now strangling us, the blockchains and thus the whole financial system would be based on a kind of work virtually any human can do, trending toward enlightened voters and away from disaster.

I have a variety of approaches in mind for faces of a universal 'velvet rack' or 'borg mother of humanity' to know each person well enough to judge trustably, involving using ML to mirror oneself and peer deep into inner galaxies over time. Anyone who wants can build some ML designed to take their own personal data of whatever type and predict what they will like or do, and see what a tangible feeling it is to feel this new kind of mirror reflecting your personality.. maybe even prefer such self-definition to disposable consumption.",t2_167fgi,False,,0,False,Proposal: a human-based 'proof of work' for blockchains,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7j4p6,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,1587799904.0,,[],{},,,True,,1587799889.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What if there were a way to reliably measure inner growth? Then, instead of being based on our formerly-ok pattern of consuming resources that is now strangling us, the blockchains and thus the whole financial system would be based on a kind of work virtually any human can do, trending toward enlightened voters and away from disaster.&lt;/p&gt;

&lt;p&gt;I have a variety of approaches in mind for faces of a universal &amp;#39;velvet rack&amp;#39; or &amp;#39;borg mother of humanity&amp;#39; to know each person well enough to judge trustably, involving using ML to mirror oneself and peer deep into inner galaxies over time. Anyone who wants can build some ML designed to take their own personal data of whatever type and predict what they will like or do, and see what a tangible feeling it is to feel this new kind of mirror reflecting your personality.. maybe even prefer such self-definition to disposable consumption.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g7j4p6,True,,phobrain,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7j4p6/proposal_a_humanbased_proof_of_work_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7j4p6/proposal_a_humanbased_proof_of_work_for/,155203,1587771089.0,0,,False,,,,
,learnmachinelearning,"My dad, who does some high-level cybersecurity consulting (and knows nothing about machine learning), was asking me the other day about an interesting problem that machine learning could be applied to. Here's the problem:

Let's say we have some database full of a whole bunch of data on a company's employees. A lot of this data is strings of fields like their street address, their phone number, or their favorite flavor of ice cream. However, we're not sure if in the entry of this data that any potential sensitive data, like a social security number, slipped in unnoticed. How would we scan a database to alert a data engineer to  double-check that a flagged entry isn't sensitive data? Is it possible to train a text classification algorithm to classify single tokens from a database as sensitive or not sensitive data?

I was interested in working on this as a short personal proof-of-concept project since I have nothing better to do with my spare time in quarantine, but I don't have much experience working with text data in machine learning. I'm brand new to all of NLP. I have a few ideas, but I'm not sure how feasible they are and I'd love to hear what opinions you might have on them or alternative routes I should research. Here are some of my ideas:

1. Named-entity recognition. I might be able to label a token as sensitive data or not sensitive data and train an NER model on a whole table of data as a CSV string with the mocked sensitive data labeled. I'm worried that this approach would be too imbalanced to work in a real scenario, given the ideally few instances of sensitive data that would slip in during entry.
2. Deep learning with word embeddings. In the research I've done, I've seen this used for a lot of document classification where a whole paragraph gets transformed into word embeddings and then a deep neural network is trained to classify a document based on those embeddings. This is where a lot of sources point me to, but I'm not sure this kind of approach can be used to classify a single token as opposed to a whole document, because as far as I understand these word embeddings are meant to get context from a lot of text and not just a single word. I'm also not sure if they can be used on a token that has a mixture of numbers and letters.
3. Regex. I've already implemented a bit of this into a test case for my own knowledge, but it's really difficult for me to capture all the edge cases of what defines some sensitive data, and my regex filter is throwing a lot of false positives and negatives. I'm not sure I'm going to be able to improve it.

In addition, suppose I had an algorithm that performs this, and gives me a list of all the cases of sensitive info it's detected in a database. From this list, I see that a portion of them are false positives, and I want to somehow feed this back into the training cycle and update the model based on these known false positives. Are there algorithms like this that support incremental learning? Or are there other ways to incorporate the known false positives into this process? If you have any other ideas or things you'd like me to research that may be helpful for this project, please let me know!

Thanks for your thoughts!",t2_96m1d8k,False,,0,False,Text classification of a single token,[],r/learnmachinelearning,False,6,,0,,False,t3_g7hf5v,False,dark,0.6,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587793809.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My dad, who does some high-level cybersecurity consulting (and knows nothing about machine learning), was asking me the other day about an interesting problem that machine learning could be applied to. Here&amp;#39;s the problem:&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s say we have some database full of a whole bunch of data on a company&amp;#39;s employees. A lot of this data is strings of fields like their street address, their phone number, or their favorite flavor of ice cream. However, we&amp;#39;re not sure if in the entry of this data that any potential sensitive data, like a social security number, slipped in unnoticed. How would we scan a database to alert a data engineer to  double-check that a flagged entry isn&amp;#39;t sensitive data? Is it possible to train a text classification algorithm to classify single tokens from a database as sensitive or not sensitive data?&lt;/p&gt;

&lt;p&gt;I was interested in working on this as a short personal proof-of-concept project since I have nothing better to do with my spare time in quarantine, but I don&amp;#39;t have much experience working with text data in machine learning. I&amp;#39;m brand new to all of NLP. I have a few ideas, but I&amp;#39;m not sure how feasible they are and I&amp;#39;d love to hear what opinions you might have on them or alternative routes I should research. Here are some of my ideas:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Named-entity recognition. I might be able to label a token as sensitive data or not sensitive data and train an NER model on a whole table of data as a CSV string with the mocked sensitive data labeled. I&amp;#39;m worried that this approach would be too imbalanced to work in a real scenario, given the ideally few instances of sensitive data that would slip in during entry.&lt;/li&gt;
&lt;li&gt;Deep learning with word embeddings. In the research I&amp;#39;ve done, I&amp;#39;ve seen this used for a lot of document classification where a whole paragraph gets transformed into word embeddings and then a deep neural network is trained to classify a document based on those embeddings. This is where a lot of sources point me to, but I&amp;#39;m not sure this kind of approach can be used to classify a single token as opposed to a whole document, because as far as I understand these word embeddings are meant to get context from a lot of text and not just a single word. I&amp;#39;m also not sure if they can be used on a token that has a mixture of numbers and letters.&lt;/li&gt;
&lt;li&gt;Regex. I&amp;#39;ve already implemented a bit of this into a test case for my own knowledge, but it&amp;#39;s really difficult for me to capture all the edge cases of what defines some sensitive data, and my regex filter is throwing a lot of false positives and negatives. I&amp;#39;m not sure I&amp;#39;m going to be able to improve it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition, suppose I had an algorithm that performs this, and gives me a list of all the cases of sensitive info it&amp;#39;s detected in a database. From this list, I see that a portion of them are false positives, and I want to somehow feed this back into the training cycle and update the model based on these known false positives. Are there algorithms like this that support incremental learning? Or are there other ways to incorporate the known false positives into this process? If you have any other ideas or things you&amp;#39;d like me to research that may be helpful for this project, please let me know!&lt;/p&gt;

&lt;p&gt;Thanks for your thoughts!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7hf5v,True,,langfosaurus,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7hf5v/text_classification_of_a_single_token/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7hf5v/text_classification_of_a_single_token/,155203,1587765009.0,0,,False,,,,
,learnmachinelearning,,t2_3a0uwyjx,False,,0,False,Stanford release Andrew Ng's Machine Learning lecture from Autumn 2018 on YT,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g6jyes,False,dark,0.99,,public,726,0,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;display_name=YouTube&amp;url=http%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 450}",140.0,,False,[],"{'oembed': {'provider_url': 'http://youtube.com', 'version': '1.0', 'type': 'video', 'height': 450, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;display_name=YouTube&amp;url=http%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jGwO_UgTS7I/hqdefault.jpg?sqp=-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ==&amp;rs=AOn4CLAKIXyiHbxEsRAvLXcVC8aXvhGbVA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;display_name=YouTube&amp;url=http%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g6jyes', 'height': 450}",,False,726,,True,https://b.thumbs.redditmedia.com/B5I8PaECBR8GYoJ_9lYdaNLXueLL6Bh1RFloq4jNdBk.jpg,False,,[],{},rich:video,,False,,1587664754.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?auto=webp&amp;s=7dad7e69084fe17dc70d1948b6f58fc13d288f0c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54a29aa3b0894b9f5f031e6ec93d6896ab4450', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'WdNtCcCE5kkWeUtaFzPEmvl-T9SFUg0GJCbpJncoGqQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6jyes,True,,TrackLabs,,64,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6jyes/stanford_release_andrew_ngs_machine_learning/,all_ads,False,https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU,155203,1587635954.0,0,"{'oembed': {'provider_url': 'http://youtube.com', 'version': '1.0', 'type': 'video', 'height': 450, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;display_name=YouTube&amp;url=http%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/jGwO_UgTS7I/hqdefault.jpg?sqp=-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ==&amp;rs=AOn4CLAKIXyiHbxEsRAvLXcVC8aXvhGbVA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hi, I'm trying to build an LSTM model which would be able to predict the classes of the next 5 time steps based on the previous 100 time steps. Each time step can have 3 classes which are exclusive. I'm not sure how to approach this? How should my output layer look like?

Thanks",t2_axav08j,False,,0,False,LSTM multi-class classification multiple time steps ahead.,[],r/learnmachinelearning,False,6,,0,,False,t3_g79qdx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587768719.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m trying to build an LSTM model which would be able to predict the classes of the next 5 time steps based on the previous 100 time steps. Each time step can have 3 classes which are exclusive. I&amp;#39;m not sure how to approach this? How should my output layer look like?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g79qdx,True,,everek123,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g79qdx/lstm_multiclass_classification_multiple_time/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g79qdx/lstm_multiclass_classification_multiple_time/,155203,1587739919.0,0,,False,,,,
,learnmachinelearning,"This is my first CNN and right now can only handle really basic, single color classification. This is a project I am submitting for school so I will not be posting the code til next week but I have attached a video. In the video, the image to be classified will appear first, then once that image is closed the prediction will appear in the terminal (working to improve this).

&amp;#x200B;

Improvements I want to make:

1. Allow single images to be submitted for live classification. Right now the test images are loaded in from a directory.
2. When predicting the colors the image will first appear but the prediction for that image does not appear until I close the image. I'd like to have it appear while the image is up.
3. A better way to display the prediction and possibly a way for the user to tell it if it is correct. 

&amp;#x200B;

Any suggestions on how to implement any/all of these would be much appreciated! While I can't post the whole code yet, it is written in Python and tensorflow was used for the model as some background.

&amp;#x200B;

https://reddit.com/link/g7bxqe/video/g6swhh1wpsu41/player",t2_5fnnk6,False,,0,False,CNN for color classification,[],r/learnmachinelearning,False,6,,0,,False,t3_g7bxqe,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587776073.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is my first CNN and right now can only handle really basic, single color classification. This is a project I am submitting for school so I will not be posting the code til next week but I have attached a video. In the video, the image to be classified will appear first, then once that image is closed the prediction will appear in the terminal (working to improve this).&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Improvements I want to make:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Allow single images to be submitted for live classification. Right now the test images are loaded in from a directory.&lt;/li&gt;
&lt;li&gt;When predicting the colors the image will first appear but the prediction for that image does not appear until I close the image. I&amp;#39;d like to have it appear while the image is up.&lt;/li&gt;
&lt;li&gt;A better way to display the prediction and possibly a way for the user to tell it if it is correct. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any suggestions on how to implement any/all of these would be much appreciated! While I can&amp;#39;t post the whole code yet, it is written in Python and tensorflow was used for the model as some background.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://reddit.com/link/g7bxqe/video/g6swhh1wpsu41/player""&gt;https://reddit.com/link/g7bxqe/video/g6swhh1wpsu41/player&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7bxqe,True,,_ygoloiB,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7bxqe/cnn_for_color_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7bxqe/cnn_for_color_classification/,155203,1587747273.0,0,,False,,,"{'g6swhh1wpsu41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/g7bxqe/asset/g6swhh1wpsu41/DASHPlaylist.mpd', 'x': 1920, 'y': 970, 'hlsUrl': 'https://v.redd.it/link/g7bxqe/asset/g6swhh1wpsu41/HLSPlaylist.m3u8', 'id': 'g6swhh1wpsu41', 'isGif': False}}",
,learnmachinelearning,,t2_xsgdp,False,,0,False,[D] Help with Path beyond MOOCs in Machine/Deep Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g7boc5,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,default,False,,[],{},,,False,,1587775225.0,text,6,,,text,self.MachineLearning,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7boc5,True,,Avistian,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7boc5/d_help_with_path_beyond_moocs_in_machinedeep/,all_ads,False,/r/MachineLearning/comments/g77ytu/d_help_with_path_beyond_moocs_in_machinedeep/,155203,1587746425.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': '[removed]', 'author_fullname': 't2_xsgdp', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[D] Help with Path beyond MOOCs in Machine/Deep Learning', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g77ytu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'default', 'edited': 1587739707.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587762081.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': 'moderator', 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g77ytu', 'is_robot_indexable': False, 'report_reasons': None, 'author': 'Avistian', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/g77ytu/d_help_with_path_beyond_moocs_in_machinedeep/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/g77ytu/d_help_with_path_beyond_moocs_in_machinedeep/', 'subreddit_subscribers': 1044414, 'created_utc': 1587733281.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_g77ytu,,
,learnmachinelearning,"Hey there!  


I know this subreddit is filled with noise and educational recommendations, so I apologize for the addition of the heartache -- though hear me out!  


As a recent grad with majors in Biomedical Sciences and Marketing, towards the end of my undergraduate, it was clear to me that bioinformatics/data science is the future of the field with regards to machine learning and big data.  


Since my graduation, I have landed a gig as a private school teacher (for science) and have been teaching myself code through MOOCS such as Coursera (i.e. IBM Data Science &amp; others, I know, controversial, but I needed somewhere to start!)  


I've been in touch with my local University (Im in Toronto if that makes a difference) with regards to potentially applying for a masters in Data Sciences to help kick start my career in the venture.  


Since then, they have recommended that I complete their night school(s) certificate program to bolster my resume to apply.  


I'm really enthusiastic about the big data realm and tough I know I require more training to become a member of the data science realm, I'm up for the task. I just dont want to be gouged and take an incorrect path.  


I had a passion for the pharmaceuticals realm, though no luck was achieved with landing any gigs. I plan to continue my self-education and look for some projects to work on to hone my skills.  


I know it's an uphill battle, but im looking for a battle strategy to accomplish my goal!  


If anyone has any insight or similar stories they'd like to share I'd love to hear them!  
And for those that made it this far and replied, I truly appreciate your time.  


Stay safe everyone!",t2_bg57drq,False,,0,False,New Science Grad Seeking Data Science Prowess (Inspiration),"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7fn8u,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1587787896.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there!  &lt;/p&gt;

&lt;p&gt;I know this subreddit is filled with noise and educational recommendations, so I apologize for the addition of the heartache -- though hear me out!  &lt;/p&gt;

&lt;p&gt;As a recent grad with majors in Biomedical Sciences and Marketing, towards the end of my undergraduate, it was clear to me that bioinformatics/data science is the future of the field with regards to machine learning and big data.  &lt;/p&gt;

&lt;p&gt;Since my graduation, I have landed a gig as a private school teacher (for science) and have been teaching myself code through MOOCS such as Coursera (i.e. IBM Data Science &amp;amp; others, I know, controversial, but I needed somewhere to start!)  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been in touch with my local University (Im in Toronto if that makes a difference) with regards to potentially applying for a masters in Data Sciences to help kick start my career in the venture.  &lt;/p&gt;

&lt;p&gt;Since then, they have recommended that I complete their night school(s) certificate program to bolster my resume to apply.  &lt;/p&gt;

&lt;p&gt;I&amp;#39;m really enthusiastic about the big data realm and tough I know I require more training to become a member of the data science realm, I&amp;#39;m up for the task. I just dont want to be gouged and take an incorrect path.  &lt;/p&gt;

&lt;p&gt;I had a passion for the pharmaceuticals realm, though no luck was achieved with landing any gigs. I plan to continue my self-education and look for some projects to work on to hone my skills.  &lt;/p&gt;

&lt;p&gt;I know it&amp;#39;s an uphill battle, but im looking for a battle strategy to accomplish my goal!  &lt;/p&gt;

&lt;p&gt;If anyone has any insight or similar stories they&amp;#39;d like to share I&amp;#39;d love to hear them!&lt;br/&gt;
And for those that made it this far and replied, I truly appreciate your time.  &lt;/p&gt;

&lt;p&gt;Stay safe everyone!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7fn8u,True,,frankiemec,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7fn8u/new_science_grad_seeking_data_science_prowess/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7fn8u/new_science_grad_seeking_data_science_prowess/,155203,1587759096.0,0,,False,,,,
,learnmachinelearning,"LSTM masks are usually applied to the input and the loss function while calculating the loss - which means the assumption is that the size of the input=size of the output. If my input and output are of different lengths both variable, how do I specify the output mask separately (preferably in the loss function)?",t2_2660fsr8,False,,0,False,Different length input/output LSTM Variable length,[],r/learnmachinelearning,False,6,,0,,False,t3_g7fkmq,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587787664.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;LSTM masks are usually applied to the input and the loss function while calculating the loss - which means the assumption is that the size of the input=size of the output. If my input and output are of different lengths both variable, how do I specify the output mask separately (preferably in the loss function)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7fkmq,True,,icyalps0808,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7fkmq/different_length_inputoutput_lstm_variable_length/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7fkmq/different_length_inputoutput_lstm_variable_length/,155203,1587758864.0,0,,False,,,,
,learnmachinelearning,"**Current: Sophomore in College (Mathematics &amp; CS)**

Over the summer I want to begin my journey in the machine learning field. 

I am doing a few thing currently.

1. Taking Introduction CS50 class \[Current WEEK 2\]  - this course is going very decent for me as I already know most of the basics but I really want a strong foundation of the basics.
2. Taking MIT Introduction To Linear Algebra - this course is going not so well, I do not really even know where to begin but I kind of understand it.

My Plan:

1. Finish CS50
2. Finish MIT Linear Algebra
3. Continue Learning Python and related libraries.
4. Learn Git
5. Learn Linux Command Line

Now here is where I need help and some I can make some comments and concerns on that plan. First off, I have never taken a calculus course in my life. I feel like it would be pretty easy based off what others say but I also get the notion that Im not the best at math (but I believe that math just takes time and you will eventually get it - I love it and have ZERO trouble learning new logic). 

I would like to know a few things, and I invite anyone who knows anything about machine learning to comment on this thread. 

1. What is worth my time out of my plan?
2. Is this plan the correct path based on my current knowledge now?
3. Anything I need to change?
4. Too much at once? (I am dedicating around 4 hours of pure in the books study time a day, which for me is not that bad because I have nothing better to do right now)
5. If you could start over from where you are now and put yourself in the shoes of someone who just got into CS and Machine Learning what would  be your plan? What would you do different? (serious answer and thoughtful answers only please, I really want to get to know what people regret)

Thanks!",t2_5kzz0sqb,False,,0,False,Advice on Plan to Learn Machine Learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g7f0ff,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587785819.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;Current: Sophomore in College (Mathematics &amp;amp; CS)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Over the summer I want to begin my journey in the machine learning field. &lt;/p&gt;

&lt;p&gt;I am doing a few thing currently.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Taking Introduction CS50 class [Current WEEK 2]  - this course is going very decent for me as I already know most of the basics but I really want a strong foundation of the basics.&lt;/li&gt;
&lt;li&gt;Taking MIT Introduction To Linear Algebra - this course is going not so well, I do not really even know where to begin but I kind of understand it.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My Plan:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Finish CS50&lt;/li&gt;
&lt;li&gt;Finish MIT Linear Algebra&lt;/li&gt;
&lt;li&gt;Continue Learning Python and related libraries.&lt;/li&gt;
&lt;li&gt;Learn Git&lt;/li&gt;
&lt;li&gt;Learn Linux Command Line&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now here is where I need help and some I can make some comments and concerns on that plan. First off, I have never taken a calculus course in my life. I feel like it would be pretty easy based off what others say but I also get the notion that Im not the best at math (but I believe that math just takes time and you will eventually get it - I love it and have ZERO trouble learning new logic). &lt;/p&gt;

&lt;p&gt;I would like to know a few things, and I invite anyone who knows anything about machine learning to comment on this thread. &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What is worth my time out of my plan?&lt;/li&gt;
&lt;li&gt;Is this plan the correct path based on my current knowledge now?&lt;/li&gt;
&lt;li&gt;Anything I need to change?&lt;/li&gt;
&lt;li&gt;Too much at once? (I am dedicating around 4 hours of pure in the books study time a day, which for me is not that bad because I have nothing better to do right now)&lt;/li&gt;
&lt;li&gt;If you could start over from where you are now and put yourself in the shoes of someone who just got into CS and Machine Learning what would  be your plan? What would you do different? (serious answer and thoughtful answers only please, I really want to get to know what people regret)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7f0ff,True,,CamBowen,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7f0ff/advice_on_plan_to_learn_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7f0ff/advice_on_plan_to_learn_machine_learning/,155203,1587757019.0,0,,False,,,,
,learnmachinelearning,,t2_5dk1rkk2,False,,0,False,Are datastrcutues and algorithms important to get a job as a machine learning engineer?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6zgct,False,dark,0.96,,public,19,0,{},,,False,[],,False,False,,{},,False,19,,False,self,False,,[],{},,,True,,1587720878.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6zgct,True,,shawn2james,,24,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6zgct/are_datastrcutues_and_algorithms_important_to_get/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6zgct/are_datastrcutues_and_algorithms_important_to_get/,155203,1587692078.0,0,,False,,,,
,learnmachinelearning,"Hi,

I've been trying to develop a vehicle detection project using [EfficientDet.](https://github.com/google/automl/tree/master/efficientdet) It works well on my test videos except one thing. There is a false positive here.

&amp;#x200B;

https://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;format=png&amp;auto=webp&amp;s=c5db9953b3e3a994b173030c14e597652fb01b08

I've tried all EfficientDet versions of d0...to d7. I've tried to re-train model with images which I've labeled. But This false positive always occurs. I am a newbie, and I have little knowledge. How can I fix it?",t2_4aky7agd,False,,0,False,I can't fix a false positive in vehicle detection?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,68.0,False,t3_g76rvs,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},HELP,False,3,,False,https://b.thumbs.redditmedia.com/PdxZIYIlD_RUWpdKNCQfLNxcVDnAeh_hUCiR_wrWYzo.jpg,False,,[],{},self,,True,,1587756632.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been trying to develop a vehicle detection project using &lt;a href=""https://github.com/google/automl/tree/master/efficientdet""&gt;EfficientDet.&lt;/a&gt; It works well on my test videos except one thing. There is a false positive here.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5db9953b3e3a994b173030c14e597652fb01b08""&gt;https://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5db9953b3e3a994b173030c14e597652fb01b08&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve tried all EfficientDet versions of d0...to d7. I&amp;#39;ve tried to re-train model with images which I&amp;#39;ve labeled. But This false positive always occurs. I am a newbie, and I have little knowledge. How can I fix it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G6aQuvFGoKT6DkvMUhT98tTWKJTprwIJGUokmf8lbFw.jpg?auto=webp&amp;s=ca4f089d991643cba8b509746dd27f8271bd4e65', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/G6aQuvFGoKT6DkvMUhT98tTWKJTprwIJGUokmf8lbFw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9b97b0ea10d56f55b5078e6f841ed979e2e610a8', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/G6aQuvFGoKT6DkvMUhT98tTWKJTprwIJGUokmf8lbFw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=697bc1655d471bbb13107195d9faa49900774e0c', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/G6aQuvFGoKT6DkvMUhT98tTWKJTprwIJGUokmf8lbFw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5f319410071c0414778a3392ddcb52ef5e4332c', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'qn7Si4DYICJWnWjv9W9aHfIC6thnbf7TpaCBcDj9hSw'}], 'enabled': False}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g76rvs,True,,hernancrespo89,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g76rvs/i_cant_fix_a_false_positive_in_vehicle_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g76rvs/i_cant_fix_a_false_positive_in_vehicle_detection/,155203,1587727832.0,0,,False,,,"{'y6bgr0h34ru41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 53, 'x': 108, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=560c84469939317779554b175da95a906b06da38'}, {'y': 106, 'x': 216, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a36088da69d21e65707d3540608b5e362bf0da7'}, {'y': 157, 'x': 320, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b09112af1a9be584a02b7a79653a5ca7d3bc4540'}, {'y': 315, 'x': 640, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a9decf1d265a55b6e682e25bf558fadfa8bbddfa'}, {'y': 472, 'x': 960, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c8250115df1e34170cacb1971012cce00834eee7'}, {'y': 531, 'x': 1080, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fd811735d641c743d9d814b930b70a7654407c72'}], 's': {'y': 592, 'x': 1202, 'u': 'https://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;format=png&amp;auto=webp&amp;s=c5db9953b3e3a994b173030c14e597652fb01b08'}, 'id': 'y6bgr0h34ru41'}}",
,learnmachinelearning,"I need to create a tensor of zeros with dimension (batch\_size, decoder\_latent\_dim).

I can't just use a placeholder like other responses suggest:

    placeholder = K.placeholder(shape=(None, decoder_latent_dim)) 
    h = K.zeros_like(placeholder, name='h')

Because I would still have to feed a value to the placeholder, and I don't want that because I don't know the batch size beforehand.

Is there a way to utilize the dynamic shapes of other tensors, say for example the Input tensor, to obtain the batch size and use it to create a tensor with custom shape (batch\_size, decoder\_latent\_dim)?",t2_1o46hvce,False,,0,False,Dynamically shaped tensor in Keras,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g7cf7b,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587777628.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to create a tensor of zeros with dimension (batch_size, decoder_latent_dim).&lt;/p&gt;

&lt;p&gt;I can&amp;#39;t just use a placeholder like other responses suggest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;placeholder = K.placeholder(shape=(None, decoder_latent_dim)) 
h = K.zeros_like(placeholder, name=&amp;#39;h&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because I would still have to feed a value to the placeholder, and I don&amp;#39;t want that because I don&amp;#39;t know the batch size beforehand.&lt;/p&gt;

&lt;p&gt;Is there a way to utilize the dynamic shapes of other tensors, say for example the Input tensor, to obtain the batch size and use it to create a tensor with custom shape (batch_size, decoder_latent_dim)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g7cf7b,True,,starzmustdie,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7cf7b/dynamically_shaped_tensor_in_keras/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7cf7b/dynamically_shaped_tensor_in_keras/,155203,1587748828.0,0,,False,,,,
,learnmachinelearning,I make my own multi layer perception in python use no libraries and I works. But the time it takes to train is vastly slower than if I build and train it using tensor flow. Why is this?,t2_49xlrsbx,False,,0,False,Why is my Ann slower than tensorflows?,[],r/learnmachinelearning,False,6,,0,,False,t3_g7c995,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587777103.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I make my own multi layer perception in python use no libraries and I works. But the time it takes to train is vastly slower than if I build and train it using tensor flow. Why is this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7c995,True,,willbill4197,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7c995/why_is_my_ann_slower_than_tensorflows/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g7c995/why_is_my_ann_slower_than_tensorflows/,155203,1587748303.0,0,,False,,,,
,learnmachinelearning,"Recent years have seen a plethora of pre-trained models such as ULMFiT, BERT, GPT, etc being open-sourced to the NLP community. One of the latest and SOTA being T5: Text-to-Text-Transfer-Transformers Model which was open-sourced near December last year. Read more at - [https://prakhartechviz.blogspot.com/2020/04/t5-text-to-text-transfer-transformer.html](https://prakhartechviz.blogspot.com/2020/04/t5-text-to-text-transfer-transformer.html) Github - [https://github.com/prakhar21/T5-Text-to-Text-Transfer-Transformer](https://github.com/prakhar21/T5-Text-to-Text-Transfer-Transformer)",t2_hkv9s,False,,0,False,Understanding T5: Text-to-Text-Transfer-Transformer,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g771pj,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},self,,True,,1587757993.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recent years have seen a plethora of pre-trained models such as ULMFiT, BERT, GPT, etc being open-sourced to the NLP community. One of the latest and SOTA being T5: Text-to-Text-Transfer-Transformers Model which was open-sourced near December last year. Read more at - &lt;a href=""https://prakhartechviz.blogspot.com/2020/04/t5-text-to-text-transfer-transformer.html""&gt;https://prakhartechviz.blogspot.com/2020/04/t5-text-to-text-transfer-transformer.html&lt;/a&gt; Github - &lt;a href=""https://github.com/prakhar21/T5-Text-to-Text-Transfer-Transformer""&gt;https://github.com/prakhar21/T5-Text-to-Text-Transfer-Transformer&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?auto=webp&amp;s=f62564e7f311ba3a3af02162e47fc5a1c9d91ac9', 'width': 1143, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=45f058ed5c52dd95ce86811c09112d1233d52b95', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dceb92abf43017c293c7ed8546f84b9ff9e08e40', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08feff2c5e023ec49250eb73c9709633eaecfcac', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e017bf5872611060088e05f35263c03b2566d56', 'width': 640, 'height': 335}, {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=736ecf392c3a7a73e8085400038e59604b42b115', 'width': 960, 'height': 503}, {'url': 'https://external-preview.redd.it/QhycRhHIkuOazD9tvuyPOvz2mopj5EXjMmr6z3azdZY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=186bdafc8f47aad72a0e82411a1ab6c0adf95257', 'width': 1080, 'height': 566}], 'variants': {}, 'id': '_KL_8aVxTTO1yjRrz5-HcaeXJcTVklDVGaMp_fUqkXY'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g771pj,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g771pj/understanding_t5_texttotexttransfertransformer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g771pj/understanding_t5_texttotexttransfertransformer/,155203,1587729193.0,0,,False,,,,
,learnmachinelearning,"Hi,

I'm looking into a project possibly using some person re ID for a small but notable part of it. I've read some survey papers and and seen a few models in the wild which look very encouraging but wanted to ask if anyone who'd actually used them how reasonable this use case seems:

 - Person reidentification of a known person within a video camera stream of 5-10 people given the ground truth (bounding box annotation of that person), and accurate person tracking. We KNOW the person exists in one of the 5-10 people. Both the ground truth and test cases can be provided as video streams. Location could be a retail outlet.

Thanks in advance!",t2_hqdpa,False,,0,False,Current State of Person Reidentification,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g79zxx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587769616.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking into a project possibly using some person re ID for a small but notable part of it. I&amp;#39;ve read some survey papers and and seen a few models in the wild which look very encouraging but wanted to ask if anyone who&amp;#39;d actually used them how reasonable this use case seems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Person reidentification of a known person within a video camera stream of 5-10 people given the ground truth (bounding box annotation of that person), and accurate person tracking. We KNOW the person exists in one of the 5-10 people. Both the ground truth and test cases can be provided as video streams. Location could be a retail outlet.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g79zxx,True,,freshprinceofuk,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g79zxx/current_state_of_person_reidentification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g79zxx/current_state_of_person_reidentification/,155203,1587740816.0,0,,False,,,,
,learnmachinelearning,"I read somewhere that decreasing the learning rate (making it smaller) will make the model train slower but will increase the accuracy as well.

However, currently I have a model, and I started with a learning rate of 0.0001, the accuracy was high, but when I decrease it further, lets say 0.000001, the accuracy gets lower and lower...

Is that normal ??",t2_14kfio,False,,0,False,"Decreasing the learning rate, decreases the accuracy",[],r/learnmachinelearning,False,6,,0,,False,t3_g79r4d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587768792.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I read somewhere that decreasing the learning rate (making it smaller) will make the model train slower but will increase the accuracy as well.&lt;/p&gt;

&lt;p&gt;However, currently I have a model, and I started with a learning rate of 0.0001, the accuracy was high, but when I decrease it further, lets say 0.000001, the accuracy gets lower and lower...&lt;/p&gt;

&lt;p&gt;Is that normal ??&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g79r4d,True,,PetterGriffinFriend,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g79r4d/decreasing_the_learning_rate_decreases_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g79r4d/decreasing_the_learning_rate_decreases_the/,155203,1587739992.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #15 | Rubik's Code,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g74y68,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://a.thumbs.redditmedia.com/hBpMVwz1sJ7rXjLc37Q1mwqEu2ECikbBqX0Qi3HIkX0.jpg,False,,[],{},link,,False,,1587746949.0,text,6,,,text,rubikscode.net,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?auto=webp&amp;s=b4323f7d09549b69625b53af08a5f93c2496993a', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=928da45c43acaad40f9d7b7dbc281b6f6abc1382', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32c5320705b03921ef1de83a016a6aab02e8fc81', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e4d8cd6b5fb1453396b6ec5ea0ce1d558532508b', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=54b6cfe0502198ae24ab2b2cabe7039570f7c90a', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee8442b6eaa7f14d1e4f5c44314020e66cabb4cd', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/Syk5OF8MIVo0iOjggn4Sjw2jDXYrKK4tLfDfJc9AgA0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e09ae365565bc4954a82b58d0000e72a513edb78', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 's6dwbluYi2XG8AekV2DFelwoVrjHLxwjESaiwjcpEqk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g74y68,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g74y68/this_week_in_ai_issue_15_rubiks_code/,all_ads,False,https://rubikscode.net/2020/04/24/this-week-in-ai-issue-15-2/,155203,1587718149.0,0,,False,,,,
,learnmachinelearning,Hi Friends. I need to implement time series analysis and prediction for a classification problem. I have daily data for various incidents that are raised with classification done. I need to predict monthly what is the count (not sum) expected in future. Please guide or share some links. Thank you.,t2_42sd7bgr,False,,0,False,Time series classification problem,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g767bt,False,light,0.76,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1587753683.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Friends. I need to implement time series analysis and prediction for a classification problem. I have daily data for various incidents that are raised with classification done. I need to predict monthly what is the count (not sum) expected in future. Please guide or share some links. Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g767bt,True,,raviacharps,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g767bt/time_series_classification_problem/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g767bt/time_series_classification_problem/,155203,1587724883.0,0,,False,,,,
,learnmachinelearning,,t2_6ahby2o7,False,,0,False,LSTM input and output shape for Sequence to Sequence model using Keras,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,16.0,False,t3_g723hv,False,light,0.84,,public,4,0,{},140.0,,False,[],,True,False,,{},HELP,False,4,,False,https://a.thumbs.redditmedia.com/SiUb9j_z6uenuJMpdu-TF8cFOZkNpEDYbxio-yHb0R0.jpg,False,,[],{},image,,False,,1587732223.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?auto=webp&amp;s=7a833ca9b809294e3581a17ca88091a1a563ef82', 'width': 1266, 'height': 153}, 'resolutions': [{'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b37b8b4356660e45ccbde0660a8b58c7af36beba', 'width': 108, 'height': 13}, {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5b707101a2647d7420412b03c8cb3235461f77bd', 'width': 216, 'height': 26}, {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d0ece7d19d1a375b0e8cbfe68db829dbc1d4fce', 'width': 320, 'height': 38}, {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a541b9197874944f49c6829639772c463678d485', 'width': 640, 'height': 77}, {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d44b14dcdbd72235e9f8086fc44f9816fe277ff3', 'width': 960, 'height': 116}, {'url': 'https://preview.redd.it/k3pyji6c3pu41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aaa23f1f0fe3f328db42565b5cee19d894a170ba', 'width': 1080, 'height': 130}], 'variants': {}, 'id': '5Wf2_uIPAUev6ipp1kWT_PyI42mUiH0VF4B8Zap5qC4'}], 'enabled': True}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g723hv,True,,PrestigiousPositiveL,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g723hv/lstm_input_and_output_shape_for_sequence_to/,all_ads,False,https://i.redd.it/k3pyji6c3pu41.png,155203,1587703423.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Really cool visualization of memorization of RNN by Andreas Madsen,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g77xoc,False,light,0.67,,public,1,0,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/QqjjV_3J9fo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'height': 344}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Really cool visualization of memorization of RNN by Andreas Madsen', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/QqjjV_3J9fo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QqjjV_3J9fo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/QqjjV_3J9fo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 459, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g77xoc', 'height': 344}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/2TnH4nlOZoFFIa8oQP5IjrLtS0plfMwCehHb9Pbj2OA.jpg,False,,[],{},rich:video,,False,,1587761953.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/iIwwl5QfyKhtAYLl1aAD-eWmty75izho7B3W9kG7Qkc.jpg?auto=webp&amp;s=2c382f5cc9a49b30ea7151b5a10c06be817c3d8b', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/iIwwl5QfyKhtAYLl1aAD-eWmty75izho7B3W9kG7Qkc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd179f2ac0313f6c3bb211cc48beb240567bf1c9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/iIwwl5QfyKhtAYLl1aAD-eWmty75izho7B3W9kG7Qkc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b89ecf68180dfb87a1b3b2809be83ec31131b0e9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/iIwwl5QfyKhtAYLl1aAD-eWmty75izho7B3W9kG7Qkc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=362a9b6eed4809e40c2a83c37facd03c9aaad854', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'r1SmyAddhCc1E8Tto3EB9X2NKiIjZSNmj7UceBOaygQ'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g77xoc,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g77xoc/really_cool_visualization_of_memorization_of_rnn/,all_ads,False,https://youtu.be/QqjjV_3J9fo,155203,1587733153.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Really cool visualization of memorization of RNN by Andreas Madsen', 'html': '&lt;iframe width=""459"" height=""344"" src=""https://www.youtube.com/embed/QqjjV_3J9fo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 344, 'width': 459, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/QqjjV_3J9fo/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g77raf,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/95E1w-4D4CI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/95E1w-4D4CI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/95E1w-4D4CI/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/95E1w-4D4CI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g77raf', 'height': 338}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/G22NXxXrYxOVlNC_iZ2ghmXoA9diVRYH9XfycBjkymU.jpg,False,,[],{},rich:video,,False,,1587761193.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8tOvhopCc4mnDYzF9hzv6gI3iAEHyBqF-N9YFGp6pXY.jpg?auto=webp&amp;s=4bf14fb4ef31a8de3a8957c3372b4d2083c89571', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/8tOvhopCc4mnDYzF9hzv6gI3iAEHyBqF-N9YFGp6pXY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6db120e2a277d01886c73b122d5ce51bbe9ed258', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/8tOvhopCc4mnDYzF9hzv6gI3iAEHyBqF-N9YFGp6pXY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fb08d832415fb32aa05ec38b3cfe03c2f3bf609', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/8tOvhopCc4mnDYzF9hzv6gI3iAEHyBqF-N9YFGp6pXY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be85999da21c97e4e5afeb77c0aa9fd514cba01c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'w_PiShjgO6fY9mzlofLupsQWUu1yNae_OyUNbvrFCSk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g77raf,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g77raf/pseudo_rgbd_for_selfimproving_monocular_slam_and/,all_ads,False,https://youtu.be/95E1w-4D4CI,155203,1587732393.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/95E1w-4D4CI?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/95E1w-4D4CI/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I'm quite familiar with time series prediction in general, but even after a following a few Keras tutorials, I still don't really get what the Time Distributed layer is doing. 




Is it basically just reshaping a flat timeseries input into batches of N time steps? Or is something cleverer going on?",t2_y21ua,False,,0,False,Could anyone please explain TimeDistributed layer wrappers in Keras?,[],r/learnmachinelearning,False,6,,0,,False,t3_g75015,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587747234.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m quite familiar with time series prediction in general, but even after a following a few Keras tutorials, I still don&amp;#39;t really get what the Time Distributed layer is doing. &lt;/p&gt;

&lt;p&gt;Is it basically just reshaping a flat timeseries input into batches of N time steps? Or is something cleverer going on?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g75015,True,,hollammi,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g75015/could_anyone_please_explain_timedistributed_layer/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g75015/could_anyone_please_explain_timedistributed_layer/,155203,1587718434.0,0,,False,,,,
,learnmachinelearning,,t2_65mrxu97,False,,0,False,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Tr...,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g74rt3,False,light,0.76,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Henry AI Labs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Axo0EtMUK90/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g74rt3', 'height': 338}",Discussion,False,2,,False,https://b.thumbs.redditmedia.com/H4SrRuUkgj86lddoZLyO-zOe8aJmMG0LbLSxAn9RnBo.jpg,False,,[],{},rich:video,,False,,1587745928.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?auto=webp&amp;s=8ef6c0357c327ae0235d0154770842d17d3ddddb', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8881568c595a505d26e397c652265f955d221b9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cd827e1843d266e8d4e34b5c4287e6b2f65eab7', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5b770d964ac052b14e71627f3dc412132e7bb52', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'I0jzYqv0Wy5AwrmP0SfnOxIZgcW5QBG2i_FvzUMO_KI'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g74rt3,True,,VisualCoding,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g74rt3/exploring_the_limits_of_transfer_learning_with_a/,all_ads,False,https://www.youtube.com/watch?v=Axo0EtMUK90&amp;feature=share,155203,1587717128.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Henry AI Labs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Axo0EtMUK90/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw'}, 'type': 'youtube.com'}",False,"[{'approved_at_utc': None, 'subreddit': 'VisualCoding', 'selftext': '', 'author_fullname': 't2_65mrxu97', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Tr...', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/VisualCoding', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 105, 'hide_score': False, 'name': 't3_g74oep', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Henry AI Labs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Axo0EtMUK90/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw'}, 'type': 'youtube.com'}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g74oep', 'height': 338}, 'link_flair_text': None, 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/H4SrRuUkgj86lddoZLyO-zOe8aJmMG0LbLSxAn9RnBo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1587745392.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtube.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?auto=webp&amp;s=8ef6c0357c327ae0235d0154770842d17d3ddddb', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8881568c595a505d26e397c652265f955d221b9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cd827e1843d266e8d4e34b5c4287e6b2f65eab7', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/HlBsCsWkIbruN2Rgzp3oskE0shdrX-As0PhMkVoMxHM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5b770d964ac052b14e71627f3dc412132e7bb52', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'I0jzYqv0Wy5AwrmP0SfnOxIZgcW5QBG2i_FvzUMO_KI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2jgq2d', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g74oep', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'VisualCoding', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/VisualCoding/comments/g74oep/exploring_the_limits_of_transfer_learning_with_a/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.youtube.com/watch?v=Axo0EtMUK90&amp;feature=share', 'subreddit_subscribers': 585, 'created_utc': 1587716592.0, 'num_crossposts': 3, 'media': {'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Axo0EtMUK90?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Henry AI Labs', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Axo0EtMUK90/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw'}, 'type': 'youtube.com'}, 'is_video': False}]",t3_g74oep,,
,learnmachinelearning,,t2_1mg76nw,False,,0,False,If youâ€™re looking to start machine learning and in search of some data to play with then here are my top 5 Open Data Sources for Machine Learning,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g7771a,False,light,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ylVNTJ0XaCQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Top 5 Open Data Sources for Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ylVNTJ0XaCQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'SATSifaction', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ylVNTJ0XaCQ/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCyVIfVgx77jslb0U5h-zhLQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ylVNTJ0XaCQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g7771a', 'height': 338}",Project,False,0,,False,https://a.thumbs.redditmedia.com/g311p-6bzL9fq2KTmHY1NP-NRNvigaXGuDQy9tR7Kl8.jpg,False,,[],{},rich:video,,False,,1587758679.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CKW-zE2aMgRTanJnqO7SLqh3Sex_5_wlM7vd8jWMYsM.jpg?auto=webp&amp;s=a55d6d2ffd965adf4434930f7b89dc7ad2704d2b', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/CKW-zE2aMgRTanJnqO7SLqh3Sex_5_wlM7vd8jWMYsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=989c7c21bd6d95918a6ba45c7e50518e43a7b6fd', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/CKW-zE2aMgRTanJnqO7SLqh3Sex_5_wlM7vd8jWMYsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5279d05b7fec8887a11eaaf2a561735cdced349c', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/CKW-zE2aMgRTanJnqO7SLqh3Sex_5_wlM7vd8jWMYsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd6d39a2740bf20b9209ec731906b3ed4b9c3833', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'VDJ2RWTrFS_GL4y9LpQ_QC2UidpVJLm3snz9fcKWb2o'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g7771a,True,,satssehgal,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7771a/if_youre_looking_to_start_machine_learning_and_in/,all_ads,False,https://youtu.be/ylVNTJ0XaCQ,155203,1587729879.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Top 5 Open Data Sources for Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ylVNTJ0XaCQ?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'SATSifaction', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ylVNTJ0XaCQ/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCyVIfVgx77jslb0U5h-zhLQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Build a sentiment classification model using BERT from the Transformers library by Hugging Face with PyTorch and Python. Learn more about what BERT is, how to use it, and fine-tune it for sentiment analysis on Google Play app reviews:

Video: https://www.youtube.com/watch?v=8N-nM3QW7O0

Text tutorial + notebook: https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/

GitHub: https://github.com/curiousily/Getting-Things-Done-with-Pytorch",t2_fztx5xf,False,,0,False,[P] Sentiment Analysis with BERT and Transformers by Hugging Face using PyTorch,[],r/learnmachinelearning,False,6,,0,,False,t3_g72xyw,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1587736421.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Build a sentiment classification model using BERT from the Transformers library by Hugging Face with PyTorch and Python. Learn more about what BERT is, how to use it, and fine-tune it for sentiment analysis on Google Play app reviews:&lt;/p&gt;

&lt;p&gt;Video: &lt;a href=""https://www.youtube.com/watch?v=8N-nM3QW7O0""&gt;https://www.youtube.com/watch?v=8N-nM3QW7O0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Text tutorial + notebook: &lt;a href=""https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/""&gt;https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GitHub: &lt;a href=""https://github.com/curiousily/Getting-Things-Done-with-Pytorch""&gt;https://github.com/curiousily/Getting-Things-Done-with-Pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/r0fET09l-8aOnk3bfc7CHlsYQb6f2uEWTW4LuSu-92A.jpg?auto=webp&amp;s=2700a204f3b2ace80f95bf986969a2fb87a95d2c', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/r0fET09l-8aOnk3bfc7CHlsYQb6f2uEWTW4LuSu-92A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e4822ab46f0f49c897e27f323762fca687400c8', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/r0fET09l-8aOnk3bfc7CHlsYQb6f2uEWTW4LuSu-92A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdc74c6850c6d6508c6f83a4349aad44461b349f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/r0fET09l-8aOnk3bfc7CHlsYQb6f2uEWTW4LuSu-92A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ff755f6a2fd897d19f7f1af054b4816a49fd4d2', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'mLoMJ5lQeprlHixCtct50DhouepG1Sx38sFMNK-1rJk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g72xyw,True,,curiousily_,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g72xyw/p_sentiment_analysis_with_bert_and_transformers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g72xyw/p_sentiment_analysis_with_bert_and_transformers/,155203,1587707621.0,0,,False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,Optimize AI Talent: Perception from Across the Globe,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,93.0,False,t3_g75x0i,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/cNyPgky0RB3jI401m9ZTCyQkJbChaIxHgcb_hHSDn2A.jpg,False,,[],{},link,,False,,1587752159.0,richtext,6,,,text,data-science-blog.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/G56Or48UDTxgo8X7lwveaTbXT8jENEFN21rz8p4Ve3M.jpg?auto=webp&amp;s=4fe53f3a8cb18188d2a8134fde5e0d536236ef3c', 'width': 361, 'height': 241}, 'resolutions': [{'url': 'https://external-preview.redd.it/G56Or48UDTxgo8X7lwveaTbXT8jENEFN21rz8p4Ve3M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93de24ebfd275a933e7fcc7c282c1048342e056a', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/G56Or48UDTxgo8X7lwveaTbXT8jENEFN21rz8p4Ve3M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=50b7b30cdc92f5010da447c4b7e9763320ec669f', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/G56Or48UDTxgo8X7lwveaTbXT8jENEFN21rz8p4Ve3M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f53a2ae03851b8b79ff1145e3535b6d9967cd124', 'width': 320, 'height': 213}], 'variants': {}, 'id': 'd2Btpj6rP1BAEHQjwoz1XgZ_RUm0fe7Yb2V4ArIvFX0'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g75x0i,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g75x0i/optimize_ai_talent_perception_from_across_the/,all_ads,False,https://data-science-blog.com/blog/2020/04/08/optimize-ai-talent-perception-from-across-the-globe/,155203,1587723359.0,0,,False,,,,
,learnmachinelearning,"I have a doubt ,

I have a set of data points each of dimnention (let say n) , i want to find hyperplane, such that loss is minimum projection these data points on that hyperplane .

If it is binary labled i can use svm to find hyperplane but in unsupervised way i have no idea how to approach this?",t2_2mbllqnq,False,,0,False,How to find the hyperplane?,[],r/learnmachinelearning,False,6,,0,,False,t3_g72t2x,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587735704.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a doubt ,&lt;/p&gt;

&lt;p&gt;I have a set of data points each of dimnention (let say n) , i want to find hyperplane, such that loss is minimum projection these data points on that hyperplane .&lt;/p&gt;

&lt;p&gt;If it is binary labled i can use svm to find hyperplane but in unsupervised way i have no idea how to approach this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g72t2x,True,,sonalgarg2806,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g72t2x/how_to_find_the_hyperplane/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g72t2x/how_to_find_the_hyperplane/,155203,1587706904.0,0,,False,,,,True
,learnmachinelearning,I just spent hours learning something which is conceptually trivial. Just print() or using matplotlib is so much easier.,t2_zygtb,False,,0,False,Does anyone else find the Tensorboard irritating?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g75brn,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,1587720400.0,,[],{},,,True,,1587749014.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just spent hours learning something which is conceptually trivial. Just print() or using matplotlib is so much easier.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g75brn,True,,niszoig,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g75brn/does_anyone_else_find_the_tensorboard_irritating/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g75brn/does_anyone_else_find_the_tensorboard_irritating/,155203,1587720214.0,0,,False,,,,
,learnmachinelearning,"Hi all, 
Is there any link or resource where I can learn to implement schwartz smith model in R?
I have been trying to research about it a lot but have been able to find a matlab implementation only. Given that I have no prior exp in matlab, is there an article where I can refer to do so? Thanks a lot in advance !",t2_2uqtqxnn,False,,0,False,Schwartz smith model in R,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g74w0y,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587746590.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, 
Is there any link or resource where I can learn to implement schwartz smith model in R?
I have been trying to research about it a lot but have been able to find a matlab implementation only. Given that I have no prior exp in matlab, is there an article where I can refer to do so? Thanks a lot in advance !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g74w0y,True,,tiredofquoraguy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g74w0y/schwartz_smith_model_in_r/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g74w0y/schwartz_smith_model_in_r/,155203,1587717790.0,0,,False,,,,
,learnmachinelearning,,t2_16diqth,False,,0,False,A Visual Guide to RNN in Keras,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,75.0,False,t3_g703ff,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},Project,False,3,,False,https://a.thumbs.redditmedia.com/SqxYURsEiaLKocscjjojcvDEV0pG1GAY_GylZAFGYy4.jpg,False,,[],{},link,,False,,1587723531.0,richtext,6,,,text,amitness.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GHNt6U5gaZJ27RAEhn_xPe_FOnb3Ha7OpvBrffs5srk.jpg?auto=webp&amp;s=ec1fa818008ac561f8c1b40b928fb85a191ef618', 'width': 648, 'height': 349}, 'resolutions': [{'url': 'https://external-preview.redd.it/GHNt6U5gaZJ27RAEhn_xPe_FOnb3Ha7OpvBrffs5srk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0692347e9f356dd89a36b05c4e10b1411d01e68', 'width': 108, 'height': 58}, {'url': 'https://external-preview.redd.it/GHNt6U5gaZJ27RAEhn_xPe_FOnb3Ha7OpvBrffs5srk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df54b5efd1263211b0a713c7461d5161e6b427d0', 'width': 216, 'height': 116}, {'url': 'https://external-preview.redd.it/GHNt6U5gaZJ27RAEhn_xPe_FOnb3Ha7OpvBrffs5srk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b47dab47046dab26ff921b7f715ffc62874f485', 'width': 320, 'height': 172}, {'url': 'https://external-preview.redd.it/GHNt6U5gaZJ27RAEhn_xPe_FOnb3Ha7OpvBrffs5srk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f659f28bf955730261d3aa6c815193d88c8afc5a', 'width': 640, 'height': 344}], 'variants': {}, 'id': 'RWQhUNYUaEmhhGffwyI8CGwpFTqHudAszkzCiI9AjoI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g703ff,True,,amitness,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g703ff/a_visual_guide_to_rnn_in_keras/,all_ads,False,https://amitness.com/2020/04/recurrent-layers-keras/,155203,1587694731.0,0,,False,,,,
,learnmachinelearning,In other words can you run information through it without risk of it being available to the public?,t2_50azgl21,False,,0,False,Is word2vec secure?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6zpvn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1587721959.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In other words can you run information through it without risk of it being available to the public?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6zpvn,True,,Sccrbrg2,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6zpvn/is_word2vec_secure/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6zpvn/is_word2vec_secure/,155203,1587693159.0,0,,False,,,,
,learnmachinelearning,"I  had a long passion to pursue art or at least the idea of art, tried a  few times didn't succeed much. But then today, I came across this [video](https://www.youtube.com/watch?v=I-EIVlHvHRM)  and I am really fascinated. Using data from different sources and  creating a visual art. This is most satisfying thing I've ever seen.

My  question is, how does he do it and how can I do it? Can someone point  me in the right direction? What is required, what to know and where to  find.

FYI: I'm a newbie to ML and ML world.",t2_5hbix9ha,False,,0,False,How to use data to create visual art?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g73z7d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1587741642.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  had a long passion to pursue art or at least the idea of art, tried a  few times didn&amp;#39;t succeed much. But then today, I came across this &lt;a href=""https://www.youtube.com/watch?v=I-EIVlHvHRM""&gt;video&lt;/a&gt;  and I am really fascinated. Using data from different sources and  creating a visual art. This is most satisfying thing I&amp;#39;ve ever seen.&lt;/p&gt;

&lt;p&gt;My  question is, how does he do it and how can I do it? Can someone point  me in the right direction? What is required, what to know and where to  find.&lt;/p&gt;

&lt;p&gt;FYI: I&amp;#39;m a newbie to ML and ML world.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QPCzxJbYkIr9QCYbT5NvHBz3upAaqttF6zOsmWFsIKw.jpg?auto=webp&amp;s=438468b5482b538486ded9a235f4c255c11e2fb9', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/QPCzxJbYkIr9QCYbT5NvHBz3upAaqttF6zOsmWFsIKw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdaa9129bef05e18ebad96d4a861a04e47e35507', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/QPCzxJbYkIr9QCYbT5NvHBz3upAaqttF6zOsmWFsIKw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e0d82c31ab2481c72301c77f1e43e0729a1d79a0', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/QPCzxJbYkIr9QCYbT5NvHBz3upAaqttF6zOsmWFsIKw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c78217e75b6ae2edb35f3f329ea44b6cdf3c2a8a', 'width': 320, 'height': 240}], 'variants': {}, 'id': '5ioOwnnrgAPnCV-1YbDsl7O139h1v5FyjvLX77Ih1AI'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g73z7d,True,,WholesomeDoktor,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g73z7d/how_to_use_data_to_create_visual_art/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g73z7d/how_to_use_data_to_create_visual_art/,155203,1587712842.0,0,,False,,,,
,learnmachinelearning,"Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.",t2_6l4z3,False,,0,False,Weekly Show-off!,[],r/learnmachinelearning,False,6,,0,,False,t3_g73sa7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1587740686.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,g73sa7,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g73sa7/weekly_showoff/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g73sa7/weekly_showoff/,155203,1587711886.0,0,,False,,,,
,learnmachinelearning,,t2_2719wkij,False,,0,False,"Officially launching Deep Learning Demystified which provided Simple, Intuitive and Visual explanation of Deep Learning. It's completely free!",[],r/learnmachinelearning,False,6,,0,73.0,False,t3_g6pkag,False,dark,0.95,,public,15,0,{},140.0,,False,[],,False,False,,{},,False,15,,False,https://b.thumbs.redditmedia.com/2Dwe3uZYj76SH-Uo3ZdVlglA-GjK2AX0N9uAT-WrmAA.jpg,False,,[],{},link,,False,,1587687378.0,text,6,,,text,deeplearningdemystified.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?auto=webp&amp;s=61cee77e8556c4a530826cf07a29fe43bd9f44e3', 'width': 3190, 'height': 1671}, 'resolutions': [{'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ca00799855aa32554ff610f7cd9b988af5b1e6a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=515210c17ff764f7b88aeb7f74290d7d6f3eae60', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=103e7db928b46ce3c061575b917e305cee3747c8', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=173947cc2f1794eab415414a9da384ca5d44171b', 'width': 640, 'height': 335}, {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d6c9a40c2c9e994cc2e66f1a6875b02bcaaf92b9', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/pla_zCO8MVFzVXUQVu7EOu1gypeMkk_MJqVJ6PaEOPA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c1f93613f998a33a373c788aea9ebe5aa6ee340', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'YrPldbsrZBpD4A1KBlnPe0E_81WCSZQkiLH75bKuf_c'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6pkag,True,,harshablast,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6pkag/officially_launching_deep_learning_demystified/,all_ads,False,https://deeplearningdemystified.com,155203,1587658578.0,0,,False,,,,
,learnmachinelearning,,t2_3qfhbsn9,False,,0,False,Free Online Resources for Data Scientists During COVID-19,[],r/learnmachinelearning,False,6,,0,64.0,False,t3_g7322b,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/crFM3Nwz8ovMEWr-Xs4cqlY3gROqzRfYTT2bmD19L1o.jpg,False,,[],{},link,,False,,1587737005.0,text,6,,,text,lionbridge.ai,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?auto=webp&amp;s=d81d5084041b9e101774de927696000b72f11159', 'width': 1368, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=442d069d4d833cafc39c1b84dd342d127ec0ff43', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=252ecc5c64730672be0c1572f2553a4fe59da1d1', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c28f0dcf7749dd59119bc3d5ab9ae3447ebeeed2', 'width': 320, 'height': 147}, {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d44f14ebb8375788d07a5c33fc58affd990f759a', 'width': 640, 'height': 295}, {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b0523be6b1934550326135ad7d9aff4b13838d2d', 'width': 960, 'height': 443}, {'url': 'https://external-preview.redd.it/gH9UrwdpnL-QsRUND3CTFYGq3-o03Te6sELd0gK8xOQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fac8cbc5f47f337359b6365985a83d907ed8a89', 'width': 1080, 'height': 498}], 'variants': {}, 'id': 'KOHd9EiG-sPMuRN5-peI_xyhkS6xPV-U0Ubn0KsS3fw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g7322b,True,,LimarcAmbalina,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g7322b/free_online_resources_for_data_scientists_during/,all_ads,False,https://lionbridge.ai/articles/free-online-resources-for-data-scientists-during-covid-19/,155203,1587708205.0,0,,False,,,,
,learnmachinelearning,"I'm pretty new to deep learning, learning about multi layer perceptrons currently and I have some basic background to programming but I haven't done any huge projects yet. 

I was wondering, I am a pretty avid competitive PC gamer and I have a strong GPU. I know that tensorflow uses CUDA which utilizes the nvidia GPU's but I also know that my university has a web based jupyter notebook that utilizes a really strong GPU. Is there anything that the online notebook can do that my own computer cant? How about the reverse statement: is there anything my PC can do that the online notebook cannot? 

&amp;#x200B;

What I'm ultimately trying to figure out is - is there any advantages to having a strong GPU with regard to ML/deep learning/neural networks? Or is having access to universities GPU remove the need for owning a strong GPU?

I'm wondering because I want to utilize my strong PC to work on a project from start to finish to put on my github and my resume. Does anyone have any cool suggestions that will utilize the GPU?",t2_6517fxym,False,,0,False,Is there any advantage to having a strong nvidia GPU for deep learning stuff?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6i79w,False,dark,0.9,,public,50,0,{},,,False,[],,False,False,,{},,False,50,,False,self,False,,[],{},,,True,,1587655002.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m pretty new to deep learning, learning about multi layer perceptrons currently and I have some basic background to programming but I haven&amp;#39;t done any huge projects yet. &lt;/p&gt;

&lt;p&gt;I was wondering, I am a pretty avid competitive PC gamer and I have a strong GPU. I know that tensorflow uses CUDA which utilizes the nvidia GPU&amp;#39;s but I also know that my university has a web based jupyter notebook that utilizes a really strong GPU. Is there anything that the online notebook can do that my own computer cant? How about the reverse statement: is there anything my PC can do that the online notebook cannot? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m ultimately trying to figure out is - is there any advantages to having a strong GPU with regard to ML/deep learning/neural networks? Or is having access to universities GPU remove the need for owning a strong GPU?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m wondering because I want to utilize my strong PC to work on a project from start to finish to put on my github and my resume. Does anyone have any cool suggestions that will utilize the GPU?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6i79w,True,,supfuh,,41,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6i79w/is_there_any_advantage_to_having_a_strong_nvidia/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6i79w/is_there_any_advantage_to_having_a_strong_nvidia/,155203,1587626202.0,0,,False,,,,
,learnmachinelearning,"Hey, I feel like this is a long shot - most people here aren't posting about text mining! But hey, worth a shot.

I'm trying to use WordNet to lemmatize &amp; tag text, then chunk it. The code I'm posting below is a combination of a lemmatization tutorial and a chunking tutorial, so if it kind of looks like Frankenstein's Monster, that'd be why. Anyway, I keep getting \`ValueError: chunk structures must contain tagged tokens or trees\`, even though the text I'm using does get both tokenized and tagged. What am I doing wrong?

Here's my code:

    import nltk
    from nltk.stem import WordNetLemmatizer
    from nltk.corpus import wordnet
    
    lemmatizer = WordNetLemmatizer()
    sentence = ""The striped p is hanging from its feet for luck""
    
    # Tokenize
    word_list = nltk.word_tokenize(sentence)
    
    # Lemmatize with POS Tags
    def get_wordnet_pos(word):
        """"""Map POS tag to first character lemmatize() accepts""""""
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {""J"": wordnet.ADJ,
                    ""N"": wordnet.NOUN,
                    ""V"": wordnet.VERB,
                    ""R"": wordnet.ADV}
    
        return tag_dict.get(tag, wordnet.NOUN)
    
    # Lemmatize a Sentence with the appropriate POS tag
    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list]
    print(lemmatized)
    #&gt; ['The', 'strip', 'p', 'be', 'hang', 'from', 'it', 'foot', 'for', 'luck']
    
    #### Chunking - Test
    grammar = ""NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}""
    nltk.RegexpParser(grammar)
    parser = nltk.RegexpParser(grammar)
    output = parser.parse(lemmatized)
    #output.draw()
    print(output)

And here's the traceback:

    Traceback (most recent call last):
      File ""D:/lemmatizing.py"", line 35, in &lt;module&gt;
        output = parser.parse(lemmatized)
      File ""C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py"", line 1275, in parse
        chunk_struct = parser.parse(chunk_struct, trace=trace)
      File ""C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py"", line 1082, in parse
        chunkstr = ChunkString(chunk_struct)
      File ""C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py"", line 93, in __init__
        tags = [self._tag(tok) for tok in self._pieces]
      File ""C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py"", line 93, in &lt;listcomp&gt;
        tags = [self._tag(tok) for tok in self._pieces]
      File ""C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py"", line 103, in _tag
        raise ValueError(""chunk structures must contain tagged "" ""tokens or trees"")
    ValueError: chunk structures must contain tagged tokens or trees

Thanks a million in advance!!",t2_48gobldk,False,,0,False,Anyone know NLP/NLTK? I'm learning lemmatizing &amp; chunking,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g71o07,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587730239.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I feel like this is a long shot - most people here aren&amp;#39;t posting about text mining! But hey, worth a shot.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to use WordNet to lemmatize &amp;amp; tag text, then chunk it. The code I&amp;#39;m posting below is a combination of a lemmatization tutorial and a chunking tutorial, so if it kind of looks like Frankenstein&amp;#39;s Monster, that&amp;#39;d be why. Anyway, I keep getting `ValueError: chunk structures must contain tagged tokens or trees`, even though the text I&amp;#39;m using does get both tokenized and tagged. What am I doing wrong?&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s my code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()
sentence = &amp;quot;The striped p is hanging from its feet for luck&amp;quot;

# Tokenize
word_list = nltk.word_tokenize(sentence)

# Lemmatize with POS Tags
def get_wordnet_pos(word):
    &amp;quot;&amp;quot;&amp;quot;Map POS tag to first character lemmatize() accepts&amp;quot;&amp;quot;&amp;quot;
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {&amp;quot;J&amp;quot;: wordnet.ADJ,
                &amp;quot;N&amp;quot;: wordnet.NOUN,
                &amp;quot;V&amp;quot;: wordnet.VERB,
                &amp;quot;R&amp;quot;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

# Lemmatize a Sentence with the appropriate POS tag
lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list]
print(lemmatized)
#&amp;gt; [&amp;#39;The&amp;#39;, &amp;#39;strip&amp;#39;, &amp;#39;p&amp;#39;, &amp;#39;be&amp;#39;, &amp;#39;hang&amp;#39;, &amp;#39;from&amp;#39;, &amp;#39;it&amp;#39;, &amp;#39;foot&amp;#39;, &amp;#39;for&amp;#39;, &amp;#39;luck&amp;#39;]

#### Chunking - Test
grammar = &amp;quot;NP: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;*&amp;lt;NN&amp;gt;}&amp;quot;
nltk.RegexpParser(grammar)
parser = nltk.RegexpParser(grammar)
output = parser.parse(lemmatized)
#output.draw()
print(output)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here&amp;#39;s the traceback:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):
  File &amp;quot;D:/lemmatizing.py&amp;quot;, line 35, in &amp;lt;module&amp;gt;
    output = parser.parse(lemmatized)
  File &amp;quot;C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py&amp;quot;, line 1275, in parse
    chunk_struct = parser.parse(chunk_struct, trace=trace)
  File &amp;quot;C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py&amp;quot;, line 1082, in parse
    chunkstr = ChunkString(chunk_struct)
  File &amp;quot;C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py&amp;quot;, line 93, in __init__
    tags = [self._tag(tok) for tok in self._pieces]
  File &amp;quot;C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py&amp;quot;, line 93, in &amp;lt;listcomp&amp;gt;
    tags = [self._tag(tok) for tok in self._pieces]
  File &amp;quot;C:\Users\matay\AppData\Local\Programs\Python\Python38\lib\site-packages\nltk\chunk\regexp.py&amp;quot;, line 103, in _tag
    raise ValueError(&amp;quot;chunk structures must contain tagged &amp;quot; &amp;quot;tokens or trees&amp;quot;)
ValueError: chunk structures must contain tagged tokens or trees
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks a million in advance!!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g71o07,True,,taylomol000,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g71o07/anyone_know_nlpnltk_im_learning_lemmatizing/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g71o07/anyone_know_nlpnltk_im_learning_lemmatizing/,155203,1587701439.0,0,,False,,,,
,learnmachinelearning,,t2_4mh1q,False,,0,False,AI Can Write Believable Stories Now: Co-writing Short Stories with GPT-2,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_g6tg6u,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/zs43j4YzEqdVoOtsRSxa4jXqOYNohnaX6O7CVwMi5wQ.jpg,False,,[],{},link,,False,,1587699959.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?auto=webp&amp;s=840972cb8a735c6cbec8d30d8774ae914e46fc05', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf513c33677bca6dcbdaa43fda8f6150e3090eaa', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5deb25a8391a554f9e6aefcb815c7d68ce80c702', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19453efad61d18f22bf36afb96b4eae19050912f', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdd817de37b811793626923cfea2f1787c1543a', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e4fda68fc42b923f09d322639be94d6cacce3bb1', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/VIDM6ew78g4ZEAGfK2yZHtJ8lHI-BbahV2RkNUi055c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e983ddbdfeb3a21e2aa2c4382536bbc039d9f836', 'width': 1080, 'height': 720}], 'variants': {}, 'id': '4V3LrExpxETrGiYGvMaz0f1ZQ3x16Ier5WpLKT6X_Lo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6tg6u,True,,iusedtobeinteresting,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6tg6u/ai_can_write_believable_stories_now_cowriting/,all_ads,False,https://medium.com/@tonytonev/ai-can-write-believable-stories-now-292391cc83ef,155203,1587671159.0,0,,False,,,,
,learnmachinelearning,"I am looking to build and train different classifiers using different technology (Naive Bayes, CNN, Bert, LSTM, Random forest and so on) to compare them in their accuracy. It will be used on short message type of texts and longer texts, similar to reviews.

I am trying to find a dataset that is managable in size and good for sentiment analysis tasks. I found the imdb 50k review dataset and that seems to fit my needs very well but I want to use another dataset as well with short messages and combine those two datasets.

I previously looked at the 1.6 million tweet sentiment140 dataset but that one seems to be too big for my rig, even if I use the Hashvectorizer in sklearn.",t2_jlp6b,False,,0,False,"What are some of the ""best"" datasets for sentiment analysis? I just need ""positive"" and ""negative""","[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6x4ba,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1587712168.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to build and train different classifiers using different technology (Naive Bayes, CNN, Bert, LSTM, Random forest and so on) to compare them in their accuracy. It will be used on short message type of texts and longer texts, similar to reviews.&lt;/p&gt;

&lt;p&gt;I am trying to find a dataset that is managable in size and good for sentiment analysis tasks. I found the imdb 50k review dataset and that seems to fit my needs very well but I want to use another dataset as well with short messages and combine those two datasets.&lt;/p&gt;

&lt;p&gt;I previously looked at the 1.6 million tweet sentiment140 dataset but that one seems to be too big for my rig, even if I use the Hashvectorizer in sklearn.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6x4ba,True,,UchihaEmre,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6x4ba/what_are_some_of_the_best_datasets_for_sentiment/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6x4ba/what_are_some_of_the_best_datasets_for_sentiment/,155203,1587683368.0,0,,False,,,,
,learnmachinelearning,"This is a Discord server where everyone wanting to learn AI is welcomed!  


Share you projects, interesting research papers, courses, ask any questions related to the field!  


The more we are, the more we learn! Spread your knowledge and Learn AI Together!

  
Join us: [https://discord.gg/SVse4Sr](https://discord.gg/SVse4Sr)",t2_c14wpji,False,,0,False,Learn AI Together! Discord server for everyone interested in the field!,[],r/learnmachinelearning,False,6,,0,,False,t3_g6q8ld,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,False,,[],{},self,,True,,1587689628.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a Discord server where everyone wanting to learn AI is welcomed!  &lt;/p&gt;

&lt;p&gt;Share you projects, interesting research papers, courses, ask any questions related to the field!  &lt;/p&gt;

&lt;p&gt;The more we are, the more we learn! Spread your knowledge and Learn AI Together!&lt;/p&gt;

&lt;p&gt;Join us: &lt;a href=""https://discord.gg/SVse4Sr""&gt;https://discord.gg/SVse4Sr&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?auto=webp&amp;s=bbf5a5f8cdd32a365c20b807d6e558010bf21ad3', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7e52d627b24baf552b6a16dd42fad4b83bfdd50', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8796fb46cfe3d0305f26fc605b5c6505c2cd4ffe', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'MId8qCjBpXwiZK4_j8Xrn-37sQy_bUZ9YanGSEY9-S8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6q8ld,True,,OnlyProggingForFun,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6q8ld/learn_ai_together_discord_server_for_everyone/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6q8ld/learn_ai_together_discord_server_for_everyone/,155203,1587660828.0,3,,False,,,,
,learnmachinelearning,,t2_3bj6omx,False,,0,False,This was great advice for following code tutorials,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g6warh,False,dark,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/X4osaNAuJH8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Should You Code Along? (In Machine Learning and Data Science)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/X4osaNAuJH8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Lazy Programmer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/X4osaNAuJH8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCdROkkPGHy6uVpUrIB4n3YQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/X4osaNAuJH8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g6warh', 'height': 338}",,False,3,,False,https://b.thumbs.redditmedia.com/uM1h8eeEIaBjvz-5DYUKBL71B92R566iSX6G1XkacRA.jpg,False,,[],{},rich:video,,False,,1587709276.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Wu_CQVd4xau6DV0SxO33-EmP5Z8q4j2Rkb-7Y0uD7EE.jpg?auto=webp&amp;s=2e57024f5b7f933a4d9f04500d8863f1524c7a10', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Wu_CQVd4xau6DV0SxO33-EmP5Z8q4j2Rkb-7Y0uD7EE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db32ff62d9c9a72986eec6aff4c753bd4ba73ab0', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Wu_CQVd4xau6DV0SxO33-EmP5Z8q4j2Rkb-7Y0uD7EE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9d6aaacf6538b528dd3bd147ae04f9754dfefe20', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Wu_CQVd4xau6DV0SxO33-EmP5Z8q4j2Rkb-7Y0uD7EE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c3d2bf56335164a204e172703252c50eef41723', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'Ly27n22xxP239OonldSDc36q98VlHyb5RR91OK7DGiI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6warh,True,,chrisatmachine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6warh/this_was_great_advice_for_following_code_tutorials/,all_ads,False,https://www.youtube.com/watch?v=X4osaNAuJH8,155203,1587680476.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Should You Code Along? (In Machine Learning and Data Science)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/X4osaNAuJH8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Lazy Programmer', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/X4osaNAuJH8/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCdROkkPGHy6uVpUrIB4n3YQ'}}",False,,,,
,learnmachinelearning,"I've recently just completed what some might call an introductory course to machine learning, building a simple neural network to recognize handwritten digits by training on the MNIST dataset. As I am very proud of that, I am looking to take it to the next level. Though I am a bit lost on where to go. I was thinking of maybe trying to tackle another classification problem or something like that but I wanted to ask for a second opinion first. Any and all suggestions welcome!",t2_1rf921gz,False,,0,False,Where To Next?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6zbsg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587720362.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve recently just completed what some might call an introductory course to machine learning, building a simple neural network to recognize handwritten digits by training on the MNIST dataset. As I am very proud of that, I am looking to take it to the next level. Though I am a bit lost on where to go. I was thinking of maybe trying to tackle another classification problem or something like that but I wanted to ask for a second opinion first. Any and all suggestions welcome!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6zbsg,True,,Mikelle16,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6zbsg/where_to_next/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6zbsg/where_to_next/,155203,1587691562.0,0,,False,,,,
,learnmachinelearning,"TL;DR -- anyone have recommendations (or suggestions) on machine-learning packages that might be suitable for creating boardgame AIs.

Details? I'm currently working on an open source boardgame engine ([https://saito.io/arcade](https://saito.io/arcade)) that runs on the Saito network. It is a nice project because the cryptographic tools guarantee things like fair dice rolls and deck shuffles. But also because all of the gaming data is public which means that the network is pretty much constantly producing game-data.

Since most of the games that people are adding don't have good single-player modes, I'm wondering about the feasibiity of hacking up the engine so that the gaming data can be fed into some sort of search-based AI training algorithm. And while I'm a reasonably skilled programmer, this would be my first real foray into machine learning, so I'm hoping someone with more experience might be willing to share tips or recommendations on places to start looking.

Any recommendations on tutorials? Or good places to start? I think it would be a really interesting project to write a Saito module that just listened on the network for gameplay (to learn) and could be invited to play games. Curious how someone with more experience would conceptualize approaching the problem and whether there are any existing open source projects that might be suitable for helping get something basic up-and-running.",t2_4v0z9,False,,0,False,[Project] Any Recommendations on Machine-Learning Tools for Creating Boardgame AI?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6yaax,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587716353.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;TL;DR -- anyone have recommendations (or suggestions) on machine-learning packages that might be suitable for creating boardgame AIs.&lt;/p&gt;

&lt;p&gt;Details? I&amp;#39;m currently working on an open source boardgame engine (&lt;a href=""https://saito.io/arcade""&gt;https://saito.io/arcade&lt;/a&gt;) that runs on the Saito network. It is a nice project because the cryptographic tools guarantee things like fair dice rolls and deck shuffles. But also because all of the gaming data is public which means that the network is pretty much constantly producing game-data.&lt;/p&gt;

&lt;p&gt;Since most of the games that people are adding don&amp;#39;t have good single-player modes, I&amp;#39;m wondering about the feasibiity of hacking up the engine so that the gaming data can be fed into some sort of search-based AI training algorithm. And while I&amp;#39;m a reasonably skilled programmer, this would be my first real foray into machine learning, so I&amp;#39;m hoping someone with more experience might be willing to share tips or recommendations on places to start looking.&lt;/p&gt;

&lt;p&gt;Any recommendations on tutorials? Or good places to start? I think it would be a really interesting project to write a Saito module that just listened on the network for gameplay (to learn) and could be invited to play games. Curious how someone with more experience would conceptualize approaching the problem and whether there are any existing open source projects that might be suitable for helping get something basic up-and-running.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6yaax,True,,trevelyan22,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6yaax/project_any_recommendations_on_machinelearning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6yaax/project_any_recommendations_on_machinelearning/,155203,1587687553.0,0,,False,,,,
,learnmachinelearning,"I'm hoping to find a source of datasets that allows for searching across things like:

* data format (csv, image,....)
* problem type (classification, regression)
* dataset size
* missing data/missing records %
* class imbalance

I'm aware of Kaggle and a few others, but I've not been able to find one with real filter/search capability like that.  Are there any out there?",t2_702gf,False,,0,False,Are there any searchable/filterable sources of datasets for learning?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6xmrv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587714009.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m hoping to find a source of datasets that allows for searching across things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data format (csv, image,....)&lt;/li&gt;
&lt;li&gt;problem type (classification, regression)&lt;/li&gt;
&lt;li&gt;dataset size&lt;/li&gt;
&lt;li&gt;missing data/missing records %&lt;/li&gt;
&lt;li&gt;class imbalance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;#39;m aware of Kaggle and a few others, but I&amp;#39;ve not been able to find one with real filter/search capability like that.  Are there any out there?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6xmrv,True,,ezeeetm,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6xmrv/are_there_any_searchablefilterable_sources_of/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6xmrv/are_there_any_searchablefilterable_sources_of/,155203,1587685209.0,0,,False,,,,
,learnmachinelearning,"I am a cognitive science student with growing familiarity in biological neural networks, neuroscience, and philosophy of mind. I have very little programming knowledge (not tech illiterate, but I am just not familiar with languages like Python or C) but I would like to learn so that I could experiment with artificial neural networks. 

What would be a good starting place to learn Python or C++ (whichever is more prevalent in the paradigm and field) while still retaining a core goal and orientation around learning about artificial neural networks? I believe I have the hardware necessaryâ€”I have a 1080 (I hear that NNs are GPU intensive), 16gb ram, and an I7-4930Kâ€”to properly run most of what I would like to fiddle with, but as a smaller question I was wondering how well CPU based NNs work? I have a T440p Thinkpad with a I7-4600M sitting around and I was curious if I could run a small NN on that.

Any help and resources on this matter would be much appreciated.",t2_xdru9,False,,0,False,Starting Place for a Total Beginner?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6x8yj,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587712647.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am a cognitive science student with growing familiarity in biological neural networks, neuroscience, and philosophy of mind. I have very little programming knowledge (not tech illiterate, but I am just not familiar with languages like Python or C) but I would like to learn so that I could experiment with artificial neural networks. &lt;/p&gt;

&lt;p&gt;What would be a good starting place to learn Python or C++ (whichever is more prevalent in the paradigm and field) while still retaining a core goal and orientation around learning about artificial neural networks? I believe I have the hardware necessaryâ€”I have a 1080 (I hear that NNs are GPU intensive), 16gb ram, and an I7-4930Kâ€”to properly run most of what I would like to fiddle with, but as a smaller question I was wondering how well CPU based NNs work? I have a T440p Thinkpad with a I7-4600M sitting around and I was curious if I could run a small NN on that.&lt;/p&gt;

&lt;p&gt;Any help and resources on this matter would be much appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6x8yj,True,,CrimsonNomad,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6x8yj/starting_place_for_a_total_beginner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6x8yj/starting_place_for_a_total_beginner/,155203,1587683847.0,0,,False,,,,
,learnmachinelearning,"is this general pseudocode/workflow correct when using CV?

* train using CV to define the best performing hyperparameters for a given model.  What you get out of this is \_not\_ a trained model, but rather a set of hyperparameters that represent the least total error averaged across all the CV folds
* now, take those HP's and train again against the *whole* training set with no CV (paying close attention to train/validate scores that signal over/underfitting to ensure the params CV gave us represents the right balance of bias/variance)
* now take that trained model and score it against a test set that was held out from all of the above (paying close attention to the difference between scores from the *previous* step and the scores against the test set to further validate not over/under fitting)

is this understanding pretty much right? am I missing anything",t2_702gf,False,,0,False,Is my understanding of 'training final model after CV' correct?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6t5qk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,1587671888.0,,[],{},,,True,,1587699018.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;is this general pseudocode/workflow correct when using CV?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;train using CV to define the best performing hyperparameters for a given model.  What you get out of this is _not_ a trained model, but rather a set of hyperparameters that represent the least total error averaged across all the CV folds&lt;/li&gt;
&lt;li&gt;now, take those HP&amp;#39;s and train again against the &lt;em&gt;whole&lt;/em&gt; training set with no CV (paying close attention to train/validate scores that signal over/underfitting to ensure the params CV gave us represents the right balance of bias/variance)&lt;/li&gt;
&lt;li&gt;now take that trained model and score it against a test set that was held out from all of the above (paying close attention to the difference between scores from the &lt;em&gt;previous&lt;/em&gt; step and the scores against the test set to further validate not over/under fitting)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;is this understanding pretty much right? am I missing anything&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6t5qk,True,,ezeeetm,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6t5qk/is_my_understanding_of_training_final_model_after/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6t5qk/is_my_understanding_of_training_final_model_after/,155203,1587670218.0,0,,False,,,,
,learnmachinelearning,"Newbie to ML. I wanted to make cloned voices saying scripts (eg. Joe Rogen narrating a script).This is for video production purposes. How should i first of all go about this problem. Are there libraries that can clone voices and then Text to speech. What technologies would i have to learn in order to make this on my own.  

Furthermore I found this web app: [https://www.voicepods.com](https://www.voicepods.com). They are in the cloning voices bossiness I think. How does this tech even work?",t2_iu54y,False,,0,False,How does this technology? (Cloning voices) and how would i go about making a ML model like this on my own. (Resemble Ai),[],r/learnmachinelearning,False,6,,0,,False,t3_g6wfjn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587709729.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Newbie to ML. I wanted to make cloned voices saying scripts (eg. Joe Rogen narrating a script).This is for video production purposes. How should i first of all go about this problem. Are there libraries that can clone voices and then Text to speech. What technologies would i have to learn in order to make this on my own.  &lt;/p&gt;

&lt;p&gt;Furthermore I found this web app: &lt;a href=""https://www.voicepods.com""&gt;https://www.voicepods.com&lt;/a&gt;. They are in the cloning voices bossiness I think. How does this tech even work?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6wfjn,True,,redditssexiestguy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6wfjn/how_does_this_technology_cloning_voices_and_how/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6wfjn/how_does_this_technology_cloning_voices_and_how/,155203,1587680929.0,0,,False,,,,
,learnmachinelearning,,t2_5c0ysq0r,False,,0,False,Andrew Ng introduces a new AI for Medicine Specialization Course!,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g60y79,False,dark,0.97,,public,419,0,{},140.0,,False,[],,False,False,,{},,False,419,,False,https://b.thumbs.redditmedia.com/1ROQKDRhLt6VcaFBAXddNVIvN_4zg3Tz-kqd9nWmqns.jpg,False,,[],{},link,,False,,1587590288.0,text,6,,,text,theclickreader.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?auto=webp&amp;s=34e0a8f5f664c4ae2f04525dbf0225e5cfd6055c', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae6e02f026d14f664eae0f3e3cf0b0ee6cff433a', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=546794131a3c221a414530b86d4e6f615b11e769', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1068daff1f5e54b99f56538418792e14c68f2fb6', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17186ea2ea8e4ddb91abcd59f28c053ff696d232', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=da6b8537500b9412c80a3babfb21af5fe8bf478f', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/qhrXOkKzs49Zhjd0LLHyBlSuPtCkPF7rPV1kII5KufI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5d735d7f5a872a80754405931cce26c741324fd1', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'NT4llBtLQzaGVz9LwnKMr3w278XX_OvYXm1bKbe4948'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g60y79,True,,thepythonprogrammer,,31,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60y79/andrew_ng_introduces_a_new_ai_for_medicine/,all_ads,False,https://theclickreader.com/andrew-ng-introduces-a-new-ai-for-medicine-specialization-course/,155203,1587561488.0,0,,False,,,,
,learnmachinelearning,"What Data Scientists donâ€™t learn in online courses or in University? Knowing the basics of these 5 skills will get you ahead in the workplace.

[https://towardsdatascience.com/5-skills-data-scientists-should-learn-27d186820d17](https://towardsdatascience.com/5-skills-data-scientists-should-learn-27d186820d17)

**TLDR;**

Machine Learning is evolving â€” it is not just about math and statistics anymore

* Software Engineering:
   * Compiled languages
   * Design patterns
* Automated Software Testing
* Cloud Computing
   * GPU Instances
   * Services like AWS S3, Redshift
   * Microservices - model has to run somewhere
* Command Line Interface
* Git",t2_k9eg8,False,,0,False,5 skills Data Scientists should learn,[],r/learnmachinelearning,False,6,,0,,False,t3_g6khzk,False,dark,0.9,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},self,,True,,1587667699.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What Data Scientists donâ€™t learn in online courses or in University? Knowing the basics of these 5 skills will get you ahead in the workplace.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://towardsdatascience.com/5-skills-data-scientists-should-learn-27d186820d17""&gt;https://towardsdatascience.com/5-skills-data-scientists-should-learn-27d186820d17&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Machine Learning is evolving â€” it is not just about math and statistics anymore&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Software Engineering:

&lt;ul&gt;
&lt;li&gt;Compiled languages&lt;/li&gt;
&lt;li&gt;Design patterns&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Automated Software Testing&lt;/li&gt;
&lt;li&gt;Cloud Computing

&lt;ul&gt;
&lt;li&gt;GPU Instances&lt;/li&gt;
&lt;li&gt;Services like AWS S3, Redshift&lt;/li&gt;
&lt;li&gt;Microservices - model has to run somewhere&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Command Line Interface&lt;/li&gt;
&lt;li&gt;Git&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?auto=webp&amp;s=5707e33ffd111b530eb1a30ca4ce61fe03ad77de', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4daf269c1fe19417dd32c393d70b46cc68788b2', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e851491f1eabc797694d1cb0bbca5ecd1e162a5a', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f6aaae49a582582901e349de69e8fd8aefece70', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10768799ff3742e9f8b2f66f41433c49c1918a1f', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c5f135bef28014d202e2511fb9d6236ae076fbcd', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/X0-buZ5YADrS8O8klTbhuRtfVdyQ-YkNweEDJwpYR_o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9b77eb0b470a7ae3696700cd621f1cb2f96c8f4', 'width': 1080, 'height': 720}], 'variants': {}, 'id': '9CNZbxRkR_cIlCfHU_yIhK-m_Ki9b6qCW5udn7S9A9c'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6khzk,True,,hiphop1987,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6khzk/5_skills_data_scientists_should_learn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6khzk/5_skills_data_scientists_should_learn/,155203,1587638899.0,0,,False,,,,
,learnmachinelearning,"https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py

Question on completely unsupervised outlier detection (using OPTICS and DBSCAN):

Normally, I thought that clustering algorithms should not be used for classification purposes (e.g. classifying whether an observation is an outlier or not). However, I came across clustering algorithms such as DBSCAN and OPTICS which seem to have specific applications to detect ""noisy data"". Does anyone think these algorithms can be used to effectively identity outliers (or noisy data)?

Thanks!",t2_xtuyc,False,,0,False,Outlier detection,[],r/learnmachinelearning,False,6,,0,,False,t3_g6rppf,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1587694409.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py""&gt;https://scikit-learn.org/stable/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Question on completely unsupervised outlier detection (using OPTICS and DBSCAN):&lt;/p&gt;

&lt;p&gt;Normally, I thought that clustering algorithms should not be used for classification purposes (e.g. classifying whether an observation is an outlier or not). However, I came across clustering algorithms such as DBSCAN and OPTICS which seem to have specific applications to detect &amp;quot;noisy data&amp;quot;. Does anyone think these algorithms can be used to effectively identity outliers (or noisy data)?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?auto=webp&amp;s=790e4de6a4da0f0ea15f26ab27e30b4127ad0dff', 'width': 1000, 'height': 700}, 'resolutions': [{'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3afde2e4c3a3acf5aa2f14a6fedd3d79dec55bcf', 'width': 108, 'height': 75}, {'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=225f93b82abc34608f63fad2cbfdb37b71f42934', 'width': 216, 'height': 151}, {'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1a97956e0a904db796a63200e8609c14b1b197d4', 'width': 320, 'height': 224}, {'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5383984926efa50b2d4d81e1fa5bcf6795dd1c13', 'width': 640, 'height': 448}, {'url': 'https://external-preview.redd.it/VWmKGTGg_ACf1JQvv0LBQZfrbgcowDD7y8z5z5gXIls.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=368ef39a7cfe84c582c4d19b56b9c7db889d8d36', 'width': 960, 'height': 672}], 'variants': {}, 'id': 'nZD3zX623agJM7ybrYwJNwmj14CLUu_I6yYBPIwkGis'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6rppf,True,,ottawalanguages,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6rppf/outlier_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6rppf/outlier_detection/,155203,1587665609.0,0,,False,,,,True
,learnmachinelearning,I'm trying to calculate the variance between predictions from a kNN algorithm. Is variance just the average difference between the means of the output values of each prediction made?,t2_95x2c,False,,0,False,How do I calculate variance for a regression function?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6v2fq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587705168.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to calculate the variance between predictions from a kNN algorithm. Is variance just the average difference between the means of the output values of each prediction made?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6v2fq,True,,cheeseisakindof,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6v2fq/how_do_i_calculate_variance_for_a_regression/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6v2fq/how_do_i_calculate_variance_for_a_regression/,155203,1587676368.0,0,,False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,Top Recommended AI and Machine Learning Articles to Read During Lock down,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,87.0,False,t3_g6j4zn,False,light,0.84,,public,8,0,{},140.0,,False,[],,False,False,,{},Discussion,False,8,,False,https://a.thumbs.redditmedia.com/FVfwP64edQeOdq0K7pC1NSQ3cMEpKtW8QNA4QDUdBO4.jpg,False,,[],{},link,,False,,1587660226.0,richtext,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?auto=webp&amp;s=7e59e433cae31386f288d35f2771692b8ee56be4', 'width': 1200, 'height': 750}, 'resolutions': [{'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2d7a815c136839ae9821243f803351675930aa8', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c49f957cca92ef71333db4f9c0525ccf704889d', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ba99b635ce377be42dd2681b6db50427a379f39', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=54f75528c39fdfcd11a1a3057134462a3438d978', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=66c78a548edc0e69d89be7ae98e424f70042df18', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/_sl9nPpl0V6AZVLispIBzlrRJq84F6iWulln6TM6CVg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e9c6eac00b0d63449d6b66ccbcfaf21069a539f', 'width': 1080, 'height': 675}], 'variants': {}, 'id': 'n1oakgv78-uqZS3tnWisvHYXJBVdi7xwtBOQuf_rb10'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g6j4zn,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6j4zn/top_recommended_ai_and_machine_learning_articles/,all_ads,False,https://medium.com/@albertchristopherr/top-recommended-ai-articles-to-read-during-lockdown-4562c9b6e202,155203,1587631426.0,0,,False,,,,
,learnmachinelearning,"can you help me with the right path to learn artificial intelligence from the beginning ?  
should i learn python first or what ?  


thank you",t2_2psmfnmi,False,,0,False,The right path to learn artificial intelligence ?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6q5x2,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587689388.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;can you help me with the right path to learn artificial intelligence from the beginning ?&lt;br/&gt;
should i learn python first or what ?  &lt;/p&gt;

&lt;p&gt;thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6q5x2,True,,MarcoAcrono,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6q5x2/the_right_path_to_learn_artificial_intelligence/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6q5x2/the_right_path_to_learn_artificial_intelligence/,155203,1587660588.0,0,,False,,,,
,learnmachinelearning,,t2_136e8t,False,,0,False,"In linear classification, what is theta_0? (ignore the question mark in the picture)","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,118.0,False,t3_g6tuqh,False,dark,0.5,,public,0,0,{},140.0,,False,[],,True,False,,{},Question,False,0,,False,https://b.thumbs.redditmedia.com/TAZdXHLgouu_Rb0tcehUH-GO-kJCBdZrhmCBoVlOOkE.jpg,False,,[],{},image,,False,,1587701281.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/hbl978upjmu41.png?auto=webp&amp;s=d62438e77d047a9934cf87c6c252e983ec0bc163', 'width': 542, 'height': 457}, 'resolutions': [{'url': 'https://preview.redd.it/hbl978upjmu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=aac119e01d87de240bc53087816ed055b3ab4c43', 'width': 108, 'height': 91}, {'url': 'https://preview.redd.it/hbl978upjmu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6b0863a2090b07557c8e69255e253cf8763a5021', 'width': 216, 'height': 182}, {'url': 'https://preview.redd.it/hbl978upjmu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=457474502a53d93e6a5e373b5abaad8579f5168a', 'width': 320, 'height': 269}], 'variants': {}, 'id': 'jBfrNLlsKpaYTiP3yqSsrhO7WyozSleb8gsuOgNvwik'}], 'enabled': True}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6tuqh,True,,JackIsNotInTheBox,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6tuqh/in_linear_classification_what_is_theta_0_ignore/,all_ads,False,https://i.redd.it/hbl978upjmu41.png,155203,1587672481.0,0,,False,,,,
,learnmachinelearning,"I'm talking about using the [model.fit](https://model.fit)(features, targets) function. 

If i run it in say, 5 times. Would it erase all that it's learned each time and overwrite, or would it update with the new features and targets?

\---------------------

CONTEXT: I'm trying to learn ML, and I'm working on my first major project other than a premade online course provided dataset. 

Obviously, it goes without saying Machine learning requires ridiculously large amounts of training and testing data, often going into a million data points. And I'm not really positive that storing an array with a million arrays nested in it is a good idea, particularly on my old pc.

So instead of training it with all the million data points at the same time, I was thinking of looping it. Each time I would load a new set of data (say 1000 arrays), and train it with that smaller set. 

It's less efficient as code, but I feel it'd be putting less stress on the memory.",t2_3c3iivc6,False,,0,False,What happens if you try to call the training function multiple times? (Python Sklearn library),"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6peuw,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1587686883.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m talking about using the &lt;a href=""https://model.fit""&gt;model.fit&lt;/a&gt;(features, targets) function. &lt;/p&gt;

&lt;p&gt;If i run it in say, 5 times. Would it erase all that it&amp;#39;s learned each time and overwrite, or would it update with the new features and targets?&lt;/p&gt;

&lt;p&gt;---------------------&lt;/p&gt;

&lt;p&gt;CONTEXT: I&amp;#39;m trying to learn ML, and I&amp;#39;m working on my first major project other than a premade online course provided dataset. &lt;/p&gt;

&lt;p&gt;Obviously, it goes without saying Machine learning requires ridiculously large amounts of training and testing data, often going into a million data points. And I&amp;#39;m not really positive that storing an array with a million arrays nested in it is a good idea, particularly on my old pc.&lt;/p&gt;

&lt;p&gt;So instead of training it with all the million data points at the same time, I was thinking of looping it. Each time I would load a new set of data (say 1000 arrays), and train it with that smaller set. &lt;/p&gt;

&lt;p&gt;It&amp;#39;s less efficient as code, but I feel it&amp;#39;d be putting less stress on the memory.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6peuw,True,,TooShyToSayILoveYou,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6peuw/what_happens_if_you_try_to_call_the_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6peuw/what_happens_if_you_try_to_call_the_training/,155203,1587658083.0,0,,False,,,,
,learnmachinelearning,I've seen Scale.ai and I'm wondering why this company is valued so much? I don't understand it's real 'value' and what it does and how it provide a real value.,t2_51w9vxrk,False,,0,False,Scale.ai,[],r/learnmachinelearning,False,6,,0,,False,t3_g6j04x,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},,False,10,,False,self,False,,[],{},,,True,,1587659462.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve seen Scale.ai and I&amp;#39;m wondering why this company is valued so much? I don&amp;#39;t understand it&amp;#39;s real &amp;#39;value&amp;#39; and what it does and how it provide a real value.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6j04x,True,,Z4glis,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6j04x/scaleai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6j04x/scaleai/,155203,1587630662.0,0,,False,,,,
,learnmachinelearning,"Hey guys,

I'm fairly new to machine learning in general and currently trying to apply a deep convolutional GAN model to a time series generation problem in python using Keras. I looked at different implementations of DCGANs for image generation and I superficially understand how they work. However I'm struggling to desing my model in a way that it accepts time series data with one feature (I'm just using generated sine and cosine series right now). I especially struggle with the way I have to shape my data and how to work with the dimensions of the convolutional layers. Do you guys have any suggestions on that?",t2_chuao,False,,0,False,Using DCGANs for generating time series,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6tai1,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587699440.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m fairly new to machine learning in general and currently trying to apply a deep convolutional GAN model to a time series generation problem in python using Keras. I looked at different implementations of DCGANs for image generation and I superficially understand how they work. However I&amp;#39;m struggling to desing my model in a way that it accepts time series data with one feature (I&amp;#39;m just using generated sine and cosine series right now). I especially struggle with the way I have to shape my data and how to work with the dimensions of the convolutional layers. Do you guys have any suggestions on that?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6tai1,True,,pantoffl,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6tai1/using_dcgans_for_generating_time_series/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6tai1/using_dcgans_for_generating_time_series/,155203,1587670640.0,0,,False,,,,
,learnmachinelearning,,t2_bjh0jhq,False,,0,False,Looking for information on what types of ML and AI could help to modernize the nuclear c3 infrastructure,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6sp8j,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,default,False,,[],{},,,False,,1587697574.0,richtext,6,,,text,self.ArtificialInteligence,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6sp8j,True,,misskatniss17,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6sp8j/looking_for_information_on_what_types_of_ml_and/,all_ads,False,/r/ArtificialInteligence/comments/g6r3pu/what_types_of_ai_will_be_used_in_the/,155203,1587668774.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ArtificialInteligence', 'selftext': 'Hey everyone, on my phone so sorry for formatting. AI has already started to infiltrate the nuclear command, control and communications (NC3) infrastructure which has been largely neglected since the Cold War. Iâ€™m not here to debate on whether or not thatâ€™s a good idea lol, but I do need help. I am a grad student researching this topic and I canâ€™t find any specific AI methods that could be used. Does anyone know of any specific algorithms or methods I can research? Thanks in advance!', 'author_fullname': 't2_bjh0jhq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What types of AI will be used in the modernization of the nuclear C3 architecture?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ArtificialInteligence', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g6r3pu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587692414.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ArtificialInteligence', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, on my phone so sorry for formatting. AI has already started to infiltrate the nuclear command, control and communications (NC3) infrastructure which has been largely neglected since the Cold War. Iâ€™m not here to debate on whether or not thatâ€™s a good idea lol, but I do need help. I am a grad student researching this topic and I canâ€™t find any specific AI methods that could be used. Does anyone know of any specific algorithms or methods I can research? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3crzr', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g6r3pu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'misskatniss17', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ArtificialInteligence/comments/g6r3pu/what_types_of_ai_will_be_used_in_the/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ArtificialInteligence/comments/g6r3pu/what_types_of_ai_will_be_used_in_the/', 'subreddit_subscribers': 34102, 'created_utc': 1587663614.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_g6r3pu,,
,learnmachinelearning,"Awkward title I know, here's the story:

Your MLP puts images into one of three classes:

Cat

Dog

Neither

Historically it has only been shown cats, dogs, people, and birds, and it does a decent job with a multi class AUC of 0.88 on test data (humans and birds go into the ""neither"" category). 

Then the model was tested on cars, and it only put half of them into ""neither"" and the rest in either cats or dogs, and when included with the test set at equal amounts as the others, and mAUC is reduced to 0.81.

So now you wish to retrain your model so that it can also put cars into the neither class. Keep in mind you cannot add new features.

Is it *likely* that doing so will reduce the model's accuracy for being able to classify the other inputs (birds/cats/dogs/humans) in order to be better at classifying the new input? Or is it a total guess and you just have to try and see?",t2_56mt1rcj,False,,0,False,"If you can't add more features, but you retrain a model to be able to classify a new type of input, will it likely lose accuracy for classifying the other types of inputs?",[],r/learnmachinelearning,False,6,,0,,False,t3_g6s4h0,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587695739.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Awkward title I know, here&amp;#39;s the story:&lt;/p&gt;

&lt;p&gt;Your MLP puts images into one of three classes:&lt;/p&gt;

&lt;p&gt;Cat&lt;/p&gt;

&lt;p&gt;Dog&lt;/p&gt;

&lt;p&gt;Neither&lt;/p&gt;

&lt;p&gt;Historically it has only been shown cats, dogs, people, and birds, and it does a decent job with a multi class AUC of 0.88 on test data (humans and birds go into the &amp;quot;neither&amp;quot; category). &lt;/p&gt;

&lt;p&gt;Then the model was tested on cars, and it only put half of them into &amp;quot;neither&amp;quot; and the rest in either cats or dogs, and when included with the test set at equal amounts as the others, and mAUC is reduced to 0.81.&lt;/p&gt;

&lt;p&gt;So now you wish to retrain your model so that it can also put cars into the neither class. Keep in mind you cannot add new features.&lt;/p&gt;

&lt;p&gt;Is it &lt;em&gt;likely&lt;/em&gt; that doing so will reduce the model&amp;#39;s accuracy for being able to classify the other inputs (birds/cats/dogs/humans) in order to be better at classifying the new input? Or is it a total guess and you just have to try and see?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6s4h0,True,,jamzwck,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6s4h0/if_you_cant_add_more_features_but_you_retrain_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6s4h0/if_you_cant_add_more_features_but_you_retrain_a/,155203,1587666939.0,0,,False,,,,
,learnmachinelearning,"Hi all!

Is anyone interested in reading through/discussing Sutton/Barto's text on Reinforcement Learning ([http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html))? I'm a mathematician making the switch into ML and have been meaning to work through this text, but it's always nice to have people to discuss concepts with. I'm also open to studying different topics as well.",t2_2cn4yrem,False,,0,False,Reinforcement Learning / ML reading group,[],r/learnmachinelearning,False,6,,0,,False,t3_g6rvuz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587694960.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all!&lt;/p&gt;

&lt;p&gt;Is anyone interested in reading through/discussing Sutton/Barto&amp;#39;s text on Reinforcement Learning (&lt;a href=""http://incompleteideas.net/book/the-book-2nd.html""&gt;http://incompleteideas.net/book/the-book-2nd.html&lt;/a&gt;)? I&amp;#39;m a mathematician making the switch into ML and have been meaning to work through this text, but it&amp;#39;s always nice to have people to discuss concepts with. I&amp;#39;m also open to studying different topics as well.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DHkhAYd-OI4G4hh8Y7b0DpVlIHW0FYYmNsPWntRCP7g.jpg?auto=webp&amp;s=3b6819db7ac8c90b343dc8dd1be22531da23c2b6', 'width': 126, 'height': 161}, 'resolutions': [{'url': 'https://external-preview.redd.it/DHkhAYd-OI4G4hh8Y7b0DpVlIHW0FYYmNsPWntRCP7g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76d611d1a931a825b16dc591224eff7c07728c9f', 'width': 108, 'height': 138}], 'variants': {}, 'id': '-GN7J3XbDwvmLUUKfIT_D8FnaNcqDOgY-spBsAjUBwk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6rvuz,True,,welcome_to_moonside,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6rvuz/reinforcement_learning_ml_reading_group/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6rvuz/reinforcement_learning_ml_reading_group/,155203,1587666160.0,0,,False,,,,
,learnmachinelearning,"Hi, 

I've read several articles/tutorials on Logistic Regression and I've come across this idea of log-odds being equal to the weighted sum of features.

i.e. if p is the probability of a sample belonging to positive class(target variable: 1), (1-p) is the probability of it belonging to the negative(target variable: 0). Then, for input features x1, x2, x3... xn and weights theta0, theta1, theta2... thetan, they write

log(p/(1-p)) = x0 \* theta0 + x1 \* theta1 + x2 \* theta2 .... xn \* thetan

where x0 is bias

Now I get what features are, I get what weighted sum of feature is, I know about the odds, I know log, what I cannot comprehend is, how are those two quantities equal? I've gone through plenty of online articles but they **always say that these two are equal**, but they don't explain *how*.",t2_6cbc7e4,False,,0,False,Logistic Regression: relationship of log-odds and weighted sum?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6rg8i,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587693560.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read several articles/tutorials on Logistic Regression and I&amp;#39;ve come across this idea of log-odds being equal to the weighted sum of features.&lt;/p&gt;

&lt;p&gt;i.e. if p is the probability of a sample belonging to positive class(target variable: 1), (1-p) is the probability of it belonging to the negative(target variable: 0). Then, for input features x1, x2, x3... xn and weights theta0, theta1, theta2... thetan, they write&lt;/p&gt;

&lt;p&gt;log(p/(1-p)) = x0 * theta0 + x1 * theta1 + x2 * theta2 .... xn * thetan&lt;/p&gt;

&lt;p&gt;where x0 is bias&lt;/p&gt;

&lt;p&gt;Now I get what features are, I get what weighted sum of feature is, I know about the odds, I know log, what I cannot comprehend is, how are those two quantities equal? I&amp;#39;ve gone through plenty of online articles but they &lt;strong&gt;always say that these two are equal&lt;/strong&gt;, but they don&amp;#39;t explain &lt;em&gt;how&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6rg8i,True,,sloppybird,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6rg8i/logistic_regression_relationship_of_logodds_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6rg8i/logistic_regression_relationship_of_logodds_and/,155203,1587664760.0,0,,False,,,,
,learnmachinelearning,"I'm deploying a model to a server, and my understanding is that when you deserialize you should have the same versions of python and any packages used. I don't want my company to be wed to, for instance, sklearn 0.22; as new features come out, being able to use them would be nice. However, we also can't afford to rebuild our models for every new version of sklearn. How have you have you dealt with this in the past?

&amp;#x200B;

At the moment, virtual environments don't seem like an option. 

&amp;#x200B;

Thanks for any help.",t2_meufi,False,,0,False,How to deal with model persistence and version control?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6rb6h,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587693100.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m deploying a model to a server, and my understanding is that when you deserialize you should have the same versions of python and any packages used. I don&amp;#39;t want my company to be wed to, for instance, sklearn 0.22; as new features come out, being able to use them would be nice. However, we also can&amp;#39;t afford to rebuild our models for every new version of sklearn. How have you have you dealt with this in the past?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;At the moment, virtual environments don&amp;#39;t seem like an option. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6rb6h,True,,LebronsForehead,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6rb6h/how_to_deal_with_model_persistence_and_version/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6rb6h/how_to_deal_with_model_persistence_and_version/,155203,1587664300.0,0,,False,,,,
,learnmachinelearning,"I used to watch the old machine learning [lectures](https://see.stanford.edu/Course/CS229) that Andrew Ng taught at Stanford in 2008. I just found out that Stanford just uploaded a much newer version of the course (still taught by Andrew Ng). You can watch the videos [here](https://www.youtube.com/watch?v=jGwO_UgTS7I&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU), and check out the syllabus and download the notes [here](http://cs229.stanford.edu/syllabus-autumn2018.html).

I hope some of you find it helpful. This course really helped me get on my feet in understanding machine learning in a deeper level.",t2_krk6q2t,False,,0,False,Andrew Ng's Stanford machine learning course (CS 229) now online with newer 2018 version,[],r/learnmachinelearning,False,6,,0,,False,t3_g63j7j,False,dark,1.0,,public,118,0,{},,,False,[],,False,False,,{},,False,118,,False,self,False,,[],{},self,,True,,1587599280.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I used to watch the old machine learning &lt;a href=""https://see.stanford.edu/Course/CS229""&gt;lectures&lt;/a&gt; that Andrew Ng taught at Stanford in 2008. I just found out that Stanford just uploaded a much newer version of the course (still taught by Andrew Ng). You can watch the videos &lt;a href=""https://www.youtube.com/watch?v=jGwO_UgTS7I&amp;amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;here&lt;/a&gt;, and check out the syllabus and download the notes &lt;a href=""http://cs229.stanford.edu/syllabus-autumn2018.html""&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I hope some of you find it helpful. This course really helped me get on my feet in understanding machine learning in a deeper level.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/u1ZbRpzqBCVH3lNpIoo8Tgf6na2QuQtU4IuEY36gkaU.jpg?auto=webp&amp;s=858b7c2c58d028738f8d9b56917597e84566d619', 'width': 148, 'height': 208}, 'resolutions': [{'url': 'https://external-preview.redd.it/u1ZbRpzqBCVH3lNpIoo8Tgf6na2QuQtU4IuEY36gkaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c50846fdcb42e52b5b27abb5d4b792a4fff5a51', 'width': 108, 'height': 151}], 'variants': {}, 'id': 'VkavWCmx-sqdru0lWWEYYi2vrSn_vKhAcyB57eI0Msg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g63j7j,True,,seismatica,,35,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g63j7j/andrew_ngs_stanford_machine_learning_course_cs/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g63j7j/andrew_ngs_stanford_machine_learning_course_cs/,155203,1587570480.0,0,,False,,,,
,learnmachinelearning,I have been thinking of using BERT for sentiment clasification on some hashtag. My question is if I need to fine tune BERT on some labelled examples of the hashtag in question. Cant I just use some general use pre-trained model on those tweets? I guess I dont understand why sentiment clasification needs to be trained on this particular hastag. Shoudnt a general pretrained model still perform with some accuracy?,t2_67qi60ts,False,,0,False,Question about Twitter sentiment clasification,[],r/learnmachinelearning,False,6,,0,,False,t3_g6r48a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587692460.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been thinking of using BERT for sentiment clasification on some hashtag. My question is if I need to fine tune BERT on some labelled examples of the hashtag in question. Cant I just use some general use pre-trained model on those tweets? I guess I dont understand why sentiment clasification needs to be trained on this particular hastag. Shoudnt a general pretrained model still perform with some accuracy?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6r48a,True,,dsmlquestions,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6r48a/question_about_twitter_sentiment_clasification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6r48a/question_about_twitter_sentiment_clasification/,155203,1587663660.0,0,,False,,,,
,learnmachinelearning,"I'm working on the Reddit Sarcasm labeled dataset and I was curious to see how well BERT performed on its own. It's doing OK (70ish% accuracy in predicting sarcastic vs serious midway through Epoch 1 of 2 on a dataset that is 50/50 sarcastic), but not great. There is other information in the dataset that I could potentially use (eg, how similar the post is to various memes, the text of the post that the post in the dataset is responding to, how many upvotes the post got, etc), but it's a little difficult to think of how to combine this data with BERT and I haven't seen it done. The only idea I can think of is just taking BERT's projected probability of sarcasm and using it as a feature for a logistic regression or random forest, but this seems clunky and not the best possible use of these powerful NLP algorithms.",t2_354dzq,False,,0,False,Is there a way to combine a natural language processing algorithm like BERT with non-text data or with a supporting text?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6r24v,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587692274.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on the Reddit Sarcasm labeled dataset and I was curious to see how well BERT performed on its own. It&amp;#39;s doing OK (70ish% accuracy in predicting sarcastic vs serious midway through Epoch 1 of 2 on a dataset that is 50/50 sarcastic), but not great. There is other information in the dataset that I could potentially use (eg, how similar the post is to various memes, the text of the post that the post in the dataset is responding to, how many upvotes the post got, etc), but it&amp;#39;s a little difficult to think of how to combine this data with BERT and I haven&amp;#39;t seen it done. The only idea I can think of is just taking BERT&amp;#39;s projected probability of sarcasm and using it as a feature for a logistic regression or random forest, but this seems clunky and not the best possible use of these powerful NLP algorithms.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6r24v,True,,RedditBadSuggestions,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6r24v/is_there_a_way_to_combine_a_natural_language/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6r24v/is_there_a_way_to_combine_a_natural_language/,155203,1587663474.0,0,,False,,,,
,learnmachinelearning,"I'm curious about what happens to a model when it hits the production stage and it begins to make predictions. Let's say 80% of these predictions are correct and 20% are incorrect. When it comes time to make more predictions, wouldn't it be problematic if the model selects rows that were previously false positives / incorrect?

Or at this stage would you retrain and hope the model has learned enough to avoid the wrong predictions?",t2_qti7h,False,,0,False,Is It Possible to Inform a Model of Incorrect Predictions?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6qyii,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1587691950.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m curious about what happens to a model when it hits the production stage and it begins to make predictions. Let&amp;#39;s say 80% of these predictions are correct and 20% are incorrect. When it comes time to make more predictions, wouldn&amp;#39;t it be problematic if the model selects rows that were previously false positives / incorrect?&lt;/p&gt;

&lt;p&gt;Or at this stage would you retrain and hope the model has learned enough to avoid the wrong predictions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6qyii,True,,XariZaru,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6qyii/is_it_possible_to_inform_a_model_of_incorrect/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6qyii/is_it_possible_to_inform_a_model_of_incorrect/,155203,1587663150.0,0,,False,,,,
,learnmachinelearning,"Hi All,

Want to introduce The Super Duper NLP Repo, a database of over 100 Colab notebooks for the NLP developer to experiment with. These notebooks hold various models from RNNs to the T5.

Hope you enjoy.

[notebooks.quantumstat.com](https://notebooks.quantumstat.com/)",t2_27omm5ij,False,,0,False,More than 100 NLP Colab Notebooks Found Here!,[],r/learnmachinelearning,False,6,,0,,False,t3_g66q3y,False,dark,0.97,,public,63,0,{},,,False,[],,False,False,,{},,False,63,,False,self,False,,[],{},self,,True,,1587609607.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi All,&lt;/p&gt;

&lt;p&gt;Want to introduce The Super Duper NLP Repo, a database of over 100 Colab notebooks for the NLP developer to experiment with. These notebooks hold various models from RNNs to the T5.&lt;/p&gt;

&lt;p&gt;Hope you enjoy.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://notebooks.quantumstat.com/""&gt;notebooks.quantumstat.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?auto=webp&amp;s=98351c20e442e4f9eed8679f76456757908c9e45', 'width': 3153, 'height': 1780}, 'resolutions': [{'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb2e6e622a73c0de98953d7046b287c3f870a22b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0d42f47a715b2259ee2c71ba08d8caa61738dc1', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf7973dde61f59cf36c68ee0e85f12507c4e5306', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d567cb49f28a355e59ee4d86c87683983c6cc29', 'width': 640, 'height': 361}, {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b4096b041f1c16f9d8c88d330ee31f08cd17a2b', 'width': 960, 'height': 541}, {'url': 'https://external-preview.redd.it/S-DuQd5Efc2B5sLISg0Lws_Zb24WWBxRYaQLB6TiJwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fcd250015eb3ce48ab1dbde570ed37d2058f75f9', 'width': 1080, 'height': 609}], 'variants': {}, 'id': '-uaan0_L_w2jdERAsqSZUlmjgEXuIszKk5PilYUtKTA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g66q3y,True,,Quantum_Stat,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g66q3y/more_than_100_nlp_colab_notebooks_found_here/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g66q3y/more_than_100_nlp_colab_notebooks_found_here/,155203,1587580807.0,0,,False,,,,
,learnmachinelearning,"Hello, I'm very new to machine learning please go easy on me.

So, I'm working on creating a regression model which predicts the concentration of the metal produced based on the dosages of the chemicals that are used for the process.

My outcome should be in the form of equation as the operations team will have the targeted concentration and will solve for the equation that is produced by the model using excel solver to find out the optimal chemical dosage. 

I can use linear regression or polynomial regression and I've used them, but I want a better model which can predict more accurately. So, I went for decision trees and random forest, and got a way better model. But the problem now is, I want some kind of output out of the model, preferably in the form of an equation.

What I was thinking was to use the predictions I get from the random forest as my input for a new polynomial regression and get a regression equation that fits the plot that fits the training set. But is this the right way to go? Will the model be back to the same accuracy or even worse?

Or is there any other way to get any output out of the decision tree or random forest?

P.S. I use KNIME for the analysis as I don't have much coding knowledge and it doesn't have any other regressions like lasso, ridge regression etc.

Tl;dr: Need some kind of regression equation from decision trees or random forest. If not possible, atleast is there any way to get any kind of output out of random forest other than the prediction?",t2_p8345p1,False,,0,False,Is it possible to get any output other than the prediction in decision trees?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6qe5e,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1587661622.0,,[],{},,,True,,1587690120.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m very new to machine learning please go easy on me.&lt;/p&gt;

&lt;p&gt;So, I&amp;#39;m working on creating a regression model which predicts the concentration of the metal produced based on the dosages of the chemicals that are used for the process.&lt;/p&gt;

&lt;p&gt;My outcome should be in the form of equation as the operations team will have the targeted concentration and will solve for the equation that is produced by the model using excel solver to find out the optimal chemical dosage. &lt;/p&gt;

&lt;p&gt;I can use linear regression or polynomial regression and I&amp;#39;ve used them, but I want a better model which can predict more accurately. So, I went for decision trees and random forest, and got a way better model. But the problem now is, I want some kind of output out of the model, preferably in the form of an equation.&lt;/p&gt;

&lt;p&gt;What I was thinking was to use the predictions I get from the random forest as my input for a new polynomial regression and get a regression equation that fits the plot that fits the training set. But is this the right way to go? Will the model be back to the same accuracy or even worse?&lt;/p&gt;

&lt;p&gt;Or is there any other way to get any output out of the decision tree or random forest?&lt;/p&gt;

&lt;p&gt;P.S. I use KNIME for the analysis as I don&amp;#39;t have much coding knowledge and it doesn&amp;#39;t have any other regressions like lasso, ridge regression etc.&lt;/p&gt;

&lt;p&gt;Tl;dr: Need some kind of regression equation from decision trees or random forest. If not possible, atleast is there any way to get any kind of output out of random forest other than the prediction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6qe5e,True,,longdong93,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6qe5e/is_it_possible_to_get_any_output_other_than_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6qe5e/is_it_possible_to_get_any_output_other_than_the/,155203,1587661320.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

Lets say I have data for a Model for 10 Years. Each observation represents one hour for which I want to make a (binary) prediction.  
Like a good DS I initally setup my 3 datasets to be

* Training: 2011-2018
* Validation: 2019
* Test: 2020

As I see (and know) that older data is not as representative for future applications.   
Is it ""okay"" to train the same model but with iteratively less data (while keeping Validation untouched and using it for model evaluation)?   
Like this:

* T0: 2011-2018
* T1: 2012-2018
* ...

It would be an easy way to tune training datasets by iteratively removing the oldest data.

By trying this I've seen massive increases in model performance. on validation.   
In my binary classification model Accuracy increased by (rel.) 5-10% compared to the baseline model (which would be a massive for applications).

Am I falling into any kind of trap here or is this approach fine?   
Extreme case: What would then stop me from dropping always the oldest datapoint?

My gut says it is fine, but have not found any literature which brings up this example",t2_117n1y,False,,0,False,Iteratively Removing Training data?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6qdx9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587690099.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Lets say I have data for a Model for 10 Years. Each observation represents one hour for which I want to make a (binary) prediction.&lt;br/&gt;
Like a good DS I initally setup my 3 datasets to be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Training: 2011-2018&lt;/li&gt;
&lt;li&gt;Validation: 2019&lt;/li&gt;
&lt;li&gt;Test: 2020&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I see (and know) that older data is not as representative for future applications.&lt;br/&gt;
Is it &amp;quot;okay&amp;quot; to train the same model but with iteratively less data (while keeping Validation untouched and using it for model evaluation)?&lt;br/&gt;
Like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;T0: 2011-2018&lt;/li&gt;
&lt;li&gt;T1: 2012-2018&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be an easy way to tune training datasets by iteratively removing the oldest data.&lt;/p&gt;

&lt;p&gt;By trying this I&amp;#39;ve seen massive increases in model performance. on validation.&lt;br/&gt;
In my binary classification model Accuracy increased by (rel.) 5-10% compared to the baseline model (which would be a massive for applications).&lt;/p&gt;

&lt;p&gt;Am I falling into any kind of trap here or is this approach fine?&lt;br/&gt;
Extreme case: What would then stop me from dropping always the oldest datapoint?&lt;/p&gt;

&lt;p&gt;My gut says it is fine, but have not found any literature which brings up this example&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6qdx9,True,,simfrep,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6qdx9/iteratively_removing_training_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6qdx9/iteratively_removing_training_data/,155203,1587661299.0,0,,False,,,,
,learnmachinelearning,"I've been trying to get started on machine learning for about 2 years now, and I've always run into logistical or technical difficulties.

From that many tries I have learned that not only is my computer unable to install tensorflow or run any machine learning locally but it is so slow and the WiFi is so bad that it's basically impossible for me to code/design any model online.

However, I think that if I could code and try the model on my android phone I would at least partially succeed. The thing is, I've not yet found a tool to design/code for Android.

For more information, please comment here. If you found a solution, I would be very grateful.


EDIT 0: I've tried Google colab, and it is really fun to experiment with but a pain in the ass to code on with mi PC and WiFi capability.

EDIT 1: My computer crashed. R.I.P. old friend",t2_1nguskoc,False,,0,False,My computer is absolute s***,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6pye5,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,1587928403.0,,[],{},,,True,,1587688685.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been trying to get started on machine learning for about 2 years now, and I&amp;#39;ve always run into logistical or technical difficulties.&lt;/p&gt;

&lt;p&gt;From that many tries I have learned that not only is my computer unable to install tensorflow or run any machine learning locally but it is so slow and the WiFi is so bad that it&amp;#39;s basically impossible for me to code/design any model online.&lt;/p&gt;

&lt;p&gt;However, I think that if I could code and try the model on my android phone I would at least partially succeed. The thing is, I&amp;#39;ve not yet found a tool to design/code for Android.&lt;/p&gt;

&lt;p&gt;For more information, please comment here. If you found a solution, I would be very grateful.&lt;/p&gt;

&lt;p&gt;EDIT 0: I&amp;#39;ve tried Google colab, and it is really fun to experiment with but a pain in the ass to code on with mi PC and WiFi capability.&lt;/p&gt;

&lt;p&gt;EDIT 1: My computer crashed. R.I.P. old friend&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6pye5,True,,-Owlsoul-,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6pye5/my_computer_is_absolute_s/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6pye5/my_computer_is_absolute_s/,155203,1587659885.0,0,,False,,,,
,learnmachinelearning,"Deploying TensorFlow models on the cloud can be a hassle. In my new tutorial, youâ€™ll learn how to spawn an AWS EC2 instance and deploy the speech recognition system I built in previous videos on the cloud.

This video is the last installment of the ""Deep Learning (Audio) Application: From Design to Deployment"" series. In this series, youâ€™ll learn how to build a simple speech recognition system and deploy it on AWS, using Flask and Docker.

Hereâ€™s the video:

https://www.youtube.com/watch?v=ceNWWxjtG3U&amp;list=PL-wATfeyAMNpCRQkKgtOZU\_ykXc63oyzp&amp;index=8

Enjoy!",t2_12ahau,False,,0,False,Deploying TensorFlow model on Amazon AWS,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6p5qg,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1587686047.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Deploying TensorFlow models on the cloud can be a hassle. In my new tutorial, youâ€™ll learn how to spawn an AWS EC2 instance and deploy the speech recognition system I built in previous videos on the cloud.&lt;/p&gt;

&lt;p&gt;This video is the last installment of the &amp;quot;Deep Learning (Audio) Application: From Design to Deployment&amp;quot; series. In this series, youâ€™ll learn how to build a simple speech recognition system and deploy it on AWS, using Flask and Docker.&lt;/p&gt;

&lt;p&gt;Hereâ€™s the video:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=ceNWWxjtG3U&amp;amp;list=PL-wATfeyAMNpCRQkKgtOZU%5C_ykXc63oyzp&amp;amp;index=8""&gt;https://www.youtube.com/watch?v=ceNWWxjtG3U&amp;amp;list=PL-wATfeyAMNpCRQkKgtOZU\_ykXc63oyzp&amp;amp;index=8&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lMA_p8VxyDh0kD0YG8eXuAJX_VRFkqP-PoL5ed-thGw.jpg?auto=webp&amp;s=033b0ec5899f66a36b520294f8b739e81384d49b', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/lMA_p8VxyDh0kD0YG8eXuAJX_VRFkqP-PoL5ed-thGw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb1da5a84832abb82f690eb61a3f3e1700a56c59', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/lMA_p8VxyDh0kD0YG8eXuAJX_VRFkqP-PoL5ed-thGw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc19906daee70c9f1dc1c140e07a750d4b6bbb58', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/lMA_p8VxyDh0kD0YG8eXuAJX_VRFkqP-PoL5ed-thGw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec45d115f988537573408a32bb1dfbe713858d72', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'bMppycuCPe-ydkERzAOZIIpYHel3Zr6tCPryHVnNX6w'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g6p5qg,True,,diabulusInMusica,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6p5qg/deploying_tensorflow_model_on_amazon_aws/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6p5qg/deploying_tensorflow_model_on_amazon_aws/,155203,1587657247.0,0,,False,,,,
,learnmachinelearning,"Are there any courses for conducting EDA?

I understand that EDA is subjective to each problem.

What I am asking is a course for the understanding mathematical tools like univariate analysis etc. used for EDA. 
It is easy to plot a distplot (seaborn), but what does this plot signify?

A course which answers questions like these, anyone?",t2_48v57lad,False,,0,False,Exploratory Data Analysis,[],r/learnmachinelearning,False,6,,0,,False,t3_g6jqja,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587663524.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Are there any courses for conducting EDA?&lt;/p&gt;

&lt;p&gt;I understand that EDA is subjective to each problem.&lt;/p&gt;

&lt;p&gt;What I am asking is a course for the understanding mathematical tools like univariate analysis etc. used for EDA. 
It is easy to plot a distplot (seaborn), but what does this plot signify?&lt;/p&gt;

&lt;p&gt;A course which answers questions like these, anyone?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6jqja,True,,raghhuveer,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6jqja/exploratory_data_analysis/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6jqja/exploratory_data_analysis/,155203,1587634724.0,0,,False,,,,
,learnmachinelearning,"I'm wondering if `tf.repeat` duplicates the input tensor in memory or stores metadata about repetitions.

To be more precise, I have an `N x D1` tensor and another tensor of length `D2`. I want to concatenate the second tensor onto each of the `N` vectors in the first tensor, resulting in an `N x (D1 + D2)` output tensor. Will this output tensor have `O(N*(D1 + D2))` memory complexity or `O(N*D1 + D2)`?",t2_116uik,False,,0,False,Does Tensorflow create virtual repetitions of data with tf.repeat?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6od4g,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587683426.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m wondering if &lt;code&gt;tf.repeat&lt;/code&gt; duplicates the input tensor in memory or stores metadata about repetitions.&lt;/p&gt;

&lt;p&gt;To be more precise, I have an &lt;code&gt;N x D1&lt;/code&gt; tensor and another tensor of length &lt;code&gt;D2&lt;/code&gt;. I want to concatenate the second tensor onto each of the &lt;code&gt;N&lt;/code&gt; vectors in the first tensor, resulting in an &lt;code&gt;N x (D1 + D2)&lt;/code&gt; output tensor. Will this output tensor have &lt;code&gt;O(N*(D1 + D2))&lt;/code&gt; memory complexity or &lt;code&gt;O(N*D1 + D2)&lt;/code&gt;?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6od4g,True,,drcopus,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6od4g/does_tensorflow_create_virtual_repetitions_of/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6od4g/does_tensorflow_create_virtual_repetitions_of/,155203,1587654626.0,0,,False,,,,
,learnmachinelearning,,t2_64dirql8,False,,0,False,What are some popular apps and websites that use machine learning/artificial intelligence at its best?,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_g6nllb,False,dark,0.5,,public,0,0,{},70.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/tb03naEs8rGdofjQMWm6abLW6KEu21IbGe-gzqacZSk.jpg,False,,[],{},link,,False,,1587680841.0,text,6,,,text,quora.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ms7hVuollbKJlfzunw8BdNnEeriCMuSlKsvzQcKvY18.jpg?auto=webp&amp;s=e55eea63d2d531fa9bd945b5e7d5109e0ba46f9e', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': '_JTXTk-V3RiYSZcBZt7ySGg5VvbuVMj-43k53OlMKh8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6nllb,True,,prud444,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6nllb/what_are_some_popular_apps_and_websites_that_use/,all_ads,False,https://www.quora.com/What-are-some-popular-apps-and-websites-that-use-machine-learning-artificial-intelligence-at-its-best?ch=10&amp;share=3f7a54dd&amp;srid=uEYba,155203,1587652041.0,0,,False,,,,
,learnmachinelearning,,t2_5a24gggr,False,,0,False,Spam Detection using Machine Learning,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g6nli8,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Python Coding - Spam Detection using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ov8ABk4_3d4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g6nli8', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/mX-dh8SV1z5JQ1p_lweYN4Zz9Z1Mm8urC0daqoaIT1k.jpg,False,,[],{},rich:video,,False,,1587680834.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?auto=webp&amp;s=0cb217b9656054ae28c504e83b19c040a94842de', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=88402a600c454af2d6a3073161f1c8bf6e230856', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad2f947960745ad195a3d290717b0373b213309b', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/RR6_o7mchSPSAvdbjn_k9SzOp7LB4BDxMLb3p6x6rMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bf94cffc6c8e1f49ee1859d831dfcd090b58f18', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zqPG334zq6oYtPVMyqN_9nWHAB2c0p748pGyPevvtgY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6nli8,True,,leooister,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6nli8/spam_detection_using_machine_learning/,all_ads,False,https://youtu.be/Ov8ABk4_3d4,155203,1587652034.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Python Coding - Spam Detection using Machine Learning', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Ov8ABk4_3d4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vinsloev Academy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Ov8ABk4_3d4/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC-OKxBgjKLBGHbueyIOWptw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,How do we set parameters for sklearn.linear_model.SGDRegressor to make it perform Batch gradient descent in front of stochastic gradient descent? I want to solve a linear-regression problem using Batch gradient descent.,t2_51ftda0c,False,,0,False,Batch gradient descent in scikit-learn,[],r/learnmachinelearning,False,6,,0,,False,t3_g6ndou,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587680051.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do we set parameters for sklearn.linear_model.SGDRegressor to make it perform Batch gradient descent in front of stochastic gradient descent? I want to solve a linear-regression problem using Batch gradient descent.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6ndou,True,,magraul98,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6ndou/batch_gradient_descent_in_scikitlearn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6ndou/batch_gradient_descent_in_scikitlearn/,155203,1587651251.0,0,,False,,,,
,learnmachinelearning,,t2_eo99ifr,False,,0,False,How to destroy my childhood game with AI?,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g6msh6,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-qkuqB2Ig5I?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to destroy my childhood game with AI? (ç”¨äººå·¥æ™ºæ…§çŽ©å°æœ‹å‹ä¸‹æ¨“æ¢¯ï¼‰', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-qkuqB2Ig5I?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Mr D', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-qkuqB2Ig5I/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCgujhE8gXiPIYDQc7HwILVQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-qkuqB2Ig5I?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g6msh6', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/L_UNPO91M244ofcLlMJhSAQqcqg80Z6-YVQ7vIj8las.jpg,False,,[],{},rich:video,,False,,1587677916.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/StMrulp8WkzIBb1JoS60L3cB4RbS946SJ7t8PESetcc.jpg?auto=webp&amp;s=e83a7e340c788ff110831368c4b4a4c0d16e3c3a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/StMrulp8WkzIBb1JoS60L3cB4RbS946SJ7t8PESetcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d64f9ca20aca252485346ea07e4a9affdf7ca7f', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/StMrulp8WkzIBb1JoS60L3cB4RbS946SJ7t8PESetcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ba9767fa3586340343579ae112d47674f49a9e2', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/StMrulp8WkzIBb1JoS60L3cB4RbS946SJ7t8PESetcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bd9d292b58b183e99dee92834182c961f13d8e7', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'UF-T--gT2H__WZ9PmEv4YSOXb4RR0W9BQxqoKncDByM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6msh6,True,,davidyu3737,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6msh6/how_to_destroy_my_childhood_game_with_ai/,all_ads,False,https://youtu.be/-qkuqB2Ig5I,155203,1587649116.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How to destroy my childhood game with AI? (ç”¨äººå·¥æ™ºæ…§çŽ©å°æœ‹å‹ä¸‹æ¨“æ¢¯ï¼‰', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/-qkuqB2Ig5I?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Mr D', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/-qkuqB2Ig5I/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCgujhE8gXiPIYDQc7HwILVQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Iâ€™m about 3/4 of the way through the Ng ML course. Really enjoying the course, itâ€™s the right balance of theory and practical considerations and the lectures are easy to follow. The course is a little dated though. What is a good follow-on that would cover modern neural networks techniques while keeping the right balance of theory/practice?

As some background, I found the assignments in the ML to be pretty easy (I have a solid Linear Algebra foundation and a lot of practice with Matlab/Octave), so I would be game for more challenge. I also would like to work in Python since that seems to be where everything is headed. 

I know this question gets asked a lot (I searched and saw the Deeper Learning specialization and fast.ai as recommendations), but wanted to ask with the additional details Iâ€™ve mentioned here.",t2_519eoyf1,False,,0,False,What course to do after Andrew Ngâ€™s ML Coursera course?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6dfbj,False,dark,0.86,,public,9,0,{},,,False,[],,False,False,,{},Question,False,9,,False,self,False,,[],{},,,True,,1587632693.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m about 3/4 of the way through the Ng ML course. Really enjoying the course, itâ€™s the right balance of theory and practical considerations and the lectures are easy to follow. The course is a little dated though. What is a good follow-on that would cover modern neural networks techniques while keeping the right balance of theory/practice?&lt;/p&gt;

&lt;p&gt;As some background, I found the assignments in the ML to be pretty easy (I have a solid Linear Algebra foundation and a lot of practice with Matlab/Octave), so I would be game for more challenge. I also would like to work in Python since that seems to be where everything is headed. &lt;/p&gt;

&lt;p&gt;I know this question gets asked a lot (I searched and saw the Deeper Learning specialization and fast.ai as recommendations), but wanted to ask with the additional details Iâ€™ve mentioned here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6dfbj,True,,pqaw,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6dfbj/what_course_to_do_after_andrew_ngs_ml_coursera/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6dfbj/what_course_to_do_after_andrew_ngs_ml_coursera/,155203,1587603893.0,0,,False,,,,
,learnmachinelearning,,t2_47jpmh5m,False,,0,False,Beginners Learning Path for Machine Learning,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g6movt,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Yko6Nf1VE6cNxoh87DYGf3qXPy-l3xJVfn70GIDVm_I.jpg,False,,[],{},link,,False,,1587677534.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?auto=webp&amp;s=80c44568144de1fb84889ea10f5cd13a35b79732', 'width': 1200, 'height': 1599}, 'resolutions': [{'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b541b5637a72c2d8eea89cafdc40a7a6b2bfcf84', 'width': 108, 'height': 143}, {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25b1288dd2782ab105139d6c2b51a9a33d1e066d', 'width': 216, 'height': 287}, {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=96b1eaf8b80c6a2f4e5bc25bc023f62d620f7e30', 'width': 320, 'height': 426}, {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=36e78b8d6b736a4a4e1bcf4fe7031a6f54a7b439', 'width': 640, 'height': 852}, {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7627491eb8ebcf7076be55286e1c9eb0c5bc44c3', 'width': 960, 'height': 1279}, {'url': 'https://external-preview.redd.it/2Mb50WSK5GE0x_mPkcMH2OM1vglVt5kSE_mOcRQICxk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c120fc92e76826b6f50ed9a8974849e75600e77e', 'width': 1080, 'height': 1439}], 'variants': {}, 'id': 'LEhU8-EtaRXyhQ60tSrtDZId_jWivB-w5pPWub7VIXc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6movt,True,,ItisAhmad,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6movt/beginners_learning_path_for_machine_learning/,all_ads,False,https://towardsdatascience.com/beginners-learning-path-for-machine-learning-5a7fb90f751a,155203,1587648734.0,0,,False,,,,
,learnmachinelearning,"I wanted to explore machine learning techniques for music generation. I have a basic understanding of HMMs and LSTMs and a degree in computational sciences. 

- Where do I go from here? Are there some good python libraries which youâ€™d recommend?
- I am looking at putting together resources and learning material for myself and anyone whoâ€™d like to pursue this path. 
- Who are some pioneers worth following in Canada/ North America?
- What companies are invested in this today and doing groundbreaking work in this field?",t2_4kx19st7,False,,0,False,ML for music,[],r/learnmachinelearning,False,6,,0,,False,t3_g6h7u3,False,dark,0.72,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587649663.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wanted to explore machine learning techniques for music generation. I have a basic understanding of HMMs and LSTMs and a degree in computational sciences. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Where do I go from here? Are there some good python libraries which youâ€™d recommend?&lt;/li&gt;
&lt;li&gt;I am looking at putting together resources and learning material for myself and anyone whoâ€™d like to pursue this path. &lt;/li&gt;
&lt;li&gt;Who are some pioneers worth following in Canada/ North America?&lt;/li&gt;
&lt;li&gt;What companies are invested in this today and doing groundbreaking work in this field?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6h7u3,True,,__stats__,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6h7u3/ml_for_music/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6h7u3/ml_for_music/,155203,1587620863.0,0,,False,,,,
,learnmachinelearning,,t2_3a0uwyjx,False,,0,False,Will I ever need to solve quadratics by factoring in ML? Because if not I can skip if on my math course. Im not sure if this is ever needed in ML?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,83.0,False,t3_g6mhat,False,dark,0.5,,public,0,0,{},140.0,,False,[],,True,False,,{},Question,False,0,,True,https://b.thumbs.redditmedia.com/zDVdyN0vv5rAjRLSDEqav4Jt4olSu9uqC6UNJoKpPrg.jpg,False,,[],{},image,,False,,1587676723.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/s1gqzzzsiku41.png?auto=webp&amp;s=acad510de004feba95c0a71ff2fd0859acacec73', 'width': 816, 'height': 487}, 'resolutions': [{'url': 'https://preview.redd.it/s1gqzzzsiku41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a439784e19a2434f874b01a14e54c4032d6d918', 'width': 108, 'height': 64}, {'url': 'https://preview.redd.it/s1gqzzzsiku41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f43ecfa4d84395b93ce183692d8e7733bc12893', 'width': 216, 'height': 128}, {'url': 'https://preview.redd.it/s1gqzzzsiku41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8caf9cd5d77af723d36515b760261cff72e44d24', 'width': 320, 'height': 190}, {'url': 'https://preview.redd.it/s1gqzzzsiku41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=01c23ac797f54eb00f33f1e9964a3cc9bc1565d7', 'width': 640, 'height': 381}], 'variants': {}, 'id': 'lwhEplpq0k2fTtQMkEQyUDLCmMi2REfDY-Jri4Vcjds'}], 'enabled': True}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6mhat,True,,TrackLabs,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6mhat/will_i_ever_need_to_solve_quadratics_by_factoring/,all_ads,False,https://i.redd.it/s1gqzzzsiku41.png,155203,1587647923.0,0,,False,,,,
,learnmachinelearning,"Hello,

I am learning about neural networks and have been spending quite some time with GANs. 

I am trying to re-discover the sample of use of GAN to interactively modify the product on the photo. It was in the same list with Neural Photo Editor, but I lost the link. The example showed how someone could drag the top of a shoe and it would extend into a boot. Does anyone know what was the name of that project? I figured I will find it again and a week later - still searching.

Thank you!",t2_82g0m,False,,0,False,User Interactive GAN similar to Neural Photo Editor but cannot find the link anymore,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6i29o,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,True,self,False,,[],{},,,True,,1587654250.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am learning about neural networks and have been spending quite some time with GANs. &lt;/p&gt;

&lt;p&gt;I am trying to re-discover the sample of use of GAN to interactively modify the product on the photo. It was in the same list with Neural Photo Editor, but I lost the link. The example showed how someone could drag the top of a shoe and it would extend into a boot. Does anyone know what was the name of that project? I figured I will find it again and a week later - still searching.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6i29o,True,,StarAvenger,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6i29o/user_interactive_gan_similar_to_neural_photo/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6i29o/user_interactive_gan_similar_to_neural_photo/,155203,1587625450.0,0,,False,,,,
,learnmachinelearning,"Free object detection with bounding boxes, suitable for training mobile models. (Speed limits, traffic lights, pedestrian crossings, stop signs). It has 877 images in it. You can try to train a model in the MakeML app in a few clicks.  
[https://makeml.app/datasets/road-signs](https://makeml.app/datasets/road-signs)",t2_1bodl8op,False,,0,False,Free Road Signs Object Detection Bounding Boxes Dataset.,[],r/learnmachinelearning,False,6,,0,,False,t3_g6lf4a,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587672240.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Free object detection with bounding boxes, suitable for training mobile models. (Speed limits, traffic lights, pedestrian crossings, stop signs). It has 877 images in it. You can try to train a model in the MakeML app in a few clicks.&lt;br/&gt;
&lt;a href=""https://makeml.app/datasets/road-signs""&gt;https://makeml.app/datasets/road-signs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6lf4a,True,,lekorotkov,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6lf4a/free_road_signs_object_detection_bounding_boxes/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6lf4a/free_road_signs_object_detection_bounding_boxes/,155203,1587643440.0,0,,False,,,,
,learnmachinelearning,,t2_3d8dg3uh,False,,0,False,Using NeuroEvolution - AI mastering Flappy Bird,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_g5u61t,False,light,0.97,,public,363,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/p2gpz4qqkau41/DASH_720?source=fallback', 'height': 674, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/p2gpz4qqkau41/DASH_96', 'dash_url': 'https://v.redd.it/p2gpz4qqkau41/DASHPlaylist.mpd', 'duration': 41, 'hls_url': 'https://v.redd.it/p2gpz4qqkau41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},Project,False,363,,False,https://b.thumbs.redditmedia.com/oRPvzXnBkKBuw8wAt_vxwMlb__dGosdNCLIo4xMTNIk.jpg,False,,[],{},hosted:video,,False,,1587556425.0,richtext,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?format=pjpg&amp;auto=webp&amp;s=c5a74c56cd9c8c0100b2f2533ebae1ec194b53d1', 'width': 1900, 'height': 1000}, 'resolutions': [{'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6417feed46a454823a2cf41ecdc1b95213ef5dec', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5da210effa782aa829995979fc32ad4b8dfa7ee3', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=338c45acd5ed4866702709e52b86e53f973a15ac', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d0f6e1716c3c431d763004c40bf0f8f5e6cc51ad', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4c953de7e5a28d216e671455800a108406eb1338', 'width': 960, 'height': 505}, {'url': 'https://external-preview.redd.it/UPvVh0z04H2CS_sSLYmILhA6dFGUYxUV9RdSfUNCOac.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2b782a16bb03048ff84092c5d513ba024cda8fff', 'width': 1080, 'height': 568}], 'variants': {}, 'id': 'UwkfUk0YhE7vtZOLHBbLnBZGnZs9uXYShM49r5zdLpI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g5u61t,True,,oFlamingo,,19,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5u61t/using_neuroevolution_ai_mastering_flappy_bird/,all_ads,False,https://v.redd.it/p2gpz4qqkau41,155203,1587527625.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/p2gpz4qqkau41/DASH_720?source=fallback', 'height': 674, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/p2gpz4qqkau41/DASH_96', 'dash_url': 'https://v.redd.it/p2gpz4qqkau41/DASHPlaylist.mpd', 'duration': 41, 'hls_url': 'https://v.redd.it/p2gpz4qqkau41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"I am looking for methods in NLP that would allow me to train a model to classify each word in a sentence based on the context of the sentence. Something like

&amp;#x200B;

* model(\[wordA, wordB\]) = (class1, class2)

&amp;#x200B;

Moreover, I need the model to be independent of the word order, so

&amp;#x200B;

* model(\[wordB, wordA\]) = (class2, class1)

&amp;#x200B;

and lastly, respect the context - some words can have different classes depending on their context:

&amp;#x200B;

* model(\[wordA, wordC\]) = (class3, class4) (so wordA is now class3, instead of class1. Akin to the word 'orange' having different meanings in different contexts).

&amp;#x200B;

Any ideas? I was thinking some RNN approaches might be required, because I can have an arbitrarily long input...",t2_59kgejji,False,,0,False,How to classify each word in a sentence based on context?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6l81p,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587671275.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking for methods in NLP that would allow me to train a model to classify each word in a sentence based on the context of the sentence. Something like&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;model([wordA, wordB]) = (class1, class2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Moreover, I need the model to be independent of the word order, so&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;model([wordB, wordA]) = (class2, class1)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;and lastly, respect the context - some words can have different classes depending on their context:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;model([wordA, wordC]) = (class3, class4) (so wordA is now class3, instead of class1. Akin to the word &amp;#39;orange&amp;#39; having different meanings in different contexts).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any ideas? I was thinking some RNN approaches might be required, because I can have an arbitrarily long input...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6l81p,True,,selling_crap_bike,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6l81p/how_to_classify_each_word_in_a_sentence_based_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6l81p/how_to_classify_each_word_in_a_sentence_based_on/,155203,1587642475.0,0,,False,,,,
,learnmachinelearning,"I ran a ttest on all of my metrics, testing the probability that differences between groups are just caused by chance.

One of the ones with the best results keeps getting rejected by RFECV (Recursive feature elimination with cross-validation).

What gives?",t2_3sqf72hz,False,,0,False,"Metric w Awesome p-value, but Recursive Feature Elimination doesn't want it??",[],r/learnmachinelearning,False,6,,0,,False,t3_g6kux5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587669539.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I ran a ttest on all of my metrics, testing the probability that differences between groups are just caused by chance.&lt;/p&gt;

&lt;p&gt;One of the ones with the best results keeps getting rejected by RFECV (Recursive feature elimination with cross-validation).&lt;/p&gt;

&lt;p&gt;What gives?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6kux5,True,,fransquaoi,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6kux5/metric_w_awesome_pvalue_but_recursive_feature/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6kux5/metric_w_awesome_pvalue_but_recursive_feature/,155203,1587640739.0,0,,False,,,,
,learnmachinelearning,"""An Introduction to Statistical Learning with Application in R"" is one of the best (if not the best) introductory books for people who are interested in Machine Learning. However, as the name suggests all the practical exercises are done in R.  


As python is a language of choice for the majority of newcomers, and as I'm myself on a journey learning machine learning I decided to reproduce all the lab works found in this book and create the application of statistical learning in Python. I've uploaded the first three chapters out of seven:

  
\- Chapter 3 Linear Regression  
\- Chapter 4 Classification  
\- Chapter 5 Resampling Methods  


The rest will be uploaded as soon as I give them a presentable form. 

 [https://github.com/bexxmodd/ITSL-with-python/blob/master/README.md](https://github.com/bexxmodd/ITSL-with-python/blob/master/README.md) 

&amp;#x200B;

Any feedback, comments are welcome, and appreciated!",t2_4c0gz9q3,False,,0,False,"""An Introduction to Statistical Learning with Application in &lt;&lt;Python&gt;&gt;",[],r/learnmachinelearning,False,6,,0,,False,t3_g68lwn,False,dark,1.0,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},self,,True,,1587615706.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;quot;An Introduction to Statistical Learning with Application in R&amp;quot; is one of the best (if not the best) introductory books for people who are interested in Machine Learning. However, as the name suggests all the practical exercises are done in R.  &lt;/p&gt;

&lt;p&gt;As python is a language of choice for the majority of newcomers, and as I&amp;#39;m myself on a journey learning machine learning I decided to reproduce all the lab works found in this book and create the application of statistical learning in Python. I&amp;#39;ve uploaded the first three chapters out of seven:&lt;/p&gt;

&lt;p&gt;- Chapter 3 Linear Regression&lt;br/&gt;
- Chapter 4 Classification&lt;br/&gt;
- Chapter 5 Resampling Methods  &lt;/p&gt;

&lt;p&gt;The rest will be uploaded as soon as I give them a presentable form. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://github.com/bexxmodd/ITSL-with-python/blob/master/README.md""&gt;https://github.com/bexxmodd/ITSL-with-python/blob/master/README.md&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Any feedback, comments are welcome, and appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/MA34Vqc9B-RY5XrBCvCM1h5hoxYHVe_DdUYxizvLKHE.jpg?auto=webp&amp;s=2898b081fb576e97310802735fa85b7e90b29028', 'width': 750, 'height': 375}, 'resolutions': [{'url': 'https://external-preview.redd.it/MA34Vqc9B-RY5XrBCvCM1h5hoxYHVe_DdUYxizvLKHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04e634224e6df611c97a88d70648f59a6baa1095', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/MA34Vqc9B-RY5XrBCvCM1h5hoxYHVe_DdUYxizvLKHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5289321b9d5fb76789aee7bbc6dca82da3d4fc5', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/MA34Vqc9B-RY5XrBCvCM1h5hoxYHVe_DdUYxizvLKHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5932ddffd1542fa0b381f6cdb66f55edcb5ceca', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/MA34Vqc9B-RY5XrBCvCM1h5hoxYHVe_DdUYxizvLKHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=45db0bd0e3d4096b14aca45237a7e1a431c07d3e', 'width': 640, 'height': 320}], 'variants': {}, 'id': 'hN-UJDab6bP-8hIO8ItzhUUZeBr4yPCTj3qn87DvIH8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g68lwn,True,,Hellr0x,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g68lwn/an_introduction_to_statistical_learning_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g68lwn/an_introduction_to_statistical_learning_with/,155203,1587586906.0,1,,False,,,,
,learnmachinelearning,"I'm a uni student doing a project on ASR. Does anyone know what ASR architecture is used for the acoustic model in IBM Watson? I found [this link](https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-science) and it says:
&gt; For acoustic modeling, IBM uses a fairly compact model to accommodate the resource limitations of the cloud. To train this compact model, IBM uses ""teacher-student training / knowledge distillation."" **Large and strong neural networks such as Long Short-Term Memory (LSTM), VGG, and the Residual Network (ResNet) are first trained**. The output of these networks is then used as teacher signals to train a compact model for actual deployment.

And then there's a reference to a paper about teacher-student training. I would like to know what kind of architecture they use for their ""teacher"" network. Anyone know where I can find this information?",t2_6g3z9,False,,0,False,IBM Watson STT architecture,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6kgnc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587667498.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a uni student doing a project on ASR. Does anyone know what ASR architecture is used for the acoustic model in IBM Watson? I found &lt;a href=""https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-science""&gt;this link&lt;/a&gt; and it says:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For acoustic modeling, IBM uses a fairly compact model to accommodate the resource limitations of the cloud. To train this compact model, IBM uses &amp;quot;teacher-student training / knowledge distillation.&amp;quot; &lt;strong&gt;Large and strong neural networks such as Long Short-Term Memory (LSTM), VGG, and the Residual Network (ResNet) are first trained&lt;/strong&gt;. The output of these networks is then used as teacher signals to train a compact model for actual deployment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And then there&amp;#39;s a reference to a paper about teacher-student training. I would like to know what kind of architecture they use for their &amp;quot;teacher&amp;quot; network. Anyone know where I can find this information?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g6kgnc,True,,Asierro,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6kgnc/ibm_watson_stt_architecture/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6kgnc/ibm_watson_stt_architecture/,155203,1587638698.0,0,,False,,,,
,learnmachinelearning,"Hi 
I just started learning about cyclic gan through online forum, I am trying to train cyclic gan to convert face to anime ,while I was trining the model I noticed that my generator loss is quite high(3000-4000) and it is not converging while the discriminator loss is low(0.02) can u please help me out here.?can u please share some material if possible on fine tuning the cyclegan? Help appreciated",t2_68jranov,False,,0,False,Difficulty in training cyclic gan,[],r/learnmachinelearning,False,6,,0,,False,t3_g6ka05,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587666512.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi 
I just started learning about cyclic gan through online forum, I am trying to train cyclic gan to convert face to anime ,while I was trining the model I noticed that my generator loss is quite high(3000-4000) and it is not converging while the discriminator loss is low(0.02) can u please help me out here.?can u please share some material if possible on fine tuning the cyclegan? Help appreciated&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6ka05,True,,kazuto9898,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6ka05/difficulty_in_training_cyclic_gan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6ka05/difficulty_in_training_cyclic_gan/,155203,1587637712.0,0,,False,,,,
,learnmachinelearning,,t2_o0pbd,False,,0,False,Worked example for diagnosing breast cancer using the famous Wisconsin data set.,[],r/learnmachinelearning,False,6,,0,,False,t3_g6k5r2,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1587665865.0,text,6,,,text,neuraldesigner.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6k5r2,True,,datapablo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6k5r2/worked_example_for_diagnosing_breast_cancer_using/,all_ads,False,https://www.neuraldesigner.com/learning/examples/breast-cancer-diagnosis,155203,1587637065.0,0,,False,,,,
,learnmachinelearning,,t2_49t2kvpy,False,,0,False,Making Of: A Free API For COVID-19 Data,[],r/learnmachinelearning,False,6,,0,56.0,False,t3_g6k3qw,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/eZo_cT-kB0tw2Vt5jQN7ypXNDT1rRJvRpiKf6kCWU_4.jpg,False,,[],{},link,,False,,1587665559.0,text,6,,,text,statworx.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?auto=webp&amp;s=80d9823c532814726d80f93261bfb3f4bcfc2ac9', 'width': 1980, 'height': 803}, 'resolutions': [{'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2d096d3fe07c33ce8cc4095c7a1c64c6ad577a4', 'width': 108, 'height': 43}, {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b922f364c815f71180fa51a9b2b867fd1df5505f', 'width': 216, 'height': 87}, {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f4620b8367dbded60f62e68b458c6b06f1c6b49', 'width': 320, 'height': 129}, {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=812d372043f259fbff44543adb24301ded37daca', 'width': 640, 'height': 259}, {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=10cc77ba0ea59597f86a2e7501a21b254d8dd373', 'width': 960, 'height': 389}, {'url': 'https://external-preview.redd.it/cM80uDu_t_ZXTZuvJTMrQhZGZdMfXjzLJrmSQNDJcDw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f38aab61cd446291b75b04c4c3bd4360cfb4c73', 'width': 1080, 'height': 438}], 'variants': {}, 'id': 'K22TabLlQ9IB76dqr3ahRbEAvUXlWlIPFaRgOVzyBpQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6k3qw,True,,datataaa,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6k3qw/making_of_a_free_api_for_covid19_data/,all_ads,False,https://www.statworx.com/de/blog/making-of-a-free-api-for-covid-19-data/,155203,1587636759.0,0,,False,,,,
,learnmachinelearning,,t2_2nvdpdl,False,,0,False,Making custom subclasses of LinearOperator,[],r/learnmachinelearning,False,6,,0,,False,t3_g6jv03,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1587664224.0,text,6,,,text,self.tensorflow,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6jv03,True,,curtlytalks,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6jv03/making_custom_subclasses_of_linearoperator/,all_ads,False,/r/tensorflow/comments/g6jukk/making_custom_subclasses_of_linearoperator/,155203,1587635424.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'tensorflow', 'selftext': ""Hi all, \n\nI've been working on a physics peoject which uses SGD to find global minima of a potential function(a landscape, essentially). So far, the limited code works okay, but i want to extend and systematize it. I have a bunch of complex variables(matrices) which i'd like to define as classes, or rather, as subclasses of the tf.linalg.LinearOperator, so that I inherit the methods(adjoint, inverse, is\\_diagonal, etc. ) while also retaining the high-performance aspects of the library. \n\nProblem is, I am not that well versed in writing subclasses, and can't make sense of the docs of the other subclasses. Could someone point me to a minimal implementation of it? Any kind of advice would be appreciated. Also let me know if it is pointless, and i can achieve all that by just simply defining a matrix object in tf."", 'author_fullname': 't2_2nvdpdl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Making custom subclasses of LinearOperator', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/tensorflow', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g6jukk', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Question', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587664159.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.tensorflow', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working on a physics peoject which uses SGD to find global minima of a potential function(a landscape, essentially). So far, the limited code works okay, but i want to extend and systematize it. I have a bunch of complex variables(matrices) which i&amp;#39;d like to define as classes, or rather, as subclasses of the tf.linalg.LinearOperator, so that I inherit the methods(adjoint, inverse, is_diagonal, etc. ) while also retaining the high-performance aspects of the library. &lt;/p&gt;\n\n&lt;p&gt;Problem is, I am not that well versed in writing subclasses, and can&amp;#39;t make sense of the docs of the other subclasses. Could someone point me to a minimal implementation of it? Any kind of advice would be appreciated. Also let me know if it is pointless, and i can achieve all that by just simply defining a matrix object in tf.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '187eeff2-afc0-11e7-822b-0e48d05df30a', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3alkk', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g6jukk', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'curtlytalks', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/tensorflow/comments/g6jukk/making_custom_subclasses_of_linearoperator/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/tensorflow/comments/g6jukk/making_custom_subclasses_of_linearoperator/', 'subreddit_subscribers': 17127, 'created_utc': 1587635359.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_g6jukk,,
,learnmachinelearning,,t2_64dirql8,False,,0,False,Are machine learning jobs overhyped?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_g6jpw3,False,light,0.4,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/kjD3FTm1V8IEcMBV0DWuURVgESKht5Wqw_qCrVTxVzk.jpg,False,,[],{},link,,False,,1587663425.0,richtext,6,,,text,quora.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jbW1dRvBv8Db-WUNlpJLtJhcV2kohiiVyb7VKcQrleA.jpg?auto=webp&amp;s=d606dc7a6827e08c8ee68530b698bd428559c1b7', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/jbW1dRvBv8Db-WUNlpJLtJhcV2kohiiVyb7VKcQrleA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=caabcdafc6c3bd463c02ce7afd4001f69258d169', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/jbW1dRvBv8Db-WUNlpJLtJhcV2kohiiVyb7VKcQrleA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6abef29c4c2b38e165d49db8b51b5147d75fdc26', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/jbW1dRvBv8Db-WUNlpJLtJhcV2kohiiVyb7VKcQrleA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce12102ad9b2a373fb26ec211c736f0b55578b61', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'y-zEjHNJG9qaEH1-dtszSotG60nDlfzwXuCnp5i9kcU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g6jpw3,True,,prud444,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6jpw3/are_machine_learning_jobs_overhyped/,all_ads,False,https://www.quora.com/Are-machine-learning-jobs-overhyped?ch=10&amp;share=0ab99b3a&amp;srid=uEYba,155203,1587634625.0,0,,False,,,,
,learnmachinelearning,,t2_2wsvqwhg,False,,0,False,"NVIDIA Open Sources MONAI, An AI Framework For Medical Imaging (Github link included)",[],r/learnmachinelearning,False,6,,0,63.0,False,t3_g695su,False,dark,1.0,,public,7,0,{},140.0,,False,[],,False,False,,{},,False,7,,False,https://b.thumbs.redditmedia.com/K7CxvadopmLzVeoNfOm6sWbri7Kg7H9eT5RWtImnsOo.jpg,False,,[],{},link,,False,,1587617498.0,text,6,,,text,marktechpost.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?auto=webp&amp;s=ac6754d1368f6db5fcf58eae85a29f8efd459b0d', 'width': 1324, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8af7b6c6da60f2070b21ad183ea8025422549c15', 'width': 108, 'height': 48}, {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7363e2352d72caf5e6d4cce798e3168d7210774e', 'width': 216, 'height': 97}, {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c6872d2c329c8159b737a5542bef64f8e863607', 'width': 320, 'height': 145}, {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ef2066fb9c874f1c0b05c580186130e8e26271d', 'width': 640, 'height': 290}, {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=99d424d9efd7ff658abd24707c7375af3060d725', 'width': 960, 'height': 435}, {'url': 'https://external-preview.redd.it/w-993m7SEy5P0KkfBV-tLyu5fN-8AjMVENLLm0h6Pcc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37f37a9cf804a887084360e9968f0f6f4b4e1415', 'width': 1080, 'height': 489}], 'variants': {}, 'id': 'YvTnyKjKEFGgoVLTKw03F-VKUrDWUsQ37GD_qdja81E'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g695su,True,,ai-lover,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g695su/nvidia_open_sources_monai_an_ai_framework_for/,all_ads,False,https://www.marktechpost.com/2020/04/22/nvidia-open-sources-monai-an-ai-framework-for-medical-imaging,155203,1587588698.0,0,,False,,,,
,learnmachinelearning,"I've really liked their flutter and web development but noticed the number of student enrolled in this course is much less than those two. Is it just because it is recent or are there much better courses available on this topic?

would love a review on this course especially compares to other well known courses like andrew's on course era and A-Z of machine learning by kirill on udemy.",t2_denva,False,,0,False,Review of Machine learning course by app brewery/angela yu?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6hh7q,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1587651082.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve really liked their flutter and web development but noticed the number of student enrolled in this course is much less than those two. Is it just because it is recent or are there much better courses available on this topic?&lt;/p&gt;

&lt;p&gt;would love a review on this course especially compares to other well known courses like andrew&amp;#39;s on course era and A-Z of machine learning by kirill on udemy.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g6hh7q,True,,godevil99,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6hh7q/review_of_machine_learning_course_by_app/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6hh7q/review_of_machine_learning_course_by_app/,155203,1587622282.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Background Matting: The World is Your Green Screen,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g63vws,False,light,0.81,,public,6,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/JE-7OcNrPao?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Background Matting: The World is Your Green Screen', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/JE-7OcNrPao?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/JE-7OcNrPao/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/JE-7OcNrPao?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g63vws', 'height': 338}",Discussion,False,6,,False,https://b.thumbs.redditmedia.com/l9O-50QmJ_mlK6Gz2ZcFTZbB6v5Ss3LUP3FUGc7q-xk.jpg,False,,[],{},rich:video,,False,,1587600445.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/FdCAb2CjHN4lpqxORgPt3p0RnGkqlD-1YH9L2ilRqpc.jpg?auto=webp&amp;s=ce3c7e5ccb04a2e2b56121ce9c0c816c28e02520', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/FdCAb2CjHN4lpqxORgPt3p0RnGkqlD-1YH9L2ilRqpc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=345817950011396c2717fc03b4421c517fdecc0c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/FdCAb2CjHN4lpqxORgPt3p0RnGkqlD-1YH9L2ilRqpc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8532655925c5f957c346ab77df70009a421854a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/FdCAb2CjHN4lpqxORgPt3p0RnGkqlD-1YH9L2ilRqpc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=761b76a272a5b48978302ae232836ea2e1c686e7', 'width': 320, 'height': 240}], 'variants': {}, 'id': '1VGzeL9RqfxFMdMb93Rl36NHxNmojB1Th5EV6ZytBWQ'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g63vws,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g63vws/background_matting_the_world_is_your_green_screen/,all_ads,False,https://youtu.be/JE-7OcNrPao,155203,1587571645.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Background Matting: The World is Your Green Screen', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/JE-7OcNrPao?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/JE-7OcNrPao/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I have a data set which let's say comprises of of values of only three kinds. Its either a,b or c. The data set would look like \[a,a,b,a,c,c,c,b,a,b....\]. Each of the data point is collect at a fixed time interval and we also have historic data to train the algorithm on. How do I determine that a particular value in the set is an outlier? The outlier in this case can be a situation when the data point should have been 'c' but has been recorded as an 'a'.

NOTE: I have tried to use MAD and DBSCAN. Both of them do not feel like the right approach to this problem. I also though about regression but am not sure how that would work, and hence haven't tried on implementing it.",t2_3pozhw8f,False,,0,False,How to detect outliers in a data set that only contains particular discrete values?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g672rf,False,light,1.0,,public,5,0,{},,,False,[],,False,False,,{},HELP,False,5,,False,self,False,,[],{},,,True,,1587610748.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a data set which let&amp;#39;s say comprises of of values of only three kinds. Its either a,b or c. The data set would look like [a,a,b,a,c,c,c,b,a,b....]. Each of the data point is collect at a fixed time interval and we also have historic data to train the algorithm on. How do I determine that a particular value in the set is an outlier? The outlier in this case can be a situation when the data point should have been &amp;#39;c&amp;#39; but has been recorded as an &amp;#39;a&amp;#39;.&lt;/p&gt;

&lt;p&gt;NOTE: I have tried to use MAD and DBSCAN. Both of them do not feel like the right approach to this problem. I also though about regression but am not sure how that would work, and hence haven&amp;#39;t tried on implementing it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g672rf,True,,saiyan_prince1998,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g672rf/how_to_detect_outliers_in_a_data_set_that_only/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g672rf/how_to_detect_outliers_in_a_data_set_that_only/,155203,1587581948.0,0,,False,,,,
,learnmachinelearning,"I've seen this mentioned a few places, regarding some feature engineering, preprocessing, and/or feature selection steps needing to go inside/outside the training loop.

my naive intuition is:

\&gt;any change to a sample, that is derived from the values of other samples in the training set, needs to be done 'inside the loop'.  This prevents samples that may not find their way into into a given iteration of training (e.g. in cross validation splits) from influencing the outcomes of other samples that do.

I ask because, by my definition above, 'scaling of numeric values' would be an 'inside the loop' thing, as it is typically done across all values in a numerical column.  But in most tutorials I see it done as a true pre-processing step (outside the loop).

So, does scaling values go inside the loop?  Is my definition above somewhat correct?  What, generally, goes inside/outside the loop as a matter of best practice?",t2_702gf,False,,0,False,what goes 'inside the training loop' and what goes 'outside the training loop'?,[],r/learnmachinelearning,False,6,,0,,False,t3_g6ahmg,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587621892.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve seen this mentioned a few places, regarding some feature engineering, preprocessing, and/or feature selection steps needing to go inside/outside the training loop.&lt;/p&gt;

&lt;p&gt;my naive intuition is:&lt;/p&gt;

&lt;p&gt;&amp;gt;any change to a sample, that is derived from the values of other samples in the training set, needs to be done &amp;#39;inside the loop&amp;#39;.  This prevents samples that may not find their way into into a given iteration of training (e.g. in cross validation splits) from influencing the outcomes of other samples that do.&lt;/p&gt;

&lt;p&gt;I ask because, by my definition above, &amp;#39;scaling of numeric values&amp;#39; would be an &amp;#39;inside the loop&amp;#39; thing, as it is typically done across all values in a numerical column.  But in most tutorials I see it done as a true pre-processing step (outside the loop).&lt;/p&gt;

&lt;p&gt;So, does scaling values go inside the loop?  Is my definition above somewhat correct?  What, generally, goes inside/outside the loop as a matter of best practice?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6ahmg,True,,ezeeetm,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6ahmg/what_goes_inside_the_training_loop_and_what_goes/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6ahmg/what_goes_inside_the_training_loop_and_what_goes/,155203,1587593092.0,0,,False,,,,
,learnmachinelearning,"[https://savannah.gnu.org/forum/forum.php?forum\_id=9721](https://savannah.gnu.org/forum/forum.php?forum_id=9721)

GNU  Parallel has been GNU for 10 years. It is useful for training models  and doing other tasks where you have to run the same command with  different arguments.",t2_3wb5n,False,,0,False,[P] 10 years anniversary release of GNU Parallel: GNU Parallel 20200422 ('10years') released [stable],[],r/learnmachinelearning,False,6,,0,,False,t3_g6a663,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1587620827.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://savannah.gnu.org/forum/forum.php?forum_id=9721""&gt;https://savannah.gnu.org/forum/forum.php?forum_id=9721&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GNU  Parallel has been GNU for 10 years. It is useful for training models  and doing other tasks where you have to run the same command with  different arguments.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/TF3skvZkfpFrbyu8c7kvQpB-k_7oDj7hXgvEf7WujQE.jpg?auto=webp&amp;s=b46426ed8a9c76e384ee601cc89815f02026ec18', 'width': 144, 'height': 125}, 'resolutions': [{'url': 'https://external-preview.redd.it/TF3skvZkfpFrbyu8c7kvQpB-k_7oDj7hXgvEf7WujQE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=856969ffa44a8aa644f9e753c97c48fd2d5a1b82', 'width': 108, 'height': 93}], 'variants': {}, 'id': 'FfclgdkICQZXLed9KalRP8XnKZbwBIWqliA2iRRpYEc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6a663,True,,OleTange,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6a663/p_10_years_anniversary_release_of_gnu_parallel/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6a663/p_10_years_anniversary_release_of_gnu_parallel/,155203,1587592027.0,0,,False,,,,
,learnmachinelearning,,t2_4z0x3ub0,False,,0,False,"We can only see a short distance ahead, but we can see plenty there that needs to be done. - Alan Turing",[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g5g7b8,False,dark,0.93,,public,518,0,{},140.0,,False,[],,True,False,,{},,False,518,,False,https://b.thumbs.redditmedia.com/2mf1TaiFVFMC1KBPnM9eqlX5HUerp-C8BHVrA_HzVaI.jpg,False,,[],{},image,,False,,1587508098.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/oehtezegl6u41.jpg?auto=webp&amp;s=8859d62547792448852fdc610325e79c219d95bd', 'width': 800, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/oehtezegl6u41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1713ed368865459b1982b78788cae9d6422b80f', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/oehtezegl6u41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7b8cc63930c3383b6f6e6c6a4b06206064638a2', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/oehtezegl6u41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=181f6b3895e3584f8ac2145bccc2c37d3e62dcc6', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/oehtezegl6u41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d7e7c05315ff264eb96eeaa3fdf9b9362d41d87', 'width': 640, 'height': 640}], 'variants': {}, 'id': '3qaGHDVI5UIuFG_8Vj92H4iDaIfUhfcppkRjtjzeo20'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5g7b8,True,,Cosmic_Ishan,,51,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5g7b8/we_can_only_see_a_short_distance_ahead_but_we_can/,all_ads,False,https://i.redd.it/oehtezegl6u41.jpg,155203,1587479298.0,1,,False,,,,
,learnmachinelearning," I'm   building a machine learning application.I try to optimize the   computitional power by breaking a for-loop when iterating over facts   inside the call function, but it returns an error saying:

&gt;OperatorNotAllowedInGraphError:   using a \`tf.Tensor\` as a Python \`bool\` is not allowed: AutoGraph did   not convert this function. Try decorating it directly with [u/tf](https://www.reddit.com/u/tf/).function.  
(I've tried adding a tf function the call method

Anyone know how to break a for-loop inside the call function in a custom tensorflow model?",t2_4cohyf78,False,,0,False,How to break a loop inside a custom made Tensorflow model (using Keras),[],r/learnmachinelearning,False,6,,0,,False,t3_g68of3,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587615928.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m   building a machine learning application.I try to optimize the   computitional power by breaking a for-loop when iterating over facts   inside the call function, but it returns an error saying:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;OperatorNotAllowedInGraphError:   using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did   not convert this function. Try decorating it directly with &lt;a href=""https://www.reddit.com/u/tf/""&gt;u/tf&lt;/a&gt;.function.&lt;br/&gt;
(I&amp;#39;ve tried adding a tf function the call method&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Anyone know how to break a for-loop inside the call function in a custom tensorflow model?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g68of3,True,,mariusjohan,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g68of3/how_to_break_a_loop_inside_a_custom_made/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g68of3/how_to_break_a_loop_inside_a_custom_made/,155203,1587587128.0,0,,False,,,,
,learnmachinelearning,"I have a idea for a project in AI, and I want to know how can I build such kind of AI models. I have good understanding of ML, and some beginner level understanding of Deep learning. What should I learn in order to build such projects?

&amp;#x200B;

I want to compare 2 audio files, of musical instruments being recorded. I want my model to tell, what differences are there in these 2 music/songs. Differences like in reverb, and delay, instruments played in wrong key, improper mixing/mastering, different waveforms in oscillators, improper automation, wrong ADSR values, etc.

One of these file will be upload by me, and other will be uploaded by the user/student. And then, my AI model will tell the student what should be improved in his/her music to be able to sound like mine.",t2_ekf5huv,False,,0,False,"What AI model should I build to find out how similar 2 audio files of musical instruments like piano, guitar and synthesizers are?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g60j5r,False,dark,0.83,,public,8,0,{},,,False,[],,False,False,,{},Question,False,8,,False,self,False,,[],{},,,True,,1587588644.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a idea for a project in AI, and I want to know how can I build such kind of AI models. I have good understanding of ML, and some beginner level understanding of Deep learning. What should I learn in order to build such projects?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I want to compare 2 audio files, of musical instruments being recorded. I want my model to tell, what differences are there in these 2 music/songs. Differences like in reverb, and delay, instruments played in wrong key, improper mixing/mastering, different waveforms in oscillators, improper automation, wrong ADSR values, etc.&lt;/p&gt;

&lt;p&gt;One of these file will be upload by me, and other will be uploaded by the user/student. And then, my AI model will tell the student what should be improved in his/her music to be able to sound like mine.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g60j5r,True,,Xenymus,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60j5r/what_ai_model_should_i_build_to_find_out_how/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g60j5r/what_ai_model_should_i_build_to_find_out_how/,155203,1587559844.0,1,,False,,,,
,learnmachinelearning,"Hi,

&amp;#x200B;

I was going through original GAN paper: Goodfellow, Ian, et al. ""Generative adversarial nets."" Advances in neural information processing systems. 2014. Link: [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)

&amp;#x200B;

For proving optimal D, eq 2, they have rewritten the objective function in equation 3. It is:

[Eq3](https://preview.redd.it/7cuo3hvxegu41.png?width=1058&amp;format=png&amp;auto=webp&amp;s=f1bd225c68b7c14885f5662ffebe49ad7eeaddc3)

So, essentially they have changed p(z) to p(x) and g(z) to x.

My question is how can this be done?

&amp;#x200B;

p.s: Is this the correct place to ask such question? Is there a dedicated place where I can ask questions related to specific sub topics of ML? I have also asked it at: [https://math.stackexchange.com/questions/3639114/doubt-in-understanding-gans](https://math.stackexchange.com/questions/3639114/doubt-in-understanding-gans)",t2_153mrt,False,,0,False,Doubt in understanding GANs,[],r/learnmachinelearning,False,6,,0,26.0,False,t3_g6bu5g,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/N8DCor8IFKRLnschZPU3BAwviy3vaL4pdE4hvyRI9Xc.jpg,1587598173.0,,[],{},,,True,,1587626672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I was going through original GAN paper: Goodfellow, Ian, et al. &amp;quot;Generative adversarial nets.&amp;quot; Advances in neural information processing systems. 2014. Link: &lt;a href=""http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf""&gt;http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;For proving optimal D, eq 2, they have rewritten the objective function in equation 3. It is:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7cuo3hvxegu41.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1bd225c68b7c14885f5662ffebe49ad7eeaddc3""&gt;Eq3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, essentially they have changed p(z) to p(x) and g(z) to x.&lt;/p&gt;

&lt;p&gt;My question is how can this be done?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;p.s: Is this the correct place to ask such question? Is there a dedicated place where I can ask questions related to specific sub topics of ML? I have also asked it at: &lt;a href=""https://math.stackexchange.com/questions/3639114/doubt-in-understanding-gans""&gt;https://math.stackexchange.com/questions/3639114/doubt-in-understanding-gans&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6bu5g,True,,aknirala,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6bu5g/doubt_in_understanding_gans/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6bu5g/doubt_in_understanding_gans/,155203,1587597872.0,0,,False,,,"{'7cuo3hvxegu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 20, 'x': 108, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=48a5274896575ceae2b82675d1cafdd182dbd2bc'}, {'y': 41, 'x': 216, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=15a8bcc38a2fab5b53c35e6150d3581c30e1ed90'}, {'y': 61, 'x': 320, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6528ac0cec22f8250f18b3b9de6f527ec9ff34b6'}, {'y': 123, 'x': 640, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2a5f9ca18832aa61ebc7ca387b857a608faa70e'}, {'y': 185, 'x': 960, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9dffca174fec80d237c44ee608178fa6c652d6b3'}], 's': {'y': 204, 'x': 1058, 'u': 'https://preview.redd.it/7cuo3hvxegu41.png?width=1058&amp;format=png&amp;auto=webp&amp;s=f1bd225c68b7c14885f5662ffebe49ad7eeaddc3'}, 'id': '7cuo3hvxegu41'}}",
,learnmachinelearning,,t2_ph5d6,False,,0,False,Videos for the Intro ML course I am teaching: Decision trees and ensemble methods (so far),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,78.0,False,t3_g65444,False,light,1.0,,public,3,0,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmfzTmt0nTtU%2Fhqdefault.jpg%3Fsqp%3D-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ%3D%3D%26rs%3DAOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 450}",140.0,,False,[],"{'oembed': {'provider_url': 'http://youtube.com', 'title': 'Decision trees - YouTube', 'type': 'video', 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmfzTmt0nTtU%2Fhqdefault.jpg%3Fsqp%3D-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ%3D%3D%26rs%3DAOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 168, 'height': 450, 'width': 600, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/mfzTmt0nTtU/hqdefault.jpg?sqp=-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ==&amp;rs=AOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA', 'thumbnail_height': 94}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmfzTmt0nTtU%2Fhqdefault.jpg%3Fsqp%3D-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ%3D%3D%26rs%3DAOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g65444', 'height': 450}",Project,False,3,,False,https://b.thumbs.redditmedia.com/g8QEsoNMusDG5GBzWXQNrDxaSxixmwULTaUKOTVjREU.jpg,False,,[],{},rich:video,,False,,1587604450.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b56XVruHPfXEAbovuwO4l---egRZ1Hl71RMKoAX605s.jpg?auto=webp&amp;s=4320ed5496531ab09c7f37c7052e98a00817aa0e', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/b56XVruHPfXEAbovuwO4l---egRZ1Hl71RMKoAX605s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6348e8b96f945ee04a75f037771723c9051aab28', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'fL5BKotwYwAHfSPTied2lqjk3WRx8wkq5JgXkJjKJJc'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g65444,True,,kamperh,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g65444/videos_for_the_intro_ml_course_i_am_teaching/,all_ads,False,https://www.youtube.com/playlist?list=PLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp,155203,1587575650.0,0,"{'oembed': {'provider_url': 'http://youtube.com', 'title': 'Decision trees - YouTube', 'type': 'video', 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fvideoseries%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fplaylist%3Flist%3DPLmZlBIcArwhPrP3H7iejBQpqtP1UHrhFp&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FmfzTmt0nTtU%2Fhqdefault.jpg%3Fsqp%3D-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ%3D%3D%26rs%3DAOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""450"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 168, 'height': 450, 'width': 600, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/mfzTmt0nTtU/hqdefault.jpg?sqp=-oaymwEWCKgBEF5IWvKriqkDCQgBFQAAiEIYAQ==&amp;rs=AOn4CLCfm9nFaLJN2nPz2gj6qVXX1qKAFA', 'thumbnail_height': 94}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning," Hello,  I'm working on a project that merges the immage of 2 or more people into an ""average result"" that should be the closest ""mix"" of the 2 or more pictures of peoples used to generate the merged immage, 

picture a ""+"" picrure b  ""+"" .... ""+""  picture n = picture z

  I would also like the image genarated to be treated  so that it dose not appear to be generated by a computer, and than possibly also upscale it to the highest resolution possible.  Can you point me int he right direction?  i'm a  a python programmer working with a small team on this project.  can you point us to a any git hub repository that dose just that, or a few project to be merged into this final result? or a course that can lead a very beginner to achive such results?  we've read of a similar project (styleGAN2) and we are thinking on implementing on a Generative Adversarial Network, so please feel free to point us in the right direction to get closer to our resoult.",t2_10q2z0,False,,0,False,Generative Adversarial Network,[],r/learnmachinelearning,False,6,,0,,False,t3_g6ayuo,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587623525.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,  I&amp;#39;m working on a project that merges the immage of 2 or more people into an &amp;quot;average result&amp;quot; that should be the closest &amp;quot;mix&amp;quot; of the 2 or more pictures of peoples used to generate the merged immage, &lt;/p&gt;

&lt;p&gt;picture a &amp;quot;+&amp;quot; picrure b  &amp;quot;+&amp;quot; .... &amp;quot;+&amp;quot;  picture n = picture z&lt;/p&gt;

&lt;p&gt;I would also like the image genarated to be treated  so that it dose not appear to be generated by a computer, and than possibly also upscale it to the highest resolution possible.  Can you point me int he right direction?  i&amp;#39;m a  a python programmer working with a small team on this project.  can you point us to a any git hub repository that dose just that, or a few project to be merged into this final result? or a course that can lead a very beginner to achive such results?  we&amp;#39;ve read of a similar project (styleGAN2) and we are thinking on implementing on a Generative Adversarial Network, so please feel free to point us in the right direction to get closer to our resoult.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g6ayuo,True,,bodytexture,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6ayuo/generative_adversarial_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6ayuo/generative_adversarial_network/,155203,1587594725.0,0,,False,,,,
,learnmachinelearning," Looking for driven beginner or intermediate Python for Data Science partner on the web to meet multiple times a week to slay Python via Google Hangout. A bit about me, I'm a Cornell Engineer transitioning to Data Science from a career in the medical field. So let's push our limits, learn, and make our dreams come true together. For the right person(s), this should be mutually inspiring, helpful, and productive.

So we are on the same page...

\-I believe mutual dependability is critical to success. Please only commit to partner study if you can keep a regular virtual meeting schedule.

\-Please be a communicative and commit to agreed upon meetings under expectation of treating as seriously as you would a job; this way, we will get our dream jobs down the line

\-Time &gt; gold &amp; rubies - please minimize cancelations/rescheduling/etc. and respect eachother's time accordingly

\-How many hours you dedicate to study is up to you, but working together on projects / collaborating is preferred

\-Be ready to mutually push each other!

Message me:

\-Your goals weekly to Python for Data Science practice (e.g. dream job, timeline for getting hired, etc.)

\-Level of Python to date

\-About you (field you work in, interest, where u live in the world)

\-Any questions (as specific as possible) you have for me

&amp;#x200B;

Best,

Leonor",t2_62t9mghw,False,,0,False,Hell-bent on Succeeding in Python for Machine Learning / Data Science? Become my Study Partner!,[],r/learnmachinelearning,False,6,,0,,False,t3_g60z8c,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1587590398.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looking for driven beginner or intermediate Python for Data Science partner on the web to meet multiple times a week to slay Python via Google Hangout. A bit about me, I&amp;#39;m a Cornell Engineer transitioning to Data Science from a career in the medical field. So let&amp;#39;s push our limits, learn, and make our dreams come true together. For the right person(s), this should be mutually inspiring, helpful, and productive.&lt;/p&gt;

&lt;p&gt;So we are on the same page...&lt;/p&gt;

&lt;p&gt;-I believe mutual dependability is critical to success. Please only commit to partner study if you can keep a regular virtual meeting schedule.&lt;/p&gt;

&lt;p&gt;-Please be a communicative and commit to agreed upon meetings under expectation of treating as seriously as you would a job; this way, we will get our dream jobs down the line&lt;/p&gt;

&lt;p&gt;-Time &amp;gt; gold &amp;amp; rubies - please minimize cancelations/rescheduling/etc. and respect eachother&amp;#39;s time accordingly&lt;/p&gt;

&lt;p&gt;-How many hours you dedicate to study is up to you, but working together on projects / collaborating is preferred&lt;/p&gt;

&lt;p&gt;-Be ready to mutually push each other!&lt;/p&gt;

&lt;p&gt;Message me:&lt;/p&gt;

&lt;p&gt;-Your goals weekly to Python for Data Science practice (e.g. dream job, timeline for getting hired, etc.)&lt;/p&gt;

&lt;p&gt;-Level of Python to date&lt;/p&gt;

&lt;p&gt;-About you (field you work in, interest, where u live in the world)&lt;/p&gt;

&lt;p&gt;-Any questions (as specific as possible) you have for me&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Best,&lt;/p&gt;

&lt;p&gt;Leonor&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g60z8c,True,,hrh_swql,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60z8c/hellbent_on_succeeding_in_python_for_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g60z8c/hellbent_on_succeeding_in_python_for_machine/,155203,1587561598.0,0,,False,,,,
,learnmachinelearning,"I have created a deep learning model, using keras, with 3 hidden layers. It works like multiple linear regression. I need to reduce the validation loss in each epoch. I tried reducing the learning rate and increasing the epoch, but it didn't work out. The lowest value i am getting is around 40 for the validation loss, but i need it to be lower. Can someone help me with that?

Dataset: Lena image (for the prediction of the target pixel, I take 4 neighbouring pixels as input and then predict the value of the target pixel.)

&amp;#x200B;

The code:

model=Sequential()

model.add(Dense(20, input\_shape=(4,), activation='relu'))

model.add(Dense(20, activation='relu'))

model.add(Dense(20, activation='relu'))

model.add(Dense(1,))

&amp;#x200B;

from keras.callbacks import EarlyStopping

from keras.optimizers import Adam

opt=Adam(lr=0.0001)

early\_stopping = EarlyStopping(monitor='val\_loss', min\_delta=0, patience=15, verbose=1, mode='auto')

model.compile(optimizer=opt, loss='mean\_squared\_error')

\#train model

[history=model.fit](https://history=model.fit)(x\_train, y\_train, batch\_size=32, validation\_split=0.2, epochs=1000, callbacks=\[early\_stopping\])",t2_1s9q8e8o,False,,0,False,How can I increase the accuracy of my neural network?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g6afl5,False,light,0.33,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,1587636269.0,,[],{},,,True,,1587621702.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have created a deep learning model, using keras, with 3 hidden layers. It works like multiple linear regression. I need to reduce the validation loss in each epoch. I tried reducing the learning rate and increasing the epoch, but it didn&amp;#39;t work out. The lowest value i am getting is around 40 for the validation loss, but i need it to be lower. Can someone help me with that?&lt;/p&gt;

&lt;p&gt;Dataset: Lena image (for the prediction of the target pixel, I take 4 neighbouring pixels as input and then predict the value of the target pixel.)&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;The code:&lt;/p&gt;

&lt;p&gt;model=Sequential()&lt;/p&gt;

&lt;p&gt;model.add(Dense(20, input_shape=(4,), activation=&amp;#39;relu&amp;#39;))&lt;/p&gt;

&lt;p&gt;model.add(Dense(20, activation=&amp;#39;relu&amp;#39;))&lt;/p&gt;

&lt;p&gt;model.add(Dense(20, activation=&amp;#39;relu&amp;#39;))&lt;/p&gt;

&lt;p&gt;model.add(Dense(1,))&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;from keras.callbacks import EarlyStopping&lt;/p&gt;

&lt;p&gt;from keras.optimizers import Adam&lt;/p&gt;

&lt;p&gt;opt=Adam(lr=0.0001)&lt;/p&gt;

&lt;p&gt;early_stopping = EarlyStopping(monitor=&amp;#39;val_loss&amp;#39;, min_delta=0, patience=15, verbose=1, mode=&amp;#39;auto&amp;#39;)&lt;/p&gt;

&lt;p&gt;model.compile(optimizer=opt, loss=&amp;#39;mean_squared_error&amp;#39;)&lt;/p&gt;

&lt;p&gt;#train model&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://history=model.fit""&gt;history=model.fit&lt;/a&gt;(x_train, y_train, batch_size=32, validation_split=0.2, epochs=1000, callbacks=[early_stopping])&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g6afl5,True,,closet_coder,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g6afl5/how_can_i_increase_the_accuracy_of_my_neural/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g6afl5/how_can_i_increase_the_accuracy_of_my_neural/,155203,1587592902.0,0,,False,,,,
,learnmachinelearning,,t2_8b6764o,False,,0,False,"[P] Minigrad: Autograd engine for Python written in Python C-API for speed, inspired by Karpathy's micrograd library",[],r/learnmachinelearning,False,6,,0,,False,t3_g63bag,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,default,False,,[],{},link,,False,,1587598564.0,text,6,,,text,self.MachineLearning,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?auto=webp&amp;s=78d7791cfa713eb5bccd39a0764b9f9f1047b6d1', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fed95294f6e40c341aa4b91e0a2309604750e96', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b039c3534eaec723bcb90802615c83c431d40d0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34a7fbd1e0d7999afefbe962cbdc582e7a87be1b', 'width': 320, 'height': 320}], 'variants': {}, 'id': '1vi3qTth1yk6L_pPMbFO8mHKjO0yD6oS8UABQMEvujg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g63bag,True,,hiBengu,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g63bag/p_minigrad_autograd_engine_for_python_written_in/,all_ads,False,/r/MachineLearning/comments/g4w4xp/p_minigrad_autograd_engine_for_python_written_in/,155203,1587569764.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MachineLearning', 'selftext': ""Source Code: [https://github.com/goktug97/minigrad](https://github.com/goktug97/minigrad)\n\nI like using my own libraries while learning stuff. I got excited when I saw Karpathy's small neural network library but it was a little bit slow for my taste. So I reimplemented it in Python C-API. It runs signifanctly faster. Of course, I would use PyTorch or Tensorflow for real projects but I learned how to implement an autograd engine and how to use Python C-API so it was worth it.\n\nPlanning to use this as far as I can while going through [https://spinningup.openai.com/en/latest/user/introduction.html](https://spinningup.openai.com/en/latest/user/introduction.html)"", 'author_fullname': 't2_wctdi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""[P] Minigrad: Autograd engine for Python written in Python C-API for speed, inspired by Karpathy's micrograd library"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MachineLearning', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'four', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g4w4xp', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.78, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Project', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587429421.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MachineLearning', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Source Code: &lt;a href=""https://github.com/goktug97/minigrad""&gt;https://github.com/goktug97/minigrad&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I like using my own libraries while learning stuff. I got excited when I saw Karpathy&amp;#39;s small neural network library but it was a little bit slow for my taste. So I reimplemented it in Python C-API. It runs signifanctly faster. Of course, I would use PyTorch or Tensorflow for real projects but I learned how to implement an autograd engine and how to use Python C-API so it was worth it.&lt;/p&gt;\n\n&lt;p&gt;Planning to use this as far as I can while going through &lt;a href=""https://spinningup.openai.com/en/latest/user/introduction.html""&gt;https://spinningup.openai.com/en/latest/user/introduction.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?auto=webp&amp;s=78d7791cfa713eb5bccd39a0764b9f9f1047b6d1', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fed95294f6e40c341aa4b91e0a2309604750e96', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b039c3534eaec723bcb90802615c83c431d40d0', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/m45QN_PIx1wKb77sPCGvAKfxuSj96M7C3tBEZllQ9og.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34a7fbd1e0d7999afefbe962cbdc582e7a87be1b', 'width': 320, 'height': 320}], 'variants': {}, 'id': '1vi3qTth1yk6L_pPMbFO8mHKjO0yD6oS8UABQMEvujg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2r3gv', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g4w4xp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'goktugkt', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MachineLearning/comments/g4w4xp/p_minigrad_autograd_engine_for_python_written_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MachineLearning/comments/g4w4xp/p_minigrad_autograd_engine_for_python_written_in/', 'subreddit_subscribers': 1044414, 'created_utc': 1587400621.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_g4w4xp,,
,learnmachinelearning,,t2_9pr4f,False,,0,False,27 Resources to Learn ML,[],r/learnmachinelearning,False,6,,0,79.0,False,t3_g69ft1,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/puNGxBQx8VY9-bUtjgbR2Pu_ibwdDeNwZ_yBPpf-uvY.jpg,False,,[],{},link,,False,,1587618391.0,text,6,,,text,serokell.io,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?auto=webp&amp;s=cd669a4c286384037fd39700be7fe02ec3fc84d1', 'width': 1200, 'height': 679}, 'resolutions': [{'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d34578bc3d729db45462fd02d275a5c9fd1148b0', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6b815e760d0fe72ee93cdb5f081caaa472782128', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ca724b5fd64d59874f1d46e2690dbd0e661277', 'width': 320, 'height': 181}, {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b8e040da353973ad333d0c84d8941ba86043c4cb', 'width': 640, 'height': 362}, {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9770ea7966683a0ae55645ed0c55cdf03bacd390', 'width': 960, 'height': 543}, {'url': 'https://external-preview.redd.it/xiEZfqanmGfQ6DxEk3k47aRR1I773VLqE80lqOauqO4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d99f91909fb01997de55c6725b5b75823b9798ce', 'width': 1080, 'height': 611}], 'variants': {}, 'id': 'PW7vyfVKAGWrs4RGw8TyvYIm270cR3oNK_Z7CHCNAYY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g69ft1,True,,NaeosPsy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g69ft1/27_resources_to_learn_ml/,all_ads,False,https://serokell.io/blog/top-resources-to-learn-ml,155203,1587589591.0,0,,False,,,,
,learnmachinelearning,,t2_2vj4dxir,False,,0,False,20 Best Machine Learning Courses to Learn,[],r/learnmachinelearning,False,6,,0,79.0,False,t3_g60rx9,False,dark,0.75,,public,4,0,{},140.0,,False,[],,False,False,,{},,False,4,,False,https://b.thumbs.redditmedia.com/0EaIAjn_PWJvdUFFH7ejQ8Ek9MWZpSdSl25qqtMP5TY.jpg,False,,[],{},link,,False,,1587589611.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?auto=webp&amp;s=f1ad193fcf8925fda7eecec2e2eddfc4978a50e5', 'width': 450, 'height': 255}, 'resolutions': [{'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=059991a004c0567d929560e64ef195b8250be27a', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c91ed9356ba77210f07c6883599fb9c9967e6c43', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/hmx7mgIqiCcutm9SDBZIFmci0ndYZdNsY_eGWBCqCSU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=28a2dd55e68edd7ddc298bbd764964413874d0cb', 'width': 320, 'height': 181}], 'variants': {}, 'id': 'hNiHZGAMdlAXHG8i2zCj8xKlnCAJpcumaPYiw_vZHrE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g60rx9,True,,cimmba,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60rx9/20_best_machine_learning_courses_to_learn/,all_ads,False,https://medium.com/@devexpert/20-best-machine-learning-courses-online-4a7863c4326a,155203,1587560811.0,0,,False,,,,
,learnmachinelearning,"While building a DQN agent, I noticed that an example solution had the following  snippet within its training code, when updating the parameters of the neural network.

&amp;#x200B;

`states, actions, rewards, next_states, dones = experiences  # Get max predicted Q values`

`model Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)`    

`Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))  # Get expected Q values from local model` 

`Q_expected = self.qnetwork_local(states).gather(1, actions)` 

&amp;#x200B;

Qnetwork here is a FC linear neural network, with an init() method and a forward() method. As it has already been initialized way before this step, I'm not sure what the 2nd and 4th line are doing. I believe they are calling .forward()? But I've never seen this notation before - is it just an abbreviation?",t2_jgca7,False,,0,False,Pytorch syntax: is calling model(states) the same as model.forward(states),[],r/learnmachinelearning,False,6,,0,,False,t3_g68qs9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587616148.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;While building a DQN agent, I noticed that an example solution had the following  snippet within its training code, when updating the parameters of the neural network.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;states, actions, rewards, next_states, dones = experiences  # Get max predicted Q values&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)&lt;/code&gt;    &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))  # Get expected Q values from local model&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Q_expected = self.qnetwork_local(states).gather(1, actions)&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Qnetwork here is a FC linear neural network, with an init() method and a forward() method. As it has already been initialized way before this step, I&amp;#39;m not sure what the 2nd and 4th line are doing. I believe they are calling .forward()? But I&amp;#39;ve never seen this notation before - is it just an abbreviation?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g68qs9,True,,dxjustice,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g68qs9/pytorch_syntax_is_calling_modelstates_the_same_as/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g68qs9/pytorch_syntax_is_calling_modelstates_the_same_as/,155203,1587587348.0,0,,False,,,,
,learnmachinelearning,"I need to make a voice classifier for my side project. However, it needs a lot of data to create it. The only way to get some voice data is that download a youtube video that has a certain person's voice and cut and extracts only that person's voice. Is there any tool to help me to reduce the manual process?

For a simple image classifier, I can use python google image downloader to download a bunch of images related to the keyword. However, I can only think of a manual way of collecting dataset for the voice.",t2_lnhovoc,False,,0,False,I am looking for a tool to collect voice data for the voice classifier.,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g68f3i,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587615097.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I need to make a voice classifier for my side project. However, it needs a lot of data to create it. The only way to get some voice data is that download a youtube video that has a certain person&amp;#39;s voice and cut and extracts only that person&amp;#39;s voice. Is there any tool to help me to reduce the manual process?&lt;/p&gt;

&lt;p&gt;For a simple image classifier, I can use python google image downloader to download a bunch of images related to the keyword. However, I can only think of a manual way of collecting dataset for the voice.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g68f3i,True,,fiddlest,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g68f3i/i_am_looking_for_a_tool_to_collect_voice_data_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g68f3i/i_am_looking_for_a_tool_to_collect_voice_data_for/,155203,1587586297.0,0,,False,,,,
,learnmachinelearning,,t2_14phk6,False,,0,False,Running TensorFlow Lite Object Detection Models in Python,[],r/learnmachinelearning,False,6,,0,102.0,False,t3_g68bji,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/vu797xBkhCvTpQSTbLkdr5ZlkZqIr9nTnTfQitd9B5o.jpg,False,,[],{},link,,False,,1587614764.0,text,6,,,text,heartbeat.fritz.ai,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?auto=webp&amp;s=a81eacebb377bfb59d83974f38040d1a09007896', 'width': 1200, 'height': 878}, 'resolutions': [{'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9494dc94116881e7d33decd6be9785ff2f7812e5', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7c732fbca4486235e6fbc6406bb9db719784164', 'width': 216, 'height': 158}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b50922bf60f463c81b602ffb73fcc3d01ee68360', 'width': 320, 'height': 234}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=625745545a14721a6008d0effc1a6b5c720668b7', 'width': 640, 'height': 468}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b636bb84fc268ee95a2ddcda3579f7f8bf6dc7d', 'width': 960, 'height': 702}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fceddd7a1ad757f9e8e6ecefd57640309a034c36', 'width': 1080, 'height': 790}], 'variants': {}, 'id': 'Dol_lRZ_25icyokEKWe8_ppr9DZ55bOAMJz78HFoXGo'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g68bji,True,,the-dagger,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g68bji/running_tensorflow_lite_object_detection_models/,all_ads,False,https://heartbeat.fritz.ai/running-tensorflow-lite-object-detection-models-in-python-8a73b77e13f8,155203,1587585964.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'tensorflow', 'selftext': '', 'author_fullname': 't2_14phk6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Running TensorFlow Lite Object Detection Models in Python', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/tensorflow', 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 102, 'hide_score': False, 'name': 't3_g68ben', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.73, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Article', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/vu797xBkhCvTpQSTbLkdr5ZlkZqIr9nTnTfQitd9B5o.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1587614751.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'heartbeat.fritz.ai', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?auto=webp&amp;s=a81eacebb377bfb59d83974f38040d1a09007896', 'width': 1200, 'height': 878}, 'resolutions': [{'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9494dc94116881e7d33decd6be9785ff2f7812e5', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7c732fbca4486235e6fbc6406bb9db719784164', 'width': 216, 'height': 158}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b50922bf60f463c81b602ffb73fcc3d01ee68360', 'width': 320, 'height': 234}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=625745545a14721a6008d0effc1a6b5c720668b7', 'width': 640, 'height': 468}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b636bb84fc268ee95a2ddcda3579f7f8bf6dc7d', 'width': 960, 'height': 702}, {'url': 'https://external-preview.redd.it/-4BqFRGm7cl8JLBndVDkOIwYnGOOdcxtq8Y7qo7KikE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fceddd7a1ad757f9e8e6ecefd57640309a034c36', 'width': 1080, 'height': 790}], 'variants': {}, 'id': 'Dol_lRZ_25icyokEKWe8_ppr9DZ55bOAMJz78HFoXGo'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '1db20040-afc0-11e7-b8c9-0e6c50199bd0', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3alkk', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g68ben', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'the-dagger', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/tensorflow/comments/g68ben/running_tensorflow_lite_object_detection_models/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://heartbeat.fritz.ai/running-tensorflow-lite-object-detection-models-in-python-8a73b77e13f8', 'subreddit_subscribers': 17127, 'created_utc': 1587585951.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_g68ben,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is the Turing Test ? | Can an Artificial Intelligence pass for a Human ? Clearly explained,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g688dy,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Hbu8VFtO2rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is the Turing Test ? | Can an Artificial Intelligence pass for a Human ? 25', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Hbu8VFtO2rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Hbu8VFtO2rE/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Hbu8VFtO2rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g688dy', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/BBWQmyQuuN2DuX3qKWdF0mzv7ctZLwFxshD1DlLoREA.jpg,False,,[],{},rich:video,,False,,1587614487.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/RPhnitYX_oOCTDzU4dpkh21F-8gArtvsn6yC83kM5K4.jpg?auto=webp&amp;s=e36b716aaf65e7708c1d1bee0ea778e006afb487', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/RPhnitYX_oOCTDzU4dpkh21F-8gArtvsn6yC83kM5K4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7533e0a690d1640ae7b91789c9806cb1655fd65b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/RPhnitYX_oOCTDzU4dpkh21F-8gArtvsn6yC83kM5K4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb1d91cafca06c8f2fbec4b2a09f0d0ffa656c14', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/RPhnitYX_oOCTDzU4dpkh21F-8gArtvsn6yC83kM5K4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8764713d6544bcb99e77a4e66bf85695bf1be95b', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ox4aa1AANr0js2BnuAzsH4dIlaBOiMuEqrqGCz0djbw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g688dy,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g688dy/what_is_the_turing_test_can_an_artificial/,all_ads,False,https://youtu.be/Hbu8VFtO2rE,155203,1587585687.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is the Turing Test ? | Can an Artificial Intelligence pass for a Human ? 25', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Hbu8VFtO2rE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Hbu8VFtO2rE/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,I am 26 working as a bank teller. I want to be a data engineer. My work experience is only retail experience like working at the bank and Verizon Wireless as a sales representative. I want to self study but I donâ€™t know where to start. Do you recommend reading books or coursera or coding academy? I am not going back to university because I donâ€™t want to take out loans. Should I start with an internship or should I wait until I have more practice building projects?,t2_5msv3kin,False,,0,False,I want to be a data engineer ! How do I do it,[],r/learnmachinelearning,False,6,,0,,False,t3_g67ew3,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1587611836.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am 26 working as a bank teller. I want to be a data engineer. My work experience is only retail experience like working at the bank and Verizon Wireless as a sales representative. I want to self study but I donâ€™t know where to start. Do you recommend reading books or coursera or coding academy? I am not going back to university because I donâ€™t want to take out loans. Should I start with an internship or should I wait until I have more practice building projects?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g67ew3,True,,rasulguidry7,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g67ew3/i_want_to_be_a_data_engineer_how_do_i_do_it/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g67ew3/i_want_to_be_a_data_engineer_how_do_i_do_it/,155203,1587583036.0,0,,False,,,,
,learnmachinelearning,"I'm working on a project involving indoor drone navigation. I'm using two drones so that a 3d image can be constructed. The drones will always fly through the same doorway (thus eliminating overfitting for the learning agent, I think). I've considered using a NN or a SIFT like algorithm for this task and I'm not quite sure what suits this task more. Thanks in adavance :-)",t2_3uf5vowp,False,,0,False,Neural Network vs. Key-Points for drone navigation,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g664gk,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1587607697.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project involving indoor drone navigation. I&amp;#39;m using two drones so that a 3d image can be constructed. The drones will always fly through the same doorway (thus eliminating overfitting for the learning agent, I think). I&amp;#39;ve considered using a NN or a SIFT like algorithm for this task and I&amp;#39;m not quite sure what suits this task more. Thanks in adavance :-)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g664gk,True,,NadavDagon,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g664gk/neural_network_vs_keypoints_for_drone_navigation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g664gk/neural_network_vs_keypoints_for_drone_navigation/,155203,1587578897.0,0,,False,,,,
,learnmachinelearning,,t2_53t39z3t,False,,0,False,Text Sentiment Prediction Flask Machine Learning Application,[],r/learnmachinelearning,False,6,,0,,False,t3_g65ut9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1587606853.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g65ut9,True,,datageekrj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g65ut9/text_sentiment_prediction_flask_machine_learning/,all_ads,False,https://www.youtube.com/playlist?list=PL3odEuBfDQml_PJjxmKoAPxPT-QUFMINZ,155203,1587578053.0,0,,False,,,,
,learnmachinelearning," Hi,

I'm  just deepen my knowledge about neural networks and taking a closer look  at the cost function. The course I'm taking says that the cost function  for a learning dataset is the sum of the cost for each row fed into the  neural network. The result is then used to adjust the weights in a  neural network.

Whey is the cost  function not somewhat adjusted to the number of rows fed into the neural  network, such as feeding the sum of all rows divided by the number of  all rows?

Thanks in advance,

Sascha",t2_430hhyt,False,,0,False,Cost function for dataset,[],r/learnmachinelearning,False,6,,0,,False,t3_g65u33,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587606784.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m  just deepen my knowledge about neural networks and taking a closer look  at the cost function. The course I&amp;#39;m taking says that the cost function  for a learning dataset is the sum of the cost for each row fed into the  neural network. The result is then used to adjust the weights in a  neural network.&lt;/p&gt;

&lt;p&gt;Whey is the cost  function not somewhat adjusted to the number of rows fed into the neural  network, such as feeding the sum of all rows divided by the number of  all rows?&lt;/p&gt;

&lt;p&gt;Thanks in advance,&lt;/p&gt;

&lt;p&gt;Sascha&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g65u33,True,,saschaandres,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g65u33/cost_function_for_dataset/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g65u33/cost_function_for_dataset/,155203,1587577984.0,0,,False,,,,
,learnmachinelearning,"I'm trying to wrap my head around how to use pretrained models from huggingface but I cannot seem to find what I'm looking for in the docs, if anyone has experience I'd love it if you could give me some hints in the right direction.

&amp;#x200B;

I'm trying to understand some code I found on kaggle:

  
`class TweetModel(transformers.BertPreTrainedModel):`  
`def __init__(self, conf):`  
`super(TweetModel, self).__init__(conf)`  
`self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)`  
`self.drop_out = nn.Dropout(0.1)`  
`self.l0 = nn.Linear(768 * 2, 2)`  
`torch.nn.init.normal_(self.l0.weight, std=0.02)`

`def forward(self, ids, mask, token_type_ids):`  
`_, _, out = self.roberta(`  
`ids,`  
`attention_mask=mask,`  
`token_type_ids=token_type_ids`  
`)`  
`...`

&amp;#x200B;

What I dont't understand is where to an explanation of what is the output of the model. I tried printing the model and the final part of the summary is this:

&amp;#x200B;

&amp;#x200B;

`...`   
`(intermediate): BertIntermediate(`  
`(dense): Linear(in_features=768, out_features=3072, bias=True)`  
`)`  
`(output): BertOutput(`  
`(dense): Linear(in_features=3072, out_features=768, bias=True)`  
`(LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)`  
`(dropout): Dropout(p=0.1, inplace=False)`  
`)`  
`)`  
`)`  
`)`  
`(pooler): BertPooler(`  
`(dense): Linear(in_features=768, out_features=768, bias=True)`  
`(activation): Tanh()`  
   `)`  
  `)`  
  `(drop_out): Dropout(p=0.1, inplace=False)`  
  `(l0): Linear(in_features=1536, out_features=2, bias=True)`  
`)`

&amp;#x200B;

is the BertOutput representing the output of the layer in the code? what are the two `_` assigned to?

On the docs of huggingface I found that the output depends on the config but I was not able to find a better explanation of it. Do you know where I should look for this stuff? Is the model sumary that I pasted relevant? 

&amp;#x200B;

Thanks for your help!",t2_p0vrw,False,,0,False,question about the output of pretrained bert models,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g64y2z,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1587603914.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to wrap my head around how to use pretrained models from huggingface but I cannot seem to find what I&amp;#39;m looking for in the docs, if anyone has experience I&amp;#39;d love it if you could give me some hints in the right direction.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to understand some code I found on kaggle:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;class TweetModel(transformers.BertPreTrainedModel):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;def __init__(self, conf):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;super(TweetModel, self).__init__(conf)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;self.drop_out = nn.Dropout(0.1)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;self.l0 = nn.Linear(768 * 2, 2)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;torch.nn.init.normal_(self.l0.weight, std=0.02)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def forward(self, ids, mask, token_type_ids):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;_, _, out = self.roberta(&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;ids,&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;attention_mask=mask,&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;token_type_ids=token_type_ids&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;What I dont&amp;#39;t understand is where to an explanation of what is the output of the model. I tried printing the model and the final part of the summary is this:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;...&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(intermediate): BertIntermediate(&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(dense): Linear(in_features=768, out_features=3072, bias=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(output): BertOutput(&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(dense): Linear(in_features=3072, out_features=768, bias=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(dropout): Dropout(p=0.1, inplace=False)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(pooler): BertPooler(&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(dense): Linear(in_features=768, out_features=768, bias=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;(activation): Tanh()&lt;/code&gt;&lt;br/&gt;
   &lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
  &lt;code&gt;)&lt;/code&gt;&lt;br/&gt;
  &lt;code&gt;(drop_out): Dropout(p=0.1, inplace=False)&lt;/code&gt;&lt;br/&gt;
  &lt;code&gt;(l0): Linear(in_features=1536, out_features=2, bias=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;is the BertOutput representing the output of the layer in the code? what are the two &lt;code&gt;_&lt;/code&gt; assigned to?&lt;/p&gt;

&lt;p&gt;On the docs of huggingface I found that the output depends on the config but I was not able to find a better explanation of it. Do you know where I should look for this stuff? Is the model sumary that I pasted relevant? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for your help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g64y2z,True,,kappa_detective,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g64y2z/question_about_the_output_of_pretrained_bert/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g64y2z/question_about_the_output_of_pretrained_bert/,155203,1587575114.0,0,,False,,,,
,learnmachinelearning,"Hello! I want to make a neural network that takes as input the presense or abscence of tags and outputs a single class (whether the user likes something or not).

The issue is: even though the tagged dataset is of a reasonable size, there's no class data (as it would change from user to user). Additionally, I don't want to rely on any data from other users. I would want the system to gradually learn the user's taste as more data is classified.

I initially thought of retraining a feedforward neural network every so often, but that sounds too computationally expensive, specially as the classified dataset increases.

My question is: Would reinforcement learning be appropriate for this task? If so, are there any good resourses on implementing one for something like this? I have only worked with supervised and unsupervised learning before, so I'm unsure if reinforcement learning would be fit for the scenario.",t2_4btlehbh,False,,0,False,Supervised or reinforcement learning for a content-based recommender system?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g64shz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587603431.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I want to make a neural network that takes as input the presense or abscence of tags and outputs a single class (whether the user likes something or not).&lt;/p&gt;

&lt;p&gt;The issue is: even though the tagged dataset is of a reasonable size, there&amp;#39;s no class data (as it would change from user to user). Additionally, I don&amp;#39;t want to rely on any data from other users. I would want the system to gradually learn the user&amp;#39;s taste as more data is classified.&lt;/p&gt;

&lt;p&gt;I initially thought of retraining a feedforward neural network every so often, but that sounds too computationally expensive, specially as the classified dataset increases.&lt;/p&gt;

&lt;p&gt;My question is: Would reinforcement learning be appropriate for this task? If so, are there any good resourses on implementing one for something like this? I have only worked with supervised and unsupervised learning before, so I&amp;#39;m unsure if reinforcement learning would be fit for the scenario.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g64shz,True,,ZuchP,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g64shz/supervised_or_reinforcement_learning_for_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g64shz/supervised_or_reinforcement_learning_for_a/,155203,1587574631.0,0,,False,,,,
,learnmachinelearning,,t2_59kxi4ej,False,,0,False,Optical Character Recognition Based on Machine Learning Technology,[],r/learnmachinelearning,False,6,,0,79.0,False,t3_g64rbq,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/b2I7o13Vr8aBF0vTCGxnsXnyTkglkQ9uXlwvk6SniaY.jpg,False,,[],{},link,,False,,1587603320.0,text,6,,,text,mobidev.biz,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?auto=webp&amp;s=243a204cd346f088e365b5ddea32691f30dc2b7b', 'width': 1488, 'height': 840}, 'resolutions': [{'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0db5b0a4d602a34ed492367f109a731b256ae508', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b3d94ff33c260119dc14dde26e0e356d6a0bdec', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08f32dc824b6e0939671eb59e9bd36d95572c737', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6017c0b78c46ecaff7aba4d25ba2c455eaa9f8e4', 'width': 640, 'height': 361}, {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ea388e58813f9cb583ad07408c7ea8070b82732', 'width': 960, 'height': 541}, {'url': 'https://external-preview.redd.it/JOm8mKyHn80L0ejljbpkJ4m7FOXZYFLD5__Z9OaLkQ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ed6caa957e00aa707b108887b95d149d15dcf62d', 'width': 1080, 'height': 609}], 'variants': {}, 'id': 'Lfbjuc8btbn4cySsiBzfD_FoaRNQNtCAnfCyWtVlTBI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g64rbq,True,,Data-Power,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g64rbq/optical_character_recognition_based_on_machine/,all_ads,False,https://mobidev.biz/blog/ocr-machine-learning-implementation?utm_source=reddit&amp;utm_medium=reddit-ds&amp;utm_campaign=reddit-ocr,155203,1587574520.0,0,,False,,,,
,learnmachinelearning,,t2_42ti910c,False,,0,False,"Used LSTM to generate music, and here is how it went",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g5tou4,False,dark,0.93,,public,12,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zoSPTb20zbU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Happy Nostalgia - Neuro Chords | Generated By Artificial Intelligence', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zoSPTb20zbU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Neuro Chords', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zoSPTb20zbU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWqnekVEEXNgGNHMcrblqzg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zoSPTb20zbU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g5tou4', 'height': 338}",,False,12,,False,https://b.thumbs.redditmedia.com/y603kYaiwloruu3RK3RgE_HmM3dJgYIKXAkdf3XxXnI.jpg,False,,[],{},rich:video,,False,,1587554371.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ZRU5bhNGNr9sD-mTsjpD8h6qfvsOtL2J3K5WOMt3chs.jpg?auto=webp&amp;s=12ba441b6059e941b9337ac00993aea7e90b17b9', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZRU5bhNGNr9sD-mTsjpD8h6qfvsOtL2J3K5WOMt3chs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a7476625996beebffa5d48e5c9986488618f65d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/ZRU5bhNGNr9sD-mTsjpD8h6qfvsOtL2J3K5WOMt3chs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f3bb5800faf4690898e2ccefa111de78dd61384', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/ZRU5bhNGNr9sD-mTsjpD8h6qfvsOtL2J3K5WOMt3chs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=581bc26685c0dedf4de5f5b676b0f607b9cbf42a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'elVsrSJnXxYQMCYLVzjeo-AVKjI6VGHjIvyKyR546Vw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5tou4,True,,vaiv101,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5tou4/used_lstm_to_generate_music_and_here_is_how_it/,all_ads,False,https://youtu.be/zoSPTb20zbU,155203,1587525571.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Happy Nostalgia - Neuro Chords | Generated By Artificial Intelligence', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zoSPTb20zbU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Neuro Chords', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zoSPTb20zbU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWqnekVEEXNgGNHMcrblqzg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hello to everyone.

I'm rather new to AI. I'm writing a midi gan as project for my thesis, but i stumbled upon some problems and i'd be really grateful if someone could help me out. I'm willing to share the code too, if it is needed (the project is Tensorflow/Keras).

I started the project building an interpreter to generate matrices from midi and the other way around, so that i could make everything work with fully-connected generator and discriminator. This method outputted decent files, but after some epochs (around 3500) something breaks and the discriminator accuracy starts going down until hits 0,00%, and from that model the output matrices will be solely composed by zeros.

Finding those difficulties i switched to a convolutional model, but it performed worse: the discriminator and generator loss do not vary during the execution, while the discriminator accuracy is always at 50% before dropping to 0% within 5 to 10 epochs. I even tried to make each nn more or least powerful by skipping some learning phase but didn't work.

What could i be doing wrong? How could i fix those models i built? Or even should i discard the midi interpreter and move along with a LSTM model, which i saw performs really good with midis?

Really thanks to everyone who is using their time to reply.",t2_12evdv,False,,0,False,Midi GAN - having some problems finding the best formula,[],r/learnmachinelearning,False,6,,0,,False,t3_g63ype,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587600697.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello to everyone.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m rather new to AI. I&amp;#39;m writing a midi gan as project for my thesis, but i stumbled upon some problems and i&amp;#39;d be really grateful if someone could help me out. I&amp;#39;m willing to share the code too, if it is needed (the project is Tensorflow/Keras).&lt;/p&gt;

&lt;p&gt;I started the project building an interpreter to generate matrices from midi and the other way around, so that i could make everything work with fully-connected generator and discriminator. This method outputted decent files, but after some epochs (around 3500) something breaks and the discriminator accuracy starts going down until hits 0,00%, and from that model the output matrices will be solely composed by zeros.&lt;/p&gt;

&lt;p&gt;Finding those difficulties i switched to a convolutional model, but it performed worse: the discriminator and generator loss do not vary during the execution, while the discriminator accuracy is always at 50% before dropping to 0% within 5 to 10 epochs. I even tried to make each nn more or least powerful by skipping some learning phase but didn&amp;#39;t work.&lt;/p&gt;

&lt;p&gt;What could i be doing wrong? How could i fix those models i built? Or even should i discard the midi interpreter and move along with a LSTM model, which i saw performs really good with midis?&lt;/p&gt;

&lt;p&gt;Really thanks to everyone who is using their time to reply.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g63ype,True,,nx_is_here,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g63ype/midi_gan_having_some_problems_finding_the_best/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g63ype/midi_gan_having_some_problems_finding_the_best/,155203,1587571897.0,0,,False,,,,
,learnmachinelearning,,t2_5hbs9pnn,False,,0,False,"[Recurrent Neural Networks - RNN] [Article] My latest article on Towards Data Science, discussing Recurrent Neural Networks as well as some special types of cells such as LSTMs and GRUs, plus the encoder-decoder architecture combined with attention mechanisms. Happy reading!",[],r/learnmachinelearning,False,6,,0,93.0,False,t3_g63mbw,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/7E4k9rssGIo9CcPH04Hu9PdUZetiXTj6XMjjqkIKH8w.jpg,False,,[],{},link,,False,,1587599572.0,text,6,,,text,towardsdatascience.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?auto=webp&amp;s=fdd1eabcf8d67902a605a321a4c8044156fe7446', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ccce4635b498b610d6c186db583044364f6e1a50', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=123a2eaf3103a70a4c33454ea96de51c3a5d3012', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb69c0782158217a0237ddda0487572df9209e8e', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=39f00765aee17cb6e36f9149bd175ec713387379', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f52376c8575ccffccb963bf924952141dd551f10', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/vFg7Lkxn6w8uu7wF8Oi0HTBwPwn1mL8EBWc5QEPa7f4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=36f496594411cfc780482c4f84e8bbb704a919ca', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'k8Iecsvbi5bYA7rJwSC05aPnEUGjjVqhFt_MBJHNqDY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g63mbw,True,,ismailmebsout,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g63mbw/recurrent_neural_networks_rnn_article_my_latest/,all_ads,False,https://towardsdatascience.com/recurrent-neural-networks-b7719b362c65,155203,1587570772.0,0,,False,,,,
,learnmachinelearning,"So I am in the requirement gathering phase of implementing CNN project for a use case. I am eager to know how picking up a suitable model for the particular use case is done in the AI/ML industry. I saw there are a lot of CNN architectures but how to choose which architecture would suit my requirement? 

&amp;#x200B;

Is it done only in a trial by error basis or are there any specific method for picking the right architecture? If it is a trial by error process, isnt it cumbersome? If we need to use only trial by error method, how long is it going to help us?",t2_2aiim15v,False,,0,False,How to choose a CNN model for a particular use case?,[],r/learnmachinelearning,False,6,,0,,False,t3_g62wmd,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587597279.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I am in the requirement gathering phase of implementing CNN project for a use case. I am eager to know how picking up a suitable model for the particular use case is done in the AI/ML industry. I saw there are a lot of CNN architectures but how to choose which architecture would suit my requirement? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is it done only in a trial by error basis or are there any specific method for picking the right architecture? If it is a trial by error process, isnt it cumbersome? If we need to use only trial by error method, how long is it going to help us?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g62wmd,True,,pikadhu,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g62wmd/how_to_choose_a_cnn_model_for_a_particular_use/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g62wmd/how_to_choose_a_cnn_model_for_a_particular_use/,155203,1587568479.0,0,,False,,,,
,learnmachinelearning,"How to derive the equations for Centerness and Polar-Centerness in [PolarMask](https://arxiv.org/abs/1909.13226) ?

&amp;#x200B;

https://preview.redd.it/hpjx2dj1ydu41.png?width=1481&amp;format=png&amp;auto=webp&amp;s=dc89ac807c1bea4064809ec320620a08c5c0304b",t2_bpftl,False,,0,False,Centerness and Polar-Centerness in BlendMask,[],r/learnmachinelearning,False,6,,0,71.0,False,t3_g62ug6,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/TQSRW-gi6zXIOOz-jcPsCykryoftDM6ttg_KdBozQoU.jpg,1587572665.0,,[],{},,,True,,1587597088.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How to derive the equations for Centerness and Polar-Centerness in &lt;a href=""https://arxiv.org/abs/1909.13226""&gt;PolarMask&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/hpjx2dj1ydu41.png?width=1481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc89ac807c1bea4064809ec320620a08c5c0304b""&gt;https://preview.redd.it/hpjx2dj1ydu41.png?width=1481&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc89ac807c1bea4064809ec320620a08c5c0304b&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g62ug6,True,,promach,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g62ug6/centerness_and_polarcenterness_in_blendmask/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g62ug6/centerness_and_polarcenterness_in_blendmask/,155203,1587568288.0,1,,False,,,"{'hpjx2dj1ydu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84177da1f5cf4927249e4c5b9313af0d40fe89c0'}, {'y': 110, 'x': 216, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=631a1fc68013536545423e3e5b3bbce4030d29d3'}, {'y': 163, 'x': 320, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=94333144ae23f5404fa085b8fa164a7bd90f3596'}, {'y': 327, 'x': 640, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=50fb80bbcdeef98be2c7ee580497ab3b72fa7225'}, {'y': 491, 'x': 960, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dac3beb0b7b4efbc8a7f81a99cbe6599a48861d6'}, {'y': 552, 'x': 1080, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1efd44b340c13bf74d8193afc7c7392b76135532'}], 's': {'y': 758, 'x': 1481, 'u': 'https://preview.redd.it/hpjx2dj1ydu41.png?width=1481&amp;format=png&amp;auto=webp&amp;s=dc89ac807c1bea4064809ec320620a08c5c0304b'}, 'id': 'hpjx2dj1ydu41'}}",
,learnmachinelearning,,,False,,0,False,[cross post],[],r/learnmachinelearning,False,6,,0,,False,t3_g62mb3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,,default,False,,,{},,,False,,1587596365.0,text,6,,,,reddit.com,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g62mb3,True,,[deleted],,0,False,all_ads,False,[],,dark,/r/learnmachinelearning/comments/g62mb3/cross_post/,all_ads,False,https://www.reddit.com/r/MachineLearning/comments/g62lt8/neuralnetworkfromscratchandmnist_thoughts/,155203,1587567565.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Remove unwanted obstructions from a short sequence of images captured by a moving camera,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g626zt,False,light,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/nreFsGrXQVw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Remove unwanted obstructions from a short sequence of images captured by a moving camera', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/nreFsGrXQVw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nreFsGrXQVw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/nreFsGrXQVw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g626zt', 'height': 338}",Discussion,False,0,,False,https://b.thumbs.redditmedia.com/AhgV1lOdDVxTy_OZMe8rM8yAqkC33Qg4q4_e7hskVGY.jpg,False,,[],{},rich:video,,False,,1587594899.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EVjRkAnhsGZXZn_8irw11dmTckHV1OZKwGwSj93ayDs.jpg?auto=webp&amp;s=637893f042cb0ee29d5c415214f81e78efed3af5', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/EVjRkAnhsGZXZn_8irw11dmTckHV1OZKwGwSj93ayDs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6576bb055d3a6e2010fcb704b05656eeebd562eb', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/EVjRkAnhsGZXZn_8irw11dmTckHV1OZKwGwSj93ayDs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56fcfcdf73d54069e1b2265e8444b9881e69a265', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/EVjRkAnhsGZXZn_8irw11dmTckHV1OZKwGwSj93ayDs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f02ea3dbd59dbab23d00a5b8a308a9509b9023f3', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'QbBxHSYiPpmXWJtP85np1AD_ONr00mtVzftmZXagiF0'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g626zt,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g626zt/remove_unwanted_obstructions_from_a_short/,all_ads,False,https://youtu.be/nreFsGrXQVw,155203,1587566099.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Remove unwanted obstructions from a short sequence of images captured by a moving camera', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/nreFsGrXQVw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/nreFsGrXQVw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"I am trying to create a speaker verification system in Python. So far i found some code the helps extract the mfcc&amp;delta features of audio files, created GMM models based on 5 different voice samples (.wav) and im currently working on the verification part. The code that i found is able to recognise the spoken user based on the highest log\_likelihood compared with the models that already exist. But when a new user tries to access the system there isn't limit that can prevent him/her to access the system since he/she is unauthorised.

    for i in range(len(models)):    
     gmm = models[i]     
    scores = np.array(gmm.score(vector))     
    log_likelihood[i] = scores.sum()

Based on the code above and after a few tests i made with different .wav files i observed that when i had a trained model based on my voice samples, everytime i tried to access it the score was significantly low (around -25) compared to other voices which is exactly how the program is supposed to work. So i had the idea of setting a value lets say -25 as a limit.

    if scores &lt; -25: 
    print(""Unauthorised"") 
    else: 
    print(""Authorised"")

I know that this is not the best option but i am currently struggling since my lack of experience around ""voice recognition/identification etc"". I tried searching for threshold value as mentioned in other questions but i'm not sure how to do that. Any help would be appreciated.",t2_5v4v8j38,False,,0,False,Setting a value for speaker verification,[],r/learnmachinelearning,False,6,,0,,False,t3_g62397,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587594537.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create a speaker verification system in Python. So far i found some code the helps extract the mfcc&amp;amp;delta features of audio files, created GMM models based on 5 different voice samples (.wav) and im currently working on the verification part. The code that i found is able to recognise the spoken user based on the highest log_likelihood compared with the models that already exist. But when a new user tries to access the system there isn&amp;#39;t limit that can prevent him/her to access the system since he/she is unauthorised.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in range(len(models)):    
 gmm = models[i]     
scores = np.array(gmm.score(vector))     
log_likelihood[i] = scores.sum()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Based on the code above and after a few tests i made with different .wav files i observed that when i had a trained model based on my voice samples, everytime i tried to access it the score was significantly low (around -25) compared to other voices which is exactly how the program is supposed to work. So i had the idea of setting a value lets say -25 as a limit.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if scores &amp;lt; -25: 
print(&amp;quot;Unauthorised&amp;quot;) 
else: 
print(&amp;quot;Authorised&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know that this is not the best option but i am currently struggling since my lack of experience around &amp;quot;voice recognition/identification etc&amp;quot;. I tried searching for threshold value as mentioned in other questions but i&amp;#39;m not sure how to do that. Any help would be appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g62397,True,,spearo1996,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g62397/setting_a_value_for_speaker_verification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g62397/setting_a_value_for_speaker_verification/,155203,1587565737.0,0,,False,,,,
,learnmachinelearning,"hey has anyone had experience with using a raspberry pi with python and tensor. I'm currently researching rn and I'm worried (more like know) that the data sets I'm going to use when training will run slow on my PC from 2008. I know little about pi but can I use it as an external processing unit in addition to my pc or will I have to run it all on the pi. Any help or direction, in general, would be appreciated. Thanks",t2_22h8833r,False,,0,False,raspberry pi and ai,[],r/learnmachinelearning,False,6,,0,,False,t3_g5x9ir,False,dark,0.67,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587572130.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;hey has anyone had experience with using a raspberry pi with python and tensor. I&amp;#39;m currently researching rn and I&amp;#39;m worried (more like know) that the data sets I&amp;#39;m going to use when training will run slow on my PC from 2008. I know little about pi but can I use it as an external processing unit in addition to my pc or will I have to run it all on the pi. Any help or direction, in general, would be appreciated. Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5x9ir,True,,yewpew,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5x9ir/raspberry_pi_and_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5x9ir/raspberry_pi_and_ai/,155203,1587543330.0,0,,False,,,,
,learnmachinelearning,"My background: I'm currently undergoing Ph.D in Radiology (medicine) field. My area of research interest are machine learning/ deep learning. I'm aiming to get postdoc positions in near future. Most postdoc positions I come across requires some short of imaging background in medicine which I have but l lack programming background/imaging analysis (lack professional certificates). 

Given that AI is taking over everything in Radiology. I want to retain my medical imaging background but at the same time learn more about AI and make it more useful in my field. If I want to jump start from medical imaging field to deep learning/imaging analysis what are my career options? I am looking for some level of course certificate which can prove my ability. 

What's the right course for me? I'm thinking to do another Ph.D in computational medicine or something similar. Is it feasible?",t2_20mjgmt,False,,0,False,What should I study for a career in medical imaging analysis/Deep learning/AI?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5vzzg,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Question,False,4,,False,self,False,,[],{},,,True,,1587565384.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;My background: I&amp;#39;m currently undergoing Ph.D in Radiology (medicine) field. My area of research interest are machine learning/ deep learning. I&amp;#39;m aiming to get postdoc positions in near future. Most postdoc positions I come across requires some short of imaging background in medicine which I have but l lack programming background/imaging analysis (lack professional certificates). &lt;/p&gt;

&lt;p&gt;Given that AI is taking over everything in Radiology. I want to retain my medical imaging background but at the same time learn more about AI and make it more useful in my field. If I want to jump start from medical imaging field to deep learning/imaging analysis what are my career options? I am looking for some level of course certificate which can prove my ability. &lt;/p&gt;

&lt;p&gt;What&amp;#39;s the right course for me? I&amp;#39;m thinking to do another Ph.D in computational medicine or something similar. Is it feasible?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g5vzzg,True,,Erythromycin500,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5vzzg/what_should_i_study_for_a_career_in_medical/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5vzzg/what_should_i_study_for_a_career_in_medical/,155203,1587536584.0,0,,False,,,,
,learnmachinelearning,,t2_qtig0,False,,0,False,How To Predict StarCraft II Battle Outcomes with Machine Learning,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,119.0,False,t3_g5y8xa,False,light,0.75,,public,2,0,{},140.0,,False,[],,False,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/YamlUXjpuyhCUmCBoCkgzh0oV8ZR6TcROyAl8YKsQlU.jpg,False,,[],{},link,,False,,1587577585.0,richtext,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_7G_7DIF-LBmrs3MRbypzBFtnFYLuVz1uc7f44bInVw.jpg?auto=webp&amp;s=4974108a8165837de68ecfcf7805eaaa51ef6cc1', 'width': 642, 'height': 546}, 'resolutions': [{'url': 'https://external-preview.redd.it/_7G_7DIF-LBmrs3MRbypzBFtnFYLuVz1uc7f44bInVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=118ba5f01cd7355c02464566f2943c86ac0d2290', 'width': 108, 'height': 91}, {'url': 'https://external-preview.redd.it/_7G_7DIF-LBmrs3MRbypzBFtnFYLuVz1uc7f44bInVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70e30bbf943b9a4bbf1e7939e29fb069ae0138d3', 'width': 216, 'height': 183}, {'url': 'https://external-preview.redd.it/_7G_7DIF-LBmrs3MRbypzBFtnFYLuVz1uc7f44bInVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac8e4c904f685e65eb98176d3405d8ad849f339b', 'width': 320, 'height': 272}, {'url': 'https://external-preview.redd.it/_7G_7DIF-LBmrs3MRbypzBFtnFYLuVz1uc7f44bInVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=33a6f2c5cce1519f4ff968a58e29a3a3d7340597', 'width': 640, 'height': 544}], 'variants': {}, 'id': 'gVSqpY0P0SQNbF_WHveHn3NX1MPMHrsdgnKY3A9VHcs'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g5y8xa,True,,Fewthp,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5y8xa/how_to_predict_starcraft_ii_battle_outcomes_with/,all_ads,False,https://medium.com//how-to-predict-starcraft-ii-battle-outcomes-with-machine-learning-40d527a2f7a4?source=friends_link&amp;sk=3c24f22d38fa17acb6fd268a671cbccf,155203,1587548785.0,0,,False,,,,
,learnmachinelearning,,t2_64nsfwjs,False,,0,False,Tutorial on how do CART models work. More info in the comments,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_g60t49,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/1ZwouRGqu4sxZRP4XWMUuY4HHWp2MSU_skl_rlNrX4g.jpg,False,,[],{},link,,False,,1587589749.0,text,6,,,text,defme.xyz,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?auto=webp&amp;s=87a5e91ee51430ca045e0f428a1fd55d19621a28', 'width': 1918, 'height': 1077}, 'resolutions': [{'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fd4a458434ef03f7dc9b8892d64b15d3d4cfff6', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47d36d5f77e5b2cd2303ceb24d74dbc39094717e', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ad4a53e940ee05a017f882424972bcc250e9a02', 'width': 320, 'height': 179}, {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=762049d326078f5458eb087f13f34ce1096be290', 'width': 640, 'height': 359}, {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=60a4d98f3754e07bcbe7cd4b068d34ac2ea01468', 'width': 960, 'height': 539}, {'url': 'https://external-preview.redd.it/DTFzfd3qxyjFHcW8LJ7BfwwT6emVPFruReWd3YGnqFI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f1c39359de97d8652f3c97f7b1962886ed7bbf7', 'width': 1080, 'height': 606}], 'variants': {}, 'id': '5oHtxbkoViQ7Xu1nt9OEEllImKBUW0o6HGtd8FlfTd8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g60t49,True,,keinekatharsis,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60t49/tutorial_on_how_do_cart_models_work_more_info_in/,all_ads,False,https://defme.xyz/post/cart/,155203,1587560949.0,0,,False,,,,
,learnmachinelearning,"I'm writing an algorithm to generate simple poems, such as haikus. I plan to use a recurrent neural net operating on the word level, and besides the normal vector representing each word to be fed into the LM, I would like to incorporate extra features, like the number of syllables that the word has.

The thing is, I would like to expand the training data for the grammar, so that it can use more words than just those found in preexisting haikus, as well as ensuring enough data so that the writing is coherent and grammatically correct.

The haikus ideally would just train, with the additional feature of # syllables, the structure, 5 7 5.

I know that with convolutional neural nets for image classification, we can pretrain many layers on a large swathe of images, and then with base, we can essentially swap in a classifier on the end that learns a specific thing, like a cat.

Has anyone done anything similar with text, to learn a basic conception of grammar on a big dataset, and then specific structure on a smaller one? Thanks!",t2_3jd0k8t1,False,,0,False,Question about Text Generation,[],r/learnmachinelearning,False,6,,0,,False,t3_g60g1x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587588264.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m writing an algorithm to generate simple poems, such as haikus. I plan to use a recurrent neural net operating on the word level, and besides the normal vector representing each word to be fed into the LM, I would like to incorporate extra features, like the number of syllables that the word has.&lt;/p&gt;

&lt;p&gt;The thing is, I would like to expand the training data for the grammar, so that it can use more words than just those found in preexisting haikus, as well as ensuring enough data so that the writing is coherent and grammatically correct.&lt;/p&gt;

&lt;p&gt;The haikus ideally would just train, with the additional feature of # syllables, the structure, 5 7 5.&lt;/p&gt;

&lt;p&gt;I know that with convolutional neural nets for image classification, we can pretrain many layers on a large swathe of images, and then with base, we can essentially swap in a classifier on the end that learns a specific thing, like a cat.&lt;/p&gt;

&lt;p&gt;Has anyone done anything similar with text, to learn a basic conception of grammar on a big dataset, and then specific structure on a smaller one? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g60g1x,True,,goatsinthegraveyard,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g60g1x/question_about_text_generation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g60g1x/question_about_text_generation/,155203,1587559464.0,0,,False,,,,
,learnmachinelearning,"Hi guys! Long time lurker first time posting. I'll go into a little bit of my background but if you want to skip down to the workflow go right ahead!

**Background:**

I recently obtained my undergraduate degree in Physics and spent a lot of time working in Python to simulate physical phenomena. I got into an undergraduate research position working with student data and was ultimately introduced to the area of Data Science. I immediately grew fascinated and ended up doing basic EDA/ML projects in R &amp; Python while taking some statistics courses.

As soon as I graduated I started looking for roles as a junior analyst to get more comfortable working with data at the professional level. As a result I began to learn different skills such as SQL and Tableau and other tips and tricks, however I believe I have never gotten down the right workflow for my projects. I searched online for examples and was immediately overwhelmed with posts from Medium, TowardsDataScience, StackOverflow, StatsExchange, Blogs. There a lot of differing opinions on how to implement ideas and while I support expressing knowledge, it can honestly be overwhelming.

\---------------------------------------------------------------------------------------------------------------------------------------------

**Project :** Mortality Predictions for Neuro ICU Patients from the open access - [eICU Collaborative Research Database](https://eicu-crd.mit.edu/)

**Problem :** Binary classification problem with *N* = \~12000 and *p* predictors.

**Workflow :**

&amp;#x200B;

* SQL
   * Query the  data from a local SQL Database I setup. (To learn how to use SQL, and  also the entire database is around 50 gb split around 31 tables)
   * Perform  most, if not all of my data aggregations within SQL to try and uncover basic patterns and relations between columns.
   * Choose variables that could possibly relate towards what I'm predicting (though I had help from reading the documentation of the database and other medical papers)
   * Write a query that I will use to move data from SQL to Python.
* Python
   * Setup the connection of my PostgreSQL database and query the data into a blank Jupyter Notebook
   * Immediately begin with splitting the data into stratified training/validation/test sets with a split of 8000/2000/2000
   * Exploratory Data Analysis/Data Wrangling on the strictly the **training** set / pre-process the data for the ML algorithms
   * Train on the  8000 training samples, validate on the 2000 training samples
   * Repeat the last step using different models and different parameters
   * Choose the  best model from training/validating models and re-train it on the 10000       samples (training + validation).
   * Take this model and use it on the test set to obtain the results of your generalized model.
   * Cry cause of  horrible performance.

I want to make sure both my SQL and Python workflow is at least on the right track. I left out some information because I wanted to explain my workflow as simple as I could so its easier to point out holes. If you would like me to go deeper into explaining other steps I am more than happy to oblige!

I recently started picking up some OOP and also trying to code my own applications and portfolio website. I saw the value in creating classes and functions so I do my best to refactor my code into functions whenever I can, however I haven't looked up the concepts of unit testing to be honest. I have a couple tutorials in mind but if you have any resources that would be awesome.

tl;dr

I've done a couple of ML projects but I consider myself a complete beginner and would like your critiques on my workflow so I can end any and all bad habits early.

edit 1: fixed some spacing",t2_3xbz2fe1,False,,0,False,Beginner Project - Critique My Workflow,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5u60r,False,light,1.0,,public,4,0,{},,,False,[],,False,False,,{},HELP,False,4,,False,self,False,,[],{},,,True,,1587556421.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys! Long time lurker first time posting. I&amp;#39;ll go into a little bit of my background but if you want to skip down to the workflow go right ahead!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I recently obtained my undergraduate degree in Physics and spent a lot of time working in Python to simulate physical phenomena. I got into an undergraduate research position working with student data and was ultimately introduced to the area of Data Science. I immediately grew fascinated and ended up doing basic EDA/ML projects in R &amp;amp; Python while taking some statistics courses.&lt;/p&gt;

&lt;p&gt;As soon as I graduated I started looking for roles as a junior analyst to get more comfortable working with data at the professional level. As a result I began to learn different skills such as SQL and Tableau and other tips and tricks, however I believe I have never gotten down the right workflow for my projects. I searched online for examples and was immediately overwhelmed with posts from Medium, TowardsDataScience, StackOverflow, StatsExchange, Blogs. There a lot of differing opinions on how to implement ideas and while I support expressing knowledge, it can honestly be overwhelming.&lt;/p&gt;

&lt;p&gt;---------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project :&lt;/strong&gt; Mortality Predictions for Neuro ICU Patients from the open access - &lt;a href=""https://eicu-crd.mit.edu/""&gt;eICU Collaborative Research Database&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem :&lt;/strong&gt; Binary classification problem with &lt;em&gt;N&lt;/em&gt; = ~12000 and &lt;em&gt;p&lt;/em&gt; predictors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Workflow :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SQL

&lt;ul&gt;
&lt;li&gt;Query the  data from a local SQL Database I setup. (To learn how to use SQL, and  also the entire database is around 50 gb split around 31 tables)&lt;/li&gt;
&lt;li&gt;Perform  most, if not all of my data aggregations within SQL to try and uncover basic patterns and relations between columns.&lt;/li&gt;
&lt;li&gt;Choose variables that could possibly relate towards what I&amp;#39;m predicting (though I had help from reading the documentation of the database and other medical papers)&lt;/li&gt;
&lt;li&gt;Write a query that I will use to move data from SQL to Python.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Python

&lt;ul&gt;
&lt;li&gt;Setup the connection of my PostgreSQL database and query the data into a blank Jupyter Notebook&lt;/li&gt;
&lt;li&gt;Immediately begin with splitting the data into stratified training/validation/test sets with a split of 8000/2000/2000&lt;/li&gt;
&lt;li&gt;Exploratory Data Analysis/Data Wrangling on the strictly the &lt;strong&gt;training&lt;/strong&gt; set / pre-process the data for the ML algorithms&lt;/li&gt;
&lt;li&gt;Train on the  8000 training samples, validate on the 2000 training samples&lt;/li&gt;
&lt;li&gt;Repeat the last step using different models and different parameters&lt;/li&gt;
&lt;li&gt;Choose the  best model from training/validating models and re-train it on the 10000       samples (training + validation).&lt;/li&gt;
&lt;li&gt;Take this model and use it on the test set to obtain the results of your generalized model.&lt;/li&gt;
&lt;li&gt;Cry cause of  horrible performance.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I want to make sure both my SQL and Python workflow is at least on the right track. I left out some information because I wanted to explain my workflow as simple as I could so its easier to point out holes. If you would like me to go deeper into explaining other steps I am more than happy to oblige!&lt;/p&gt;

&lt;p&gt;I recently started picking up some OOP and also trying to code my own applications and portfolio website. I saw the value in creating classes and functions so I do my best to refactor my code into functions whenever I can, however I haven&amp;#39;t looked up the concepts of unit testing to be honest. I have a couple tutorials in mind but if you have any resources that would be awesome.&lt;/p&gt;

&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve done a couple of ML projects but I consider myself a complete beginner and would like your critiques on my workflow so I can end any and all bad habits early.&lt;/p&gt;

&lt;p&gt;edit 1: fixed some spacing&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g5u60r,True,,SteveMWolf,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5u60r/beginner_project_critique_my_workflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5u60r/beginner_project_critique_my_workflow/,155203,1587527621.0,0,,False,,,,
,learnmachinelearning,,t2_pj954,False,,0,False,How to get yourself to the point of being able to do research in Machine Learning ?,[],r/learnmachinelearning,False,6,,0,,False,t3_g5g9at,False,dark,0.93,,public,46,0,{},,,False,[],,False,False,,{},,False,46,,False,default,False,,[],{},,,False,,1587508292.0,text,6,,,text,self.math,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5g9at,True,,Zophike1,,9,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5g9at/how_to_get_yourself_to_the_point_of_being_able_to/,all_ads,False,/r/math/comments/g4ybn2/how_to_get_yourself_to_the_point_of_being_able_to/,155203,1587479492.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'math', 'selftext': 'Lets say you have a masters degree in mathematics and certain areas that you like in mathematics (or any other similar science subject). How do you get yourself to the point of being able to conduct research as a hobby (ie no phd/industrial lab input)\n\n1. Do you just exhaust textbooks in a niche and then read a good quantity of papers?\n2. How do you find (relatively easier) problems upon which to work? This seems like the harder problem\n\nWhat would be a good strategy for this?\n\nPossibly this question should be split by pure vs applied mathematics as applied mathematics papers are pretty accessible ', 'author_fullname': 't2_zqp1wbh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to get yourself to the point of being able to do research in mathematics', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/math', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_g4ybn2', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 446, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 446, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1587451711.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587436034.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.math', 'allow_live_comments': True, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lets say you have a masters degree in mathematics and certain areas that you like in mathematics (or any other similar science subject). How do you get yourself to the point of being able to conduct research as a hobby (ie no phd/industrial lab input)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Do you just exhaust textbooks in a niche and then read a good quantity of papers?&lt;/li&gt;\n&lt;li&gt;How do you find (relatively easier) problems upon which to work? This seems like the harder problem&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What would be a good strategy for this?&lt;/p&gt;\n\n&lt;p&gt;Possibly this question should be split by pure vs applied mathematics as applied mathematics papers are pretty accessible &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qh0n', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g4ybn2', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'SilurianWenlock', 'discussion_type': None, 'num_comments': 64, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/math/comments/g4ybn2/how_to_get_yourself_to_the_point_of_being_able_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/math/comments/g4ybn2/how_to_get_yourself_to_the_point_of_being_able_to/', 'subreddit_subscribers': 1140611, 'created_utc': 1587407234.0, 'num_crossposts': 5, 'media': None, 'is_video': False}]",t3_g4ybn2,,
,learnmachinelearning,"Learning rate, optimal batch_size  if it is not hardcoded, what else?

Cheers.",t2_3v76dvmu,False,,0,False,What hyperparameters does Kerasâ€™ LSTM model learns during validation?,[],r/learnmachinelearning,False,6,,0,,False,t3_g5z7fw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587582559.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Learning rate, optimal batch_size  if it is not hardcoded, what else?&lt;/p&gt;

&lt;p&gt;Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5z7fw,True,,19Summer,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5z7fw/what_hyperparameters_does_keras_lstm_model_learns/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5z7fw/what_hyperparameters_does_keras_lstm_model_learns/,155203,1587553759.0,0,,False,,,,
,learnmachinelearning,"It doesn't matter if it's something trivial. As long as it's new information about machine learning you didn't know until this week, feel free to share!",t2_6l4z3,False,,0,False,TWIL (This Week I Learned) - Share something new that you have learned this week!,[],r/learnmachinelearning,False,6,,0,,False,t3_g5wh3o,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,True,self,False,,[],{},,,True,,1587567874.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It doesn&amp;#39;t matter if it&amp;#39;s something trivial. As long as it&amp;#39;s new information about machine learning you didn&amp;#39;t know until this week, feel free to share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,g5wh3o,True,,AutoModerator,,3,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5wh3o/twil_this_week_i_learned_share_something_new_that/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5wh3o/twil_this_week_i_learned_share_something_new_that/,155203,1587539074.0,0,,False,,,,
,learnmachinelearning,"Hi,

I've used [this repo](https://github.com/xuannianz/EfficientDet). They used COCO dataset. I want to improve model. So I will train it again with 1500 images which I've labeled. My question is: Can I use the val images (it has 5000 images) on COCO dataset again?",t2_4aky7agd,False,,0,False,Can use same val images on transfer learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g5yq7i,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1587580182.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve used &lt;a href=""https://github.com/xuannianz/EfficientDet""&gt;this repo&lt;/a&gt;. They used COCO dataset. I want to improve model. So I will train it again with 1500 images which I&amp;#39;ve labeled. My question is: Can I use the val images (it has 5000 images) on COCO dataset again?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AlAh3fSL7rcl4vuXDYT9jrxREXB9zxg77LalwRsGm24.jpg?auto=webp&amp;s=264079ab623b2d0147af30a3b459cda4b4e33361', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/AlAh3fSL7rcl4vuXDYT9jrxREXB9zxg77LalwRsGm24.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=874c1cf03f6fe5dea4dfee6797cdce319a4a1271', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/AlAh3fSL7rcl4vuXDYT9jrxREXB9zxg77LalwRsGm24.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=00469abaaba49191052cdcf4527b7e9ae4e33d64', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/AlAh3fSL7rcl4vuXDYT9jrxREXB9zxg77LalwRsGm24.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b770d1c2a878906e0dc4f45a60c8226a07579918', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'usUfEOQ8rifNLNCQUA4fZT9by1rV13ktNy7EvaS64X8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5yq7i,True,,hernancrespo89,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5yq7i/can_use_same_val_images_on_transfer_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5yq7i/can_use_same_val_images_on_transfer_learning/,155203,1587551382.0,0,,False,,,,
,learnmachinelearning," I'm doing some research to find a topic for my bachelor thesis in mathematics and I'm looking for a direction, so far I thought about matrix product states used in Tensorflow. Are there other topics that would be a good fit for a bachelor thesis regarding machine learning? Ideally I look for a topic I can start with in the bachelor thesis and finish it in the master thesis since I will write one right after the other.",t2_47vdfp68,False,,0,False,What are some current topics in machine learning for a Bachelor/ Master thesis in mathematics?,[],r/learnmachinelearning,False,6,,0,,False,t3_g5yoqm,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587579969.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing some research to find a topic for my bachelor thesis in mathematics and I&amp;#39;m looking for a direction, so far I thought about matrix product states used in Tensorflow. Are there other topics that would be a good fit for a bachelor thesis regarding machine learning? Ideally I look for a topic I can start with in the bachelor thesis and finish it in the master thesis since I will write one right after the other.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5yoqm,True,,Chiray1,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5yoqm/what_are_some_current_topics_in_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5yoqm/what_are_some_current_topics_in_machine_learning/,155203,1587551169.0,0,,False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,Why Python is Still the Ruling Language in the AI world,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,60.0,False,t3_g5yhar,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://a.thumbs.redditmedia.com/usr3rt9We648KD7qAum2AVyaTlFRT8EIMCn7msdlU58.jpg,False,,[],{},link,,False,,1587578861.0,richtext,6,,,text,brainstormingbox.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?auto=webp&amp;s=915731b16d5ad83a8cb2d75a1eb4a8498a6ceeda', 'width': 1800, 'height': 772}, 'resolutions': [{'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a47ebea997ea2e3057e44c20746d03e1f7ab3d', 'width': 108, 'height': 46}, {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=237b416b20ec4afea174cbc3af38defca661d3e9', 'width': 216, 'height': 92}, {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfb78d08c4841c2dc70b53c624a3772a1228becb', 'width': 320, 'height': 137}, {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea75a5fae5b647dff83b1e49e02c8659d5cea830', 'width': 640, 'height': 274}, {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a9457caa21e20ed19fa79cb90899e3356ef96ee', 'width': 960, 'height': 411}, {'url': 'https://external-preview.redd.it/CHLRy9FGPIlYFRXyTrJfhMqTlxr2oQrh2xCZy_EwWYM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79cc37133813a4db36c8fb51ba02285be445c988', 'width': 1080, 'height': 463}], 'variants': {}, 'id': 'te3VQwK8ol59NZgZCjQzyq28-2PE_ZktfGSXlMPOOSk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g5yhar,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5yhar/why_python_is_still_the_ruling_language_in_the_ai/,all_ads,False,http://brainstormingbox.org/why-python-is-still-the-ruling-language-in-the-ai-world/,155203,1587550061.0,0,,False,,,,
,learnmachinelearning,"I am new to time series analysis. I have a ACF plot of the time series, which is added below.

&amp;#x200B;

https://preview.redd.it/7vyo1kk3ecu41.png?width=380&amp;format=png&amp;auto=webp&amp;s=c5ee45a4bc9f4a1d0d927de66eff077b2b96eb96

From what I have read, If there is a peak in correlation coefficient above confidence interval then the point will be used as a lag for auto-correlation. It seems that there is not strong auto-correlation at any point. Also , no seasonality is present in the time series as the pattern doesn't seem to repeat. Data is collected on monthly basis, but there are no peaks on 12,24.. marks. 

Is my interpretation correct? If not, what can we say about auto-correlation and seasonality? What more information can we get from the ACF plot ?",t2_4u32m7ko,False,,0,False,Interpretation of seasonality and auto-correlation from the ACF plot of time series.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,97.0,False,t3_g5yf7d,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},HELP,False,1,,False,https://a.thumbs.redditmedia.com/4h05_xRnX5WrwKHSMgJ-iPWTxTomPOYdBzB_xTR81s0.jpg,False,,[],{},,,True,,1587578552.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am new to time series analysis. I have a ACF plot of the time series, which is added below.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7vyo1kk3ecu41.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ee45a4bc9f4a1d0d927de66eff077b2b96eb96""&gt;https://preview.redd.it/7vyo1kk3ecu41.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5ee45a4bc9f4a1d0d927de66eff077b2b96eb96&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From what I have read, If there is a peak in correlation coefficient above confidence interval then the point will be used as a lag for auto-correlation. It seems that there is not strong auto-correlation at any point. Also , no seasonality is present in the time series as the pattern doesn&amp;#39;t seem to repeat. Data is collected on monthly basis, but there are no peaks on 12,24.. marks. &lt;/p&gt;

&lt;p&gt;Is my interpretation correct? If not, what can we say about auto-correlation and seasonality? What more information can we get from the ACF plot ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,g5yf7d,True,,sdbhavsar3,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5yf7d/interpretation_of_seasonality_and_autocorrelation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5yf7d/interpretation_of_seasonality_and_autocorrelation/,155203,1587549752.0,0,,False,,,"{'7vyo1kk3ecu41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 75, 'x': 108, 'u': 'https://preview.redd.it/7vyo1kk3ecu41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=231866c0d1880dd88ce464066786addbcf19c097'}, {'y': 150, 'x': 216, 'u': 'https://preview.redd.it/7vyo1kk3ecu41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7fb4e997a5affa1e7ff7af3a423e0f7579ca73a5'}, {'y': 222, 'x': 320, 'u': 'https://preview.redd.it/7vyo1kk3ecu41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3629ad0ca0dd01955091a580fc19bd6817bd5df4'}], 's': {'y': 264, 'x': 380, 'u': 'https://preview.redd.it/7vyo1kk3ecu41.png?width=380&amp;format=png&amp;auto=webp&amp;s=c5ee45a4bc9f4a1d0d927de66eff077b2b96eb96'}, 'id': '7vyo1kk3ecu41'}}",
,learnmachinelearning,,t2_68t27oaz,False,,0,False,Reading multiple files in Tensorflow 2,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5vsjb,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,default,False,,[],{},,,False,,1587564312.0,richtext,6,,,text,biswajitsahoo1111.github.io,False,,,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g5vsjb,True,,biswajitsahoo1111,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5vsjb/reading_multiple_files_in_tensorflow_2/,all_ads,False,https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/,155203,1587535512.0,0,,False,,,,
,learnmachinelearning,"Hi, I'm teaching a class of young students about machine learning and I'm looking for an interactive playground to inspect data, train it with different algorithms, and evaluate the results.

I know a few demos on visual data but couldn't find anything for tabular data.

The students have very limited knowledge in coding, and we've got no budget for Azure etc.

Have you heard of something that can fit our needs?

&amp;#x200B;

It can even be a colab notebook controlled by UI.",t2_lenun,False,,0,False,Interactive tabular data playground - without code,[],r/learnmachinelearning,False,6,,0,,False,t3_g5y842,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587577446.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m teaching a class of young students about machine learning and I&amp;#39;m looking for an interactive playground to inspect data, train it with different algorithms, and evaluate the results.&lt;/p&gt;

&lt;p&gt;I know a few demos on visual data but couldn&amp;#39;t find anything for tabular data.&lt;/p&gt;

&lt;p&gt;The students have very limited knowledge in coding, and we&amp;#39;ve got no budget for Azure etc.&lt;/p&gt;

&lt;p&gt;Have you heard of something that can fit our needs?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;It can even be a colab notebook controlled by UI.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5y842,True,,Fartin_dog,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5y842/interactive_tabular_data_playground_without_code/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5y842/interactive_tabular_data_playground_without_code/,155203,1587548646.0,0,,False,,,,
,learnmachinelearning,Pairwise Euclidean distance calculation is described in [this blog](https://omoindrot.github.io/triplet-loss). I assume this method utilizes GPU's fast matrix calculation. I wonder if we can calculate pairwise Manhattan distance in a similar manner?,t2_4cpzgig7,False,,0,False,Is there any way to calculate pairwise Manhattan distance by matrix multiplication or matrix calculation in general?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5tjyp,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},self,,True,,1587553779.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Pairwise Euclidean distance calculation is described in &lt;a href=""https://omoindrot.github.io/triplet-loss""&gt;this blog&lt;/a&gt;. I assume this method utilizes GPU&amp;#39;s fast matrix calculation. I wonder if we can calculate pairwise Manhattan distance in a similar manner?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?auto=webp&amp;s=88156354eccd0d641b9ccb795667b1d6a4210480', 'width': 1016, 'height': 763}, 'resolutions': [{'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9c966fb2f9ca41e0cf1a384367a66e9e75d5d84', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13a5b3d118b916efc3a4e53a7ca2c66bc342783a', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4010d557b2d624cf08e6b6369eb28e0489f3aec3', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d1fad291bec3e93540ecb4be31e8efc29f5b7ae', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/beekcAW4jgqdsvfitntaBbDbc6fVvOpSgD8P3Ytgn84.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af04c0e710f157faef958b9aeec897f6bb1da492', 'width': 960, 'height': 720}], 'variants': {}, 'id': 'lEM2I32rL31BsiRLzBiJmWiMKaMsSu0LPS2HvLPT4uI'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g5tjyp,True,,biggest_oversight,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5tjyp/is_there_any_way_to_calculate_pairwise_manhattan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5tjyp/is_there_any_way_to_calculate_pairwise_manhattan/,155203,1587524979.0,0,,False,,,,
,learnmachinelearning,,t2_ggszd,False,,0,False,Track the model performance metrics in Federated training,[],r/learnmachinelearning,False,6,,0,30.0,False,t3_g5ws1e,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/HLGQ2_J3Hhx4dFVmRAuNrHx-9wZsvCI34fY8kL2-Odk.jpg,False,,[],{},link,,False,,1587569490.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?auto=webp&amp;s=3900337afd35274e036a5614e359e5b039519012', 'width': 1200, 'height': 260}, 'resolutions': [{'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e07d0e00db2e177e1c005107581f50ea33001e73', 'width': 108, 'height': 23}, {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f7961f266e204f2b989fdf0c02717dc22dcb884', 'width': 216, 'height': 46}, {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0cc3899eb24bb3d27181e5d7b07462499fbd3185', 'width': 320, 'height': 69}, {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=976378b9bd80c2560d9c630ab57a18ec2b0a7ab0', 'width': 640, 'height': 138}, {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e5530a0f7a972103da202dac1dabf15839ddcd2', 'width': 960, 'height': 208}, {'url': 'https://external-preview.redd.it/DSthM7UT579EVSWFj3a1QRLVIgUMucmo3otZwNwgoAQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52506b20fedd55560e32f4a4a1422d048dfafa54', 'width': 1080, 'height': 234}], 'variants': {}, 'id': 'kLv_pXRjVRtJZpgerJ4IiQ6Al8UvVJizD3CqH1QrvXQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5ws1e,True,,preslavrachev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5ws1e/track_the_model_performance_metrics_in_federated/,all_ads,False,https://medium.com/ki-labs-engineering/track-the-model-performance-metrics-in-federated-training-2c53f18ca9da,155203,1587540690.0,0,,False,,,,
,learnmachinelearning,"as I'm learning more, I'm making some naive realizations/assumptions about feature importance/feature engineering.

Looking for some validation (or correction) on my intuitive assumptions:  


1. Data preprocessing and feature engineering are *at least* as important, if not more important than algo selection, model training, and tuning.  Saying that another way:  its typically more effective to improve model accuracy score by engineering better features than anything you could do with model tuning.
2. The best features have 3 properties:
   1. Highly correlated with the target feature
   2. Highly uncorrelated with other features
   3. High variability of the feature values

&amp;#x200B;

am I close on those?",t2_702gf,False,,0,False,please validate/correct my assumptions about feature importance/feature engineering,[],r/learnmachinelearning,False,6,,0,,False,t3_g5s4sm,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587547870.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;as I&amp;#39;m learning more, I&amp;#39;m making some naive realizations/assumptions about feature importance/feature engineering.&lt;/p&gt;

&lt;p&gt;Looking for some validation (or correction) on my intuitive assumptions:  &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Data preprocessing and feature engineering are &lt;em&gt;at least&lt;/em&gt; as important, if not more important than algo selection, model training, and tuning.  Saying that another way:  its typically more effective to improve model accuracy score by engineering better features than anything you could do with model tuning.&lt;/li&gt;
&lt;li&gt;The best features have 3 properties:

&lt;ol&gt;
&lt;li&gt;Highly correlated with the target feature&lt;/li&gt;
&lt;li&gt;Highly uncorrelated with other features&lt;/li&gt;
&lt;li&gt;High variability of the feature values&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;am I close on those?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5s4sm,True,,ezeeetm,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5s4sm/please_validatecorrect_my_assumptions_about/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5s4sm/please_validatecorrect_my_assumptions_about/,155203,1587519070.0,0,,False,,,,
,learnmachinelearning,,t2_1328lx,False,,0,False,500 Free Computer Science Courses from the Worldâ€™s Top CS Universities,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g5mofq,False,dark,0.81,,public,6,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/6Ko40RUo-9M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '500 Free Computer Science Courses from the Worldâ€™s Top CS Universities | 2020', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/6Ko40RUo-9M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'The Last Benchers', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6Ko40RUo-9M/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCE-MW-oqDTnW1BP_GereEaw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/6Ko40RUo-9M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g5mofq', 'height': 338}",,False,6,,False,https://b.thumbs.redditmedia.com/BEfT4ryiRdW7-kWQUC3g8YKmAFZKJ9D0erdG0YjQFwk.jpg,False,,[],{},rich:video,,False,,1587528740.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/VbiSa-4vHx2D_xAF5AFNsV7GnTXPol6p44wrhd9umkY.jpg?auto=webp&amp;s=9d9f34190f4ad7cee7fe2e75e717acc7b2b3f6a9', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/VbiSa-4vHx2D_xAF5AFNsV7GnTXPol6p44wrhd9umkY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6054e4c62607a1866537b0f6a1dfe833025ba9e2', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/VbiSa-4vHx2D_xAF5AFNsV7GnTXPol6p44wrhd9umkY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de2f4e61f961c86c0fce5987e21531903f845d59', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/VbiSa-4vHx2D_xAF5AFNsV7GnTXPol6p44wrhd9umkY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5d4b1a72abf0b826c31d352b21a38c892a5742f', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'IkQ1nkXBLOXB0G71h8V0RJr7l_4CTPG4haKGBOSjTQg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5mofq,True,,AjeyKoushik,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5mofq/500_free_computer_science_courses_from_the_worlds/,all_ads,False,https://youtu.be/6Ko40RUo-9M,155203,1587499940.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': '500 Free Computer Science Courses from the Worldâ€™s Top CS Universities | 2020', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/6Ko40RUo-9M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'The Last Benchers', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/6Ko40RUo-9M/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCE-MW-oqDTnW1BP_GereEaw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_p98c3j5,False,,0,False,Template for Reading Research Papers,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g5qb4g,False,dark,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/w9MY-cTQ_VKhhQ37mytkhbw0DZzb7cd_ZMz2TzbmxWM.jpg,False,,[],{},link,,False,,1587540828.0,text,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/W1sb2cwNBxZUxvfWkqLdJlcCWgk01Zou8v48-A0v1ng.jpg?auto=webp&amp;s=6c6be5bb202e6b8adddd3259f3354ac5414c548d', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/W1sb2cwNBxZUxvfWkqLdJlcCWgk01Zou8v48-A0v1ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bea469e1f07eb88a0ac3ad1adca3c69d9300e26', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/W1sb2cwNBxZUxvfWkqLdJlcCWgk01Zou8v48-A0v1ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=11c3485628fb1dfae175660de9c333fb8e3b2d91', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/W1sb2cwNBxZUxvfWkqLdJlcCWgk01Zou8v48-A0v1ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc7bff9d4c46333b7dbefdf211734585803b0082', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'IhCELoGoL394oBp4JQhPZOmO0WFSiIeumF8qKbbv36Q'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5qb4g,True,,ktessera,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5qb4g/template_for_reading_research_papers/,all_ads,False,https://github.com/KaleabTessera/Research-Paper-Reading-Template,155203,1587512028.0,0,,False,,,,
,learnmachinelearning,"At [https://talktotransformer.com/](https://talktotransformer.com/), you can type a prompt and the transformer will autogenerate the text for you using OpenAI's GPT-2 1.5 billion parameter model.

I'm not asking how GPT-2 works, I'm asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.

And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.

I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don't think so because that would cost a shit ton of money), or am I missing something fundamental here?

This same question can be applied to this website too: [https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is](https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is)

Please keep in mind that I'm relatively new to all of this. Thanks in advance!",t2_slnv7,False,,0,False,How does the talktotransformer website work so fast?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5pddn,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,False,,[],{},,,True,,1587537491.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;At &lt;a href=""https://talktotransformer.com/""&gt;https://talktotransformer.com/&lt;/a&gt;, you can type a prompt and the transformer will autogenerate the text for you using OpenAI&amp;#39;s GPT-2 1.5 billion parameter model.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not asking how GPT-2 works, I&amp;#39;m asking something else. When I ran the GPT-2 1.5B model on the free TPU in Google Colab, it took around 20 to 40ish seconds to generate around the same about of text as the website generates per prompt.&lt;/p&gt;

&lt;p&gt;And yet, the website is somehow generating text from the prompt almost instantaneously using the 1.5B model. This is on top of all the X number of people who must be using the website at the same time I am, so it is doing text generation concurrently and near-instantaneously using a gigantic model.&lt;/p&gt;

&lt;p&gt;I am very confused. Did the creator of the website just use a lot more TPUs/GPUs behind the scenes and is letting them run 24/7 (I don&amp;#39;t think so because that would cost a shit ton of money), or am I missing something fundamental here?&lt;/p&gt;

&lt;p&gt;This same question can be applied to this website too: &lt;a href=""https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is""&gt;https://demo.allennlp.org/next-token-lm?text=AllenNLP%20is&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Please keep in mind that I&amp;#39;m relatively new to all of this. Thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g5pddn,True,,parrot15,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5pddn/how_does_the_talktotransformer_website_work_so/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5pddn/how_does_the_talktotransformer_website_work_so/,155203,1587508691.0,0,,False,,,,
,learnmachinelearning,,t2_tlqt0,False,,0,False,Created a script that runs your face through a convolutional neural network and matches it with the most similar celebrity. Here is a free link. Happy programming!,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_g53eo2,False,light,0.94,,public,232,0,{},140.0,,False,[],,False,False,,{},Project,False,232,,False,https://b.thumbs.redditmedia.com/etwVgIMtZDt8mP8Ood-rvT-OIZwtEWoiq9XAjcmFzHA.jpg,False,,[],{},link,,False,,1587452412.0,richtext,6,,,text,towardsdatascience.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oPoM1rX7_LLyPX4MvCTeKoi3vGLEW8yxdJm1kE7ks3Q.jpg?auto=webp&amp;s=f48088594ba0c4fddf4f6745d70d1182d412b75f', 'width': 330, 'height': 434}, 'resolutions': [{'url': 'https://external-preview.redd.it/oPoM1rX7_LLyPX4MvCTeKoi3vGLEW8yxdJm1kE7ks3Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=922d5349e7f7c356a8ce43b9ffcc00f3612e872d', 'width': 108, 'height': 142}, {'url': 'https://external-preview.redd.it/oPoM1rX7_LLyPX4MvCTeKoi3vGLEW8yxdJm1kE7ks3Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b83baa7db9d0142a6c079dab86b0779c043a44f', 'width': 216, 'height': 284}, {'url': 'https://external-preview.redd.it/oPoM1rX7_LLyPX4MvCTeKoi3vGLEW8yxdJm1kE7ks3Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=91e8bd6ae88287c4a0a6cdfbdb3b58ab9ed1851b', 'width': 320, 'height': 420}], 'variants': {}, 'id': 'WJbAdDZYsQagsk8t23XzbSbECw2bquPWqABL0rOAFys'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g53eo2,True,,bogmaestro,,22,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g53eo2/created_a_script_that_runs_your_face_through_a/,all_ads,False,https://towardsdatascience.com/which-celebrity-are-you-d8c6507f21c9?source=friends_link&amp;sk=1045eabec8b7d85b3b04814a0473111a,155203,1587423612.0,1,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,Learning to See Through Obstructions,[],r/learnmachinelearning,False,6,,0,35.0,False,t3_g5uozj,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/0sXS3b6utRuXAShHnwCAoyq7X4cLw9HI_Pzay0YNHgk.jpg,False,,[],{},link,,False,,1587558822.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?auto=webp&amp;s=0b2cda9b02fde189d6d517c97384f0c863022ba9', 'width': 1366, 'height': 344}, 'resolutions': [{'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f06c0bca85b6698dd9643c7e1c1ad823b0a21625', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4d8be53bf920d428fae26470b4f2a1cbcb14d6c', 'width': 216, 'height': 54}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e23f5db3c2ee729e8518b19aa533ebee2c536a65', 'width': 320, 'height': 80}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cb8296ccce949b99c3cad4e7a19ee7c7e6251aa', 'width': 640, 'height': 161}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa49baaf38d07c7898bd60df63123f25a25b39e6', 'width': 960, 'height': 241}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=764c56567b5ce78b1da16b7a9ce4de36697f22d9', 'width': 1080, 'height': 271}], 'variants': {}, 'id': 'X0Rz0oO580mtjFwuYrWrP7JszB1rnW6IIYP4rKJ3kA0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5uozj,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5uozj/learning_to_see_through_obstructions/,all_ads,False,/r/LatestInML/comments/g5um8x/learning_to_see_through_obstructions/,155203,1587530022.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'Remove unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera!\n\n[link to paper](https://www.profillic.com/paper/arxiv:2004.01180)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/g5um8x/video/cyhjwmq3rau41/player', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learning to See Through Obstructions', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 35, 'hide_score': False, 'media_metadata': {'cyhjwmq3rau41': {'status': 'valid', 'e': 'RedditVideo', 'dashUrl': 'https://v.redd.it/link/g5um8x/asset/cyhjwmq3rau41/DASHPlaylist.mpd', 'x': 1280, 'y': 720, 'hlsUrl': 'https://v.redd.it/link/g5um8x/asset/cyhjwmq3rau41/HLSPlaylist.m3u8', 'id': 'cyhjwmq3rau41', 'isGif': False}}, 'name': 't3_g5um8x', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 21, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/0sXS3b6utRuXAShHnwCAoyq7X4cLw9HI_Pzay0YNHgk.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1587558477.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Remove unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.profillic.com/paper/arxiv:2004.01180""&gt;link to paper&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://reddit.com/link/g5um8x/video/cyhjwmq3rau41/player""&gt;https://reddit.com/link/g5um8x/video/cyhjwmq3rau41/player&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?auto=webp&amp;s=0b2cda9b02fde189d6d517c97384f0c863022ba9', 'width': 1366, 'height': 344}, 'resolutions': [{'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f06c0bca85b6698dd9643c7e1c1ad823b0a21625', 'width': 108, 'height': 27}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4d8be53bf920d428fae26470b4f2a1cbcb14d6c', 'width': 216, 'height': 54}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e23f5db3c2ee729e8518b19aa533ebee2c536a65', 'width': 320, 'height': 80}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cb8296ccce949b99c3cad4e7a19ee7c7e6251aa', 'width': 640, 'height': 161}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa49baaf38d07c7898bd60df63123f25a25b39e6', 'width': 960, 'height': 241}, {'url': 'https://external-preview.redd.it/qBuq_zFCPlNCD88x281mCXT3IJdlxUzWaR715FZEnN4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=764c56567b5ce78b1da16b7a9ce4de36697f22d9', 'width': 1080, 'height': 271}], 'variants': {}, 'id': 'X0Rz0oO580mtjFwuYrWrP7JszB1rnW6IIYP4rKJ3kA0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'g5um8x', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/g5um8x/learning_to_see_through_obstructions/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/g5um8x/learning_to_see_through_obstructions/', 'subreddit_subscribers': 3386, 'created_utc': 1587529677.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",t3_g5um8x,,
,learnmachinelearning,,t2_52wdx,False,,0,False,Using Machine Learning to Predict Battle Outcomes in StarCraft 2,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_g5pbf6,False,dark,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/64pH7BkhEwE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How To Predict StarCraft 2 Battle Outcomes Using Machine Learning - Bits of Code', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/64pH7BkhEwE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'ESChamp Competitive A.I', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/64pH7BkhEwE/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWc8nq-7grjx378yV5RiMQ'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/64pH7BkhEwE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g5pbf6', 'height': 338}",,False,3,,False,https://b.thumbs.redditmedia.com/5XI8Ow7cVUsVN1JwIsupVttDPX-VkegBHAHyTO1ftwc.jpg,False,,[],{},rich:video,,False,,1587537310.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/EpIsYA0P4ytUuhTJtE98sfZZoc2YICyou0FyDUjSFn8.jpg?auto=webp&amp;s=22f1b861637dea50c2c29985ae0f96f5b0e118e2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/EpIsYA0P4ytUuhTJtE98sfZZoc2YICyou0FyDUjSFn8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ae66ad9500a8243a41da84b3a1453e6f4ad8b09d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/EpIsYA0P4ytUuhTJtE98sfZZoc2YICyou0FyDUjSFn8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=931d5abd869baa7e4373f7cafa0c233e8635da3f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/EpIsYA0P4ytUuhTJtE98sfZZoc2YICyou0FyDUjSFn8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed8cf5f6af111192844f9c5162cc5939157b9b47', 'width': 320, 'height': 240}], 'variants': {}, 'id': '-QkJogEQLrpa33EzJ4r3lrpmHbKoRw7GsdX9LmWXuAg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5pbf6,True,,Drekkonis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5pbf6/using_machine_learning_to_predict_battle_outcomes/,all_ads,False,https://youtu.be/64pH7BkhEwE,155203,1587508510.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'How To Predict StarCraft 2 Battle Outcomes Using Machine Learning - Bits of Code', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/64pH7BkhEwE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'ESChamp Competitive A.I', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/64pH7BkhEwE/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWc8nq-7grjx378yV5RiMQ'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hey smart people with data since background! Im new to ML and mostly have experience building classification models and general SWE. So one of my models is identification of customer and support dialogs as complaint or not, and over a month this model labeled a LOT of text data. So I got assigned to look on to the results, what data science magic can I do here ? So far I tried topic modeling, LDA, k means but im not quite sure what im looking for. Any ideas, articles, notebooks you can recommend me ? What can I do with this text data ?",t2_l0sviry,False,,0,False,Help with topic modeling and beyond!,[],r/learnmachinelearning,False,6,,0,,False,t3_g5ubjd,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1587557097.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey smart people with data since background! Im new to ML and mostly have experience building classification models and general SWE. So one of my models is identification of customer and support dialogs as complaint or not, and over a month this model labeled a LOT of text data. So I got assigned to look on to the results, what data science magic can I do here ? So far I tried topic modeling, LDA, k means but im not quite sure what im looking for. Any ideas, articles, notebooks you can recommend me ? What can I do with this text data ?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5ubjd,True,,nikita1923666,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5ubjd/help_with_topic_modeling_and_beyond/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5ubjd/help_with_topic_modeling_and_beyond/,155203,1587528297.0,0,,False,,,,
,learnmachinelearning,,t2_4csu169w,False,,0,False,"In this project, I used Neural Networks and Genetic Algorithm to make a game AI. I've included the source code also. I'd love to hear your thoughts about this.","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_g5h9e2,False,light,0.84,,public,8,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/rxzioiG-Vnk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'A.I. learns to play | Neural Network + Genetic Algorithm', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/rxzioiG-Vnk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rxzioiG-Vnk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/rxzioiG-Vnk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/g5h9e2', 'height': 338}",Project,False,8,,False,https://b.thumbs.redditmedia.com/xEoITc3DXSbcLa4jXtqHGTthN16ddaznUppqePhDKqo.jpg,False,,[],{},rich:video,,False,,1587511596.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cS11wYR91nn7IT1pBMUw-D03hCmOwU-LJfKi0mo9ENw.jpg?auto=webp&amp;s=a1d5b41d805c2a4927c18f4924cb5bc671cd7f2f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/cS11wYR91nn7IT1pBMUw-D03hCmOwU-LJfKi0mo9ENw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=90354bbbaa08f4fbb9b0cfc27452c87b275ff1c9', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/cS11wYR91nn7IT1pBMUw-D03hCmOwU-LJfKi0mo9ENw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=28becc3b24e99c4427e7f3119bbe42c315a8e61d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/cS11wYR91nn7IT1pBMUw-D03hCmOwU-LJfKi0mo9ENw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b0abb5ed6f461e1a49910a0bff51be108b61542', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'BGMEQ4iz6k1nQZpxqiabXLFQosCo7uQ22aAyuQZSYGg'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g5h9e2,True,,nerdy_wits,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5h9e2/in_this_project_i_used_neural_networks_and/,all_ads,False,https://youtu.be/rxzioiG-Vnk,155203,1587482796.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'A.I. learns to play | Neural Network + Genetic Algorithm', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/rxzioiG-Vnk?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Normalized Nerd', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rxzioiG-Vnk/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UC7Fs-Fdpe0I8GYg3lboEuXw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,,False,,0,False,"Free Textbook ""The Elements of Statistical Learning"" from Springer",[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g5lq3d,False,dark,0.81,,public,3,0,{},140.0,,False,[],,False,False,,{},,False,3,,,https://b.thumbs.redditmedia.com/5fq_JvdgRgcxZTOMsbKtamlqFmm8tVef8w4SnH7MIIk.jpg,False,,,{},link,,False,,1587525749.0,text,6,,,,link.springer.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b6cH3vJm_GQtN3XBxrhgod7MkW_CyzaPdYejw1rF7BM.jpg?auto=webp&amp;s=6835535429abc0d2c4587e00d12eff3bf6fad811', 'width': 306, 'height': 501}, 'resolutions': [{'url': 'https://external-preview.redd.it/b6cH3vJm_GQtN3XBxrhgod7MkW_CyzaPdYejw1rF7BM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc562b81941a0c9d1531bf1d9c891bdaf857ec9d', 'width': 108, 'height': 176}, {'url': 'https://external-preview.redd.it/b6cH3vJm_GQtN3XBxrhgod7MkW_CyzaPdYejw1rF7BM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=94e5a3ea252d4a2ae5d842a463193c4f9bbf20dc', 'width': 216, 'height': 353}], 'variants': {}, 'id': 'ict2LBkV4GI_nJQRQkkO4nz6gFu5iZmNckEbuep8VZY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5lq3d,True,,[deleted],,2,True,all_ads,False,[],,dark,/r/learnmachinelearning/comments/g5lq3d/free_textbook_the_elements_of_statistical/,all_ads,False,https://link.springer.com/book/10.1007/978-0-387-84858-7,155203,1587496949.0,0,,False,,,,
,learnmachinelearning,"To earn practice with machine learning, I'm working on creating a texas holdem poker ai that can compete with professional tier players. If anyone is interested in joining the project send me a message!

I'm using google colab and tensorflow.",t2_2npwokzh,False,,0,False,Practice project; Creating a professional poker AI.,[],r/learnmachinelearning,False,6,,0,,False,t3_g5mm93,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1587528540.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;To earn practice with machine learning, I&amp;#39;m working on creating a texas holdem poker ai that can compete with professional tier players. If anyone is interested in joining the project send me a message!&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using google colab and tensorflow.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5mm93,True,,Starting2Decay,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5mm93/practice_project_creating_a_professional_poker_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5mm93/practice_project_creating_a_professional_poker_ai/,155203,1587499740.0,0,,False,,,,
,learnmachinelearning,"I am very new to ML, so, sorry if this is a stupid question. I have a 2019 MacBook Pro and only have \~60GB/256GB free. Will I be able to install and use Pytorch? I have a 2TB external hard drive. Would I be able to store data on the external hard drive for Pytorch to train off of? Or would I be able to run Pytorch off of the external hard drive if I don't have enough room on my Mac?

If it is impossible for me to get it to work on my MacBook, do you know any cheap computers that would offer me the minimum to be able to do what I need for Pytorch? Either to buy or to build.

Any advice or help would be appreciated. I've been trying to look online for answers but I haven't been able to find anything.",t2_bt4rwqq,False,,0,False,Am I able to run Pytorch or TensorFlow?,[],r/learnmachinelearning,False,6,,0,,False,t3_g5qpjt,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1587514260.0,,[],{},,,True,,1587542270.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am very new to ML, so, sorry if this is a stupid question. I have a 2019 MacBook Pro and only have ~60GB/256GB free. Will I be able to install and use Pytorch? I have a 2TB external hard drive. Would I be able to store data on the external hard drive for Pytorch to train off of? Or would I be able to run Pytorch off of the external hard drive if I don&amp;#39;t have enough room on my Mac?&lt;/p&gt;

&lt;p&gt;If it is impossible for me to get it to work on my MacBook, do you know any cheap computers that would offer me the minimum to be able to do what I need for Pytorch? Either to buy or to build.&lt;/p&gt;

&lt;p&gt;Any advice or help would be appreciated. I&amp;#39;ve been trying to look online for answers but I haven&amp;#39;t been able to find anything.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5qpjt,True,,wyattgumball,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5qpjt/am_i_able_to_run_pytorch_or_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5qpjt/am_i_able_to_run_pytorch_or_tensorflow/,155203,1587513470.0,0,,False,,,,
,learnmachinelearning,Is it worthwhile to try and learn machine learning without knowing statistics (calc + lin alg. based) first?,t2_4yqkvgto,False,,0,False,statistics,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5q5re,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,False,self,False,,[],{},,,True,,1587540283.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is it worthwhile to try and learn machine learning without knowing statistics (calc + lin alg. based) first?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,g5q5re,True,,lil_faucet,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5q5re/statistics/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5q5re/statistics/,155203,1587511483.0,0,,False,,,,
,learnmachinelearning,,t2_3ps7w65l,False,,0,False,Pooling in Convolutional Neural Network: https://www.instagram.com/machinelearning/,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_g56msr,False,dark,0.91,,public,48,0,{},140.0,,False,[],,True,False,,{},,False,48,,True,https://b.thumbs.redditmedia.com/uhIjtU5rx4yElbUi3HcrOreRsFKtzevxCMMvAZAzgZY.jpg,False,,[],{},image,,False,,1587464254.0,text,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?auto=webp&amp;s=920ab12fd774b53564c96acaf9130a64a379a165', 'width': 1080, 'height': 1080}, 'resolutions': [{'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cfa893ae6a06b165e946d17b39a30c7b30ec215f', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a834d600f56d9bb3035236c428cd43fbb5b98b', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8c60c02990d50cab89b1f86b32aa31d3e09734f', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=98f9bb5298abbf9c39bad50510298c79d5d34268', 'width': 640, 'height': 640}, {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3426680ee2af701e093f85a49c40d0ea390a94f1', 'width': 960, 'height': 960}, {'url': 'https://preview.redd.it/61tcfy2xy2u41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=998d0b203d830ade90121244bf1c942d42f08223', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'HdinszkBkyIRse9g0tUGUkLUEitM72YPhtQY2ufdYNY'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g56msr,True,,wstcpyt1988,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g56msr/pooling_in_convolutional_neural_network/,all_ads,False,https://i.redd.it/61tcfy2xy2u41.png,155203,1587435454.0,0,,False,,,,
,learnmachinelearning,"Hey Guys,

I am pretty new into Object Detection and Tracking.

I want to create a Tool, which can do following things:

1. Detect Objects (not only one)
2. Track them
3. Create Trajectories from the Trackings
4. Convert the Trajectories from image coordinates to real world coordinates
5. Give me a .csv or something similar with the trajectories for each object

What would you say is the best way to create such a tool?

Where do i start? Are there some good tutorial to begin with?

&amp;#x200B;

Thank you!",t2_68aa0nxm,False,,0,False,Creating Object Detection and Tracking Tool,[],r/learnmachinelearning,False,6,,0,,False,t3_g5j4wy,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587517620.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;

&lt;p&gt;I am pretty new into Object Detection and Tracking.&lt;/p&gt;

&lt;p&gt;I want to create a Tool, which can do following things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Detect Objects (not only one)&lt;/li&gt;
&lt;li&gt;Track them&lt;/li&gt;
&lt;li&gt;Create Trajectories from the Trackings&lt;/li&gt;
&lt;li&gt;Convert the Trajectories from image coordinates to real world coordinates&lt;/li&gt;
&lt;li&gt;Give me a .csv or something similar with the trajectories for each object&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What would you say is the best way to create such a tool?&lt;/p&gt;

&lt;p&gt;Where do i start? Are there some good tutorial to begin with?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5j4wy,True,,Chillo4747,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5j4wy/creating_object_detection_and_tracking_tool/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5j4wy/creating_object_detection_and_tracking_tool/,155203,1587488820.0,0,,False,,,,
,learnmachinelearning,"Hey all- my company [Beastnotes](https://www.beastnotes.com/) built a note taking chrome extension that is designed to work with Coursera. It has the unique feature of allowing note-taking in the same browser you are watching your lecture, no longer do you have to pause and flip between browsers to take your notes and itâ€™s free to use for the first three coursebooks. I wrote an article about it [here](https://medium.com/@beastnotes/online-note-taking-without-distraction-e47a440aa890) if you want to learn more!",t2_2r4lqxm8,False,,0,False,In-browser note taking tool designed for online learning,[],r/learnmachinelearning,False,6,,0,,False,t3_g5idem,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1587515171.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all- my company &lt;a href=""https://www.beastnotes.com/""&gt;Beastnotes&lt;/a&gt; built a note taking chrome extension that is designed to work with Coursera. It has the unique feature of allowing note-taking in the same browser you are watching your lecture, no longer do you have to pause and flip between browsers to take your notes and itâ€™s free to use for the first three coursebooks. I wrote an article about it &lt;a href=""https://medium.com/@beastnotes/online-note-taking-without-distraction-e47a440aa890""&gt;here&lt;/a&gt; if you want to learn more!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,g5idem,True,,GiletsJaunesKiki,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5idem/inbrowser_note_taking_tool_designed_for_online/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5idem/inbrowser_note_taking_tool_designed_for_online/,155203,1587486371.0,0,,False,,,,
,learnmachinelearning,"Over the quarantine, a couple of friends and I decided to scrape local news sites for articles related to covid-19 crisis. What we want to do now is to categorize the test either as a problem or solution. 

Do I use *sentiment analysis*? 

How should I approach categorizing the texts?

Thank you!",t2_yas0o,False,,0,False,How should I approach categorizing a text as either a Problem or Solution?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5lwy4,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1587526346.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Over the quarantine, a couple of friends and I decided to scrape local news sites for articles related to covid-19 crisis. What we want to do now is to categorize the test either as a problem or solution. &lt;/p&gt;

&lt;p&gt;Do I use &lt;em&gt;sentiment analysis&lt;/em&gt;? &lt;/p&gt;

&lt;p&gt;How should I approach categorizing the texts?&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,g5lwy4,True,,tomy8910,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5lwy4/how_should_i_approach_categorizing_a_text_as/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5lwy4/how_should_i_approach_categorizing_a_text_as/,155203,1587497546.0,0,,False,,,,
,learnmachinelearning,"A really cool way to do data annotation, a much faster and easy way for easy ones atleast. Check out at [https://prakhartechviz.blogspot.com/2020/04/interactive-annotations-in-jupyter-notebook.html](https://prakhartechviz.blogspot.com/2020/04/interactive-annotations-in-jupyter-notebook.html)",t2_hkv9s,False,,0,False,Interactive Data Annotations in Jupyter Notebook,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_g5lqay,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1587525769.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A really cool way to do data annotation, a much faster and easy way for easy ones atleast. Check out at &lt;a href=""https://prakhartechviz.blogspot.com/2020/04/interactive-annotations-in-jupyter-notebook.html""&gt;https://prakhartechviz.blogspot.com/2020/04/interactive-annotations-in-jupyter-notebook.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fh6vvoh8tYE1p8HTBKeYjeIvAMvy8Jh7rvWzCRuc5lI.jpg?auto=webp&amp;s=9f4cb6d0b83ca8c317a0ccf9466e493b63cae238', 'width': 762, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/fh6vvoh8tYE1p8HTBKeYjeIvAMvy8Jh7rvWzCRuc5lI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb7f5c1c2a99c48e0b27ea9cf7e4c022d0b24f74', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/fh6vvoh8tYE1p8HTBKeYjeIvAMvy8Jh7rvWzCRuc5lI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac9cb5acc1b4c7930a399f80ebd6e34c66165210', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/fh6vvoh8tYE1p8HTBKeYjeIvAMvy8Jh7rvWzCRuc5lI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0499ba5385d5d9a66949e9e86e9f63a4f67e96cd', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/fh6vvoh8tYE1p8HTBKeYjeIvAMvy8Jh7rvWzCRuc5lI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c95eaffe892a891092b774f5f756e81b0eb795a2', 'width': 640, 'height': 335}], 'variants': {}, 'id': '8D5aqlBLkP5o2W0Cuxx7OEKrMS1sG9KEqC4xfNqJMLY'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,g5lqay,True,,prakhar21,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/g5lqay/interactive_data_annotations_in_jupyter_notebook/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/g5lqay/interactive_data_annotations_in_jupyter_notebook/,155203,1587496969.0,0,,False,,,,
,learnmachinelearning,,t2_4r7zftln,False,,0,False,Control the car by using your index finger swinging in the air. Made using TensorFlow's Handpose model. Check out the live simulation (Link in the comment section),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,68.0,False,t3_ggz8pz,False,light,0.92,,public,46,0,{},140.0,,False,[],,True,False,,{},Project,False,46,,False,https://b.thumbs.redditmedia.com/PE15Vs7lAno-O86IPJT0YbaS6l-_ZQCpeziScN9oMtA.jpg,False,,[],{},image,,False,,1589136022.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?format=png8&amp;s=95427a2f62a7fe10defa4c0b21362192c30642a9', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=b5cd184a1460a3a84490125d433d6f30326b6e11', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=432b90ffc0c513427a9adf6ebe0d4bc1cfe7f5a8', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=9d503f3c1df0b684ae10f34b56a48d23fea0eec7', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=01ce3c699382ace866516b329784e672a55dc15c', 'width': 640, 'height': 314}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?s=7b6d30868ca6fc9fa332156b34c65ea9bc5007e8', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;crop=smart&amp;s=252a2bc32709b6f9f9747981443d41d78af64f36', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;crop=smart&amp;s=380654721b85ca25278878682b9b3e7b7840346d', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;crop=smart&amp;s=41ac613a6d800acdc96928edfd4c25b86e395bea', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;crop=smart&amp;s=7f1f7aee2fad8d9b0c0012da361bf47499e107d2', 'width': 640, 'height': 314}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/abniciln1xx41.gif?format=mp4&amp;s=81c2b69d52618118bb2bc3dbeac400970c60e1f5', 'width': 640, 'height': 314}, 'resolutions': [{'url': 'https://preview.redd.it/abniciln1xx41.gif?width=108&amp;format=mp4&amp;s=b301ff122d1e0ca967222f008330495e6b7f2201', 'width': 108, 'height': 52}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=216&amp;format=mp4&amp;s=f1628766de4a4eb4d2e7b55de7ad255498b9762b', 'width': 216, 'height': 105}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=320&amp;format=mp4&amp;s=7bc82dcb20f8ca2d2a0cd20f66ac9aec66c18f08', 'width': 320, 'height': 157}, {'url': 'https://preview.redd.it/abniciln1xx41.gif?width=640&amp;format=mp4&amp;s=d17075ed2b10c5e50bfafcac1190597c67f8e4f8', 'width': 640, 'height': 314}]}}, 'id': 'EMThTd8mK0bzLpxVTWROT-Pip3UIlJKNtK5PD0GbXpA'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggz8pz,True,,h4wk_3y3,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggz8pz/control_the_car_by_using_your_index_finger/,all_ads,False,https://i.redd.it/abniciln1xx41.gif,155203,1589107222.0,0,,False,,,,
,learnmachinelearning,"Hi all,

Iâ€™ve read a lot of posts asking for recommendations for textbooks to learn the math behind machine learning so I figured Iâ€™d make a self-study guide that walks you through it all including the recommended subjects and corresponding textbooks. You should have more than enough mathematical maturity to work through ESL and the Deep Learning book by the time youâ€™re done. Iâ€™ll update it periodically and if anyone has any questions or comments those are always welcome!

https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0",t2_4qzspo30,False,,0,False,A comprehensive self-study guide for the math behind machine learning.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggpzk2,False,dark,0.98,,public,326,1,{},,,False,[],,False,False,,{},,False,326,,False,self,False,,[],{},self,,True,,1589094933.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;Iâ€™ve read a lot of posts asking for recommendations for textbooks to learn the math behind machine learning so I figured Iâ€™d make a self-study guide that walks you through it all including the recommended subjects and corresponding textbooks. You should have more than enough mathematical maturity to work through ESL and the Deep Learning book by the time youâ€™re done. Iâ€™ll update it periodically and if anyone has any questions or comments those are always welcome!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0""&gt;https://www.dropbox.com/s/mffzmuo9fvs5j6m/Study_Guide.pdf?dl=0&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?auto=webp&amp;s=8844a2f59ff2ffe318ad0e2b7846e1e37aed5128', 'width': 160, 'height': 160}, 'resolutions': [{'url': 'https://external-preview.redd.it/cmn-k_-F7L54OygS2lqkHISYKPfRXpKLouzuOXaxXGo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=20970d5f7f7c343396aff132ae5ffcee0ce03cb8', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'KMbn3CCPvk6_ukzEK1zzIufQuyyC0aay4raAkv3Slng'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 50, 'id': 'award_a2506925-fc82-4d6c-ae3b-b7217e09d7f0', 'penny_donate': None, 'coin_reward': 0, 'icon_url': 'https://i.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png', 'days_of_premium': 0, 'icon_height': 2048, 'resized_icons': [{'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=16&amp;height=16&amp;auto=webp&amp;s=4e475e8c3265ec7148d7f4204f07d33949482f21', 'width': 16, 'height': 16}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=32&amp;height=32&amp;auto=webp&amp;s=42e32a4b9f1e70791716c3be283e89951e212a69', 'width': 32, 'height': 32}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=48&amp;height=48&amp;auto=webp&amp;s=5adb621fede4e8e66b952a379ad038fcc1b8ad13', 'width': 48, 'height': 48}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=64&amp;height=64&amp;auto=webp&amp;s=6161edea19569bbee73ef322a2e5470535ec1787', 'width': 64, 'height': 64}, {'url': 'https://preview.redd.it/award_images/t5_22cerq/80j20o397jj41_NarwhalSalute.png?width=128&amp;height=128&amp;auto=webp&amp;s=5d2c75f44f176f430e936204f9a53b8a2957f2fc', 'width': 128, 'height': 128}], 'icon_width': 2048, 'start_date': None, 'is_enabled': True, 'description': 'A golden splash of respect', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'name': 'Narwhal Salute', 'icon_format': None, 'award_sub_type': 'PREMIUM', 'penny_price': None, 'award_type': 'global'}]",[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggpzk2,True,,PersonalPsychology2,,37,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggpzk2/a_comprehensive_selfstudy_guide_for_the_math/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggpzk2/a_comprehensive_selfstudy_guide_for_the_math/,155203,1589066133.0,0,,False,,,,
,learnmachinelearning,"I have written a multi-part series on the concepts and implementation of Counterfactual Regret Minimization (CFR), with Python code and toy example. CFR is at the heart of most advanced Poker AIs including the famed [Pluribus](https://en.wikipedia.org/wiki/Pluribus_(poker_bot))

I hope you find it interesting: [Steps to building a Poker AI (Part 1)](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-1-outline-and-history-58fbedaf6ded)

If there's enough interest I plan to continue the series in some form to talk more about the aspects specific to Texas Hold'em and some more advanced stuff that is necessary to make a strong AI.

Direct links to the other parts:

[Part 2: Modelling Imperfect Information Games](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-2-modelling-imperfect-information-games-c384b7c71edf)  
[Part 3: Regrets and Minimizing Regrets in One-Shot Games](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-3-regrets-and-minimizing-regrets-in-one-shot-games-ca7cdc8f66d0)  
[Part 4: Regret matching for Rock-Paper-Scissors in Python](https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-4-regret-matching-for-rock-paper-scissors-in-python-168411edbb13)  
[Part 5: Sequential Games, Kuhn Poker and Counterfactual Regrets](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-5-sequential-games-kuhn-poker-and-counterfactual-regrets-311f533f786e)  
[Part 6: Beating Kuhn Poker with CFR using Python](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python-1b4172a6ab2d)  
[Part 7: Exploitability, Multiplayer CFR and 3-player Kuhn Poker](https://medium.com/ai-in-plain-english/building-a-poker-ai-part-7-exploitability-multiplayer-cfr-and-3-player-kuhn-poker-25f313bf83cf)

(X-post from r/poker)",t2_9t1bg,False,,0,False,Gentle introduction to the basics of Poker AI,[],r/learnmachinelearning,False,6,,0,,False,t3_gh01ca,False,dark,0.94,,public,15,0,{},,,False,[],,False,False,,{},,False,15,,False,self,False,,[],{},,,True,,1589139811.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have written a multi-part series on the concepts and implementation of Counterfactual Regret Minimization (CFR), with Python code and toy example. CFR is at the heart of most advanced Poker AIs including the famed &lt;a href=""https://en.wikipedia.org/wiki/Pluribus_(poker_bot""&gt;Pluribus&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I hope you find it interesting: &lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-1-outline-and-history-58fbedaf6ded""&gt;Steps to building a Poker AI (Part 1)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If there&amp;#39;s enough interest I plan to continue the series in some form to talk more about the aspects specific to Texas Hold&amp;#39;em and some more advanced stuff that is necessary to make a strong AI.&lt;/p&gt;

&lt;p&gt;Direct links to the other parts:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-2-modelling-imperfect-information-games-c384b7c71edf""&gt;Part 2: Modelling Imperfect Information Games&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-3-regrets-and-minimizing-regrets-in-one-shot-games-ca7cdc8f66d0""&gt;Part 3: Regrets and Minimizing Regrets in One-Shot Games&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/steps-to-building-a-poker-ai-part-4-regret-matching-for-rock-paper-scissors-in-python-168411edbb13""&gt;Part 4: Regret matching for Rock-Paper-Scissors in Python&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-5-sequential-games-kuhn-poker-and-counterfactual-regrets-311f533f786e""&gt;Part 5: Sequential Games, Kuhn Poker and Counterfactual Regrets&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-6-beating-kuhn-poker-with-cfr-using-python-1b4172a6ab2d""&gt;Part 6: Beating Kuhn Poker with CFR using Python&lt;/a&gt;&lt;br/&gt;
&lt;a href=""https://medium.com/ai-in-plain-english/building-a-poker-ai-part-7-exploitability-multiplayer-cfr-and-3-player-kuhn-poker-25f313bf83cf""&gt;Part 7: Exploitability, Multiplayer CFR and 3-player Kuhn Poker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(X-post from &lt;a href=""/r/poker""&gt;r/poker&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh01ca,True,,tt293,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh01ca/gentle_introduction_to_the_basics_of_poker_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh01ca/gentle_introduction_to_the_basics_of_poker_ai/,155203,1589111011.0,0,,False,,,,
,learnmachinelearning,,t2_10lnxu,False,,0,False,A Hackerâ€™s Guide to Efficiently Train Deep Learning Models ðŸš€,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_ggyyj0,False,dark,1.0,,public,5,0,{},140.0,,False,[],,False,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/m9EnopsuRsbSI5bQc-Vhth-12231vsIhEjlcpi0mNio.jpg,False,,[],{},link,,False,,1589134575.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?auto=webp&amp;s=352f5f8dc8a416c2beed67b2de1f66119d994e25', 'width': 1200, 'height': 899}, 'resolutions': [{'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2de1e06f8a9cabf293ba5d8e58d61bdd69a5a445', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=26e139597dee2c395b38a530c9fe2ede540b55dc', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc52dd1aa9c63a64f8d293a64a9ef011296b7295', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc511a35831290685235b4747a75d0eb310a63a', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d10673ec1555bd3198f20b0b80deae7a2af4b487', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/QqXzN3SQeZCGCwAi7arDo1pF4wXiK8VXbcCF_QbyBqg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9f747225a0b4467e62c0300566668629520643a9', 'width': 1080, 'height': 809}], 'variants': {}, 'id': '0qpsjUZBTl7oyb3qI6C8Ab094Znns3q0ATMDI8M1Etc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggyyj0,True,,ahmedbesbes,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyyj0/a_hackers_guide_to_efficiently_train_deep/,all_ads,False,https://medium.com/@ahmedbesbes/a-hackers-guide-to-efficiently-train-deep-learning-models-b2cccbd1bc0a,155203,1589105775.0,0,,False,,,,
,learnmachinelearning,"I'm really fascinated by apps like Spotify and Shazam, and would like to pursue a career related to music information retrieval (MIR) and recommender systems. It seems like an esoteric field, especially since deep learning these days seems to primarily be focused on image and text data.

Any of you interested in (or working in) similar areas? Maybe we can connect and form a study group! Maybe we can share interesting papers (e.g. from ISMIR and ACM RecSys conferences), work on projects together, and share study materials.",t2_3loxj2cz,False,,0,False,[D] Anyone interested in music recommendation?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggwqus,False,light,1.0,,public,8,0,{},,,False,[],,False,False,,{},Discussion,False,8,,False,self,False,,[],{},,,True,,1589122994.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m really fascinated by apps like Spotify and Shazam, and would like to pursue a career related to music information retrieval (MIR) and recommender systems. It seems like an esoteric field, especially since deep learning these days seems to primarily be focused on image and text data.&lt;/p&gt;

&lt;p&gt;Any of you interested in (or working in) similar areas? Maybe we can connect and form a study group! Maybe we can share interesting papers (e.g. from ISMIR and ACM RecSys conferences), work on projects together, and share study materials.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggwqus,True,,hedgehogist,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwqus/d_anyone_interested_in_music_recommendation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwqus/d_anyone_interested_in_music_recommendation/,155203,1589094194.0,0,,False,,,,
,learnmachinelearning,"This video explains how to claim the offer, consider watching:

 [https://www.youtube.com/watch?v=i8MQIlLXLIM](https://www.youtube.com/watch?v=i8MQIlLXLIM)",t2_5ggm5svc,False,,0,False,"Hey all, consider looking at IBM Data Science and AI programs free for 30 days","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggxhj8,False,light,0.72,,public,6,0,{},,,False,[],,False,False,,{},Discussion,False,6,,False,self,False,,[],{},self,,True,,1589126857.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This video explains how to claim the offer, consider watching:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=i8MQIlLXLIM""&gt;https://www.youtube.com/watch?v=i8MQIlLXLIM&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?auto=webp&amp;s=3f39d9840b0aef690cedb12db73aac501db37cf2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=39c3ba0bbbdb04d0a9baa011585f7d29788d8095', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=421deb33acebaa5c3ad15395c22f13df00c2c62e', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/1NZA9BGSgh_Ld-XPpgdGmeFVjccoEvo5b-we3chj3Ic.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72d6737b82ee38552392be0bac5e39fe3998d074', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'jBGO9k_PntalzUMXgd5n09Ji5dIE8PWC6C-HbXdTtq8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggxhj8,True,,awsconsultant,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxhj8/hey_all_consider_looking_at_ibm_data_science_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggxhj8/hey_all_consider_looking_at_ibm_data_science_and/,155203,1589098057.0,0,,False,,,,
,learnmachinelearning,"I am thinking of doing Stanford's CS229 Machine Learning course. It's the heavier version of Coursera's ML course. Stanford released 2018 version of this course on [YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) recently. Also, this version uses Python, which is a plus.

Andrew mentions in first few minutes of [first lecture](https://youtu.be/jGwO_UgTS7I?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) that having a study group would help a lot in getting through the course and since I am doing this as self-study, it becomes even more important.

So let me know if you are interested. Please remember that the course is of intermediate or intermediate+ level - this will only make all the efforts even more worthwhile. Check out [Problem Set 1](https://drive.google.com/file/d/1K_h2dCHfAQCnboCVxF49ay9B9UjB-a_l/view?usp=sharing) and [Syllabus](http://cs229.stanford.edu/syllabus-autumn2018.html) to get an idea.

P.S. I will admit that Problem Set 1 scared the shit out of me on first view, and that's what motivated me to do the course.

&amp;#x200B;

Update2:Since group grew far too quickly than expected - it wasn't possible to manage things on Telegram.

[Lentor3579](https://www.reddit.com/user/Lentor3579/) has created a discord server ([https://discord.gg/RcVEVuX](https://discord.gg/RcVEVuX)). So please join there - all further information will be on discord only. Telegram link won't work anymore.

&amp;#x200B;

Update1:

~~Ok, so around 10 or so people seem interested. I have created a Telegram group (~~[~~https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ~~](https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ)~~) where we ask normal doubts and then have a weekly session on Zoom or Teams (or whatever people agree on) to discuss the progress made. I will make a Google sheet to track what we are doing for a given week once people reply to the poll regarding pacing.~~

~~If you guys have a better idea, please suggest.~~

~~On telegram, your number is only visible to people who already have your contact saved or you have their contact saved. Others can only see your username/name.Source:~~ [~~https://telegram.org/faq#q-who-can-see-my-phone-number~~](https://telegram.org/faq#q-who-can-see-my-phone-number)",t2_37d2oexf,False,,0,False,Stanford's CS229 ML Course Study Partner,[],r/learnmachinelearning,False,6,,0,,False,t3_ggg8e5,False,dark,0.97,,public,136,0,{},,,False,[],,False,False,,{},,False,136,,False,self,1589106475.0,,[],{},self,,True,,1589062945.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am thinking of doing Stanford&amp;#39;s CS229 Machine Learning course. It&amp;#39;s the heavier version of Coursera&amp;#39;s ML course. Stanford released 2018 version of this course on &lt;a href=""https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;YouTube&lt;/a&gt; recently. Also, this version uses Python, which is a plus.&lt;/p&gt;

&lt;p&gt;Andrew mentions in first few minutes of &lt;a href=""https://youtu.be/jGwO_UgTS7I?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;first lecture&lt;/a&gt; that having a study group would help a lot in getting through the course and since I am doing this as self-study, it becomes even more important.&lt;/p&gt;

&lt;p&gt;So let me know if you are interested. Please remember that the course is of intermediate or intermediate+ level - this will only make all the efforts even more worthwhile. Check out &lt;a href=""https://drive.google.com/file/d/1K_h2dCHfAQCnboCVxF49ay9B9UjB-a_l/view?usp=sharing""&gt;Problem Set 1&lt;/a&gt; and &lt;a href=""http://cs229.stanford.edu/syllabus-autumn2018.html""&gt;Syllabus&lt;/a&gt; to get an idea.&lt;/p&gt;

&lt;p&gt;P.S. I will admit that Problem Set 1 scared the shit out of me on first view, and that&amp;#39;s what motivated me to do the course.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Update2:Since group grew far too quickly than expected - it wasn&amp;#39;t possible to manage things on Telegram.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.reddit.com/user/Lentor3579/""&gt;Lentor3579&lt;/a&gt; has created a discord server (&lt;a href=""https://discord.gg/RcVEVuX""&gt;https://discord.gg/RcVEVuX&lt;/a&gt;). So please join there - all further information will be on discord only. Telegram link won&amp;#39;t work anymore.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Update1:&lt;/p&gt;

&lt;p&gt;&lt;del&gt;Ok, so around 10 or so people seem interested. I have created a Telegram group (&lt;/del&gt;&lt;a href=""https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ""&gt;&lt;del&gt;https://t.me/joinchat/FvYNhRyL1ZB15QAoi3DhrQ&lt;/del&gt;&lt;/a&gt;&lt;del&gt;) where we ask normal doubts and then have a weekly session on Zoom or Teams (or whatever people agree on) to discuss the progress made. I will make a Google sheet to track what we are doing for a given week once people reply to the poll regarding pacing.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;If you guys have a better idea, please suggest.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;On telegram, your number is only visible to people who already have your contact saved or you have their contact saved. Others can only see your username/name.Source:&lt;/del&gt; &lt;a href=""https://telegram.org/faq#q-who-can-see-my-phone-number""&gt;&lt;del&gt;https://telegram.org/faq#q-who-can-see-my-phone-number&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?auto=webp&amp;s=7dad7e69084fe17dc70d1948b6f58fc13d288f0c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54a29aa3b0894b9f5f031e6ec93d6896ab4450', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'WdNtCcCE5kkWeUtaFzPEmvl-T9SFUg0GJCbpJncoGqQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggg8e5,True,,kaiNbleu,,52,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggg8e5/stanfords_cs229_ml_course_study_partner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggg8e5/stanfords_cs229_ml_course_study_partner/,155203,1589034145.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggnghh,False,dark,0.87,,public,37,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZnpZsiy_p2M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggnghh', 'height': 338}",,False,37,,False,https://b.thumbs.redditmedia.com/XIiBENV4Jh8C1XtbgKh57N-_DRf1hSh6wF6AEKDkY2Q.jpg,False,,[],{},rich:video,,False,,1589086393.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?auto=webp&amp;s=238eba7e302b2bad4e95eb10c7c2a81ec22fbc64', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fd76389c456600962df909dda5856ab3eaeb983', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7beefb5236f0e5e75b2c75ab26ef7bdfccc15a89', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/KLRyOdQhFf9hB8aMuCog3giiSAix2dzENDjv1e1T7qY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a61edef3c746f461398d18b95c73661903d6935', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fnLVMezrmrrxLFHs_XstZpLKuC8tnGlQPsQW-zWY_Es'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggnghh,True,,OnlyProggingForFun,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggnghh/what_are_gans_introduction_to_generative/,all_ads,False,https://www.youtube.com/watch?v=ZnpZsiy_p2M,155203,1589057593.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'What are GANs ? | Introduction to Generative Adversarial Networks | Face Generation &amp; Editing - 30', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/ZnpZsiy_p2M?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/ZnpZsiy_p2M/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}}",False,,,,
,learnmachinelearning,"Hi everyone,

I'm interested in image generation and image-to-image translation, and through my reads I have noticed that people favour the use of upsampling and convolution layers over transposed convolution layers.
In ([StarGAN v2](https://arxiv.org/abs/1912.01865)), regarding their generator architecture (Section E. Table 5.), the encoding part uses pooling layers, instead of convolutions with ```stride = 2```, while their decoder uses upsampling layers, instead of transposed convolutions with ```stride = 2```.

If I got it right, both pooling and upsampling operations can be learnt by convolution and transposed convolution layers. If this is true, then why not learning transposed convolutions instead of upsampling? Same goes for convolutions and pooling?

Also, can convolutions and transposed convolutions be equivalent?
Here's a small example:
```py
import torch
from torch.nn import Conv2d, ConvTranspose2d

n = 1
h, w = 128, 128
i, o = 16, 32
k, p = 3, 1

a = Conv2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)
b = ConvTranspose2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)

x = torch.ones(n, i, h, w)
print(x.shape)

y = a(x)
z = b(x)

print(y.shape)
print(z.shape)

```
Given the same input, can both layers produce the same output? If yes, is there a relation between their weights (and bias)?

Thanks in advance,

Piollinas",t2_5zbb2ujo,False,,0,False,Why are Upsampling+Convolutions better than Transposed Convolutions?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggyz05,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,1589106321.0,,[],{},,,True,,1589134650.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m interested in image generation and image-to-image translation, and through my reads I have noticed that people favour the use of upsampling and convolution layers over transposed convolution layers.
In (&lt;a href=""https://arxiv.org/abs/1912.01865""&gt;StarGAN v2&lt;/a&gt;), regarding their generator architecture (Section E. Table 5.), the encoding part uses pooling layers, instead of convolutions with &lt;code&gt;stride = 2&lt;/code&gt;, while their decoder uses upsampling layers, instead of transposed convolutions with &lt;code&gt;stride = 2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If I got it right, both pooling and upsampling operations can be learnt by convolution and transposed convolution layers. If this is true, then why not learning transposed convolutions instead of upsampling? Same goes for convolutions and pooling?&lt;/p&gt;

&lt;p&gt;Also, can convolutions and transposed convolutions be equivalent?
Here&amp;#39;s a small example:
```py
import torch
from torch.nn import Conv2d, ConvTranspose2d&lt;/p&gt;

&lt;p&gt;n = 1
h, w = 128, 128
i, o = 16, 32
k, p = 3, 1&lt;/p&gt;

&lt;p&gt;a = Conv2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)
b = ConvTranspose2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)&lt;/p&gt;

&lt;p&gt;x = torch.ones(n, i, h, w)
print(x.shape)&lt;/p&gt;

&lt;p&gt;y = a(x)
z = b(x)&lt;/p&gt;

&lt;p&gt;print(y.shape)
print(z.shape)&lt;/p&gt;

&lt;p&gt;```
Given the same input, can both layers produce the same output? If yes, is there a relation between their weights (and bias)?&lt;/p&gt;

&lt;p&gt;Thanks in advance,&lt;/p&gt;

&lt;p&gt;Piollinas&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggyz05,True,,Piollinas,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyz05/why_are_upsamplingconvolutions_better_than/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggyz05/why_are_upsamplingconvolutions_better_than/,155203,1589105850.0,0,,False,,,,
,learnmachinelearning,,t2_10lnxu,False,,0,False,"Complete end to end machine learning tutorial: from data collection to deployment: scrape and collect data, train a model, design an app, and deploy it to AWS + Full code in Python","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,93.0,True,t3_gh1uyp,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Project,False,1,,False,https://b.thumbs.redditmedia.com/OgKSJCBSEZ6mSgu1KsbH7XyEfbG4qhezDPDIVYJeQ5Q.jpg,False,,[],{},link,,False,,1589147565.0,richtext,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?auto=webp&amp;s=9e166dab7a82900f94cc841798746f7b587863c5', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5282f0a54dd7969b78ed7b59d8a32b9ad7f39cd7', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f41a4c90c2dae002dbb0df8aa52485db14effff3', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=254234782ad9497172f41d0dfa08de7a3802f420', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09eecc8b7e2cc52a1056320b990e96723184fd78', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c35680018f4f76c3e26e72ca06179a30f615628d', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/B7WdpS-fT6Kp6156fkVgdiyljYYF1gij_pH-pysty9c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e509dac49de5552616980151b13603ee7be56a84', 'width': 1080, 'height': 720}], 'variants': {}, 'id': '7aFHhITed9tDeUdnl57ECf3ekWwSCkVJbtmvq7bmvSk'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gh1uyp,True,,ahmedbesbes,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh1uyp/complete_end_to_end_machine_learning_tutorial/,all_ads,False,https://medium.com/datadriveninvestor/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203,155203,1589118765.0,0,,False,,,,
,learnmachinelearning,"Hi, 
I have few texts that have been graded ( out of 10). Now, I want to  predict the score on my test data (that hasn't been scored ).

Regression, BOW, NLP, etc can be used but i cannot figure out how to predict based on similarity w. R. T my trained dataset.

Idk how should I go about it.",t2_30p1s3ws,False,,0,False,Predict marks from the text,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,True,t3_gh1cf3,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589145471.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, 
I have few texts that have been graded ( out of 10). Now, I want to  predict the score on my test data (that hasn&amp;#39;t been scored ).&lt;/p&gt;

&lt;p&gt;Regression, BOW, NLP, etc can be used but i cannot figure out how to predict based on similarity w. R. T my trained dataset.&lt;/p&gt;

&lt;p&gt;Idk how should I go about it.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gh1cf3,True,,Mayank008,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh1cf3/predict_marks_from_the_text/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh1cf3/predict_marks_from_the_text/,155203,1589116671.0,0,,False,,,,
,learnmachinelearning,"so currently im a IT diploma student, and im pretty new in programming. I'll be continuing my bachelor of computer science in the following month and im going to take data science as my major. Im interested in data science and im planning to take short online course before going to my bachelor's degree. can someone guide to the right direction?",t2_692wpcym,False,,0,False,Data science advice needed,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0w4y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589143629.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;so currently im a IT diploma student, and im pretty new in programming. I&amp;#39;ll be continuing my bachelor of computer science in the following month and im going to take data science as my major. Im interested in data science and im planning to take short online course before going to my bachelor&amp;#39;s degree. can someone guide to the right direction?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0w4y,True,,Scorlibpl,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0w4y/data_science_advice_needed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0w4y/data_science_advice_needed/,155203,1589114829.0,0,,False,,,,
,learnmachinelearning,"Greetings,

&amp;#x200B;

I'm doing a research project on an experimental process that contains 2 main control loops (One SISO for force via airpressure and one MIMO for temperature with 6 pyrometers and 3 IR emitter). As a part of this project I'm supposed to implement ML to speed up the fine adjustments for the control process. The conditions are as following: I have a data set of vectors containing the target position,velocity, pressure and temperature for each point of a trajectory. I have a bunch of sensors to observe the controlled variable and a bunch of actuators to influence the reference value. The values aren't bound that hard, but they have a small influence on each other. The AI is supposed to learn the control process parameters and to tell the actuators when and how to start actuating. Now, I have read into some basics, but I'm missing experience with ML so I don't know where to start. Do you guys have some thoughts, advice or ideas on what would be the best suited ML approach to such a problem and where I can find some good (non general) guides or tutorials?",t2_57fvtgr,False,,0,False,Simple ML in MIMO controll process ?,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0lhl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589142338.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings,&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m doing a research project on an experimental process that contains 2 main control loops (One SISO for force via airpressure and one MIMO for temperature with 6 pyrometers and 3 IR emitter). As a part of this project I&amp;#39;m supposed to implement ML to speed up the fine adjustments for the control process. The conditions are as following: I have a data set of vectors containing the target position,velocity, pressure and temperature for each point of a trajectory. I have a bunch of sensors to observe the controlled variable and a bunch of actuators to influence the reference value. The values aren&amp;#39;t bound that hard, but they have a small influence on each other. The AI is supposed to learn the control process parameters and to tell the actuators when and how to start actuating. Now, I have read into some basics, but I&amp;#39;m missing experience with ML so I don&amp;#39;t know where to start. Do you guys have some thoughts, advice or ideas on what would be the best suited ML approach to such a problem and where I can find some good (non general) guides or tutorials?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0lhl,True,,Nemonekto,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0lhl/simple_ml_in_mimo_controll_process/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0lhl/simple_ml_in_mimo_controll_process/,155203,1589113538.0,0,,False,,,,
,learnmachinelearning,,t2_x8ze3l8,False,,0,False,Understand LSTMS with an Illustrated Guide with a step by step explanation,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gg9agg,False,dark,0.99,,public,312,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Illustrated Guide to LSTM's and GRU's: A step by step explanation"", 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Michael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8HyCNIVRbSU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gg9agg', 'height': 338}",,False,312,,False,https://b.thumbs.redditmedia.com/frgFRjdVN2jVxvaz-xuMQlyl7ObAkhaoMr5O3jLK2Gg.jpg,False,,[],{},rich:video,,False,,1589029602.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?auto=webp&amp;s=e31ba2f9138801dcf0280de960072e93c615ab2a', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab52e4909b466a3e617fe52357bafa662f80247', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f00c0f43fe9fbca1525d5fcbe9c429e4293cd26', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/StjGILOnyQsOSqcc5h2o8qngdRiR-5N-n8HVs05oa1I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecea310beba44167a1963bf1b14dffbdad062481', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'udBbbGTWJYKhiFOWBI2jaHP8TR7u6wcU0Cjls77_HOI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg9agg,True,,LearnedVector,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9agg/understand_lstms_with_an_illustrated_guide_with_a/,all_ads,False,https://www.youtube.com/watch?v=8HyCNIVRbSU,155203,1589000802.0,1,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': ""Illustrated Guide to LSTM's and GRU's: A step by step explanation"", 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/8HyCNIVRbSU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Michael Phi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/8HyCNIVRbSU/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCYpBgT4riB-VpsBBBQkblqQ'}}",False,,,,
,learnmachinelearning,,t2_6d8czqvi,False,,0,False,What parts of AI work/development are generally seen as the most interesting?,[],r/learnmachinelearning,False,6,,0,,True,t3_gh0c43,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589141156.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gh0c43,True,,begintomorrow,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh0c43/what_parts_of_ai_workdevelopment_are_generally/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh0c43/what_parts_of_ai_workdevelopment_are_generally/,155203,1589112356.0,0,,False,,,,
,learnmachinelearning,,t2_6fkdqkbf,False,,0,False,Tool to quickly add single or multiclass labels to images without using your mouse,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,104.0,False,t3_ggsr0y,False,light,1.0,,public,7,0,{},140.0,,False,[],,True,False,,{},Project,False,7,,False,https://b.thumbs.redditmedia.com/iW95-MeMMSy64glEP17cuthvxoHZuv_7oQmsUvEtW5w.jpg,False,,[],{},image,,False,,1589105092.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?format=png8&amp;s=35d252f811ffaaf44b003403a20f3d71e3213617', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=ca2e8a3d6a2de36a7366b73f0ffc587abcada928', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=43b4d0972942b9ea9521b322ab3d53bf3a59fd5c', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=ad87ce2c15ec25fa3f797a9fa1d8df94a1216c2d', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=686e5b6da191b6345a103bfb6b3aeb5b1a1808e2', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=a305faf010774f5e2acbb35fd04a5b122c9aedc9', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=8fb92ad6569e85f16039e1ce387fdfe63146c735', 'width': 1080, 'height': 808}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?s=bfbbb6aa4b64b9836ebf590f46e1bc3fef280238', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;crop=smart&amp;s=74f3d073695943e2dbdde3e6e442d69d264932cf', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;crop=smart&amp;s=a14b6fdeafe6e8144faec22a749ba8a485cd0d0b', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;crop=smart&amp;s=aa09aceeab44827b17f74bbf71bf9b634f3c2a93', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;crop=smart&amp;s=ec7e7d51d4610aa3a7162c82dc7173f81664cac7', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;crop=smart&amp;s=b900016aa781f5fde6eb2350b9fa2e6f3532353b', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;crop=smart&amp;s=e57f007755c7ce1c254ad975133e14ca83aa0521', 'width': 1080, 'height': 808}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?format=mp4&amp;s=6891ac6fd19ad534a334960b1d73e57795dd6d18', 'width': 1620, 'height': 1213}, 'resolutions': [{'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=108&amp;format=mp4&amp;s=2cd0e6b1b26b8a70661631753210ae85d78fbb04', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=216&amp;format=mp4&amp;s=332623600d17adfd362095813a7a913d1c89da25', 'width': 216, 'height': 161}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=320&amp;format=mp4&amp;s=33101b4610a37eca01dc119e70d4e3937393179d', 'width': 320, 'height': 239}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=640&amp;format=mp4&amp;s=850270644dcaf9ab08dcd42df130e5b226faef40', 'width': 640, 'height': 479}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=960&amp;format=mp4&amp;s=38e01e4b22b004ae12d999bd424cc5b8f273ff90', 'width': 960, 'height': 718}, {'url': 'https://preview.redd.it/b2j0wm5ehux41.gif?width=1080&amp;format=mp4&amp;s=cb51b88f8fd88412b519f7ef7e16bb427303d4d5', 'width': 1080, 'height': 808}]}}, 'id': 'rE_kFMT_CVGUvLfwT6S1ksJiB8bOKDPcI7u4f7UVBrc'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggsr0y,True,,diffu5e,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggsr0y/tool_to_quickly_add_single_or_multiclass_labels/,all_ads,False,https://i.redd.it/b2j0wm5ehux41.gif,155203,1589076292.0,0,,False,,,,
,learnmachinelearning,"How do articles which explain CNN generate images( like over-imposong kernel grids on images) 

Not the best example [google search result](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQyQzgHCZvW-xYtZCPq-UbI-Xz5rYc5O2ZzH9Ll1WiLEk1IjpEQ&amp;usqp=CAU)",t2_5cuwjp5q,False,,0,False,Cool animations or still images explaining cnn,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gh01rj,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1589139867.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do articles which explain CNN generate images( like over-imposong kernel grids on images) &lt;/p&gt;

&lt;p&gt;Not the best example &lt;a href=""https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQyQzgHCZvW-xYtZCPq-UbI-Xz5rYc5O2ZzH9Ll1WiLEk1IjpEQ&amp;amp;usqp=CAU""&gt;google search result&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?auto=webp&amp;s=75ce352f178579adec3ef61b3528f21df15c7e0d', 'width': 638, 'height': 479}, 'resolutions': [{'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aea76a051418f62e54cd29dd03aab1169b4b896', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b3099e307da1c2386ec9974d2b07b48a5e29067', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/mz1VBWC8kULoHFGnWeW9fAQto2rNeBjIaUsgXN9g4Io.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9f4e35eb9a0c1d3e33b15ab7dc2829c979f728a', 'width': 320, 'height': 240}], 'variants': {}, 'id': '22vBmEUCfXCxxqlGrwamhcVQA6BhRkZx03_LM6CHuYM'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gh01rj,True,,r2d2FortNite,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gh01rj/cool_animations_or_still_images_explaining_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gh01rj/cool_animations_or_still_images_explaining_cnn/,155203,1589111067.0,0,,False,,,,
,learnmachinelearning,,t2_6awn0nqk,False,,0,False,Humble Book Bundle: Definitive Guides to All Things Programming by O'Reilly (pay what you want and help charity),"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_ggzs42,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,False,https://a.thumbs.redditmedia.com/9sEEZTYgufRV_l9JANb2QXL_XLIDlCVzqgvCLYgYTf8.jpg,False,,[],{},link,,False,,1589138632.0,richtext,6,,,text,humblebundle.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?auto=webp&amp;s=cc7ee2b99a7c43423dd1de45202ecbe68a66f75e', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1471c31024e209de5e30ef75cb9709355e6b3d06', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e83e37a65359332cf95abf71d2dc331bbce502a5', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b21cd8c75dd690a5561f07fc65efdecfa54c1d8', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=10c26d04996b393f69d8ecda3a5edc2f786caf62', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6847adb66fb19cb5856719aaf94f9a7271b372d', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/SiJR7r7DmrmgNYGzDXtoJxJnFKuAlGccAO_FyZA5w5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6f67268c24241f85cef951e8add494d0ec45282', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'W1dtqkKZOrsWPWsN7_TNU7cM-pajRzK1ke6VAHOGqWo'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggzs42,True,,big_clips,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggzs42/humble_book_bundle_definitive_guides_to_all/,all_ads,False,https://www.humblebundle.com/books/definitive-guides-to-all-things-programming-oreilly-books?partner=repsup,155203,1589109832.0,0,,False,,,,
,learnmachinelearning,,t2_5bi9w5n3,False,,0,False,"Hello everyone, i finished learning python and i wanted to learn machine learning as i'm a beginner and need to know where to start, can you help me in this?",[],r/learnmachinelearning,False,6,,0,,False,t3_ggzpwv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589138340.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggzpwv,True,,imashadowguys,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggzpwv/hello_everyone_i_finished_learning_python_and_i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggzpwv/hello_everyone_i_finished_learning_python_and_i/,155203,1589109540.0,0,,False,,,,
,learnmachinelearning,"Fast gradient sign method/attack is popularized with the panda-to-gibbon picture on GoogLeNet

My question is simple: does it also work for other types of model with well-defined loss functions, such as SVM, logistic regression, etc? (for inputs that are perhaps not images)

I'm trying to think up a toy example to illustrate FGSM without having to write 50 lines of Pytorch code.",t2_2kpphupx,False,,0,False,Does fast gradient sign attack also work for non-neural network models?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwe6r,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589121200.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Fast gradient sign method/attack is popularized with the panda-to-gibbon picture on GoogLeNet&lt;/p&gt;

&lt;p&gt;My question is simple: does it also work for other types of model with well-defined loss functions, such as SVM, logistic regression, etc? (for inputs that are perhaps not images)&lt;/p&gt;

&lt;p&gt;I&amp;#39;m trying to think up a toy example to illustrate FGSM without having to write 50 lines of Pytorch code.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwe6r,True,,fromnighttilldawn,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwe6r/does_fast_gradient_sign_attack_also_work_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwe6r/does_fast_gradient_sign_attack_also_work_for/,155203,1589092400.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

**MACHINE LEARNING : Ensemble Learning Methods**

Combine all â€œweakâ€ learner to form ensemble.

Averaging : Equal weights are assigned to different model.

&amp;#x200B;

Bagging (reduces variance) :

Bagging or bootstrap aggregation â€˜reduces varianceâ€™ of an estimate by taking mean of multiple estimates.

Steps :

1. Create randomly sampled datasets of the original training data.
2. 2. Build and fit several classifiers to each of these diverse copies.
3. 3. Take the average of all the predictions to make final overall prediction.

Boosting (reduces bias) :

Boosting â€˜reduces biasâ€™ by training weak learner sequentially,each trying to correct its predecessor.

Steps:

1. Train a Classfier H1 that best classifies the data with respect to accuracy.
2. 2. Identify the region where H1 produces errors,add weights to it and produce a H2 classifier.
3. 3. Exaggerate those samples for which H1 gives a different result from H2 and produces H3 classifier. Repeat step 02 for a new classifier.

Adaboost : Consider a scenario, where there are â€˜+â€™ and â€˜-â€˜

Adaboost Working : Step 1

1. Assign equal weights to each data point.
2. 2. Apply a decision stump to classify them as +(plus) and -(minus)
3. 3. Decision stump (D1) has generated vertical plane at left side to classify.
4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.

Adaboost Working : Step 2

1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.
2. 2. The second decision stump (D2) will try to predict them correctly.
3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.
4. 4. D2 has also caused mis-classification errors to three -(minus)

Adaboost Working : Step 3

1. D3 adds higher weights to three â€“ (minus)
2. 2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.

Adaboost Working : Step 4

1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.

Adaboost Algorithm :

Step 1 : Initially each data point is weighted equally with weight .

. Wi = 1/n

. Where n is the number of samples.

Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.

Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.

. @\^t =1/2 ln 1-e/ e

. @ â€“ alpha , ln â€“ log ,

. e â€“ error , t â€“ time ,

. \^ â€“ to power

Step 4 : Weight after time â€˜tâ€™ is given as :

. Wi\^t+1/z e\^-at.h1(x).y(x)

.

. Z â€“ Normalizing factor

. h1(x).y(x) â€“ sign of the current output

Gradient Boosting (GBM) : Gradient boosting involves three elements.

1. A loss function to be optimised.
2. 2. A weak learner to make predictions.
3. 3. An additive model to add weak learners to minimize the loss function.

GBM Mechanism :

1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.
2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.
3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.
4. 4. Modeling is stopped when residuals do not have any pattern that can be modeled.

Adaboost Working : Step 1

1. Assign equal weights to each data point.
2. 2. Apply a decision stump to classify them as +(plus) and -(minus)
3. 3. Decision stump (D1) has generated vertical plane at left side to classify.
4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.

Adaboost Working : Step 2

1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.
2. 2. The second decision stump (D2) will try to predict them correctly.
3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.
4. 4. D2 has also caused mis-classification errors to three -(minus)

Adaboost Working : Step 3

1. D3 adds higher weights to three â€“ (minus)
2. 2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.

Adaboost Working : Step 4

1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.

Adaboost Algorithm :

Step 1 : Initially each data point is weighted equally with weight .

. Wi = 1/n

. Where n is the number of samples.

Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.

Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.

. @\^t =1/2 ln 1-e/ e

. @ â€“ alpha , ln â€“ log ,

. e â€“ error , t â€“ time ,

. \^ â€“ to power

Step 4 : Weight after time â€˜tâ€™ is given as :

. Wi\^t+1/z e\^-at.h1(x).y(x)

.

. Z â€“ Normalizing factor

. h1(x).y(x) â€“ sign of the current output

Gradient Boosting (GBM) : Gradient boosting involves three elements.

1. A loss function to be optimised.
2. 2. A weak learner to make predictions.
3. 3. An additive model to add weak learners to minimize the loss function.

GBM Mechanism :

1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.
2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.
3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.
4. 4. Modelling is stopped when residuals do not have any pattern that can be modelled.

&amp;#x200B;

GBM Algorithm Steps :

1. Fit a simple regression or classification model.
2. 2. Calculate error residuals (actual value â€“ predicted value)
3. 3. Fit a new model on error residuals as targets variable with same input variables.
4. 4. Add the predicted residuals to the previous predictions.
5. 5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.

XGBoost: eXtreme Gradient Boosting is a library for developing fast and high -performance gradient boosting tree models.XGBoost is extensively used in ML competitions as it is almost 10 times faster than other gradient boosting techniques.

XGBoost Parameters :

1. General Parameters: Number of threads.
2. 2. Booster Parameters: a. Step size. b. Regularisation
3. 3. Task Parameters: a. Objective. b. Evaluation metric.

General Parameters :

nthread :

1. Number of parallel threads.
2. 2. If no value is entered,algorithm automatically detects the number of cores and runs on all the cores.

booster :

1. gbtree : tree-based model
2.  gblinear : linear function

Silent \[default =0\] :

1. If set to 1, no running messages will be printed.Hence,keep it â€˜0â€™ as the messages might help in understanding the model.

Booster Parameters : Booster parameters guide individual booster (Tree/Regression) at each.

Parameters for tree booster :

eta : Step size shrinkage is used in update to prevent overfitting. Range in \[0,1\], default 0.3

gamma : Minimum loss reduction required to make a split. Range \[0,infinite\],default 0

max\_depth : Maximum depth of the tree. Range \[1,infinite\],default 6

min\_child\_weight : minimum sum of instance weight needed in a child. Range \[0,Range\] , default 1

**For Videos and More:** [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)

 **Blog Source:** 

[https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41](https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41)",t2_205ygpnb,False,,0,False,What is 'Ensemble Learning Methods' and How to do it?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggum9d,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1589112721.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MACHINE LEARNING : Ensemble Learning Methods&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Combine all â€œweakâ€ learner to form ensemble.&lt;/p&gt;

&lt;p&gt;Averaging : Equal weights are assigned to different model.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Bagging (reduces variance) :&lt;/p&gt;

&lt;p&gt;Bagging or bootstrap aggregation â€˜reduces varianceâ€™ of an estimate by taking mean of multiple estimates.&lt;/p&gt;

&lt;p&gt;Steps :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create randomly sampled datasets of the original training data.&lt;/li&gt;
&lt;li&gt;2. Build and fit several classifiers to each of these diverse copies.&lt;/li&gt;
&lt;li&gt;3. Take the average of all the predictions to make final overall prediction.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Boosting (reduces bias) :&lt;/p&gt;

&lt;p&gt;Boosting â€˜reduces biasâ€™ by training weak learner sequentially,each trying to correct its predecessor.&lt;/p&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train a Classfier H1 that best classifies the data with respect to accuracy.&lt;/li&gt;
&lt;li&gt;2. Identify the region where H1 produces errors,add weights to it and produce a H2 classifier.&lt;/li&gt;
&lt;li&gt;3. Exaggerate those samples for which H1 gives a different result from H2 and produces H3 classifier. Repeat step 02 for a new classifier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost : Consider a scenario, where there are â€˜+â€™ and â€˜-â€˜&lt;/p&gt;

&lt;p&gt;Adaboost Working : Step 1&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assign equal weights to each data point.&lt;/li&gt;
&lt;li&gt;2. Apply a decision stump to classify them as +(plus) and -(minus)&lt;/li&gt;
&lt;li&gt;3. Decision stump (D1) has generated vertical plane at left side to classify.&lt;/li&gt;
&lt;li&gt;4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 2&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.&lt;/li&gt;
&lt;li&gt;2. The second decision stump (D2) will try to predict them correctly.&lt;/li&gt;
&lt;li&gt;3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.&lt;/li&gt;
&lt;li&gt;4. D2 has also caused mis-classification errors to three -(minus)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 3&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D3 adds higher weights to three â€“ (minus)&lt;/li&gt;
&lt;li&gt;2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 4&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Algorithm :&lt;/p&gt;

&lt;p&gt;Step 1 : Initially each data point is weighted equally with weight .&lt;/p&gt;

&lt;p&gt;. Wi = 1/n&lt;/p&gt;

&lt;p&gt;. Where n is the number of samples.&lt;/p&gt;

&lt;p&gt;Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.&lt;/p&gt;

&lt;p&gt;Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.&lt;/p&gt;

&lt;p&gt;. @^t =1/2 ln 1-e/ e&lt;/p&gt;

&lt;p&gt;. @ â€“ alpha , ln â€“ log ,&lt;/p&gt;

&lt;p&gt;. e â€“ error , t â€“ time ,&lt;/p&gt;

&lt;p&gt;. ^ â€“ to power&lt;/p&gt;

&lt;p&gt;Step 4 : Weight after time â€˜tâ€™ is given as :&lt;/p&gt;

&lt;p&gt;. Wi^t+1/z e^-at.h1(x).y(x)&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;. Z â€“ Normalizing factor&lt;/p&gt;

&lt;p&gt;. h1(x).y(x) â€“ sign of the current output&lt;/p&gt;

&lt;p&gt;Gradient Boosting (GBM) : Gradient boosting involves three elements.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A loss function to be optimised.&lt;/li&gt;
&lt;li&gt;2. A weak learner to make predictions.&lt;/li&gt;
&lt;li&gt;3. An additive model to add weak learners to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GBM Mechanism :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GBM predicts the residuals or error of prior models and then sums them to make the final prediction.&lt;/li&gt;
&lt;li&gt;2. One weak learner is added at a time and existing weak learners in the model are left unchanged.&lt;/li&gt;
&lt;li&gt;3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.&lt;/li&gt;
&lt;li&gt;4. Modeling is stopped when residuals do not have any pattern that can be modeled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 1&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assign equal weights to each data point.&lt;/li&gt;
&lt;li&gt;2. Apply a decision stump to classify them as +(plus) and -(minus)&lt;/li&gt;
&lt;li&gt;3. Decision stump (D1) has generated vertical plane at left side to classify.&lt;/li&gt;
&lt;li&gt;4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 2&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.&lt;/li&gt;
&lt;li&gt;2. The second decision stump (D2) will try to predict them correctly.&lt;/li&gt;
&lt;li&gt;3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.&lt;/li&gt;
&lt;li&gt;4. D2 has also caused mis-classification errors to three -(minus)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 3&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D3 adds higher weights to three â€“ (minus)&lt;/li&gt;
&lt;li&gt;2. Horizontal line is generated to classify + (plus) and â€“ (minus) based on higher weight of mis-classified observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Working : Step 4&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Adaboost Algorithm :&lt;/p&gt;

&lt;p&gt;Step 1 : Initially each data point is weighted equally with weight .&lt;/p&gt;

&lt;p&gt;. Wi = 1/n&lt;/p&gt;

&lt;p&gt;. Where n is the number of samples.&lt;/p&gt;

&lt;p&gt;Step 2 : A classifier â€˜H1â€™ is picked up the best classifies the data with minimal error rate.&lt;/p&gt;

&lt;p&gt;Step 3 : The weighing factor â€˜@ (alpha)â€™ is dependant on errors (e) caused by the H1 classifier.&lt;/p&gt;

&lt;p&gt;. @^t =1/2 ln 1-e/ e&lt;/p&gt;

&lt;p&gt;. @ â€“ alpha , ln â€“ log ,&lt;/p&gt;

&lt;p&gt;. e â€“ error , t â€“ time ,&lt;/p&gt;

&lt;p&gt;. ^ â€“ to power&lt;/p&gt;

&lt;p&gt;Step 4 : Weight after time â€˜tâ€™ is given as :&lt;/p&gt;

&lt;p&gt;. Wi^t+1/z e^-at.h1(x).y(x)&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;. Z â€“ Normalizing factor&lt;/p&gt;

&lt;p&gt;. h1(x).y(x) â€“ sign of the current output&lt;/p&gt;

&lt;p&gt;Gradient Boosting (GBM) : Gradient boosting involves three elements.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A loss function to be optimised.&lt;/li&gt;
&lt;li&gt;2. A weak learner to make predictions.&lt;/li&gt;
&lt;li&gt;3. An additive model to add weak learners to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GBM Mechanism :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GBM predicts the residuals or error of prior models and then sums them to make the final prediction.&lt;/li&gt;
&lt;li&gt;2. One weak learner is added at a time and existing weak learners in the model are left unchanged.&lt;/li&gt;
&lt;li&gt;3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.&lt;/li&gt;
&lt;li&gt;4. Modelling is stopped when residuals do not have any pattern that can be modelled.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;GBM Algorithm Steps :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fit a simple regression or classification model.&lt;/li&gt;
&lt;li&gt;2. Calculate error residuals (actual value â€“ predicted value)&lt;/li&gt;
&lt;li&gt;3. Fit a new model on error residuals as targets variable with same input variables.&lt;/li&gt;
&lt;li&gt;4. Add the predicted residuals to the previous predictions.&lt;/li&gt;
&lt;li&gt;5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;XGBoost: eXtreme Gradient Boosting is a library for developing fast and high -performance gradient boosting tree models.XGBoost is extensively used in ML competitions as it is almost 10 times faster than other gradient boosting techniques.&lt;/p&gt;

&lt;p&gt;XGBoost Parameters :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;General Parameters: Number of threads.&lt;/li&gt;
&lt;li&gt;2. Booster Parameters: a. Step size. b. Regularisation&lt;/li&gt;
&lt;li&gt;3. Task Parameters: a. Objective. b. Evaluation metric.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;General Parameters :&lt;/p&gt;

&lt;p&gt;nthread :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Number of parallel threads.&lt;/li&gt;
&lt;li&gt;2. If no value is entered,algorithm automatically detects the number of cores and runs on all the cores.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;booster :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;gbtree : tree-based model&lt;/li&gt;
&lt;li&gt; gblinear : linear function&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Silent [default =0] :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If set to 1, no running messages will be printed.Hence,keep it â€˜0â€™ as the messages might help in understanding the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Booster Parameters : Booster parameters guide individual booster (Tree/Regression) at each.&lt;/p&gt;

&lt;p&gt;Parameters for tree booster :&lt;/p&gt;

&lt;p&gt;eta : Step size shrinkage is used in update to prevent overfitting. Range in [0,1], default 0.3&lt;/p&gt;

&lt;p&gt;gamma : Minimum loss reduction required to make a split. Range [0,infinite],default 0&lt;/p&gt;

&lt;p&gt;max_depth : Maximum depth of the tree. Range [1,infinite],default 6&lt;/p&gt;

&lt;p&gt;min_child_weight : minimum sum of instance weight needed in a child. Range [0,Range] , default 1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For Videos and More:&lt;/strong&gt; &lt;a href=""https://www.facebook.com/seevecoding""&gt;www.facebook.com/seevecoding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Blog Source:&lt;/strong&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41""&gt;https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vKw6ateLXak-cgwwbDwy3FEyQp7ntHJChB8-5DhkkkQ.jpg?auto=webp&amp;s=ec1415d6df378dd491dad04991f28c1ee5a52d18', 'width': 200, 'height': 200}, 'resolutions': [{'url': 'https://external-preview.redd.it/vKw6ateLXak-cgwwbDwy3FEyQp7ntHJChB8-5DhkkkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a51180563bf098a2101bb79f4b2caad7ca3e7bcf', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'wukwke95vIPcLfRmc-wWi8RZ2RVJUsRM30JL36CjMWE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggum9d,True,,iamrealadvait,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/,155203,1589083921.0,0,,False,,,,
,learnmachinelearning,,t2_6dcghpa8,False,,0,False,Step wise guide to build handwritten Digit Recognition using cnn and opencv,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggyumn,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Handwritten Digit Recognition using machine learning | keras and opencv | [github] |full explanation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'ProgrammingHut', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qmY36LzdPHo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCx1_WfGX9D9rmsJNBM5qsMA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggyumn', 'height': 338}",Project,False,1,,False,https://b.thumbs.redditmedia.com/XtBkLCQnPdyLpZFdzuA2uxZvMddPt3tNR78f72awlkk.jpg,False,,[],{},rich:video,,False,,1589134009.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?auto=webp&amp;s=31f862a3e09805072774d557184a0068e1d45f77', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b740404c0b0d2ef7d74a84c95ca2a692f65be505', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=926868336c7a747215182e7531f1e6d354577238', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/SCErtW97AsabqrUVATVgHhx4arPwNHaJcukNLV6hI9Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6c169579a3953b0a0aacf4af73335f673bfbf2f', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'wg8I7HoSBM_S_yx7864fojmVuGyghz-jMbMki6tD5_s'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggyumn,True,,Pawan315,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyumn/step_wise_guide_to_build_handwritten_digit/,all_ads,False,https://youtu.be/qmY36LzdPHo,155203,1589105209.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Handwritten Digit Recognition using machine learning | keras and opencv | [github] |full explanation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/qmY36LzdPHo?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'ProgrammingHut', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/qmY36LzdPHo/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCx1_WfGX9D9rmsJNBM5qsMA'}}",False,,,,
,learnmachinelearning,,t2_4w7oamkl,False,,0,False,"In Variational Autoencoders, does the generative model generates samples from latent variables which are sampled from a multivariate distribution? If yes, then is this similar in case of GANs?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggyh0m,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589132040.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggyh0m,True,,HTKasd,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggyh0m/in_variational_autoencoders_does_the_generative/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggyh0m/in_variational_autoencoders_does_the_generative/,155203,1589103240.0,0,,False,,,,
,learnmachinelearning,,t2_6fnjwm42,False,,0,False,Guys check out our FREE AI and machine learning course,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggydpl,False,light,1.0,,public,1,0,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'm.youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'description': ""An intro on Artificial intelligence and Machine Learning Ep : 1 PS that's a crow in the beginning"", 'title': 'Learn AI and Machine learning basics', 'url': 'https://www.youtube.com/watch?v=dqWUIJjFrmM', 'type': 'video', 'author_name': 'AI washingmachine', 'height': 338, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,False,,"{'content': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggydpl', 'height': 338}",Project,False,1,,False,https://a.thumbs.redditmedia.com/dY5JfMGrsw9r9TGoMZxlwrkr1sCcyBTIMHTOBSXxVY0.jpg,False,,[],{},rich:video,,False,,1589131563.0,richtext,6,,,text,m.youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?auto=webp&amp;s=1733febf5a0bd6fae80eb674d48df8d52d71e9fc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9be822395a8b233483be8ba694b9810839ebb07', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c54724b4e15a9584f83a5404e64a9f6472582e6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8833c15b014479bdbf037635f34c11ea2cd2662', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KtY5la4OC1bioPef9YIyQumQxjs6NhTgWt9gt1HjmLU'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggydpl,True,,AI_washingmachine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggydpl/guys_check_out_our_free_ai_and_machine_learning/,all_ads,False,https://m.youtube.com/watch?feature=youtu.be&amp;v=dqWUIJjFrmM,155203,1589102763.0,0,"{'type': 'm.youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'description': ""An intro on Artificial intelligence and Machine Learning Ep : 1 PS that's a crow in the beginning"", 'title': 'Learn AI and Machine learning basics', 'url': 'https://www.youtube.com/watch?v=dqWUIJjFrmM', 'type': 'video', 'author_name': 'AI washingmachine', 'height': 338, 'width': 600, 'html': '&lt;iframe class=""embedly-embed"" src=""https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FdqWUIJjFrmM%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DdqWUIJjFrmM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FdqWUIJjFrmM%2Fhqdefault.jpg&amp;key=ed8fa8699ce04833838e66ce79ba05f1&amp;type=text%2Fhtml&amp;schema=youtube"" width=""600"" height=""338"" scrolling=""no"" title=""YouTube embed"" frameborder=""0"" allow=""autoplay; fullscreen"" allowfullscreen=""true""&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'version': '1.0', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,"Awesome talk on Tensorflow.js, Machine Learning in javascript by Jason Mayes Senior Advocate at Google",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggxz3x,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tensorflow.js : Machine Learning in JavaScript', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/x-608hSAhCA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggxz3x', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/FAoo3VgO6L-UrZGx6hbJjX9T_wEZs-Fqg6uH4uBonTs.jpg,False,,[],{},rich:video,,False,,1589129434.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?auto=webp&amp;s=5b594ea3d7cb5c146c1eb9a8c3f89d08b39e6bea', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3af1cebb089cf1f826c5e0102b8b71d8b8ca70a0', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=246df40b605e32386c6fa4fd1c3d1bd8f061e606', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/oTByQrbjUGxb8d7dkJ2Ege6A-tqy-39Ma5YjiR9EatY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b77451d214bbdf1ed7475220c108372a4a205e5', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'oHrzZyYUjrYCiVRz9gXci67sj67Xe5e1wCTo94LX_Ho'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggxz3x,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxz3x/awesome_talk_on_tensorflowjs_machine_learning_in/,all_ads,False,https://www.youtube.com/watch?v=x-608hSAhCA&amp;feature=emb_logo,155203,1589100634.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Tensorflow.js : Machine Learning in JavaScript', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/x-608hSAhCA?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/x-608hSAhCA/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,,,,
,learnmachinelearning,"I've an matrix X with size (20000000, 10), and set KFold split into 5 folds.

When I split the data iteratively, as:

`for train_id, test_id in kfold.split(x):`

  `x_train = data[train_id]`

It shows the error:

 `'Passing list-likes to .loc or [] with any missing labels is no longer supported` 

So I check for the train\_id if there is any problem, and it shows the first fold ID:

 `[ 4022530  4022531  4022532 ... 20112646 20112647 20112648]` 

&amp;#x200B;

This is strange result, for I should get 4000000 each fold (total size / n\_fold).

I believe this is the problem, but I can't figure out why. My setting of KFold doesn't seem to have any problem:

`kfold = sklearn.model_selection.KFold(n_splits = 5)`

Could anyone help me?",t2_11cquw,False,,0,False,KFold in sklearn give strange results of train_id,[],r/learnmachinelearning,False,6,,0,,False,t3_ggxo9c,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589127847.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve an matrix X with size (20000000, 10), and set KFold split into 5 folds.&lt;/p&gt;

&lt;p&gt;When I split the data iteratively, as:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;for train_id, test_id in kfold.split(x):&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x_train = data[train_id]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It shows the error:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;&amp;#39;Passing list-likes to .loc or [] with any missing labels is no longer supported&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;So I check for the train_id if there is any problem, and it shows the first fold ID:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;[ 4022530  4022531  4022532 ... 20112646 20112647 20112648]&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;This is strange result, for I should get 4000000 each fold (total size / n_fold).&lt;/p&gt;

&lt;p&gt;I believe this is the problem, but I can&amp;#39;t figure out why. My setting of KFold doesn&amp;#39;t seem to have any problem:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kfold = sklearn.model_selection.KFold(n_splits = 5)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Could anyone help me?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggxo9c,True,,Laurence-Lin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggxo9c/kfold_in_sklearn_give_strange_results_of_train_id/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggxo9c/kfold_in_sklearn_give_strange_results_of_train_id/,155203,1589099047.0,0,,False,,,,
,learnmachinelearning,"Iâ€™ve been struggling to find a thesis topic and after a lot of research I finally found something Iâ€™m interested it and want to get some opinions on it. 

Basically I was wondering if ML learning could be used to look at information about a case and connect it to existing legal precedents? 

Iâ€™m in a combined bs/ms program and just starting out so I could use some direction!",t2_i7bmsnh,False,,0,False,Machine learning to Identify Legal Precedent. MS DS thesis topic?,[],r/learnmachinelearning,False,6,,0,,False,t3_gguebw,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589111779.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™ve been struggling to find a thesis topic and after a lot of research I finally found something Iâ€™m interested it and want to get some opinions on it. &lt;/p&gt;

&lt;p&gt;Basically I was wondering if ML learning could be used to look at information about a case and connect it to existing legal precedents? &lt;/p&gt;

&lt;p&gt;Iâ€™m in a combined bs/ms program and just starting out so I could use some direction!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gguebw,True,,whyshali,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gguebw/machine_learning_to_identify_legal_precedent_ms/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gguebw/machine_learning_to_identify_legal_precedent_ms/,155203,1589082979.0,0,,False,,,,
,learnmachinelearning,I completed the first assignment in deeplearning.ai and got 100/100.But i still dont understand how numbers equate image values and it all seems like magic to me.Is it normal or there's a better explanation or am i doing something wrong?,t2_1af95696,False,,0,False,Need help with deeper understanding of what's going on in neural networks.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggx4fu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1589124966.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I completed the first assignment in deeplearning.ai and got 100/100.But i still dont understand how numbers equate image values and it all seems like magic to me.Is it normal or there&amp;#39;s a better explanation or am i doing something wrong?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggx4fu,True,,_notdivyanshuuuu,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggx4fu/need_help_with_deeper_understanding_of_whats/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggx4fu/need_help_with_deeper_understanding_of_whats/,155203,1589096166.0,0,,False,,,,
,learnmachinelearning,"In google colab, I'd turn on the GPU in the setting. However, recently it kept showing up ""You are connected to GPU, but not utlilizing it.""

And I found that my lightgbm training is really slow. I tried to set the parameters in lightgbm 'device\_type' to 'gpu', but it's still slow and the message still indicate that GPU is not utilized.

Now I'm wondering if I should reinstall lightgbm to configure GPU setting. 

Should I set up any GPU settings while my device contains one, in order to achieve speed up? I'd never done that before, this is the first time problem occur.

&amp;#x200B;

Thanks for any help!",t2_11cquw,False,,0,False,"If my envorinment(ex: Google Colab) enables GPU, should I set up GPU environment for speed up training and other code running?",[],r/learnmachinelearning,False,6,,0,,False,t3_ggwrsv,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589123130.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In google colab, I&amp;#39;d turn on the GPU in the setting. However, recently it kept showing up &amp;quot;You are connected to GPU, but not utlilizing it.&amp;quot;&lt;/p&gt;

&lt;p&gt;And I found that my lightgbm training is really slow. I tried to set the parameters in lightgbm &amp;#39;device_type&amp;#39; to &amp;#39;gpu&amp;#39;, but it&amp;#39;s still slow and the message still indicate that GPU is not utilized.&lt;/p&gt;

&lt;p&gt;Now I&amp;#39;m wondering if I should reinstall lightgbm to configure GPU setting. &lt;/p&gt;

&lt;p&gt;Should I set up any GPU settings while my device contains one, in order to achieve speed up? I&amp;#39;d never done that before, this is the first time problem occur.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwrsv,True,,Laurence-Lin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwrsv/if_my_envorinmentex_google_colab_enables_gpu/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwrsv/if_my_envorinmentex_google_colab_enables_gpu/,155203,1589094330.0,0,,False,,,,
,learnmachinelearning,"Hi Iâ€™m high school student 

Iâ€™m now studying intro to machine learning with TF nanodegree and I finished Ai programming with python nano degree 

but I feel like Udacity doesnâ€™t provide the full information so I sometimes use google or hands on machine learning with sklear..

But I forgot some of what 
I learnt Before two weeks

Is it because Iâ€™m learning 
fast or thatâ€™s regular ?

Or I have to  practicing everything ?
Like practice in kaggle notebooks 

How much time I have to put in algorithm ?

Please help me 
I already learn a lot of linear algebra 
and some of probability and calculus
And I have some knowledge in 
python ,NumPy ,pandas ,sklearn",t2_3xc4pyo7,False,,0,False,Iâ€™m LOST,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggwqco,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589122921.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Iâ€™m high school student &lt;/p&gt;

&lt;p&gt;Iâ€™m now studying intro to machine learning with TF nanodegree and I finished Ai programming with python nano degree &lt;/p&gt;

&lt;p&gt;but I feel like Udacity doesnâ€™t provide the full information so I sometimes use google or hands on machine learning with sklear..&lt;/p&gt;

&lt;p&gt;But I forgot some of what 
I learnt Before two weeks&lt;/p&gt;

&lt;p&gt;Is it because Iâ€™m learning 
fast or thatâ€™s regular ?&lt;/p&gt;

&lt;p&gt;Or I have to  practicing everything ?
Like practice in kaggle notebooks &lt;/p&gt;

&lt;p&gt;How much time I have to put in algorithm ?&lt;/p&gt;

&lt;p&gt;Please help me 
I already learn a lot of linear algebra 
and some of probability and calculus
And I have some knowledge in 
python ,NumPy ,pandas ,sklearn&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggwqco,True,,i3zM,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwqco/im_lost/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwqco/im_lost/,155203,1589094121.0,0,,False,,,,
,learnmachinelearning,"Hey guys, has anyone used [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/) to organize their projects in the past? I'm fairly new to it and am trying to figure out how to integrate my existing data pipeline code into a cookiecutter format.   


If you have used cookiecutter for data science before, please message me. Any help appreciated. Thanks!",t2_3loxj2cz,False,,0,False,Help with using Cookiecutter Data Science,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwlof,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589122248.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, has anyone used &lt;a href=""https://drivendata.github.io/cookiecutter-data-science/""&gt;Cookiecutter Data Science&lt;/a&gt; to organize their projects in the past? I&amp;#39;m fairly new to it and am trying to figure out how to integrate my existing data pipeline code into a cookiecutter format.   &lt;/p&gt;

&lt;p&gt;If you have used cookiecutter for data science before, please message me. Any help appreciated. Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwlof,True,,hedgehogist,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwlof/help_with_using_cookiecutter_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwlof/help_with_using_cookiecutter_data_science/,155203,1589093448.0,0,,False,,,,
,learnmachinelearning,,t2_21n30k0y,False,,0,False,Unit Neurons v1.0 (C++ Neural Network Library) Release Trailer,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggtgr0,False,light,1.0,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggtgr0', 'height': 338}",Project,False,2,,False,https://b.thumbs.redditmedia.com/8Ic4laTF05YVxRsRJ0NuRknbRTJbJKAtEnN5KkFkzzE.jpg,False,,[],{},rich:video,,False,,1589107900.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?auto=webp&amp;s=7558654250d1f6bba3e596f2aa61e5168d718a79', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60b7736699cec361507c993cb3acd3d1e4e58683', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eb26974ca87809c0d5b6bb511eee0e1b0790e67', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e86e8b32bd9bbf7b6d4f3da9b87cd37ef85fc39', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cLsNNIo94xZ7ympHy6nSQb3vA1nSiSo5r5OnkWpcHKA'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggtgr0,True,,johnlime3301,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggtgr0/unit_neurons_v10_c_neural_network_library_release/,all_ads,False,https://youtu.be/3yW3_18dBIg,155203,1589079100.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}",False,"[{'approved_at_utc': None, 'subreddit': 'DecisionTheory', 'selftext': '', 'author_fullname': 't2_21n30k0y', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Unit Neurons v1.0 (C++ Neural Network Library) Release Trailer', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/DecisionTheory', 'hidden': False, 'pwls': None, 'link_flair_css_class': 'reinforcementLearning', 'downs': 0, 'thumbnail_height': 105, 'hide_score': False, 'name': 't3_ggi4gu', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.56, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggi4gu', 'height': 338}, 'link_flair_text': 'RL', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'image', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1589069467.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?auto=webp&amp;s=7558654250d1f6bba3e596f2aa61e5168d718a79', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60b7736699cec361507c993cb3acd3d1e4e58683', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eb26974ca87809c0d5b6bb511eee0e1b0790e67', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/LcOzYR_tAvQMi9uLvLgIXkwGyqGlaj6hLGI6ltaeoKU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e86e8b32bd9bbf7b6d4f3da9b87cd37ef85fc39', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cLsNNIo94xZ7ympHy6nSQb3vA1nSiSo5r5OnkWpcHKA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '5ada5386-b7b1-11e5-b99a-0eed2e8dfa3b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_38gi4', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ggi4gu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'johnlime3301', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/DecisionTheory/comments/ggi4gu/unit_neurons_v10_c_neural_network_library_release/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://youtu.be/3yW3_18dBIg', 'subreddit_subscribers': 1649, 'created_utc': 1589040667.0, 'num_crossposts': 4, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Unit Neurons v1.0 Release Trailer', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/3yW3_18dBIg?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'John Lime', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/3yW3_18dBIg/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCO8uZURqxMK7105RPXBlEoQ'}}, 'is_video': False}]",t3_ggi4gu,,
,learnmachinelearning,"I'm currently learning about CNN (Convolutional Neural Network) and apparently  MLP (Multi Layer Perceptrion) is commonly used as fully connected in CNN.

Recently i heard about GAP (Global Average Pooling) as alternative of MLP as fully connected in CNN and it works well on few simple cases i tested.

So i wonder if there are any alternative for fully connected in CNN besides MLP and GAP?",t2_4weapg0g,False,,0,False,Are there alternative for fully connected in CNN besides MLP and GAP?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggwf44,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589121325.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently learning about CNN (Convolutional Neural Network) and apparently  MLP (Multi Layer Perceptrion) is commonly used as fully connected in CNN.&lt;/p&gt;

&lt;p&gt;Recently i heard about GAP (Global Average Pooling) as alternative of MLP as fully connected in CNN and it works well on few simple cases i tested.&lt;/p&gt;

&lt;p&gt;So i wonder if there are any alternative for fully connected in CNN besides MLP and GAP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwf44,True,,05e981ae,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwf44/are_there_alternative_for_fully_connected_in_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggwf44/are_there_alternative_for_fully_connected_in_cnn/,155203,1589092525.0,0,,False,,,,
,learnmachinelearning,,t2_6fnjwm42,False,,0,False,Guysss check out our FREE AI and machine learning course,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggwexd,False,dark,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Learn AI and Machine learning basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI washingmachine', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggwexd', 'height': 338}",,False,1,,False,https://b.thumbs.redditmedia.com/exEYZe8wBZihfyhoIkuvtdBfxF9VoU1uLu7YdgRfGNc.jpg,False,,[],{},rich:video,,False,,1589121301.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?auto=webp&amp;s=1733febf5a0bd6fae80eb674d48df8d52d71e9fc', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9be822395a8b233483be8ba694b9810839ebb07', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c54724b4e15a9584f83a5404e64a9f6472582e6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/XkK0Nshj57f9W_o2_3dGAEsHyNpbR7YbIo4ULZvAFH4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8833c15b014479bdbf037635f34c11ea2cd2662', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'KtY5la4OC1bioPef9YIyQumQxjs6NhTgWt9gt1HjmLU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggwexd,True,,AI_washingmachine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggwexd/guysss_check_out_our_free_ai_and_machine_learning/,all_ads,False,https://youtu.be/dqWUIJjFrmM,155203,1589092501.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Learn AI and Machine learning basics', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/dqWUIJjFrmM?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AI washingmachine', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/dqWUIJjFrmM/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLI38xJS64oFl2__it1C76A'}}",False,,,,
,learnmachinelearning,"Hey! I just created a blog linking to some of the best resources out there to get started working with Machine Learning, from a Deep Learning approach. Feel free to let me know what you think.  [https://sciteens.org/blog/zero-to-hero-ai](https://sciteens.org/blog/zero-to-hero-ai)",t2_9bn8ubx,False,,0,False,Zero to Hero: The (Mostly) Free Guide to Getting Started in A.I.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggipa5,False,dark,0.93,,public,12,0,{},,,False,[],,False,False,,{},,False,12,,False,self,False,,[],{},,,True,,1589071240.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey! I just created a blog linking to some of the best resources out there to get started working with Machine Learning, from a Deep Learning approach. Feel free to let me know what you think.  &lt;a href=""https://sciteens.org/blog/zero-to-hero-ai""&gt;https://sciteens.org/blog/zero-to-hero-ai&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggipa5,True,,JSutie,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggipa5/zero_to_hero_the_mostly_free_guide_to_getting/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggipa5/zero_to_hero_the_mostly_free_guide_to_getting/,155203,1589042440.0,0,,False,,,,
,learnmachinelearning,"Hi, so I recently started studying about data science, machine learning, neural networks, deep learning etc and my PC is so old that my cpu goes 100% even in the simplest kaggle tasks. I think that investing in threadripper 2920x , a x399 mobo , 32gb ram and one graphics card is a good idea for now. Do you have any suggestions? Thank you.",t2_2jobwlgs,False,,0,False,Pc build for machine learning and data science.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggqtlb,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589097885.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, so I recently started studying about data science, machine learning, neural networks, deep learning etc and my PC is so old that my cpu goes 100% even in the simplest kaggle tasks. I think that investing in threadripper 2920x , a x399 mobo , 32gb ram and one graphics card is a good idea for now. Do you have any suggestions? Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggqtlb,True,,CarelessWrangler3,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggqtlb/pc_build_for_machine_learning_and_data_science/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggqtlb/pc_build_for_machine_learning_and_data_science/,155203,1589069085.0,0,,False,,,,
,learnmachinelearning,,t2_1084g6,False,,0,False,Multilabel Image Classifier for predicting Fashion clothing Type,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggw0rx,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clothing Feature Detection Demo', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YX9uwoPgG5E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggw0rx', 'height': 338}",Project,False,1,,False,https://a.thumbs.redditmedia.com/dhHDSMw5oT8uUxC9KBjKy3VOYDNkboJPu3a-nvhuzw0.jpg,False,,[],{},rich:video,,False,,1589119326.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?auto=webp&amp;s=d2259c26a8fd38457e0521a2db9cc0f93dd75e13', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f84e46a98a210e84480d5c5bf683ea1e449188b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a58310ec673b47208a596dcf7fc7f2ae4132300', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/QYbO5ck03GUo_Tsmz5_UW9GMkA5O5AgV1EPrGMmWSHA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c8122068f8df8a0acec4d295eb33a6b241beb57', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'MCzyABrx413d1EjvUD377rlmIlkuVL7gvPVxqLBy4Ys'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggw0rx,True,,rednivrug,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggw0rx/multilabel_image_classifier_for_predicting/,all_ads,False,https://www.youtube.com/watch?v=YX9uwoPgG5E,155203,1589090526.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Clothing Feature Detection Demo', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/YX9uwoPgG5E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YX9uwoPgG5E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,,,,
,learnmachinelearning,"What is the difference between additive attention and concat alignment method in multiplicative attention paper ?

I know that they use decoder hidden state from different locations.

But the thing I don't understand is , in additive attention do we do a element-wise addition , as the name ?

And in concat do we concatenate each encoder hidden state with with decoder hidden state ?

Thank you",t2_4fsxelmr,False,,0,False,Additive Attention and Multiplicative Attention Question,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggvx0e,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589118820.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What is the difference between additive attention and concat alignment method in multiplicative attention paper ?&lt;/p&gt;

&lt;p&gt;I know that they use decoder hidden state from different locations.&lt;/p&gt;

&lt;p&gt;But the thing I don&amp;#39;t understand is , in additive attention do we do a element-wise addition , as the name ?&lt;/p&gt;

&lt;p&gt;And in concat do we concatenate each encoder hidden state with with decoder hidden state ?&lt;/p&gt;

&lt;p&gt;Thank you&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggvx0e,True,,chirathpansilu,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggvx0e/additive_attention_and_multiplicative_attention/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggvx0e/additive_attention_and_multiplicative_attention/,155203,1589090020.0,0,,False,,,,
,learnmachinelearning,,t2_4an3hpeg,False,,0,False,"This chair model was trained in RunwayML. The projection was made in GoogleColab, using StyleGAN2, to explore the latent space between the ChairGAN with the three well-known chairs.","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,87.0,False,t3_ggib63,False,light,0.92,,public,9,0,{},140.0,,False,[],,True,False,,{},Discussion,False,9,,False,https://a.thumbs.redditmedia.com/ca4tlrLrbasM0tlTVcA5FIKSe4VuuriK9kD-2AlPLI4.jpg,False,,[],{},link,,False,,1589070038.0,richtext,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?auto=webp&amp;s=31cf1745f48c7939b423e01c1c0dcdd2bc28463c', 'width': 1152, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7aa46bcf0e9b9439ed3d9fe45a8918f8ded2717c', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ba3aa360495a2dcf97f250539be33c0e37a8b083', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=69ea0ff433860d13a4926baf1c95b38f5541d195', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b3a102a75d93faea80bf612f66b9b81afe7b978', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2c704b653622e499ec44b61ad65d9adfbbac2e9', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52a04959135cf357898927c1f77f52a84bbeda76', 'width': 1080, 'height': 675}], 'variants': {}, 'id': '3yQRNas98pOcf_girT9Tuy94g63nNxxUzDdon56PecU'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggib63,True,,PopescuG,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggib63/this_chair_model_was_trained_in_runwayml_the/,all_ads,False,https://v.redd.it/l5sbj44ipox41,155203,1589041238.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'VisualMath', 'selftext': '', 'author_fullname': 't2_65o7vhtm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'This chair model was trained in RunwayML. The projection was made in GoogleColab, using StyleGAN2, to explore the latent space between the ChairGAN with the three well-known chairs.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/VisualMath', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 87, 'hide_score': False, 'name': 't3_ggaf5x', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.99, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'reddit_video': {'fallback_url': 'https://v.redd.it/l5sbj44ipox41/DASH_720?source=fallback', 'height': 720, 'width': 1152, 'scrubber_media_url': 'https://v.redd.it/l5sbj44ipox41/DASH_96', 'dash_url': 'https://v.redd.it/l5sbj44ipox41/DASHPlaylist.mpd', 'duration': 30, 'hls_url': 'https://v.redd.it/l5sbj44ipox41/HLSPlaylist.m3u8', 'is_gif': True, 'transcoding_status': 'completed'}}, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 55, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/ca4tlrLrbasM0tlTVcA5FIKSe4VuuriK9kD-2AlPLI4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'hosted:video', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1589034942.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'v.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?format=pjpg&amp;auto=webp&amp;s=13efca55ef411d589478829a1b9c279d5287f08a', 'width': 1152, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2fc3441a0fce7fe9505a7a354dcc7992205d6c94', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5a2197bcf4d2c30db9211e505617e81bfba3b349', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5547f809f62ed3b7b0537e604b51d5989cdac8ee', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5f07504187a32eadf6b0c71e39facffd55938068', 'width': 640, 'height': 400}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e4889ff75679a2a5cfdbacfc3b2d137186587b2a', 'width': 960, 'height': 600}, {'url': 'https://external-preview.redd.it/c7AMjOXNVn2j3fLO3icxVITjbSAtsmEL4whwK4UDzGQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=827670d76145c925a56b92e780da95067a479b69', 'width': 1080, 'height': 675}], 'variants': {}, 'id': '3yQRNas98pOcf_girT9Tuy94g63nNxxUzDdon56PecU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2jaju8', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'ggaf5x', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'FunVisualMath', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/VisualMath/comments/ggaf5x/this_chair_model_was_trained_in_runwayml_the/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://v.redd.it/l5sbj44ipox41', 'subreddit_subscribers': 2862, 'created_utc': 1589006142.0, 'num_crossposts': 3, 'media': {'reddit_video': {'fallback_url': 'https://v.redd.it/l5sbj44ipox41/DASH_720?source=fallback', 'height': 720, 'width': 1152, 'scrubber_media_url': 'https://v.redd.it/l5sbj44ipox41/DASH_96', 'dash_url': 'https://v.redd.it/l5sbj44ipox41/DASHPlaylist.mpd', 'duration': 30, 'hls_url': 'https://v.redd.it/l5sbj44ipox41/HLSPlaylist.m3u8', 'is_gif': True, 'transcoding_status': 'completed'}}, 'is_video': True}]",t3_ggaf5x,,
,learnmachinelearning,"Hi, I'm learning the reinforcement learning algorithms.  I came across the advantage function and state value function.  Most places say that the advantage function A(***s***,***a***) = Q function Q(***s***,***a***) minus the state value function V(***s***).  Some places say that  Q**(*****s, a***) is the value of action ***a*** taken at state ***s***, and V(***s***) which is the value of the state, or the average of all rewards (caused by all actions taken) at state ***s*** .  But when I look at the actual equations for Q(s,a) and V(s), they are actually the same, e.g., both equal to the expected value of the sum of discounted rewards.  I'm confused.  Could somebody explain to me what's exactly the advantage function (or maybe the difference between Q and V)?  Thank you.",t2_643sg3ws,False,,0,False,Could somebody clarify the difference b/w advantage function and state value function?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggusc6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589113481.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I&amp;#39;m learning the reinforcement learning algorithms.  I came across the advantage function and state value function.  Most places say that the advantage function A(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;,&lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt;) = Q function Q(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;,&lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt;) minus the state value function V(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;).  Some places say that  Q&lt;strong&gt;(&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;s, a&lt;/em&gt;&lt;/strong&gt;) is the value of action &lt;strong&gt;&lt;em&gt;a&lt;/em&gt;&lt;/strong&gt; taken at state &lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;, and V(&lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt;) which is the value of the state, or the average of all rewards (caused by all actions taken) at state &lt;strong&gt;&lt;em&gt;s&lt;/em&gt;&lt;/strong&gt; .  But when I look at the actual equations for Q(s,a) and V(s), they are actually the same, e.g., both equal to the expected value of the sum of discounted rewards.  I&amp;#39;m confused.  Could somebody explain to me what&amp;#39;s exactly the advantage function (or maybe the difference between Q and V)?  Thank you.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggusc6,True,,TobinC1,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggusc6/could_somebody_clarify_the_difference_bw/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggusc6/could_somebody_clarify_the_difference_bw/,155203,1589084681.0,0,,False,,,,
,learnmachinelearning," As the title says, me and probably many others have alot of free time now, I'd like to learn more about Cloud and probably have more hands-on experience rather than just theortical knowledge, I personally have digged abit into AWS technologies such as EC2, but I feel this is very minimal and can be futher improved, which cloud services do you find the most beneficial for a data scientist to learn?

Thanks!",t2_kfdra,False,,0,False,"I have alot of free time, I want to learn some cloud which technologies/cloud services should I mostly be learning about",[],r/learnmachinelearning,False,6,,0,,False,t3_ggorpx,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589090712.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;As the title says, me and probably many others have alot of free time now, I&amp;#39;d like to learn more about Cloud and probably have more hands-on experience rather than just theortical knowledge, I personally have digged abit into AWS technologies such as EC2, but I feel this is very minimal and can be futher improved, which cloud services do you find the most beneficial for a data scientist to learn?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggorpx,True,,Unchart3disOP,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggorpx/i_have_alot_of_free_time_i_want_to_learn_some/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggorpx/i_have_alot_of_free_time_i_want_to_learn_some/,155203,1589061912.0,0,,False,,,,
,learnmachinelearning,"So right now my CNN is getting about 40% accuracy on 12 classes, which isn't horrible for my first neural network, but whenever I change the network architecture(like num of neurons and layers) it just stops learning and I need to go back to 16,32,64. Is there any sort of patter to how to optimize the architecture?",t2_n3wq5,False,,0,False,Advice on optimizing CNN Architecture?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggq8s6,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589095816.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So right now my CNN is getting about 40% accuracy on 12 classes, which isn&amp;#39;t horrible for my first neural network, but whenever I change the network architecture(like num of neurons and layers) it just stops learning and I need to go back to 16,32,64. Is there any sort of patter to how to optimize the architecture?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggq8s6,True,,10macattack,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggq8s6/advice_on_optimizing_cnn_architecture/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggq8s6/advice_on_optimizing_cnn_architecture/,155203,1589067016.0,0,,False,,,,
,learnmachinelearning,"Hi guys,

I am currently at a loss for ideas. I need to create a sudoku solver in C++ that uses ML. I know how to solve sudoku, but the ML part simply beats me. 

I've read on different sites, articles and in different publications that reinforcement learning might be the way to go, since I already have a dataset and is mandatory to use it (this is a college project). And that RNN are a good idea too. And that wouldn't be a problem if coding in python, since it has all it's wonderful libraries. But what would you choose if you had to do this using C++ and some libraries for it?",t2_5uttszv3,False,,0,False,What would be the best approach for a ML algorithm that has to be written in C++ and solves sudoku?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggt84m,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1589106963.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I am currently at a loss for ideas. I need to create a sudoku solver in C++ that uses ML. I know how to solve sudoku, but the ML part simply beats me. &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve read on different sites, articles and in different publications that reinforcement learning might be the way to go, since I already have a dataset and is mandatory to use it (this is a college project). And that RNN are a good idea too. And that wouldn&amp;#39;t be a problem if coding in python, since it has all it&amp;#39;s wonderful libraries. But what would you choose if you had to do this using C++ and some libraries for it?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggt84m,True,,ATiredRedHead,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggt84m/what_would_be_the_best_approach_for_a_ml/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggt84m/what_would_be_the_best_approach_for_a_ml/,155203,1589078163.0,0,,False,,,,
,learnmachinelearning,"Does anyone know of a dataset (preferably large-scale) for distinguishing programming code from text? I know this would be easy to put together, but just curious if one already exists out there in the wild (I couldn't find anything on Google Dataset Search)",t2_6f7z197p,False,,0,False,Datasets for code detection?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggry04,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589101984.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Does anyone know of a dataset (preferably large-scale) for distinguishing programming code from text? I know this would be easy to put together, but just curious if one already exists out there in the wild (I couldn&amp;#39;t find anything on Google Dataset Search)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggry04,True,,awaythrow9508,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggry04/datasets_for_code_detection/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggry04/datasets_for_code_detection/,155203,1589073184.0,0,,False,,,,
,learnmachinelearning,"I'm a student studying computer engineering and I'm planning on teaching myself machine learning concepts and Tensorflow over the summer with several Coursera courses. At the moment my plan of action is to do a theory of machine learning course in tandem with an intro to Tensorflow course. My question is, should I wait to do the Tensorflow course until after I've learned the theory of ML?",t2_38jhws92,False,,0,False,Learning theory in tandem with practice?,[],r/learnmachinelearning,False,6,,0,,False,t3_gglfoi,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589079852.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a student studying computer engineering and I&amp;#39;m planning on teaching myself machine learning concepts and Tensorflow over the summer with several Coursera courses. At the moment my plan of action is to do a theory of machine learning course in tandem with an intro to Tensorflow course. My question is, should I wait to do the Tensorflow course until after I&amp;#39;ve learned the theory of ML?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gglfoi,True,,Magma_not_Lava,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gglfoi/learning_theory_in_tandem_with_practice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gglfoi/learning_theory_in_tandem_with_practice/,155203,1589051052.0,0,,False,,,,
,learnmachinelearning,"Applying LSTMs to stock prices seems to be a fairly popular exercise, at least for learning purposes. As far as I could tell from googling, the fairly broad consensus is that it doesn't really work, as there's no much information to be gleened beyond the current stock price.

However, when looking for papers to back this up, the few I found seemed to show 'promising results'.  E.g.

[https://www.researchgate.net/profile/Adriano\_Pereira3/publication/318329563\_Stock\_market%27s\_price\_movement\_prediction\_with\_LSTM\_neural\_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf](https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf)

Which has a fair number of citations (however 55% accuracy doesn't seem particularly impressive if the dataset is skewed). Just wanted to double check my initial impressions were correct and the papers I've found are just poorly conducted, or if I'm wrong and LSTMs are actually used in stock price prediction (and some credible papers that show this).",t2_6dilbrzl,False,,0,False,LSTMs in stock price prediction,[],r/learnmachinelearning,False,6,,0,,False,t3_ggjkew,False,dark,0.75,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1589073920.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Applying LSTMs to stock prices seems to be a fairly popular exercise, at least for learning purposes. As far as I could tell from googling, the fairly broad consensus is that it doesn&amp;#39;t really work, as there&amp;#39;s no much information to be gleened beyond the current stock price.&lt;/p&gt;

&lt;p&gt;However, when looking for papers to back this up, the few I found seemed to show &amp;#39;promising results&amp;#39;.  E.g.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf""&gt;https://www.researchgate.net/profile/Adriano_Pereira3/publication/318329563_Stock_market%27s_price_movement_prediction_with_LSTM_neural_networks/links/5a4d5545aca2729b7c8b3b7d/Stock-markets-price-movement-prediction-with-LSTM-neural-networks.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Which has a fair number of citations (however 55% accuracy doesn&amp;#39;t seem particularly impressive if the dataset is skewed). Just wanted to double check my initial impressions were correct and the papers I&amp;#39;ve found are just poorly conducted, or if I&amp;#39;m wrong and LSTMs are actually used in stock price prediction (and some credible papers that show this).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggjkew,True,,econtextthrow,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggjkew/lstms_in_stock_price_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggjkew/lstms_in_stock_price_prediction/,155203,1589045120.0,0,,False,,,,
,learnmachinelearning,,t2_1jyhaoq,False,,0,False,LSTM and back propagation with numpy,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggdp8u,False,light,0.91,,public,9,0,{},,,False,[],,False,False,,{},Project,False,9,,False,default,False,,[],{},,,False,,1589051629.0,richtext,6,,,text,blog.varunajayasiri.com,False,,,,,,False,False,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggdp8u,True,,mlvpj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdp8u/lstm_and_back_propagation_with_numpy/,all_ads,False,http://blog.varunajayasiri.com/numpy_lstm.html,155203,1589022829.0,0,,False,,,,
,learnmachinelearning,,t2_5yd6bcu6,False,,0,False,5 CORE Data Science Skills You Should Master,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_ggpabf,False,light,0.4,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '5 CORE Data Science Skills You Should Master', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lfn6u2K4oHE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggpabf', 'height': 338}",Discussion,False,0,,False,https://b.thumbs.redditmedia.com/KSft4rolzdRmNGpfPaFJxSKlKfjpwQpdn92-e0JX3VE.jpg,False,,[],{},rich:video,,False,,1589092461.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?auto=webp&amp;s=bb84a9141cd85cde01f4e63a5be70bc129b4acbf', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4e59deb112d8c3e60da8cb4147d7ae06c1f0658b', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59a45d92b637e87cbbbe5b18e19915bd8143a4c5', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/5W_pWqe_H0M5NsjOILfNoFjmnu1hiGmHyW-1DmQnyZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8d8f4435bf0b6cdf655ed57e8465d75800c47bca', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'HGwIi2xzEWiaik-fE8Ih8Y2NovJck78ghH4hShArPss'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggpabf,True,,JK_Bielan,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggpabf/5_core_data_science_skills_you_should_master/,all_ads,False,https://www.youtube.com/watch?v=lfn6u2K4oHE,155203,1589063661.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': '5 CORE Data Science Skills You Should Master', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lfn6u2K4oHE?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Machine Learning Jack', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lfn6u2K4oHE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCGihA5NxOruVkCNfcjafY6Q'}}",False,,,,
,learnmachinelearning,"I started with Andrew Ng's Machine Learning course on Coursera last year but quit by week 5 due to school and not putting in enough time. I have decided to start again and have enrolled in the course again. 

I was thinking that having a discord to discuss each week's content would be helpful since that would be a study group apart from the discussions forum on Coursera which I don't really like. Let me know if any of you are interested and we can discuss on connecting through discord and setting up regular meetings. We can set goals and be accountable for the group's progress, trying to reach deadlines for discussions.",t2_67ndk8lg,False,,0,False,Discord for Ng's ML Course (Coursera),[],r/learnmachinelearning,False,6,,0,,False,t3_ggo29g,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589088376.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I started with Andrew Ng&amp;#39;s Machine Learning course on Coursera last year but quit by week 5 due to school and not putting in enough time. I have decided to start again and have enrolled in the course again. &lt;/p&gt;

&lt;p&gt;I was thinking that having a discord to discuss each week&amp;#39;s content would be helpful since that would be a study group apart from the discussions forum on Coursera which I don&amp;#39;t really like. Let me know if any of you are interested and we can discuss on connecting through discord and setting up regular meetings. We can set goals and be accountable for the group&amp;#39;s progress, trying to reach deadlines for discussions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggo29g,True,,newtonseitz,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggo29g/discord_for_ngs_ml_course_coursera/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggo29g/discord_for_ngs_ml_course_coursera/,155203,1589059576.0,0,,False,,,,
,learnmachinelearning,,t2_2xz6764h,False,,0,False,Why is reinforcement learning often used with neural networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggnddn,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589086100.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggnddn,True,,User1377420,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggnddn/why_is_reinforcement_learning_often_used_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggnddn/why_is_reinforcement_learning_often_used_with/,155203,1589057300.0,0,,False,,,,
,learnmachinelearning,"I want to increase the output class size of a pre-trained YOLOV3 model, to detect an object (crosswalk) not originally existing in the COCO dataset it was trained with. I also want to keep the pre-existing detection accuracy for most of the original COCO objects.

The specific object I'm targeting are crosswalks, and my network code is an adaptation of [https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3\_tf2/models.py#L204](https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204)

How I think I want to accomplish this is via [transfer learning](https://medium.com/starschema-blog/transfer-learning-the-dos-and-donts-165729d66625). What I understand training requirements are:

* The new target object needs resemblance to original dataset objects the pre-trained model worked with. Crosswalks are primarily simple shapes with slight warping. The weight's I'm reusing were trained under COCO.
* I only reuse the n-1 layers of the pre-trained model and freeze those to avoid adjustments.
* Because I'm only fine tunning the final layer, my learning rate can be larger than 10\^-6 (used learning is 10\^-4)

# Problems

The general problem is after training, the model can't detect sidewalks nor the original objects. But this could be a symptom of the following:

# Sub Issue 1: Force Fit Layer Weights

The original model was built for 80 class outputs. Since I want to keep those as classification outputs, The model I train is built for 81 classes. This results in slight layer weights shape mismatch.

To resolve this I force set the weights by iterating through the problematic layer and identify which weights have mismatching shape. Those specific subset weights get adjusted using `np.resize(weights, target_size)`. This results in padding randomized weights of similar values to the respective weight set.

This ""Force Fit"" does not happen at ever layer, nor at every weight set in the respective layer. Here's an example log that might help paint the situation:

* a layer weights is a tuple of Nd-arrays. Depth is the index within that respective tuple
* `Shape at depth 5: (1, 1, 512, 255) | expected: (1, 1, 512, 258)`translates to `model.get_layers(""yolo_output_1"").get_weights[4]`

&amp;#x200B;

    Network consists of layers [['input', 'yolo_darknet', 'yolo_conv_0', 'yolo_conv_1', 'yolo_conv_2', 'yolo_output_0', 'yolo_output_1', 'yolo_output_2']]
    Transfering weights for layer [input]
    Transfer success
    Freezing layer [input]
    Transfering weights for layer [yolo_darknet]
    Transfer success
    Freezing layer [yolo_darknet]
    Transfering weights for layer [yolo_conv_0]
    Transfer success
    Freezing layer [yolo_conv_0]
    Transfering weights for layer [yolo_conv_1]
    Transfer success
    Freezing layer [yolo_conv_1]
    Transfering weights for layer [yolo_conv_2]
    Transfer success
    Freezing layer [yolo_conv_2]
    Transfering weights for layer [yolo_output_0]
    Reshaping layer [yolo_output_0] source weights to fit expected size
    Mismatch found at layer depth 5
    Shape at depth 5: (1, 1, 1024, 255)	| expected: (1, 1, 1024, 258)
    Mismatch found at layer depth 6
    Shape at depth 6: (255,)	| expected: (258,)
    Attempting to set reshaped weights to layer [yolo_output_0]
    Transfer success
    Freezing layer [yolo_output_0]
    Transfering weights for layer [yolo_output_1]
    Reshaping layer [yolo_output_1] source weights to fit expected size
    Mismatch found at layer depth 5
    Shape at depth 5: (1, 1, 512, 255)	| expected: (1, 1, 512, 258)
    Mismatch found at layer depth 6
    Shape at depth 6: (255,)	| expected: (258,)
    Attempting to set reshaped weights to layer [yolo_output_1]
    Transfer success
    Freezing layer [yolo_output_1]

# Questions (Part 1)

1. Is this ""Force Fit"" approach stupid? should I just train everything from scratch if I want to increase the possible classification output while inheriting the pre-trained models accuracy?
2. If the ""Force Fit"" isn't an issue, do I need to reconsider how many layers I leave unfrozen for training? What I understand of DNN layers is the bottom most builds up simple-generic understanding of structures. So if I shave off the top most on a pre-trained model that detects traffic-lights/people, to now work with crosswalks, then I'm far too deep in the network layers and should consider a smaller architecture?

# Sub Issue 2: Not Enough Data

My current annotated data of crosswalks images is about 400 images. I partition the dataset as 300 to train, and 100 to validate. This [Intel case study of transfer learning](https://software.intel.com/content/www/us/en/develop/articles/traffic-light-detection-using-the-tensorflow-object-detection-api.html) used around 600 images for training.

# Questions (Part 2)

1. Is the minimum training data requirements to build on-top of COCO dataset typically 600-1k sized?
2. My goal is to have my final model inherit the detection accuracy of some COCO objects. So is it correct to consider that I need to provide training data for those respective objects, in addition to my new target object (cross walks)? If I do does each object also require 600-1k sized samples each?
3. If I need to retrain the model with the COCO objects I want to retain in my final model, this can potentially reduce my classification class output to less than 80. Should I force fit the loaded weights to the smaller network size if there's a mismatch? Or should I still keep network size equipped to predict 80 output types, as they'll default to zeros due to not having representation in the training set?",t2_2a11d7b5,False,,0,False,"How to expand detection class output in a pre-trained model, and retrain original objects classification success?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggncn6,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,1589057408.0,,[],{},self,,True,,1589086026.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I want to increase the output class size of a pre-trained YOLOV3 model, to detect an object (crosswalk) not originally existing in the COCO dataset it was trained with. I also want to keep the pre-existing detection accuracy for most of the original COCO objects.&lt;/p&gt;

&lt;p&gt;The specific object I&amp;#39;m targeting are crosswalks, and my network code is an adaptation of &lt;a href=""https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204""&gt;https://github.com/zzh8829/yolov3-tf2/blob/master/yolov3_tf2/models.py#L204&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How I think I want to accomplish this is via &lt;a href=""https://medium.com/starschema-blog/transfer-learning-the-dos-and-donts-165729d66625""&gt;transfer learning&lt;/a&gt;. What I understand training requirements are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The new target object needs resemblance to original dataset objects the pre-trained model worked with. Crosswalks are primarily simple shapes with slight warping. The weight&amp;#39;s I&amp;#39;m reusing were trained under COCO.&lt;/li&gt;
&lt;li&gt;I only reuse the n-1 layers of the pre-trained model and freeze those to avoid adjustments.&lt;/li&gt;
&lt;li&gt;Because I&amp;#39;m only fine tunning the final layer, my learning rate can be larger than 10^-6 (used learning is 10^-4)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;Problems&lt;/h1&gt;

&lt;p&gt;The general problem is after training, the model can&amp;#39;t detect sidewalks nor the original objects. But this could be a symptom of the following:&lt;/p&gt;

&lt;h1&gt;Sub Issue 1: Force Fit Layer Weights&lt;/h1&gt;

&lt;p&gt;The original model was built for 80 class outputs. Since I want to keep those as classification outputs, The model I train is built for 81 classes. This results in slight layer weights shape mismatch.&lt;/p&gt;

&lt;p&gt;To resolve this I force set the weights by iterating through the problematic layer and identify which weights have mismatching shape. Those specific subset weights get adjusted using &lt;code&gt;np.resize(weights, target_size)&lt;/code&gt;. This results in padding randomized weights of similar values to the respective weight set.&lt;/p&gt;

&lt;p&gt;This &amp;quot;Force Fit&amp;quot; does not happen at ever layer, nor at every weight set in the respective layer. Here&amp;#39;s an example log that might help paint the situation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a layer weights is a tuple of Nd-arrays. Depth is the index within that respective tuple&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shape at depth 5: (1, 1, 512, 255) | expected: (1, 1, 512, 258)&lt;/code&gt;translates to &lt;code&gt;model.get_layers(&amp;quot;yolo_output_1&amp;quot;).get_weights[4]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Network consists of layers [[&amp;#39;input&amp;#39;, &amp;#39;yolo_darknet&amp;#39;, &amp;#39;yolo_conv_0&amp;#39;, &amp;#39;yolo_conv_1&amp;#39;, &amp;#39;yolo_conv_2&amp;#39;, &amp;#39;yolo_output_0&amp;#39;, &amp;#39;yolo_output_1&amp;#39;, &amp;#39;yolo_output_2&amp;#39;]]
Transfering weights for layer [input]
Transfer success
Freezing layer [input]
Transfering weights for layer [yolo_darknet]
Transfer success
Freezing layer [yolo_darknet]
Transfering weights for layer [yolo_conv_0]
Transfer success
Freezing layer [yolo_conv_0]
Transfering weights for layer [yolo_conv_1]
Transfer success
Freezing layer [yolo_conv_1]
Transfering weights for layer [yolo_conv_2]
Transfer success
Freezing layer [yolo_conv_2]
Transfering weights for layer [yolo_output_0]
Reshaping layer [yolo_output_0] source weights to fit expected size
Mismatch found at layer depth 5
Shape at depth 5: (1, 1, 1024, 255) | expected: (1, 1, 1024, 258)
Mismatch found at layer depth 6
Shape at depth 6: (255,)    | expected: (258,)
Attempting to set reshaped weights to layer [yolo_output_0]
Transfer success
Freezing layer [yolo_output_0]
Transfering weights for layer [yolo_output_1]
Reshaping layer [yolo_output_1] source weights to fit expected size
Mismatch found at layer depth 5
Shape at depth 5: (1, 1, 512, 255)  | expected: (1, 1, 512, 258)
Mismatch found at layer depth 6
Shape at depth 6: (255,)    | expected: (258,)
Attempting to set reshaped weights to layer [yolo_output_1]
Transfer success
Freezing layer [yolo_output_1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;Questions (Part 1)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Is this &amp;quot;Force Fit&amp;quot; approach stupid? should I just train everything from scratch if I want to increase the possible classification output while inheriting the pre-trained models accuracy?&lt;/li&gt;
&lt;li&gt;If the &amp;quot;Force Fit&amp;quot; isn&amp;#39;t an issue, do I need to reconsider how many layers I leave unfrozen for training? What I understand of DNN layers is the bottom most builds up simple-generic understanding of structures. So if I shave off the top most on a pre-trained model that detects traffic-lights/people, to now work with crosswalks, then I&amp;#39;m far too deep in the network layers and should consider a smaller architecture?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;Sub Issue 2: Not Enough Data&lt;/h1&gt;

&lt;p&gt;My current annotated data of crosswalks images is about 400 images. I partition the dataset as 300 to train, and 100 to validate. This &lt;a href=""https://software.intel.com/content/www/us/en/develop/articles/traffic-light-detection-using-the-tensorflow-object-detection-api.html""&gt;Intel case study of transfer learning&lt;/a&gt; used around 600 images for training.&lt;/p&gt;

&lt;h1&gt;Questions (Part 2)&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;Is the minimum training data requirements to build on-top of COCO dataset typically 600-1k sized?&lt;/li&gt;
&lt;li&gt;My goal is to have my final model inherit the detection accuracy of some COCO objects. So is it correct to consider that I need to provide training data for those respective objects, in addition to my new target object (cross walks)? If I do does each object also require 600-1k sized samples each?&lt;/li&gt;
&lt;li&gt;If I need to retrain the model with the COCO objects I want to retain in my final model, this can potentially reduce my classification class output to less than 80. Should I force fit the loaded weights to the smaller network size if there&amp;#39;s a mismatch? Or should I still keep network size equipped to predict 80 output types, as they&amp;#39;ll default to zeros due to not having representation in the training set?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?auto=webp&amp;s=29c6af9777795fc4a07c9bd886cf006df323b2e5', 'width': 252, 'height': 252}, 'resolutions': [{'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ac66d19f565defbcb4ef10a1c1f7703091a3cbb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/YqKP5A6h8KztglbxnpS-ZLn8WTSI0E8InSsWAYMQ6Bo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9e4d60edea4e2c5bd0bbf274ec777b04ee35040', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'hiozEMqFCp1Wb4R27LrsnnDsknwoNS55yOsGQVAZ4_o'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggncn6,True,,SuspiciousSimple,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggncn6/how_to_expand_detection_class_output_in_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggncn6/how_to_expand_detection_class_output_in_a/,155203,1589057226.0,0,,False,,,,
,learnmachinelearning,"&amp;#x200B;

[Accuracy as a function of epoch and batch sizes in one specific case](https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;format=pjpg&amp;auto=webp&amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040)

I'm just trying to understand machine learning. I was working a simple example and the author gave certain values for epoch and batch size, and said that one just develops 'a feel' for what  the right numbers are. With 0 experience, I wanted to understand what considerations go into that guesstimate. I trained the same model on the same dataset, varying batch size and number of epochs, trying to visualize where the best accuracy lies. I'm sure it varies per dataset, per model, per run on the same machine, but I was hoping for something to develop an intuition on. I have to say I'm still in the dark on what to use as numbers for these two factors. How does one know where to set these?",t2_3ybjnaub,False,,0,False,How do you intuit epoch and batch sizes?,[],r/learnmachinelearning,False,6,,0,139.0,False,t3_ggiyzf,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/ukLAHqYSaYf9rwiL6OHn7lJqztQYS8Hpdo3jmGx33is.jpg,False,,[],{},,,True,,1589072056.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040""&gt;Accuracy as a function of epoch and batch sizes in one specific case&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m just trying to understand machine learning. I was working a simple example and the author gave certain values for epoch and batch size, and said that one just develops &amp;#39;a feel&amp;#39; for what  the right numbers are. With 0 experience, I wanted to understand what considerations go into that guesstimate. I trained the same model on the same dataset, varying batch size and number of epochs, trying to visualize where the best accuracy lies. I&amp;#39;m sure it varies per dataset, per model, per run on the same machine, but I was hoping for something to develop an intuition on. I have to say I&amp;#39;m still in the dark on what to use as numbers for these two factors. How does one know where to set these?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggiyzf,True,,TechIsSoCool,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggiyzf/how_do_you_intuit_epoch_and_batch_sizes/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggiyzf/how_do_you_intuit_epoch_and_batch_sizes/,155203,1589043256.0,0,,False,,,"{'3dpyb9fcqrx41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 107, 'x': 108, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77c0079044709e68e25ede361f5590b90382f246'}, {'y': 215, 'x': 216, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6a5bd5cf71e94cac334bb652de74717ea577582'}, {'y': 319, 'x': 320, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d63db89943f5c45b8bd28ff5cd3c325a28645a26'}, {'y': 638, 'x': 640, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73d1e20bd272730449bdf303e2634343bef86350'}, {'y': 957, 'x': 960, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73d4b210c2084a3975716e98e093cdbb5b51a620'}, {'y': 1076, 'x': 1080, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28987acbd1ec4a96bf0af7b9f0e8c1946b74558e'}], 's': {'y': 2577, 'x': 2585, 'u': 'https://preview.redd.it/3dpyb9fcqrx41.jpg?width=2585&amp;format=pjpg&amp;auto=webp&amp;s=e0f5e0e9c42345daed247da3aaacc130798d9040'}, 'id': '3dpyb9fcqrx41'}}",
,learnmachinelearning,Transformer i have seen on papers typically has 12 heads and 12 layers. Does it suffer from vanishing gradient and how is the problem overcome normally?,t2_3qui5vln,False,,0,False,Does transformer suffer from vanishing gradient problem?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggggaj,False,dark,0.67,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589063754.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Transformer i have seen on papers typically has 12 heads and 12 layers. Does it suffer from vanishing gradient and how is the problem overcome normally?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggggaj,True,,lifesaboxofchoco,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggggaj/does_transformer_suffer_from_vanishing_gradient/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggggaj/does_transformer_suffer_from_vanishing_gradient/,155203,1589034954.0,0,,False,,,,True
,learnmachinelearning,"I wanna start learning AI /ML.
I learnt java (2+ years) + now python(6+ months) 
I learned flask/numpy/pillow..etc. 

Now kinda lost where to start? 
In my CS Bachelor course at Uni we took Intro to AI needless to say (Agent Env model and three jar problems) didnâ€™t really tell me what next to do and how AI and ML works. 

So I have zero idea now at what next ?
Do I jump to sci-kit and NLTK or maybe tenserflow ?  
Or maybe there are some theoretical knowledge first ? Am too lost right now .
Would ohevif there is a road map just how web dev have one.

Am kinda scared that I need to go under heavy math courses for a year and then start learn another year AI then another year ML.

(Sry for my bad English)",t2_6cacicc9,False,,0,False,Lost at how to start learning ML/AI as CS student.,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggmmb7,False,light,0.67,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,1589060122.0,,[],{},,,True,,1589083641.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wanna start learning AI /ML.
I learnt java (2+ years) + now python(6+ months) 
I learned flask/numpy/pillow..etc. &lt;/p&gt;

&lt;p&gt;Now kinda lost where to start? 
In my CS Bachelor course at Uni we took Intro to AI needless to say (Agent Env model and three jar problems) didnâ€™t really tell me what next to do and how AI and ML works. &lt;/p&gt;

&lt;p&gt;So I have zero idea now at what next ?
Do I jump to sci-kit and NLTK or maybe tenserflow ?&lt;br/&gt;
Or maybe there are some theoretical knowledge first ? Am too lost right now .
Would ohevif there is a road map just how web dev have one.&lt;/p&gt;

&lt;p&gt;Am kinda scared that I need to go under heavy math courses for a year and then start learn another year AI then another year ML.&lt;/p&gt;

&lt;p&gt;(Sry for my bad English)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggmmb7,True,,r-_-mark,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggmmb7/lost_at_how_to_start_learning_mlai_as_cs_student/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggmmb7/lost_at_how_to_start_learning_mlai_as_cs_student/,155203,1589054841.0,0,,False,,,,
,learnmachinelearning,,t2_zhsd6,False,,0,False,TensorFlow 2.0 Tutorial : Deploying Machine Learning models with REST API,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_ggdbeh,False,dark,0.73,,public,5,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Deploying Tensorflow 2.0 with REST API', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zpYAQIA1z4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/ggdbeh', 'height': 338}",,False,5,,False,https://b.thumbs.redditmedia.com/IFPqEBuGC5Ud78kuKEcInYmhebcC-mZ9KLK87NbRx_Q.jpg,False,,[],{},rich:video,,False,,1589049730.0,text,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?auto=webp&amp;s=1c0815185cb5762085f76a69dda22b922d882686', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f95e26d6b21270b0ccda5fd3a61b5f9a47691198', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75be5c3ec52cc7057a0423bb885aeef3248dbff2', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/Y4BY__xRO7dR8In5D7txNKOaUtkv09QAiMCAoy3yfwQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26a712e955ad607fc8f537b84c12300174a2b7bb', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'XxlnRd7qWzk9yKonMvAHgTxuCHRGbObr__dvBol8G-Q'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdbeh,True,,gautamrbharadwaj,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdbeh/tensorflow_20_tutorial_deploying_machine_learning/,all_ads,False,https://youtu.be/zpYAQIA1z4Y,155203,1589020930.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Deploying Tensorflow 2.0 with REST API', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/zpYAQIA1z4Y?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Gautam Ramachandra', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/zpYAQIA1z4Y/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UClhkVM6ma94mc5spq68c67Q'}}",False,,,,
,learnmachinelearning,"Hi guys,

I am working on the FIFA 19 data set for a project in college using R.
I was wondering if there was a algorithm which could help me select the best team from the dataset of all the players under different parameters. Such as age, nation, or for a given budget. 

Any and all help would be appreciated.

Thanks.",t2_5cmgmwoi,False,,0,False,Suggestions for algorithms for selecting teams from FIFA 19 dataset from kaggel,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggli77,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1589080063.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi guys,&lt;/p&gt;

&lt;p&gt;I am working on the FIFA 19 data set for a project in college using R.
I was wondering if there was a algorithm which could help me select the best team from the dataset of all the players under different parameters. Such as age, nation, or for a given budget. &lt;/p&gt;

&lt;p&gt;Any and all help would be appreciated.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,ggli77,True,,imyaash,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggli77/suggestions_for_algorithms_for_selecting_teams/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggli77/suggestions_for_algorithms_for_selecting_teams/,155203,1589051263.0,0,,False,,,,
,learnmachinelearning,"I have been working on a video series that uses Python to build a variety of cool projects in Machine Learning using just Python and recently started a tutorial series on Python. I would love to have constructive feedback in order to improvise on any particular front that you want to suggest.

Some of the features of both the series are these:

1. Linear Regression Project using Python (we work with a dataset)
2. Implementation of Multiple Linear Regression using Gradient Descent Algorithm (Working with a dataset)
3. Intuition and Conceptual Videos
4. As a pre-requisite, I have posted some Python Tutorial Series (both are in progress and ongoing series)

This is what we will be covering from absolute scratch in the ongoing series. I have added some videos already (12+) so that would be enough for you to know how the content is. 

https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274

I have already put up around 13 videos  on Python and more than 10 videos on Machine Learning in the respective YouTube Playlists : [Python Tutorials with Projects](https://www.youtube.com/watch?v=q6V0cBzQ7bc&amp;list=PLXgqhtspYCM8eUX94Ng4SQ-3kWMTZ7zFM)  &amp; [Machine Learning Tutorials with Projects](https://www.youtube.com/playlist?list=PLXgqhtspYCM9-eMFw31mJZnQFYjj2SQLO) and  will be uploading more content on a regular basis soon.",t2_51mclnu7,False,,0,False,FREE MACHINE LEARNING TUTORIAL SERIES ALONG WITH PYTHON (FROM SCRATCH),[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfpypp,False,dark,0.94,,public,385,0,{},140.0,,False,[],,False,False,,{},,False,385,,False,https://b.thumbs.redditmedia.com/bl9FBWcj_JAfgMWb6rBptoCaIYXcKMCVUOzqdyg3jfE.jpg,False,,[],{},self,,True,,1588958841.0,text,6,,,text,self.learnmachinelearning,True,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have been working on a video series that uses Python to build a variety of cool projects in Machine Learning using just Python and recently started a tutorial series on Python. I would love to have constructive feedback in order to improvise on any particular front that you want to suggest.&lt;/p&gt;

&lt;p&gt;Some of the features of both the series are these:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Linear Regression Project using Python (we work with a dataset)&lt;/li&gt;
&lt;li&gt;Implementation of Multiple Linear Regression using Gradient Descent Algorithm (Working with a dataset)&lt;/li&gt;
&lt;li&gt;Intuition and Conceptual Videos&lt;/li&gt;
&lt;li&gt;As a pre-requisite, I have posted some Python Tutorial Series (both are in progress and ongoing series)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is what we will be covering from absolute scratch in the ongoing series. I have added some videos already (12+) so that would be enough for you to know how the content is. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274""&gt;https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have already put up around 13 videos  on Python and more than 10 videos on Machine Learning in the respective YouTube Playlists : &lt;a href=""https://www.youtube.com/watch?v=q6V0cBzQ7bc&amp;amp;list=PLXgqhtspYCM8eUX94Ng4SQ-3kWMTZ7zFM""&gt;Python Tutorials with Projects&lt;/a&gt;  &amp;amp; &lt;a href=""https://www.youtube.com/playlist?list=PLXgqhtspYCM9-eMFw31mJZnQFYjj2SQLO""&gt;Machine Learning Tutorials with Projects&lt;/a&gt; and  will be uploading more content on a regular basis soon.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?auto=webp&amp;s=71171898179a0e65790a09a05886abc30af3be34', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7688ac46afc0b87978baa7f6c2f22e68572767f6', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa7d86a201fa49f7d6ad1dfec22a921704cd2b83', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/hpHcIhUwU5GSfxEMU-pA4i4Xd-JFDt8L51zYa48x6So.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=90b80a83a872775c1c925d01c846b6b64377ae16', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'zq0x9-UdUCMi4WdCmIecPstk_D-SEz9vNP1XVsIu850'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfpypp,True,,TheNerdyDevYT,,48,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfpypp/free_machine_learning_tutorial_series_along_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfpypp/free_machine_learning_tutorial_series_along_with/,155203,1588930041.0,1,,False,,,"{'mgyugbuweix41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 60, 'x': 108, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=472ca82dfe9c9d5a0e1afbd92f952df4f2bdb152'}, {'y': 121, 'x': 216, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fdc98131b22dcb15eea21cabfa5d341fa5ba8124'}, {'y': 180, 'x': 320, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44cfd139f2cf2a75e32367c8f1adec4a7fd1771f'}, {'y': 360, 'x': 640, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c85919667399855f3d7701de4dbac3cb60f9015'}, {'y': 540, 'x': 960, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b2023164290504f8ee447a709d6ac8c780724394'}, {'y': 607, 'x': 1080, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3009d5e669f7676f59adb581d0415785598ec89'}], 's': {'y': 720, 'x': 1280, 'u': 'https://preview.redd.it/mgyugbuweix41.png?width=1280&amp;format=png&amp;auto=webp&amp;s=63d3a4243b769bafa377e2ee3e1d2c609051b274'}, 'id': 'mgyugbuweix41'}}",
,learnmachinelearning,,t2_16diqth,False,,0,False,A Commit History of BERT and its Forks,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggf5r8,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,default,False,,[],{},,,False,,1589058547.0,richtext,6,,,text,amitness.com,False,,,,,,False,False,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggf5r8,True,,amitness,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggf5r8/a_commit_history_of_bert_and_its_forks/,all_ads,False,https://amitness.com/2020/05/git-log-of-bert/,155203,1589029747.0,0,,False,,,,
,learnmachinelearning,"I'm interested in creating a crude working demo of this: [https://arxiv.org/pdf/2004.02349.pdf](https://arxiv.org/pdf/2004.02349.pdf)

Essentially, it's using state of the art NLP (specifically, BERT) to map statements like:

&amp;#x200B;

&gt;How many world champions are there with only one reign?

to

    select count(*) where column(""No. of reigns"") == 1;

I have a solid background in python, and an intermediate level of experience with ML (mostly sklearn and fast.ai) on tabular data.  But I am completely self-taught, and have a very weak background in both math and NLP, i'm stronger on the practical/coding side.  As such, the 'mathy' explanation in that paper is tough for me to translate into code intuitively.

Any advice?  Anybody want to collaborate?  give me breadcrumbs and I'll do the heavy lifting...",t2_702gf,False,,0,False,how to turn an ARVIX paper into working code example?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggixcz,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,1589043413.0,,[],{},,,True,,1589071919.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m interested in creating a crude working demo of this: &lt;a href=""https://arxiv.org/pdf/2004.02349.pdf""&gt;https://arxiv.org/pdf/2004.02349.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Essentially, it&amp;#39;s using state of the art NLP (specifically, BERT) to map statements like:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How many world champions are there with only one reign?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;select count(*) where column(&amp;quot;No. of reigns&amp;quot;) == 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I have a solid background in python, and an intermediate level of experience with ML (mostly sklearn and fast.ai) on tabular data.  But I am completely self-taught, and have a very weak background in both math and NLP, i&amp;#39;m stronger on the practical/coding side.  As such, the &amp;#39;mathy&amp;#39; explanation in that paper is tough for me to translate into code intuitively.&lt;/p&gt;

&lt;p&gt;Any advice?  Anybody want to collaborate?  give me breadcrumbs and I&amp;#39;ll do the heavy lifting...&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,ggixcz,True,,ezeeetm,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggixcz/how_to_turn_an_arvix_paper_into_working_code/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggixcz/how_to_turn_an_arvix_paper_into_working_code/,155203,1589043119.0,1,,False,,,,
,learnmachinelearning,"After a lot of work I've created a machine learning api for asking questions, but I'm having quite some trouble publishing it.  
I've tried using the Google App Engine but it crashes due to my large model which I'm uploading.   
Heroku didn't work neither because the storage to low.

Any suggestions on which cloud provider I could use would be really helpful (not too complicated).",t2_4cohyf78,False,,0,False,Can someone help me publish my api to production,[],r/learnmachinelearning,False,6,,0,,False,t3_ggeyfi,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589057688.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;After a lot of work I&amp;#39;ve created a machine learning api for asking questions, but I&amp;#39;m having quite some trouble publishing it.&lt;br/&gt;
I&amp;#39;ve tried using the Google App Engine but it crashes due to my large model which I&amp;#39;m uploading.&lt;br/&gt;
Heroku didn&amp;#39;t work neither because the storage to low.&lt;/p&gt;

&lt;p&gt;Any suggestions on which cloud provider I could use would be really helpful (not too complicated).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggeyfi,True,,mariusjohan,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggeyfi/can_someone_help_me_publish_my_api_to_production/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggeyfi/can_someone_help_me_publish_my_api_to_production/,155203,1589028888.0,0,,False,,,,
,learnmachinelearning,,t2_xf2t5,False,,0,False,Data Science. Probability distributions,[],r/learnmachinelearning,False,6,,0,104.0,False,t3_gg62za,False,dark,0.94,,public,16,0,{},140.0,,False,[],,False,False,,{},,False,16,,False,https://b.thumbs.redditmedia.com/MUCTC_5KCGeUgULClVsSjViN5JVoAPIjpWKDL_7UiAc.jpg,False,,[],{},link,,False,,1589016282.0,text,6,,,text,luminousmen.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?auto=webp&amp;s=051606fa10335d9dbcd97728c06b2894853312c4', 'width': 1400, 'height': 1049}, 'resolutions': [{'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf775e8b7ad6e70779fab043c940ad7226a8c45c', 'width': 108, 'height': 80}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b80dc192efe7f6e7ab7f8cae88a23b840e37636', 'width': 216, 'height': 161}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=88d021c095649ac06f5445e28baf70edb9f06c8a', 'width': 320, 'height': 239}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b25959d4294aae27aa2a70f758d5b3bbc75fb7fc', 'width': 640, 'height': 479}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc918eb45ea0de5293b6f953bae4776e7b2447b5', 'width': 960, 'height': 719}, {'url': 'https://external-preview.redd.it/jMcWWpIt3Wk_0yE_UaPQps3VVsRsM5-437LlDXQYPmY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=116ddd61c5f9de2d0da4a6a15b0db04c3c797452', 'width': 1080, 'height': 809}], 'variants': {}, 'id': '1jYcsUcnKCWVynpCtNXuF3QYUAsWpKbDM4oRP8DxhXc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg62za,True,,luminoumen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg62za/data_science_probability_distributions/,all_ads,False,https://luminousmen.com/post/data-science-probability-distributions,155203,1588987482.0,0,,False,,,,
,learnmachinelearning,"Lets consider [this image](https://imgur.com/a/du3qznX) with some notation on a [3 layer neural network](https://i.imgur.com/VE8FcI4.png) with 2 inputs, 3 hidden neurons and 1 out put.

&amp;#x200B;

Is this math basically telling me that to back propagate my error through the network I need to:

1. Find  âˆ‚ J/ âˆ‚ z^((3))  =  âˆ‚ J/ âˆ‚ Å·  \*  âˆ‚Å·/âˆ‚z^((3)) =  ð›¿^((3))
2. Then multiply my error through W^((2))
3. Take the weights multiplied by the error and transpose it so that when the weights are cross multiplied by the vector gradient of the derivative of the activation function, ð›¿^((2)) , becomes a scalar value.
4. Finally update the weights(W^((1))) with ð›¿^((2))X^(T)

The last part I am confused about because it seems like ð›¿^((2))X^(T) becomes a 1x2 matrix and W^((1)) is a 2x3 matrix. How does that work out?

I have read multiple resources on BP including [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html) (multiple times). I just need some conversation about it please.",t2_7jjem,False,,0,False,I'd like help understanding the maths behind Back propagation,[],r/learnmachinelearning,False,6,,0,,False,t3_ggig40,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1589070445.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Lets consider &lt;a href=""https://imgur.com/a/du3qznX""&gt;this image&lt;/a&gt; with some notation on a &lt;a href=""https://i.imgur.com/VE8FcI4.png""&gt;3 layer neural network&lt;/a&gt; with 2 inputs, 3 hidden neurons and 1 out put.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Is this math basically telling me that to back propagate my error through the network I need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Find  âˆ‚ J/ âˆ‚ z&lt;sup&gt;(3&lt;/sup&gt;)  =  âˆ‚ J/ âˆ‚ Å·  *  âˆ‚Å·/âˆ‚z&lt;sup&gt;(3&lt;/sup&gt;) =  ð›¿&lt;sup&gt;(3&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;Then multiply my error through W&lt;sup&gt;(2&lt;/sup&gt;)&lt;/li&gt;
&lt;li&gt;Take the weights multiplied by the error and transpose it so that when the weights are cross multiplied by the vector gradient of the derivative of the activation function, ð›¿&lt;sup&gt;(2&lt;/sup&gt;) , becomes a scalar value.&lt;/li&gt;
&lt;li&gt;Finally update the weights(W&lt;sup&gt;(1&lt;/sup&gt;)) with ð›¿&lt;sup&gt;(2&lt;/sup&gt;)X&lt;sup&gt;T&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The last part I am confused about because it seems like ð›¿&lt;sup&gt;(2&lt;/sup&gt;)X&lt;sup&gt;T&lt;/sup&gt; becomes a 1x2 matrix and W&lt;sup&gt;(1&lt;/sup&gt;) is a 2x3 matrix. How does that work out?&lt;/p&gt;

&lt;p&gt;I have read multiple resources on BP including &lt;a href=""http://neuralnetworksanddeeplearning.com/chap2.html""&gt;http://neuralnetworksanddeeplearning.com/chap2.html&lt;/a&gt; (multiple times). I just need some conversation about it please.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?auto=webp&amp;s=5dcba14cfa9a087ba0c5abd12bcc149210f6f04b', 'width': 1160, 'height': 1036}, 'resolutions': [{'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a89fca42af5a62be0e7de1815ae5e59a71f6647', 'width': 108, 'height': 96}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f22d48c638c5b05a94b9ebef97bba758bc2d1e9', 'width': 216, 'height': 192}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d33a332c8b9e00d89798644e4f37dc1a5b77bc3a', 'width': 320, 'height': 285}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc30a8196453fd108b0deb90c8ca29707d2d0d1b', 'width': 640, 'height': 571}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e8d6f7099ccb00f1965a189aa74208f76026531d', 'width': 960, 'height': 857}, {'url': 'https://external-preview.redd.it/AostKrVjUfpWDnKh5oGyuKSGOahQC93r2_M9VIy2NEM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c22aa875d9a568bebe07590320f2e59e578e7336', 'width': 1080, 'height': 964}], 'variants': {}, 'id': 'ryYx1suE-qF2PtBO9I6gK0yoP448CugMTk3sww4kjKM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggig40,True,,raidicy,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggig40/id_like_help_understanding_the_maths_behind_back/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggig40/id_like_help_understanding_the_maths_behind_back/,155203,1589041645.0,0,,False,,,,
,learnmachinelearning,"Hey guys.. I've been on this for the past few days and couldn't figure it out. Posted on various groups,  StackOverflow etc and got suggestions from many users. I implemented these suggestions into the code shown below, but still having the same issue. Sorry for the lengthy post, but I want to be as clear as possible. All relevant code snippets are shown below:

Setting up image paths:

    imagepaths = []
    
    for root, dirs, files in os.walk(""."", topdown=False): 
      for name in files:
        path = os.path.join(root, name)
        if path.endswith(""jpg""): # We want only the images
          imagepaths.append(path)

Loading into arrays, preprocessing:

    X = [] # Image data
    y = [] # Labels
    
    datagen = ImageDataGenerator(rescale=1./255, samplewise_center=True)
    
    # Loops through imagepaths to load images and labels into arrays
    for path in imagepaths:
      img = cv2.imread(path) # Reads image and returns np.array
      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY) #find rgb 
      img = cv2.resize(img, (75, 75)) # Reduce image size so training can be faster
      img = image.img_to_array(img)
      img = datagen.standardize(img)
      X.append(img)
      
      # Processing label in image path
      category = path.split(""\\"")[1]
      #print(category)
      split = (category.split(""_""))     
      if int(split[0]) == 0:
        label = int(split[1])
      else:
        label = int(split[0])
      y.append(label)
    
    # Turn X and y into np.array to speed up train_test_split
    
    X = np.array(X, dtype=""float32"") #ORIGINAL uint8
    X = X.reshape(len(imagepaths), 75, 75, 1) # Needed to reshape so CNN knows it's different images, 1 for bw change to 3 for rgb
    y = np.array(y)
    tf.keras.utils.to_categorical(X, num_classes=None, dtype=""float32"")
    tf.keras.utils.to_categorical(y, num_classes=None, dtype=""float32"")

Creating test set:

    ts = 0.3 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)

Creating model. Yes I know its super small, just 1 layer, but I was suggested to cut down to start from the base and build up. Originally it was 5 layers, but the results are still the same.

    model = Sequential()
    
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(75, 75, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(26, activation='softmax'))

Compiling the model. And fitting. I was told that the gradient could be exploding, so was suggested to add the first line with the clipnorm..

    adam = keras.optimizers.Adam(clipnorm=1.)
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  
    
    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1, validation_data=(X_test, y_test))   

The final training can be seen here. The issues are, losses are NAN and accuracies are 0.

    Train on 54600 samples, validate on 23400 samples
    Epoch 1/5
    54600/54600 [==============================] - 14s 265us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 2/5
    54600/54600 [==============================] - 15s 269us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 3/5
    54600/54600 [==============================] - 15s 273us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 4/5
    54600/54600 [==============================] - 15s 267us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
    Epoch 5/5
    54600/54600 [==============================] - 14s 263us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00

Here are a list of things which I did wrong and was suggested to do by others:

1. I didn't standardize the data originally - So, I did ImageDataGenerator, rescaled, and standardized it.
2. I was suggested to turn the data to categorical, which I did use the to\_categorical function (I think i did that right) but I'm not sure if there's anything else required.
3. Reduce model complexity. I did that brought it to only one layer to debug.
4. Possible exploding gradient - so changed the adam optimizer with clipnorm = 1

BACKGROUND: This model trains and recognizes the 26 letters of the alphabet. I know the dataset is fine because when I use it to train a model for 10 letters at a time (A-J) for example it works fine. The issue is only when I go from 10-26. Yes, I did try to change the dense to 26 on the original code but that did not work.

I've been staring at this and trying everything for the past two days...

ANY HELP IS APPRECIATED",t2_7ajy1,False,,0,False,"Loss of NAN, Accuracy of 0 - Any idea why? Full code provided..",[],r/learnmachinelearning,False,6,,0,,False,t3_ggi8eg,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589069802.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys.. I&amp;#39;ve been on this for the past few days and couldn&amp;#39;t figure it out. Posted on various groups,  StackOverflow etc and got suggestions from many users. I implemented these suggestions into the code shown below, but still having the same issue. Sorry for the lengthy post, but I want to be as clear as possible. All relevant code snippets are shown below:&lt;/p&gt;

&lt;p&gt;Setting up image paths:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;imagepaths = []

for root, dirs, files in os.walk(&amp;quot;.&amp;quot;, topdown=False): 
  for name in files:
    path = os.path.join(root, name)
    if path.endswith(&amp;quot;jpg&amp;quot;): # We want only the images
      imagepaths.append(path)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loading into arrays, preprocessing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = [] # Image data
y = [] # Labels

datagen = ImageDataGenerator(rescale=1./255, samplewise_center=True)

# Loops through imagepaths to load images and labels into arrays
for path in imagepaths:
  img = cv2.imread(path) # Reads image and returns np.array
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY) #find rgb 
  img = cv2.resize(img, (75, 75)) # Reduce image size so training can be faster
  img = image.img_to_array(img)
  img = datagen.standardize(img)
  X.append(img)

  # Processing label in image path
  category = path.split(&amp;quot;\\&amp;quot;)[1]
  #print(category)
  split = (category.split(&amp;quot;_&amp;quot;))     
  if int(split[0]) == 0:
    label = int(split[1])
  else:
    label = int(split[0])
  y.append(label)

# Turn X and y into np.array to speed up train_test_split

X = np.array(X, dtype=&amp;quot;float32&amp;quot;) #ORIGINAL uint8
X = X.reshape(len(imagepaths), 75, 75, 1) # Needed to reshape so CNN knows it&amp;#39;s different images, 1 for bw change to 3 for rgb
y = np.array(y)
tf.keras.utils.to_categorical(X, num_classes=None, dtype=&amp;quot;float32&amp;quot;)
tf.keras.utils.to_categorical(y, num_classes=None, dtype=&amp;quot;float32&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating test set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ts = 0.3 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating model. Yes I know its super small, just 1 layer, but I was suggested to cut down to start from the base and build up. Originally it was 5 layers, but the results are still the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()

model.add(Conv2D(32, (3, 3), activation=&amp;#39;relu&amp;#39;, input_shape=(75, 75, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(26, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling the model. And fitting. I was told that the gradient could be exploding, so was suggested to add the first line with the clipnorm..&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adam = keras.optimizers.Adam(clipnorm=1.)

model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])  

model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1, validation_data=(X_test, y_test))   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final training can be seen here. The issues are, losses are NAN and accuracies are 0.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train on 54600 samples, validate on 23400 samples
Epoch 1/5
54600/54600 [==============================] - 14s 265us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/5
54600/54600 [==============================] - 15s 269us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/5
54600/54600 [==============================] - 15s 273us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 4/5
54600/54600 [==============================] - 15s 267us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 5/5
54600/54600 [==============================] - 14s 263us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here are a list of things which I did wrong and was suggested to do by others:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I didn&amp;#39;t standardize the data originally - So, I did ImageDataGenerator, rescaled, and standardized it.&lt;/li&gt;
&lt;li&gt;I was suggested to turn the data to categorical, which I did use the to_categorical function (I think i did that right) but I&amp;#39;m not sure if there&amp;#39;s anything else required.&lt;/li&gt;
&lt;li&gt;Reduce model complexity. I did that brought it to only one layer to debug.&lt;/li&gt;
&lt;li&gt;Possible exploding gradient - so changed the adam optimizer with clipnorm = 1&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;BACKGROUND: This model trains and recognizes the 26 letters of the alphabet. I know the dataset is fine because when I use it to train a model for 10 letters at a time (A-J) for example it works fine. The issue is only when I go from 10-26. Yes, I did try to change the dense to 26 on the original code but that did not work.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been staring at this and trying everything for the past two days...&lt;/p&gt;

&lt;p&gt;ANY HELP IS APPRECIATED&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggi8eg,True,,MrMegaGamerz,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggi8eg/loss_of_nan_accuracy_of_0_any_idea_why_full_code/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggi8eg/loss_of_nan_accuracy_of_0_any_idea_why_full_code/,155203,1589041002.0,0,,False,,,,
,learnmachinelearning,I  am beginner in ML. And sometimes I feel lost as I don't know someone personally who is studying ML. It would be awesome If I had someone as mentor or study buddy whom I could DM without any hesitation  .,t2_4q46suxl,False,,0,False,Need study buddy or mentor.,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg9ohu,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},Request,False,6,,False,self,False,,[],{},,,True,,1589031374.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I  am beginner in ML. And sometimes I feel lost as I don&amp;#39;t know someone personally who is studying ML. It would be awesome If I had someone as mentor or study buddy whom I could DM without any hesitation  .&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gg9ohu,True,,dark_--knight,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9ohu/need_study_buddy_or_mentor/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg9ohu/need_study_buddy_or_mentor/,155203,1589002574.0,0,,False,,,,
,learnmachinelearning," I was looking into some things on the internet and I stumbled across [https://nptel.ac.in/courses/106/103/106103205/](https://nptel.ac.in/courses/106/103/106103205/). Should I go ahead and utilise this course, considering I have an interest in ML and this could help me cover some of the mathematics behind it? (And probably help me learn other data science topics later)?",t2_43a4imr8,False,,0,False,Discrete mathematics as a prerequisite to Machine learning?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gghrxg,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589068379.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was looking into some things on the internet and I stumbled across &lt;a href=""https://nptel.ac.in/courses/106/103/106103205/""&gt;https://nptel.ac.in/courses/106/103/106103205/&lt;/a&gt;. Should I go ahead and utilise this course, considering I have an interest in ML and this could help me cover some of the mathematics behind it? (And probably help me learn other data science topics later)?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gghrxg,True,,AficionadoDS,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gghrxg/discrete_mathematics_as_a_prerequisite_to_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gghrxg/discrete_mathematics_as_a_prerequisite_to_machine/,155203,1589039579.0,0,,False,,,,
,learnmachinelearning,"I am doing a master's degree in Machine Learning and I would like to gather more resources regarding the subject. For me ML has a steep learning curve but I really enjoy. So what I would like to do is to add ML in my daily life from ""unofficial"" sources (eg. podcasts on ML, books, websites etc) 

Could you suggest such resources? Thank you in advance for your time",t2_mmqygks,False,,0,False,Machine Learning everyday resources,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggho5q,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1589068011.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am doing a master&amp;#39;s degree in Machine Learning and I would like to gather more resources regarding the subject. For me ML has a steep learning curve but I really enjoy. So what I would like to do is to add ML in my daily life from &amp;quot;unofficial&amp;quot; sources (eg. podcasts on ML, books, websites etc) &lt;/p&gt;

&lt;p&gt;Could you suggest such resources? Thank you in advance for your time&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,ggho5q,True,,platico_dev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggho5q/machine_learning_everyday_resources/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggho5q/machine_learning_everyday_resources/,155203,1589039211.0,0,,False,,,,
,learnmachinelearning,"In this project tutorial, youâ€™ll learn how to use machine learning to develop a stock trading robot. Youâ€™ll gain all the essential skills to create a full-fledged stock trading algorithm that investors and traders can utilize in their trading. 

 [https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/](https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/)",t2_3ovm77le,False,,0,False,Machine Learning for Stock Trading,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gghcqs,False,light,0.43,,public,0,0,{},,,False,[],,False,False,,{},Project,False,0,,False,self,False,,[],{},self,,True,,1589066947.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this project tutorial, youâ€™ll learn how to use machine learning to develop a stock trading robot. Youâ€™ll gain all the essential skills to create a full-fledged stock trading algorithm that investors and traders can utilize in their trading. &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/""&gt;https://www.education-ecosystem.com/andreybu/l9kEd-machine-learning-for-stock-trading/9b4Dv-intro-video-machine-learning-for-stock-trading/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?auto=webp&amp;s=9e43ee8c006d992808d8c071cfd5a99a66707ea3', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d92fb3ccc07110d0511be9542e29e56f2b471ef', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ObS9pwfXO15jhOuF1OvhBppiXjgDGGt8V5g9N9Ol_dA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c83730b89e16d0391d3a5a0c3a3b56010ffe9313', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'NjWIsUOwVenqDHwTtkeo-58F5m1HUc2DwROccUE0-Cs'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gghcqs,True,,juancarlospro,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gghcqs/machine_learning_for_stock_trading/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gghcqs/machine_learning_for_stock_trading/,155203,1589038147.0,0,,False,,,,
,learnmachinelearning,"This is the situation. I'm training a model to recognize letters of the Alphabet. There are, 26 classes. When writing the code for 26 classes, and loading nearly 100,000 images to train, I'm having a lot of issues. I could however successfully train the model to work on 10 letter increments. As in, A-J, K-T, and then U-Z. These three work perfectly fine (A-Z does not).   


Question: Can I train my A-J model, save the H5. Then, train the K-T, and U-Z after and then MERGE the H5 files together? I understand it's possible to train all A-Z using less images and then retrain the model with a different image set, but the issues are coming when I'm doing a 26 class system - hence I'm asking if I can do it in increments of 10 and merge it after.",t2_7ajy1,False,,0,False,Can I add categories to a dataset after and retrain an H5 file?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggh2vu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589066001.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is the situation. I&amp;#39;m training a model to recognize letters of the Alphabet. There are, 26 classes. When writing the code for 26 classes, and loading nearly 100,000 images to train, I&amp;#39;m having a lot of issues. I could however successfully train the model to work on 10 letter increments. As in, A-J, K-T, and then U-Z. These three work perfectly fine (A-Z does not).   &lt;/p&gt;

&lt;p&gt;Question: Can I train my A-J model, save the H5. Then, train the K-T, and U-Z after and then MERGE the H5 files together? I understand it&amp;#39;s possible to train all A-Z using less images and then retrain the model with a different image set, but the issues are coming when I&amp;#39;m doing a 26 class system - hence I&amp;#39;m asking if I can do it in increments of 10 and merge it after.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggh2vu,True,,MrMegaGamerz,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggh2vu/can_i_add_categories_to_a_dataset_after_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggh2vu/can_i_add_categories_to_a_dataset_after_and/,155203,1589037201.0,0,,False,,,,
,learnmachinelearning," 

Starting from the fact that handwritten documents style are  gender-dependent (male and female have different writing styles), I'm  trying to predict writer's gender from its handwritten scripts using  Convolutional Neural Network (CNN). I choose the IAM and KHATT datasets  for English and Arabic respectively.  After I read research articles  related to this problem, I realized that few of them used deep learning  (with handwritten word and/or sentence as input to CNN) and the majority  used classic methods like (LBP, HOG, GLCM, SFTA), the state of the art  accuracy is 80.79% and 85% (for CNN and classic methods respectively), I  also read this [Article](https://www.etsmtl.ca/ETS/media/ImagesETS/Labo/LIVIA/Publications/2012/Hanusiak_IJDAR_2012.pdf)  about identifying/verifying writer from its handwritten scripts which  uses texture blocks from the written documents that give good results.  As data preprocessing I used the method of texture blocks, after line  and word segmentation, i constructed texture image from handwriting  words, and segment the texture image to texture blocks of size  100\*100px.

I used 60 writers per gender for training (which give 42,000 texture  blocks), and 7 writers per gender for validation (which give 4,000  texture blocks) and 7 writers per gender for testing (which give 4,000  texture blocks).

I'm using TensorFlow and Keras as framework, I started with simple  (LeNet-like architectures) for base line, I got 60% test accuracy. As a  second approach, I used different state of the art architecture in image  classification like (VGG16, VGG19, RESNET34, RESNET50) and I got 64%  test accuracy.

So, my questions are:

&amp;#x200B;

1. How to reduce overfitting though I tried regularization methods (Dropout, L1, L2, Batch Norm) to reduce overfitting ?
2. I observed high variance in accuracy, if I re-shuffle the data (e.g from 64% to 50%) ?
3. How to modify CNN architectures to (multi-input CNN) to make the decision on multiple texture blocks, instead of one?

Texture blocks examples : 

https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;format=png&amp;auto=webp&amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de",t2_3i7gc514,False,,0,False,Gender Prediction from Offline Handwriting Using Convolutional Neural Networks,[],r/learnmachinelearning,False,6,,0,72.0,False,t3_gggtve,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/29_S0BvJBfrLJkf2qCgmPyY317IaRiQLd01xyQefTDI.jpg,False,,[],{},,,True,,1589065124.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Starting from the fact that handwritten documents style are  gender-dependent (male and female have different writing styles), I&amp;#39;m  trying to predict writer&amp;#39;s gender from its handwritten scripts using  Convolutional Neural Network (CNN). I choose the IAM and KHATT datasets  for English and Arabic respectively.  After I read research articles  related to this problem, I realized that few of them used deep learning  (with handwritten word and/or sentence as input to CNN) and the majority  used classic methods like (LBP, HOG, GLCM, SFTA), the state of the art  accuracy is 80.79% and 85% (for CNN and classic methods respectively), I  also read this &lt;a href=""https://www.etsmtl.ca/ETS/media/ImagesETS/Labo/LIVIA/Publications/2012/Hanusiak_IJDAR_2012.pdf""&gt;Article&lt;/a&gt;  about identifying/verifying writer from its handwritten scripts which  uses texture blocks from the written documents that give good results.  As data preprocessing I used the method of texture blocks, after line  and word segmentation, i constructed texture image from handwriting  words, and segment the texture image to texture blocks of size  100*100px.&lt;/p&gt;

&lt;p&gt;I used 60 writers per gender for training (which give 42,000 texture  blocks), and 7 writers per gender for validation (which give 4,000  texture blocks) and 7 writers per gender for testing (which give 4,000  texture blocks).&lt;/p&gt;

&lt;p&gt;I&amp;#39;m using TensorFlow and Keras as framework, I started with simple  (LeNet-like architectures) for base line, I got 60% test accuracy. As a  second approach, I used different state of the art architecture in image  classification like (VGG16, VGG19, RESNET34, RESNET50) and I got 64%  test accuracy.&lt;/p&gt;

&lt;p&gt;So, my questions are:&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How to reduce overfitting though I tried regularization methods (Dropout, L1, L2, Batch Norm) to reduce overfitting ?&lt;/li&gt;
&lt;li&gt;I observed high variance in accuracy, if I re-shuffle the data (e.g from 64% to 50%) ?&lt;/li&gt;
&lt;li&gt;How to modify CNN architectures to (multi-input CNN) to make the decision on multiple texture blocks, instead of one?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Texture blocks examples : &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de""&gt;https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gggtve,True,,khalilmeftah,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gggtve/gender_prediction_from_offline_handwriting_using/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gggtve/gender_prediction_from_offline_handwriting_using/,155203,1589036324.0,0,,False,,,"{'lp3ekbfa7rx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 55, 'x': 108, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d124ec402061952e70ec2431eff2bb65af367e'}, {'y': 111, 'x': 216, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c64cc4d841f8e26b385e64883fb0a80d1f47c2d8'}, {'y': 165, 'x': 320, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3f6ddbcb49cbfc6280466872314fb85b5edb400'}], 's': {'y': 289, 'x': 558, 'u': 'https://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;format=png&amp;auto=webp&amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de'}, 'id': 'lp3ekbfa7rx41'}}",
,learnmachinelearning,"Hi all,

I'm developing some models for image classification and object detection, and at the very end i understood I don't have good enough grasps on statistics for model evaluation. For instance, I wanted to plot Precision/Recall plot to learn value for optimal threshold for given class. I know how to do it but I have a feeling that i don't fully understand it. Also, I want to use some Platt's scaling ( [https://en.wikipedia.org/wiki/Platt\_scaling](https://en.wikipedia.org/wiki/Platt_scaling) ) to map models scores to probability and again I think i can do it, but i want someone to patiently explain to me what exactly I should do and what does it do.  

Do you have any idea for a coursera courses (or other websites, books, materials) that will let easily understand these concepts?",t2_1owxkdl1,False,,0,False,Appropriate courses to learn model evaluation,[],r/learnmachinelearning,False,6,,0,,False,t3_ggg20d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1589062249.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m developing some models for image classification and object detection, and at the very end i understood I don&amp;#39;t have good enough grasps on statistics for model evaluation. For instance, I wanted to plot Precision/Recall plot to learn value for optimal threshold for given class. I know how to do it but I have a feeling that i don&amp;#39;t fully understand it. Also, I want to use some Platt&amp;#39;s scaling ( &lt;a href=""https://en.wikipedia.org/wiki/Platt_scaling""&gt;https://en.wikipedia.org/wiki/Platt_scaling&lt;/a&gt; ) to map models scores to probability and again I think i can do it, but i want someone to patiently explain to me what exactly I should do and what does it do.  &lt;/p&gt;

&lt;p&gt;Do you have any idea for a coursera courses (or other websites, books, materials) that will let easily understand these concepts?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?auto=webp&amp;s=78838a66125bf7aa03489f673313601508eae5ad', 'width': 1200, 'height': 545}, 'resolutions': [{'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=648d3b34a2e5f5e6a07f9d274c6223f9e0e611a4', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3483de6dc00e14dac35b27240761b96b09da7496', 'width': 216, 'height': 98}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dba4ab2e8503305436507058815213788d1b53a', 'width': 320, 'height': 145}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=070b91604de8ab8a73274edcc276a23ed194d1e3', 'width': 640, 'height': 290}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c8f5a11df21ad558dbb7aa3d50aa0d478be7321b', 'width': 960, 'height': 436}, {'url': 'https://external-preview.redd.it/wBmAdgyX3pN7Bx14ghZgucheTSb6cVP9lSKwK36HDxU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21959f74b60bfbf454e05276edb49e37d32f3ccb', 'width': 1080, 'height': 490}], 'variants': {}, 'id': 'othF90FfeIQC66ekAeqIYR_nvXmSV79Jb3-4hkfnoVg'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggg20d,True,,buniosmieci,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggg20d/appropriate_courses_to_learn_model_evaluation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggg20d/appropriate_courses_to_learn_model_evaluation/,155203,1589033449.0,0,,False,,,,
,learnmachinelearning,"Hey, I'm an ML enthusiast who's trying to learn ML the mathematical way. One of the things that gave me a lot of intuition into Neural Networks and other ML algos was implementing them myself  on Python, using NumPy. 

You can check these out on [my GitHub](https://github.com/Vikhyat2603/Machine-Learning).

As of now, I've uploaded :

1. A complete feed-forward Neural Network implementation
2. Using Genetic Algorithms to train Neural Networks on Reinforcement Learning tasks
3. A visualisation for Gradient Descent using linear regression (named 'Perceptron Model')
4. A visualisation for K-Means Clustering

I've tried to make these projects understandable and modular. Feel free to reach out to me for any questions, and I'd love to get some suggestions/feedback on this!

GitHub -  [https://github.com/Vikhyat2603/Machine-Learning](https://github.com/Vikhyat2603/Machine-Learning) 

Email - [vikhyat2603@gmail.com](mailto:vikhyat2603@gmail.com)

Linkedin - [https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/](https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/)",t2_2y9k6ek4,False,,0,False,Check out these NumPy Implementations for Machine Learning algorithms,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg0e1a,False,light,0.91,,public,27,0,{},,,False,[],,False,False,,{},Project,False,27,,False,self,False,,[],{},self,,True,,1588996746.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey, I&amp;#39;m an ML enthusiast who&amp;#39;s trying to learn ML the mathematical way. One of the things that gave me a lot of intuition into Neural Networks and other ML algos was implementing them myself  on Python, using NumPy. &lt;/p&gt;

&lt;p&gt;You can check these out on &lt;a href=""https://github.com/Vikhyat2603/Machine-Learning""&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As of now, I&amp;#39;ve uploaded :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A complete feed-forward Neural Network implementation&lt;/li&gt;
&lt;li&gt;Using Genetic Algorithms to train Neural Networks on Reinforcement Learning tasks&lt;/li&gt;
&lt;li&gt;A visualisation for Gradient Descent using linear regression (named &amp;#39;Perceptron Model&amp;#39;)&lt;/li&gt;
&lt;li&gt;A visualisation for K-Means Clustering&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;ve tried to make these projects understandable and modular. Feel free to reach out to me for any questions, and I&amp;#39;d love to get some suggestions/feedback on this!&lt;/p&gt;

&lt;p&gt;GitHub -  &lt;a href=""https://github.com/Vikhyat2603/Machine-Learning""&gt;https://github.com/Vikhyat2603/Machine-Learning&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Email - [&lt;a href=""mailto:vikhyat2603@gmail.com""&gt;vikhyat2603@gmail.com&lt;/a&gt;](mailto:&lt;a href=""mailto:vikhyat2603@gmail.com""&gt;vikhyat2603@gmail.com&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Linkedin - &lt;a href=""https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/""&gt;https://www.linkedin.com/in/vikhyat-agarwal-bba30618b/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?auto=webp&amp;s=7b5c198d1f8a5f492e7baeead69c994c93b2a2bf', 'width': 420, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c237e6b3f9608a9faf28ff948516e22fccb9bc41', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fa01f7df346e5680c13191d74df6d9990e43b9e', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/6Sa2WfyjBgXCIXN4yxByXxotZa9mRAI9VFsHNybs97s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3bd1d7aa2ac1b7a6e51dad32bfbc31b34801e970', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'Blxa0lT7ynyYVYTvJarl813SqGKxe_nB7Hr5nidACVc'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gg0e1a,True,,Vikhyat333,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg0e1a/check_out_these_numpy_implementations_for_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg0e1a/check_out_these_numpy_implementations_for_machine/,155203,1588967946.0,0,,False,,,,
,learnmachinelearning,,t2_64cu1j8e,False,,0,False,Suggestions for first ML project for resume building and overall development.,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg55x0,False,dark,0.92,,public,10,0,{},,,False,[],,False,False,,{},Request,False,10,,False,self,False,,[],{},,,True,,1589012775.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gg55x0,True,,redeyetree,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg55x0/suggestions_for_first_ml_project_for_resume/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg55x0/suggestions_for_first_ml_project_for_resume/,155203,1588983975.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,How to monitor boiling milk - Homemade AI recipe,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gfrv28,False,light,0.96,,public,84,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How to monitor boiling milk - Homemade AI recipe', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_CiOCvD1--Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfrv28', 'height': 338}",Discussion,False,84,,False,https://b.thumbs.redditmedia.com/mUad4fceyad_fXkH1qYss1f1gJfQcJ7fXf1DXygzdRQ.jpg,False,,[],{},rich:video,,False,,1588967844.0,richtext,6,,,text,youtu.be,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?auto=webp&amp;s=479b55d98a9e1c30270a70cf8709fabfe5949beb', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68bcc4222ebec2364fbf2219e6ee9dfec210cc9c', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be7434b6716d90e95f1903424f1540b0fe674045', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/8rAiKKLreivF6cpiPhs2Q2yeRtk1FCFgvnXIfICK8Ss.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4734d8bfa8a68291e6a9be462c536d99d9dc776e', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'IsINZF1zS5Hd6b1lkm1e6eXa_IaJBhGCgV3Y1gTWdf8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfrv28,True,,cmillionaire9,,13,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrv28/how_to_monitor_boiling_milk_homemade_ai_recipe/,all_ads,False,https://youtu.be/_CiOCvD1--Q,155203,1588939044.0,2,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'How to monitor boiling milk - Homemade AI recipe', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/_CiOCvD1--Q?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/_CiOCvD1--Q/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}}",False,,,,
,learnmachinelearning,"I have  compiled a video file using yolo and i want to upload it directly to google drive.

I searched online but can't find any command which works.

This is what I'm doing after mounting drive:

!cpÂ out.aviÂ ""/content/drive/MyÂ Drive/images/out1.avi""

And the error i get is:

 Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(""/content/gdrive"", force\_remount=True). cp: failed to access '/content/drive/My Drive/images/out1.avi': Transport endpoint is not connected .

Can anyone help me how to solve this.",t2_6dqc4uv8,False,,0,False,[HELP] Upload file from colab straight to google drive,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg9qb8,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1589031615.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have  compiled a video file using yolo and i want to upload it directly to google drive.&lt;/p&gt;

&lt;p&gt;I searched online but can&amp;#39;t find any command which works.&lt;/p&gt;

&lt;p&gt;This is what I&amp;#39;m doing after mounting drive:&lt;/p&gt;

&lt;p&gt;!cpÂ out.aviÂ &amp;quot;/content/drive/MyÂ Drive/images/out1.avi&amp;quot;&lt;/p&gt;

&lt;p&gt;And the error i get is:&lt;/p&gt;

&lt;p&gt;Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&amp;quot;/content/gdrive&amp;quot;, force_remount=True). cp: failed to access &amp;#39;/content/drive/My Drive/images/out1.avi&amp;#39;: Transport endpoint is not connected .&lt;/p&gt;

&lt;p&gt;Can anyone help me how to solve this.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg9qb8,True,,RayS0l0,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9qb8/help_upload_file_from_colab_straight_to_google/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg9qb8/help_upload_file_from_colab_straight_to_google/,155203,1589002815.0,0,,False,,,,
,learnmachinelearning,Expecting replies from people in industry.,t2_11uipv0u,False,,0,False,Put down your top 3 favorite machine learning algorithms.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggdl13,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589051031.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Expecting replies from people in industry.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdl13,True,,k_anu7,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdl13/put_down_your_top_3_favorite_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggdl13/put_down_your_top_3_favorite_machine_learning/,155203,1589022231.0,0,,False,,,,
,learnmachinelearning,"What if instead of Humans, machines are controlling the Financial System of the Country. With this idea in mind created a small project which used Reinforcement learning to control this simulated Environment called 'Chair The Fed'. This simulation is created by the Federal Reserve Bank of San Francisco to teach about the effects of external factors like news and how manipulating the fed funds rate can control the inflation and unemployment rates. The link to code and demo video is below.

Project - https://github.com/lucky630/Chair_The_Fed_Rl

Demo - https://www.youtube.com/watch?v=vDVLj1d361A",t2_1084g6,False,,0,False,Controlling Unemployment in a simulated environment using Reinforcement learning,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_ggav83,False,light,0.67,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},self,,True,,1589037247.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;What if instead of Humans, machines are controlling the Financial System of the Country. With this idea in mind created a small project which used Reinforcement learning to control this simulated Environment called &amp;#39;Chair The Fed&amp;#39;. This simulation is created by the Federal Reserve Bank of San Francisco to teach about the effects of external factors like news and how manipulating the fed funds rate can control the inflation and unemployment rates. The link to code and demo video is below.&lt;/p&gt;

&lt;p&gt;Project - &lt;a href=""https://github.com/lucky630/Chair_The_Fed_Rl""&gt;https://github.com/lucky630/Chair_The_Fed_Rl&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Demo - &lt;a href=""https://www.youtube.com/watch?v=vDVLj1d361A""&gt;https://www.youtube.com/watch?v=vDVLj1d361A&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?auto=webp&amp;s=5212052bfff458308e142ae6bb980a9ff4963f0b', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d768890f2c3a54a148b167073df5a4b9117ec8ae', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ba35ec24ac399217e9cc435f56ccfcfed538b97', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/eaFk-eKdssPP9inIqQTca2Ntq-T8kiOZ8miDtatTBNE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b904b957ae053574684aa8a5b60d3997f3919674', 'width': 320, 'height': 320}], 'variants': {}, 'id': '_PSC4biw4O45uAH1nz6aApKjE1sS06yocyrxCBhbdzI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,ggav83,True,,rednivrug,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggav83/controlling_unemployment_in_a_simulated/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggav83/controlling_unemployment_in_a_simulated/,155203,1589008447.0,0,,False,,,,
,learnmachinelearning,"Let's have a  meeting!

1. What have you accomplished since last week?
2. What are your goals for next week?
3. Do you have any blockers that need helps from the /r/LearnMachineLearning community?

Don't be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.",t2_6l4z3,False,,0,False,"Weekly Status Check Meeting - Share your progress, your goals, and whatever is stopping you from achieving those goals!",[],r/learnmachinelearning,False,6,,0,,False,t3_ggar50,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,True,self,False,,[],{},,,True,,1589036674.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Let&amp;#39;s have a  meeting!&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What have you accomplished since last week?&lt;/li&gt;
&lt;li&gt;What are your goals for next week?&lt;/li&gt;
&lt;li&gt;Do you have any blockers that need helps from the &lt;a href=""/r/LearnMachineLearning""&gt;/r/LearnMachineLearning&lt;/a&gt; community?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Don&amp;#39;t be pressured to fill out all three questions! If you just want to celebrate your progress, just share that. If you have been planning something, you can make this meeting your first step towards your goal. If you are stuck from making a progress, feel free to just ask a question.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,ggar50,True,,AutoModerator,,4,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggar50/weekly_status_check_meeting_share_your_progress/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggar50/weekly_status_check_meeting_share_your_progress/,155203,1589007874.0,0,,False,,,,
,learnmachinelearning,"Hey everyone, I am new to the world of machine learning and a hobbyist. I am thinking of a fun side project that would involve making predictions based on file structures, and wondering if anyone had any papers on it or reading material. This seems like a pretty straight forward classification problem if I'm not mistaken and any thoughts or advice would be greatly appreciated.",t2_7z7uy,False,,0,False,File Structure Prediction?,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg919h,False,light,0.67,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},,,True,,1589028473.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone, I am new to the world of machine learning and a hobbyist. I am thinking of a fun side project that would involve making predictions based on file structures, and wondering if anyone had any papers on it or reading material. This seems like a pretty straight forward classification problem if I&amp;#39;m not mistaken and any thoughts or advice would be greatly appreciated.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gg919h,True,,Phizy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg919h/file_structure_prediction/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg919h/file_structure_prediction/,155203,1588999673.0,0,,False,,,,
,learnmachinelearning,"In logistic regression one of the first ideas is to equate the logit(p) = a straight line, how can we equate the logit of probability to a straight line?

&amp;#x200B;

Thanks, in advance",t2_3rsotruc,False,,0,False,Logistic Regression Help,[],r/learnmachinelearning,False,6,,0,,False,t3_ggdcfh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589049876.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In logistic regression one of the first ideas is to equate the logit(p) = a straight line, how can we equate the logit of probability to a straight line?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks, in advance&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggdcfh,True,,Jamhead2000,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggdcfh/logistic_regression_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggdcfh/logistic_regression_help/,155203,1589021076.0,0,,False,,,,
,learnmachinelearning,"Hi all, 

Could you recommend some resources for learning Graphical Neural Networks? My attempts to learn the concepts from review papers didn't go well. 

Thanks for your time and consideration.",t2_61xh5qps,False,,0,False,Resources for learning Graphical Neural Networks?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg64k7,False,dark,1.0,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1589016452.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, &lt;/p&gt;

&lt;p&gt;Could you recommend some resources for learning Graphical Neural Networks? My attempts to learn the concepts from review papers didn&amp;#39;t go well. &lt;/p&gt;

&lt;p&gt;Thanks for your time and consideration.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg64k7,True,,ppsrs,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg64k7/resources_for_learning_graphical_neural_networks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg64k7/resources_for_learning_graphical_neural_networks/,155203,1588987652.0,0,,False,,,,
,learnmachinelearning,"Hi! Iâ€™m new to python and the instructions they put up donâ€™t really make a lot of sense to me, would someone be able to give an easy walkthrough for setting it up? I tried using the instructions provided but it didnâ€™t work.",t2_x05qd,False,,0,False,Is there an easy way to set up Jukebox AI?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8jvm,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1589026359.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi! Iâ€™m new to python and the instructions they put up donâ€™t really make a lot of sense to me, would someone be able to give an easy walkthrough for setting it up? I tried using the instructions provided but it didnâ€™t work.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg8jvm,True,,WAFFLED_II,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8jvm/is_there_an_easy_way_to_set_up_jukebox_ai/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8jvm/is_there_an_easy_way_to_set_up_jukebox_ai/,155203,1588997559.0,0,,False,,,,
,learnmachinelearning,"\[Actual problem statement\]

For example and account posts about the same topic for at least 10 posts, shifts to a new topic for at least 10 posts.

How can I deal with the statement above?",t2_tabm4,False,,0,False,How to detect if the user is talking about the same topic in his/her posts?,[],r/learnmachinelearning,False,6,,0,,False,t3_ggcdid,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589044936.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;[Actual problem statement]&lt;/p&gt;

&lt;p&gt;For example and account posts about the same topic for at least 10 posts, shifts to a new topic for at least 10 posts.&lt;/p&gt;

&lt;p&gt;How can I deal with the statement above?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggcdid,True,,waheed0332,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggcdid/how_to_detect_if_the_user_is_talking_about_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggcdid/how_to_detect_if_the_user_is_talking_about_the/,155203,1589016136.0,0,,False,,,,
,learnmachinelearning,I finished Andrew's MOOC on Coursera and i've been wasting 1 day and half without anything to do lol.What do you guys suggest i do now? I want to learn more theory and Implementation of Machine Learning before moving on to deep learning. I plan on taking the deep learning specialization course offered by [deeplearning.ai](https://deeplearning.ai) on coursera. But i want to cover more machine learning and master it before  I move on . What do i do now? Please drop some suggestions for me,t2_2ov8dfiu,False,,0,False,I just finished Andrew Ng's Machine Learning MOOC on coursera and i have no idea what to do now.Any suggestions?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gftmrs,False,light,0.91,,public,33,0,{},,,False,[],,False,False,,{},Discussion,False,33,,False,self,False,,[],{},self,,True,,1588974790.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I finished Andrew&amp;#39;s MOOC on Coursera and i&amp;#39;ve been wasting 1 day and half without anything to do lol.What do you guys suggest i do now? I want to learn more theory and Implementation of Machine Learning before moving on to deep learning. I plan on taking the deep learning specialization course offered by &lt;a href=""https://deeplearning.ai""&gt;deeplearning.ai&lt;/a&gt; on coursera. But i want to cover more machine learning and master it before  I move on . What do i do now? Please drop some suggestions for me&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?auto=webp&amp;s=f2427c47bea6d58850d6911cf474069a18cc9b62', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b07bfe98cdf9df387c029da8df49b5de69c706a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b5db315d49d86527b77a361b9a2c28d75a3b90b', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=47549465daed6c2193229dd479b317c23b543247', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d398aae35768742b8943fa84352b11665c8d238', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=56bea95f704ddfd895b407e0e304fb0df53866e9', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/K7LaAPj58Iw6wg6xDG7v1g8jbz7uxtsma3TB5cU9eWQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df0ba01e8a1c6732cd9ae449a51e95d19f18cedf', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '5zNMs90HSxmwgVyfBrk-wTVOjvtEek3FpmHWS_2ZKP4'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gftmrs,True,,FaizRahim,,20,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gftmrs/i_just_finished_andrew_ngs_machine_learning_mooc/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gftmrs/i_just_finished_andrew_ngs_machine_learning_mooc/,155203,1588945990.0,0,,False,,,,
,learnmachinelearning,"Looks like Andrew Ng's ML course on Coursera is being offered for free (there are charges if you want the certificate), with classes starting May 11.

Is this a big deal? Has this already been discussed here? Cheers.",t2_rlnpqcu,False,,0,False,Andrew Ng's Coursera course.,[],r/learnmachinelearning,False,6,,0,,False,t3_ggbrer,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,1589013279.0,,[],{},,,True,,1589041787.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Looks like Andrew Ng&amp;#39;s ML course on Coursera is being offered for free (there are charges if you want the certificate), with classes starting May 11.&lt;/p&gt;

&lt;p&gt;Is this a big deal? Has this already been discussed here? Cheers.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggbrer,True,,cosmictypist,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggbrer/andrew_ngs_coursera_course/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/ggbrer/andrew_ngs_coursera_course/,155203,1589012987.0,0,,False,,,,
,learnmachinelearning,"Hello, my summer plans were screwed because of the current pandemic, so I'm trying to spend some of my time this summer learning some machine learning. Given the current situation, I'd like to see if I can use some coronavirus related datasets, as a beginner I obviously don't expect to find out amything useful, but I still think it would be a good idea. Are there any coronavirus related datasets online?
Maybe like something were doctors post patient responses to different treatments and info about the patients, or maybe something more related to the spread of the virus. If there isn't anything like this, do you guys think, something like that would be useful in any way?",t2_1zkqj8ti,False,,0,False,Public datasets on coronavirus information?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg8s7n,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589027370.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, my summer plans were screwed because of the current pandemic, so I&amp;#39;m trying to spend some of my time this summer learning some machine learning. Given the current situation, I&amp;#39;d like to see if I can use some coronavirus related datasets, as a beginner I obviously don&amp;#39;t expect to find out amything useful, but I still think it would be a good idea. Are there any coronavirus related datasets online?
Maybe like something were doctors post patient responses to different treatments and info about the patients, or maybe something more related to the spread of the virus. If there isn&amp;#39;t anything like this, do you guys think, something like that would be useful in any way?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8s7n,True,,flyingwizard1,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8s7n/public_datasets_on_coronavirus_information/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8s7n/public_datasets_on_coronavirus_information/,155203,1588998570.0,0,,False,,,,
,learnmachinelearning,Should i really go in depth into regression/classification and do some projects before diving into deep learning? What would you guys recommend?,t2_5dk1rkk2,False,,0,False,When should i start deep learning?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg8nfl,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1589026794.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Should i really go in depth into regression/classification and do some projects before diving into deep learning? What would you guys recommend?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8nfl,True,,shawn2james,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8nfl/when_should_i_start_deep_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8nfl/when_should_i_start_deep_learning/,155203,1588997994.0,0,,False,,,,
,learnmachinelearning,,t2_djtutca,False,,0,False,"How do you choose between the different Pooling layers (mean, max...) in a CNN ?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg535t,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Question,False,4,,False,self,False,,[],{},,,True,,1589012495.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg535t,True,,Avditvs,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg535t/how_do_you_choose_between_the_different_pooling/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg535t/how_do_you_choose_between_the_different_pooling/,155203,1588983695.0,0,,False,,,,
,learnmachinelearning,,t2_3baoayyx,False,,0,False,Help with linear alegbra projections question,[],r/learnmachinelearning,False,6,,0,26.0,False,t3_ggb78z,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/_s6B9cdgqTCQE-gE52ilDoXdUBzHhxfQ2hj3Gg2Uaxo.jpg,False,,[],{},,,False,,1589038966.0,text,6,,,text,self.askmath,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,ggb78z,True,,gimlidorf,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/ggb78z/help_with_linear_alegbra_projections_question/,all_ads,False,/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/,155203,1589010166.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'askmath', 'selftext': ""Can someone tell me what's wrong with my logic in answering this question from strang. Problem set 4.4 q 37\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pt3qux47wfx41.png?width=499&amp;format=png&amp;auto=webp&amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6\n\ni think you should subtract *QQ**^(T)**a*    but the answer is subtract *Q**^(T)**a*. But *Q**^(T)**a* would be a would be a (*n by m)* matrix times (*m* by *n*) col which would leave a (*n* by 1)  column but a is a column of length *n* rather than *m*"", 'author_fullname': 't2_3baoayyx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help with linear alegbra projections question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/askmath', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 26, 'hide_score': False, 'media_metadata': {'pt3qux47wfx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 20, 'x': 108, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2327f28593718cff795743a1c60dadb9b2c7e7b3'}, {'y': 40, 'x': 216, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0db109ae37d2b2c08c2af12f50399aefbd3c7e0'}, {'y': 59, 'x': 320, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=33cd3db602c03a465560c29cc2d056c4ea812c70'}], 's': {'y': 93, 'x': 499, 'u': 'https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;format=png&amp;auto=webp&amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6'}, 'id': 'pt3qux47wfx41'}}, 'name': 't3_gfjb3s', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588928483.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.askmath', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Can someone tell me what&amp;#39;s wrong with my logic in answering this question from strang. Problem set 4.4 q 37&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6""&gt;https://preview.redd.it/pt3qux47wfx41.png?width=499&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f1e3e73a5d014d5d35decf672ab29b5e48ba9d6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;i think you should subtract &lt;em&gt;QQ&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;    but the answer is subtract &lt;em&gt;Q&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;. But &lt;em&gt;Q&lt;/em&gt;&lt;em&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;/em&gt;&lt;em&gt;a&lt;/em&gt; would be a would be a (&lt;em&gt;n by m)&lt;/em&gt; matrix times (&lt;em&gt;m&lt;/em&gt; by &lt;em&gt;n&lt;/em&gt;) col which would leave a (&lt;em&gt;n&lt;/em&gt; by 1)  column but a is a column of length &lt;em&gt;n&lt;/em&gt; rather than &lt;em&gt;m&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qm4f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfjb3s', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'gimlidorf', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/askmath/comments/gfjb3s/help_with_linear_alegbra_projections_question/', 'subreddit_subscribers': 20697, 'created_utc': 1588899683.0, 'num_crossposts': 2, 'media': None, 'is_video': False}]",t3_gfjb3s,,
,learnmachinelearning,"Hello! I'm trying to create a NN which can recognize the letters of the alphabet (26 classes). I apologize for the lengthy post, but I've included all my relevant code to be as clear as possible. In the end I've explained the issue.

The following block is where I name the paths correctly, and standardize/normalize the image and get it ready for training.

    X = [] # Image data
    y = [] # Labels
    
    datagen = ImageDataGenerator(samplewise_center=True)
    
    for path in imagepaths:
      img = cv2.imread(path)
      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
      img = cv2.resize(img, (200, 200))
      img = image.img_to_array(img)
      img = datagen.standardize(img)
      X.append(img)
    
      # Processing label in image path
      category = path.split(""\\"")[1]
      #print(category)
      split = (category.split(""_""))     
      if int(split[0]) == 0:
        label = int(split[1])
      else:
        label = int(split[0])
      y.append(label)
    
    # Turn X and y into np.array to speed up train_test_split
    X = np.array(X, dtype=""uint8"")
    X = X.reshape(len(imagepaths), 200, 200, 1) 
    y = np.array(y)

Creating the test set.

    ts = 0.3 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)

Creating a model. Dense of 26, one output for each letter, and size 200,200,1 to match input:

    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(26, activation='softmax'))

Model compiler and fit:

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 
    model.fit(X_train, y_train, epochs=1, batch_size=64, verbose=1, validation_data=(X_test, y_test))

The issue comes here where the output of my [model.fit](https://model.fit/) is:

    Train on 54600 samples, validate on 23400 samples
    Epoch 1/1
    54600/54600 [==============================] - 79s 1ms/step - loss: nan - accuracy: 1.8315e-05 - val_loss: nan - val_accuracy: 0.0000e+00

I understand that it may not be high accuracy or anything from the get-go, but why are the losses nan? I posted elsewhere and I was first told to normalize my data (which I fixed for this post). Then I was told that it was possible that my dataset is corrupt or leaking - this is not the case because when I don't do 26 letters at once, it works perfectly fine. (Meaning I tested the code, using letters A-J, dense = 10, etc) and got a high accuracy of about 95%.

Any help is appreciated, I've been scratching my had at this for hours!",t2_7ajy1,False,,0,False,"Losses of NAN, Accuracy of 0, I tried everything! (CNN)",[],r/learnmachinelearning,False,6,,0,,False,t3_gg4qs7,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},,,True,,1589011206.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! I&amp;#39;m trying to create a NN which can recognize the letters of the alphabet (26 classes). I apologize for the lengthy post, but I&amp;#39;ve included all my relevant code to be as clear as possible. In the end I&amp;#39;ve explained the issue.&lt;/p&gt;

&lt;p&gt;The following block is where I name the paths correctly, and standardize/normalize the image and get it ready for training.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = [] # Image data
y = [] # Labels

datagen = ImageDataGenerator(samplewise_center=True)

for path in imagepaths:
  img = cv2.imread(path)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
  img = cv2.resize(img, (200, 200))
  img = image.img_to_array(img)
  img = datagen.standardize(img)
  X.append(img)

  # Processing label in image path
  category = path.split(&amp;quot;\\&amp;quot;)[1]
  #print(category)
  split = (category.split(&amp;quot;_&amp;quot;))     
  if int(split[0]) == 0:
    label = int(split[1])
  else:
    label = int(split[0])
  y.append(label)

# Turn X and y into np.array to speed up train_test_split
X = np.array(X, dtype=&amp;quot;uint8&amp;quot;)
X = X.reshape(len(imagepaths), 200, 200, 1) 
y = np.array(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating the test set.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ts = 0.3 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Creating a model. Dense of 26, one output for each letter, and size 200,200,1 to match input:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (3, 3), activation=&amp;#39;relu&amp;#39;, input_shape=(200, 200, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(26, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Model compiler and fit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) 
model.fit(X_train, y_train, epochs=1, batch_size=64, verbose=1, validation_data=(X_test, y_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The issue comes here where the output of my &lt;a href=""https://model.fit/""&gt;model.fit&lt;/a&gt; is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Train on 54600 samples, validate on 23400 samples
Epoch 1/1
54600/54600 [==============================] - 79s 1ms/step - loss: nan - accuracy: 1.8315e-05 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I understand that it may not be high accuracy or anything from the get-go, but why are the losses nan? I posted elsewhere and I was first told to normalize my data (which I fixed for this post). Then I was told that it was possible that my dataset is corrupt or leaking - this is not the case because when I don&amp;#39;t do 26 letters at once, it works perfectly fine. (Meaning I tested the code, using letters A-J, dense = 10, etc) and got a high accuracy of about 95%.&lt;/p&gt;

&lt;p&gt;Any help is appreciated, I&amp;#39;ve been scratching my had at this for hours!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg4qs7,True,,MrMegaGamerz,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg4qs7/losses_of_nan_accuracy_of_0_i_tried_everything_cnn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg4qs7/losses_of_nan_accuracy_of_0_i_tried_everything_cnn/,155203,1588982406.0,0,,False,,,,
,learnmachinelearning,,t2_2f64gmd6,False,,0,False,Is it natural that an image captioning model becomes spammy and a lot slower to train when adding attention?!,[],r/learnmachinelearning,False,6,,0,80.0,False,t3_gg9lqd,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/HUgFuLB4-WJGHg0Qad2yj_4Oc-Cgu-hkjid7QzIhUcQ.jpg,False,,[],{},image,,False,,1589031013.0,text,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?auto=webp&amp;s=f5cb39273bdf3155ca33a4ca55629f92be1221f5', 'width': 432, 'height': 248}, 'resolutions': [{'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=31ddd4f4a0de50eaa0a6924ea0e426dafe675241', 'width': 108, 'height': 62}, {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6ca58168f2859318f4e11e19fa352a0f4fca09c', 'width': 216, 'height': 124}, {'url': 'https://preview.redd.it/nhrbd6kvdox41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5e860f086c5e5947e43000a859e27aa795754a1', 'width': 320, 'height': 183}], 'variants': {}, 'id': 'WZHFZ8UPbXH2W9JQZqdfgFT-kWYXzoeSIqjI385EfrY'}], 'enabled': True}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg9lqd,True,,Abdalrahman12,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg9lqd/is_it_natural_that_an_image_captioning_model/,all_ads,False,https://i.redd.it/nhrbd6kvdox41.png,155203,1589002213.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying - How a good memory makes you wealthy $$$,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8x18,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1589027951.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?auto=webp&amp;s=66e486c81869feaad60df238edaa128388f3110a', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e642309c726659985affc46be1040421897ce8d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9453b1ab1344ef827e5e7bc71f6e87fc973649b6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dcded9a7211251b822a8a446cbaf8c647b0f52a', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef3d77ac395bf51c23ea92070a0e6a5320802e4e', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=666bfe10bbef0f8584a838c844d4c1c5c9d639ef', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a77cb1ce3d021ab0e49f1235bbe929c8254ab0a6', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'JhDjM1_cc6F88dvY5zq9T1V-OXjf34-ySM2rRo5JObg'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg8x18,True,,jianfa-ben-tsai,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8x18/studying_how_a_good_memory_makes_you_wealthy/,all_ads,False,/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/,155203,1588999151.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Benefits of a Good Memory\n\nâ€œGreat memory at work leads to be a star = promotion &amp; $$$. Not forgetting your partnerâ€™s birthday or your wedding anniversary leads to a happier life.â€\n\nIMAGINATION\n\nAlbert Einstein once said: Knowledge takes you from A to Z but imagination takes you everywhere.\n\nPay attention and convert the words of a book page into a story or an image.\n\nMake the learning of personally meaningful material easy by leveraging on your imagination.\n\nLook for meaning and relationship between data points to extract action points that translate into money or benefits. See it in your mind like a movie as clearly as you can. Hear &amp; feel it. Use your entire being and emotions to invest in making the material part of who you are and the knowledge will never leave you. Look for alternative words that sound like the difficult word that I am learning. This is associative thinking in play.\n\nWhen you have trouble making a mental connection in your mind about the material that you are trying to remember, that means that you failed to understand the material fully. Go back and look at learning this with a different strategy or ask for help from a friend, colleague, expert or stranger.\n\nSPACED REPETITION\n\nThe art of memorisation lies in practice recalling - not repeat reading\n\nSpaced repetition helps to retain and reinforce what you have learnt.\n\nAsk: how is the information I am learning going to help me right now and in the future?\n\nHow is the new information move me a step towards my goals?\n\nHow is the new material going to tell me something that not many people know?\n\nFind action steps from the new material.\n\nLearn and remember by applying creativity to everything that they know, i.e. produce images in our minds. Use your five senses. Manipulate the size of the image and make the image ridiculous. Colour the images with the rainbow shades of life. Use action, singing, talking and dancing to animate the images. Donâ€™t let the rules of inhibition prevent you from playing with your images.\n\nExtract a familiar word or phrase from familiar and link it with the sound that is the same or similar to the abstract word.\n\nPlant images at markers along mind palace journey or in mindâ€™s buildings.\n\nTurn the abstract concept into something tangible.\n\nReview journey/building backwards &amp; forwards a few times.\n\nPeriodic self-test and continued use.\n\nGiving personal meaning to numbers.\n\nInterest level is measured by how much you remember.\n\nMake an image vocabulary for key subject concepts.\n\nReview images 1 hr later, 1 day, 2 days, 3 days, 1 week, 2 weeks, 3 weeks, | month, 2 months, 3 months.\n\nOTHER\n\nSelectively take 10% of a book page out and rephrase into a single sentence in your own words and covert that single sentence into an image and story. Add sound to that image.\n\nForgetting irrelevant information helps us to prioritise.\n\nThe art of thinking is different from having eidetic memory or being able to Google well.\n\nTeach others.\n\nAsk for help when you are stuck.\n\nPeople often study a field when they are young and naive and simply goes with what is popular and will get a job even if they are disinterested in the field. It makes the learning curve steep and learning unpleasant. They wonâ€™t succeed in their job would have wasted decades of their life.\n\nHard Facts that are hard to derive meaning needs to be learned by rote. Even if learning by rote, you could adopt spaced timed interval repetition for review and revision.\n\nFind the easiest way to learn the material. Work hard but think smart, find out how it applies to learn and to work too.\n\nFind out and appreciate the reason behind each point. When you gain an appreciation of the underlying principles behind why something exists, you will be emotionally moved by its beauty and it will be effortless to remember the information. Bridge new information to old information. By making association from old information to new information it makes it easy to develop neural pathways in your both left and right hemispheres of your brain.\n\nUse your family and friends body parts to remember your material. Make it intimate.\n\nHow to remember numbers:\n\nZero for (Z,S)\n\nOne for One-way street (t)\n\nTwo for On/Off Switch (n)\n\nThree for Three Blind Mice (m)\n\nFour for 4ingers chicken (R)\n\nFive for Five Star Hotel (L)\n\nSix for Sixth Sense (J)\n\nSeven for 7 Eleven (K)\n\nEight for 8 Ball (G)\n\nNine for Cat with 9 Lives (P)\n\nTen for Ten Fingers &amp; Toes\n\nSummarise what you have written. People are busy. Deliver 70% of value in 3 minutes is better than 0% of the value with wordy text.\n\nWhy should students put in so much effort to memorize equations and facts, when they could easily find that information at the click of a mouse button?\n\nJianfa: â€œWhy didnâ€™t schools articulate specifically how does learning this subject help student make money in the near future directly other than some vague â€“ this graduation paper will get you a job which we know that getting a good job requires referrals and human connections which you get from the dark arts and partying.\n\nFive minutes of light physical exercise after any learning session can give your memory a boost.\n\nWhen you have to put something in an unfamiliar place, say what you are doing out loud: ""I am putting my sunglasses on the table by the door."" By letting your ears register the information, you increase your chances of remembering it later.\n\nMake up a story - break the information down and make up a story linking together each piece of information.\n\nAsk yourself Who, What, When, Why, Where, How.\n\nCreate mind maps, colour-code your notes, rely on written instructions for assignments and projects, use visual aids such as flashcards, diagrams, charts and pictures, watch a short clip or video that explains your topic.\n\nRecord your lectures and class notes, attend group study sessions, participate in class discussions, read the material out loud, make up a rhyme or song about the topic, use word association.\n\nTake detailed notes in class, rewrite your notes, turn diagrams and charts into words, ask your teacher for handouts, write regular summaries.\n\nAdd a physical activity while you study, act out concepts and theories, build 3D models to apply complex information, study in short blocks, study with others, including plenty of examples in your notes, attend field trips to make information more memorable.\n\nAvoid over-studying and filling your brain with useless information. Many people have a tendency to focus on unnecessary facts\n\nTo remember any piece of information, begin the review/rehearsal process as soon as possible after the information enters your short-term memory.\n\nIt is easier to remember information that has been grouped, organized, or clustered than information that has not.\n\nIndividuals remember more of what was learned at the beginning and end of a learning session.\n\nmake a conscious decision and effort to remember something.\n\nAvoid heavy meals before exams. Drink water earlier in the day to avoid dehydration.\n\nIn order to remember something thoroughly, we must be interested in it.\n\nBe positive about remembering -\n\nMost academic subjects can be classified into 3 categories,\n\na) problem solving;\n\nb) concept-driven\n\nc) interpretation\n\nSegregate ""core material"" from ""elaborative material"".\n\nDistil notes, repeat and write. Read ideas out loud in a dramatic manner. Re-organize ideas (think)\n\nGet an overview. Limit the amount to learn. Visualise, or draw an image. Link concepts. Test yourself repeatedly. Explain ideas to someone. Develop or use photographic memory. Move while repeating the main points, walk, dance, kick a ball, then write them. Develop a mnemonic for the most important points.\n\nWe cannot write our notes down logical because we tend to forget logical stuff more than creative stuff. Jianfa Thesis: Typing your notes are fast, but you should turn your typed notes in your own words into questions and answers. Take the Q&amp;A into Anki free flashcards app, as you test yourself with the flashcards, handwrite the answer down with pen and paper and draw simple images to illustrate the answer. Voice record the Q&amp;A into a voice memo app and listen to your recording while travelling on public transport.\n\nWhat feelings must you employ to improve your memory?\n\n\\- Emotion - (If you love what you do, you will remember what you love) &gt; Be Present - (Center yourself in the present and be aware) &gt; Pay Attention - (Not paying attention leads to costly mistakes)&gt; Do 1 thing at a time (Unless you are breathing and talking, humans can realistically do 1 thing well at a time.) &gt; Interesting - (If you love your job, you will find even the most mundane of tasks interesting?) &gt; Attractive - (Your mind remembers beautiful people? Capitalise that strength) &gt; Ridiculous - (What was your first memory of a silly thing that you or someone else did decades ago?\n\nHow come you still remember that without initial effortful thinking?\n\nWhat are the Associative Techniques to improve your memory?\n\nRemembering what you read\n\nDraw diagrams to see the structure of key ideas.â€\n\nIngrid Spielman recommends interacting with the text by asking yourself questions as you go along.\n\nIf youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€\n\nIf youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€\n\nskim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.\n\nThe first part is the impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.\n\nThe second part is an association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.\n\nThe third part is the repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.\n\nExperts say that, if you want to remember what you experience, itâ€™s important to do something with that information.\n\nTwo Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.\n\nPlus, if you find that you canâ€™t explain it, you might want to go back and reread.\n\nReadout loud to myself most of the time to understand and remember what I just read.â€\n\nStart by reading a Wikipedia article on the subject as preparation\n\nThe key is to see, connect, and then repeat.\n\nThe more you actively engage with the content that you are consuming, the more readily youâ€™ll remember it.\n\nTo remember something, you need to apply it. Instead of passively taking in information or actively trying to memorize it by rote, itâ€™s important to make connections. If you can apply what youâ€™ve learned, get feedback, and re-apply a concept with feedback, itâ€™s much more likely to stick.\n\nFor example, reading a recipe alone wonâ€™t help you learn to cook. Cooking a meal and having the combined feedback of your taste-buds and the comments of others will stand out in your mind. Watching someone do an exercise never has the same impact as doing it yourself. A framework is all but useless unless you apply it.\n\nWhen you apply a concept or practise to your life, it becomes easier to internalize the information.\n\nWhen you pick up a book or sit down to watch a movie, have a purpose in mind.\n\nWhen you spot related chapters or ideas in books, find ways to connect them. Highlight them, write notes, or clip the sections that are related. Taking notes by hand is an especially valuable way to help you remember important concepts.\n\nPeople who watch lots of movies or read lots of books, but canâ€™t remember them, waste a lot of time. They havenâ€™t taken in any information that will actually help them. To avoid forgetting everything you see, apply it immediately after you see it, and revisit the concepts often.\n\nWatching movies and reading mindlessly is a waste of time. Make the most of everything that you see and read by finding ways to engage with the content. Think of what youâ€™ll be missing if you allow these learning opportunities to pass you by.\n\nWhat are the mental traps that lead to learning difficulties? Avoid being duped that it is easy to remember so you donâ€™t put the effort in repetitive recall thus you quickly forget.\n\nThink in both images and words.\n\nHow do u recall information u have learnt? Give your mind a command to recall\n\nHow do you store information that you have read into your long term memory? Taking notes by hand is more effective than typing for memory retention\n\nPause after each page of reading, close your eyes to rest it and reflect on what you have read\n\nDonâ€™t read over and over, write a one-page summary. Recall information from memory\n\nSelf-test using flashcards using spaced repetition. Reflect &amp; repeat over &amp; over.\n\nDraw it out. Learn the difficult stuff in the morning\n\nIdentify the order in which to remember\n\nUse Colour, Sexuality, Humour, Outrageous Images, Exaggeration, Size Manipulation, Quantity Manipulation, 3D, time and space, to lock information into your long term memory - draw it out to play with images from Google.  \n \n\n8 tricks for remembering everything you read\n\nShana Lebowitz, Business Insider US\n\nReference: Public Accessed: 09. May 2020 [https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;IR=T](https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;IR=T)\n\n1. Take notes on the page.\n\nâ€œNever read without a pencil,â€ says Quora user Deniz AteÅŸ. â€œUnderline sentences you find confusing, interesting, or important. Draw lines along the side of important paragraphs. Draw diagrams to see the structure of key ideas.â€\n\n   \n \n\n2. Ask yourself questions about the material.\n\nIngrid Spielman recommends interacting with the text by asking yourself questions as you go along.\n\n   \n \n\nIf youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€\n\n   \n \n\nIf youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€\n\n   \n \n\n3. Skim the text first.\n\nAn anonymous user cites an article by Bill Klemm, Ph.D., a professor of neuroscience, which highlights skimming as a key strategy for retaining information.\n\n   \n \n\nThe idea here isnâ€™t to skip the whole reading process. Instead, youâ€™ll want to skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.\n\n   \n \n\nMake associations between the information youâ€™re reading and facts you already know. Francisco Osorio/Flickr\n\n4. Impress, associate, repeat.\n\nStack Exchange user TRdH says that memory is a three-pronged process. (His answer was reproduced on Lifehacker.)\n\n   \n \n\nThe first part is impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.\n\n   \n \n\nThe second part is association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.\n\n   \n \n\nThe third part is repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.\n\n   \n \n\n5. Introduce the information to others.\n\nExperts say that, if you want to remember what you experience, itâ€™s important to do something with that information.\n\n   \n \n\nTwo Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.\n\n   \n \n\nPlus, if you find that you canâ€™t explain it, you might want to go back and reread.\n\n   \n \n\n6. Read out loud.\n\nAnother anonymous Quora user says, â€œI actually have to read out loud to myself most of the time to understand and remember what I just read.â€\n\n   \n \n\nWriting in Psychology Today, psychologist Art Markman, Ph.D., says this strategy might work best when there are a few key items you need to remember. Thatâ€™s because the sentences you speak (or even whisper) out loud take on a distinctiveness. You remember producing and hearing the items and so your memory for them is different from the memory of the words you read silently.\n\n   \n \n\nResearch suggests reading on Kindle, instead of on paper, hurts your ability to remember a storyâ€™s plot. Flickr/Rich Mitchell\n\n7. Read on paper.\n\nE-readers are convenient tools for when you want to bring a ton of books on vacation and for downloading stories in an instant.\n\n   \n \n\nBut research suggests that they could also undermine the strength of your memories. One study found that, when people read the same short story in a paperback or on a Kindle, the paperback readers were better able to remember the storyâ€™s chronology.\n\n   \n \n\nLead study author Anne Mangen, Ph.D., says thatâ€™s possibly because the piles of pages in your hands creates a â€œtactile sense of progressâ€ that you donâ€™t get from a Kindle. (Of course, itâ€™s possible that people who are more accustomed to reading online may not have this problem.)\n\n   \n \n\nMeanwhile, Mangenâ€™s other research found that high-school students performed better on a test of reading comprehension when they read a text in print instead of on a computer screen.\n\n   \n \n\n8. Become familiar with the topic first.\n\nBlogger Ryan Battles recommends gaining some background knowledge before you dive into a particular text.\n\n   \n \n\nâ€œThe more you understand about a particular subject,â€ he writes, â€œthe more â€˜hooksâ€™ keep the facts in there.â€ Presumably, thatâ€™s because youâ€™re able to make more associations between the new information and what you already know.\n\n   \n \n\nYou can even start by reading a Wikipedia article on the subject as preparation.\n\nInformation + Emotion = Long-Term Memory\n\nMemory is the power of association.\n\nYour memory is not fixed. You donâ€™t have a memory, you do a memory.\n\nThere is a learning curve, but also a forgetting curve. Within 48 hours of learning something new, 80% of it can be gone.\n\nThe art of memory is the art of attention.\n\nAn incredible memory and a powerful presence come from being powerfully present.\n\nUse Spaced Repetition as a way to review the information to consolidate it from short to long-term memory.\n\nMake the information memorable. Make it silly, shocking or different.\n\nUse visualization and emotion when remembering things. What do you see and how it makes you feel.\n\nRemember: Information + Emotion = Long-Term Memory.\n\nProper sleep is very important for your memory and your brain.\n\nIt consolidates your short to long-term memory.\n\nDuring sleep, you clear the metabolic waste in your brain that leads to dementia and Alzheimer.\n\nWhen you are dreaming is when you come up with new solutions and ideas.\n\nMemory Principles:\n\nFirst or Primacy\n\nLast or Recency\n\nOrganized or Chunked\n\nEmotional\n\nDifferent or Unique\n\nFamiliar\n\nWe can visualize\n\nConnected\n\nAssociated\n\nPictures are a universal language.\n\nT.I.P. - Turn each element Into a Picture.\n\nConnect each picture to the next.\n\nUse emotion, visualization, action, and exaggeration.\n\nUse how your memory works, so that you can work your memory.\n\nBrain Bites from this lesson:\n\nThe Sun List provides you with 20 pegs. Use them!\n\nThe right answer is whatever works for you.\n\nThe best practice is teaching someone.\n\nT.I.P. - Turn Into Picture.\n\nThe challenge is not your retention, itâ€™s your attention.\n\nM.O.M. - Motivation, Observation, and Mechanics.\n\nBE SUAVE - Believe, Exercise, Say, Use, Ask, Visualize, and End.\n\nPractice makes permanent.\n\nMaking pictures helps you remember better.\n\nPIE: Place, Imagine, and Entwine.\n\nThe PIE Method works like this: Find a place on the person that pops out, imagine the personâ€™s name turned into a picture, then entwine or link the place and the image.\n\nGenius leaves clues. There is always a method behind what appears to be magical.\n\nVisual - Write the name on the person\'s forehead. You can use your favorite colour.\n\nAuditory - Repeat the name 2-3 times. \\[Remember also the S in BE SUAVE\\]\n\nKinaesthetic - Use micro-movements to write the name with your hand on the side of your body.\n\nT.I.P. - Turn Into Picture.\n\nLink images using the Vowels.\n\nThe Vowels: Action, Emotion, Illogical, Outstanding, Unusual.\n\nYour memory has three parts: Encode, Store, Retrieve.\n\nTake a picture and substitute it for the word.\n\nTurn the ordinary into extraordinary using intensifiers. \\[imagination, visualization, emotion, association, etc.\\]\n\nYou can learn using frequency, duration, or (the best way) intensity.\n\nTurn the words you want to memorize into pictures and link them via intensifiers. \\[imagination, visualization, emotion, association, etc.\\]\n\nMemory has 3 parts: Encode, Store, Retrieve.\n\nP.I.E. - Place, Imagine, Entwine.\n\nThe Location Method\n\nFind 5 places\n\nGo clockwise\n\nPick unique items\n\nPick large items\n\nNo empty spaces\n\nJimâ€™s Morning Routine\n\nRemember dreams\n\nMake the bed\n\nDrink water\n\nPhysical exercise\n\nBreathing techniques\n\nCold shower\n\nBrush teeth with the opposite hand\n\nSuperbrain Smoothie\n\nJournaling\n\nNew learnings\n\nT.I.P. - Turn Into Picture.\n\nIf you can clearly imagine it, you\'ll clearly remember it.\n\nTake the Sun List images and use Chain Linking.\n\nUse visual or auditory Basic Association to build your own lists.\n\nAuditory Basic Association\n\n1 - Bun\n\n2 - Shoe\n\n3 - Tree\n\n4 - Door\n\n5 - Hive\n\n6 - Sticks\n\n7 - Heaven\n\n8 - Gate\n\n9 - Wine\n\n10 - Zen\n\nNumbers are abstract, words are easier to remember.\n\nUse the numbers 0 to 9 and assign a consonant sound to each number.\n\nAlphanumeric Code Of Memory\n\n1 = T, D, Th (strokes)\n\n2 = N (upside down N)\n\n3 = M (3M)\n\n4 = R (Four)\n\n5 = L (hand)\n\n6 = J, G (soft), Sh, Ch (Mirror Image)\n\n7 = C (hard), K, G (hard) (top bottom K)\n\n8 = F, V (V8)\n\n9 = B, P (Mirror Image)\n\n0 = S, C (soft), Z (Zorro)\n\nRules\n\nVowels have no value.\n\nSilent letters have no value.\n\nW, H, Y have no value.\n\nDouble letters count once.\n\nNumbers are abstract, words are easier to remember.\n\nPick the word that is the easiest to picture.\n\nExample of 1 to 10\n\n1 = T = Tie\n\n2 = N = Noah\n\n3 = M = Ma (Mother)\n\n4 = R = Rye\n\n5 = L = Law\n\n6 = Sh = Shoe\n\n7 = K = Key\n\n8 = V = Ivy\n\n9 = B = Bee\n\n10 = T, S = Toes\n\nA single number can create multiple words\n\n72 = K, N = Can | Cone | Gone\n\n72 = G (hard), N = Gone | Gun | Goon\n\n72 = C (hard), N = Cane\n\nOther examples\n\n33 = M, M = Mummy\n\n47 = R, K = Rock\n\n49 = R, P = Rope\n\n51 = L, T = Lite\n\n60 = Ch, S = Cheese\n\n80 = F, C = Face\n\n97 = B, K = Book\n\nWords transformed into numbers\n\nTable = T, B, L = 195\n\nCat = K, T = 71\n\nCarpet = K, R, P, T = 7491\n\nButter = B, T, R = 914\n\nBody Folders in Numeric Code\n\n\\*\\*\\*We use just the 1st sound of each element in the Body Folders.\n\nTop = T = 1\n\nNose = N = 2\n\nMouth = M = 3\n\nEars = R = 4\n\nLarynx = L = 5\n\nShoulders = Sh = 6\n\nCollar = C = 7\n\nFingers = F = 8\n\nBelly = B = 9\n\nSeat = S = 10\n\nThere is no learning without memory.\n\nThe champion pushes past the pain period.\n\nAll behaviour is belief-driven.\n\nThe key to better comprehension is by asking better questions.\n\nBehaviour what, Capability how, beliefs and values why, identity who, environment where &amp; when.\n\nAsk a question, reticular activation system.\n\nThe 8 C\'s To Muscle Memory\n\nCompetency\n\nChunking\n\nCombining\n\nConsequences\n\nCharacter\n\nConsistency\n\nCommit by burning the bridges. Make a decision by cutting from other possibilities.\n\nHave a Coach that challenge you.\n\nClose my eyes and use my imagination and imagine I am that person.\n\nYou will never need to memorise a book word for word. \n\nTake your textbook, and take a good look at it:\n\nLook at the front cover.\n\nLook at the back cover.\n\nLook over the introduction.\n\nRead the conclusion, and\n\nBe sure to scan through the index, if your book has one.\n\nAnd read information about the bookâ€™s publication, like the place of publication, the publisher, and the publication date\n\nRead the table of contents, introduction and conclusion. \n\nTurn the memorised information into knowledge that you can use over and over â€” not just for this single test or exam.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying - How a good memory makes you wealthy $$$', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg7vxh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.64, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': 1588999068.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589023618.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Benefits of a Good Memory&lt;/p&gt;\n\n&lt;p&gt;â€œGreat memory at work leads to be a star = promotion &amp;amp; $$$. Not forgetting your partnerâ€™s birthday or your wedding anniversary leads to a happier life.â€&lt;/p&gt;\n\n&lt;p&gt;IMAGINATION&lt;/p&gt;\n\n&lt;p&gt;Albert Einstein once said: Knowledge takes you from A to Z but imagination takes you everywhere.&lt;/p&gt;\n\n&lt;p&gt;Pay attention and convert the words of a book page into a story or an image.&lt;/p&gt;\n\n&lt;p&gt;Make the learning of personally meaningful material easy by leveraging on your imagination.&lt;/p&gt;\n\n&lt;p&gt;Look for meaning and relationship between data points to extract action points that translate into money or benefits. See it in your mind like a movie as clearly as you can. Hear &amp;amp; feel it. Use your entire being and emotions to invest in making the material part of who you are and the knowledge will never leave you. Look for alternative words that sound like the difficult word that I am learning. This is associative thinking in play.&lt;/p&gt;\n\n&lt;p&gt;When you have trouble making a mental connection in your mind about the material that you are trying to remember, that means that you failed to understand the material fully. Go back and look at learning this with a different strategy or ask for help from a friend, colleague, expert or stranger.&lt;/p&gt;\n\n&lt;p&gt;SPACED REPETITION&lt;/p&gt;\n\n&lt;p&gt;The art of memorisation lies in practice recalling - not repeat reading&lt;/p&gt;\n\n&lt;p&gt;Spaced repetition helps to retain and reinforce what you have learnt.&lt;/p&gt;\n\n&lt;p&gt;Ask: how is the information I am learning going to help me right now and in the future?&lt;/p&gt;\n\n&lt;p&gt;How is the new information move me a step towards my goals?&lt;/p&gt;\n\n&lt;p&gt;How is the new material going to tell me something that not many people know?&lt;/p&gt;\n\n&lt;p&gt;Find action steps from the new material.&lt;/p&gt;\n\n&lt;p&gt;Learn and remember by applying creativity to everything that they know, i.e. produce images in our minds. Use your five senses. Manipulate the size of the image and make the image ridiculous. Colour the images with the rainbow shades of life. Use action, singing, talking and dancing to animate the images. Donâ€™t let the rules of inhibition prevent you from playing with your images.&lt;/p&gt;\n\n&lt;p&gt;Extract a familiar word or phrase from familiar and link it with the sound that is the same or similar to the abstract word.&lt;/p&gt;\n\n&lt;p&gt;Plant images at markers along mind palace journey or in mindâ€™s buildings.&lt;/p&gt;\n\n&lt;p&gt;Turn the abstract concept into something tangible.&lt;/p&gt;\n\n&lt;p&gt;Review journey/building backwards &amp;amp; forwards a few times.&lt;/p&gt;\n\n&lt;p&gt;Periodic self-test and continued use.&lt;/p&gt;\n\n&lt;p&gt;Giving personal meaning to numbers.&lt;/p&gt;\n\n&lt;p&gt;Interest level is measured by how much you remember.&lt;/p&gt;\n\n&lt;p&gt;Make an image vocabulary for key subject concepts.&lt;/p&gt;\n\n&lt;p&gt;Review images 1 hr later, 1 day, 2 days, 3 days, 1 week, 2 weeks, 3 weeks, | month, 2 months, 3 months.&lt;/p&gt;\n\n&lt;p&gt;OTHER&lt;/p&gt;\n\n&lt;p&gt;Selectively take 10% of a book page out and rephrase into a single sentence in your own words and covert that single sentence into an image and story. Add sound to that image.&lt;/p&gt;\n\n&lt;p&gt;Forgetting irrelevant information helps us to prioritise.&lt;/p&gt;\n\n&lt;p&gt;The art of thinking is different from having eidetic memory or being able to Google well.&lt;/p&gt;\n\n&lt;p&gt;Teach others.&lt;/p&gt;\n\n&lt;p&gt;Ask for help when you are stuck.&lt;/p&gt;\n\n&lt;p&gt;People often study a field when they are young and naive and simply goes with what is popular and will get a job even if they are disinterested in the field. It makes the learning curve steep and learning unpleasant. They wonâ€™t succeed in their job would have wasted decades of their life.&lt;/p&gt;\n\n&lt;p&gt;Hard Facts that are hard to derive meaning needs to be learned by rote. Even if learning by rote, you could adopt spaced timed interval repetition for review and revision.&lt;/p&gt;\n\n&lt;p&gt;Find the easiest way to learn the material. Work hard but think smart, find out how it applies to learn and to work too.&lt;/p&gt;\n\n&lt;p&gt;Find out and appreciate the reason behind each point. When you gain an appreciation of the underlying principles behind why something exists, you will be emotionally moved by its beauty and it will be effortless to remember the information. Bridge new information to old information. By making association from old information to new information it makes it easy to develop neural pathways in your both left and right hemispheres of your brain.&lt;/p&gt;\n\n&lt;p&gt;Use your family and friends body parts to remember your material. Make it intimate.&lt;/p&gt;\n\n&lt;p&gt;How to remember numbers:&lt;/p&gt;\n\n&lt;p&gt;Zero for (Z,S)&lt;/p&gt;\n\n&lt;p&gt;One for One-way street (t)&lt;/p&gt;\n\n&lt;p&gt;Two for On/Off Switch (n)&lt;/p&gt;\n\n&lt;p&gt;Three for Three Blind Mice (m)&lt;/p&gt;\n\n&lt;p&gt;Four for 4ingers chicken (R)&lt;/p&gt;\n\n&lt;p&gt;Five for Five Star Hotel (L)&lt;/p&gt;\n\n&lt;p&gt;Six for Sixth Sense (J)&lt;/p&gt;\n\n&lt;p&gt;Seven for 7 Eleven (K)&lt;/p&gt;\n\n&lt;p&gt;Eight for 8 Ball (G)&lt;/p&gt;\n\n&lt;p&gt;Nine for Cat with 9 Lives (P)&lt;/p&gt;\n\n&lt;p&gt;Ten for Ten Fingers &amp;amp; Toes&lt;/p&gt;\n\n&lt;p&gt;Summarise what you have written. People are busy. Deliver 70% of value in 3 minutes is better than 0% of the value with wordy text.&lt;/p&gt;\n\n&lt;p&gt;Why should students put in so much effort to memorize equations and facts, when they could easily find that information at the click of a mouse button?&lt;/p&gt;\n\n&lt;p&gt;Jianfa: â€œWhy didnâ€™t schools articulate specifically how does learning this subject help student make money in the near future directly other than some vague â€“ this graduation paper will get you a job which we know that getting a good job requires referrals and human connections which you get from the dark arts and partying.&lt;/p&gt;\n\n&lt;p&gt;Five minutes of light physical exercise after any learning session can give your memory a boost.&lt;/p&gt;\n\n&lt;p&gt;When you have to put something in an unfamiliar place, say what you are doing out loud: &amp;quot;I am putting my sunglasses on the table by the door.&amp;quot; By letting your ears register the information, you increase your chances of remembering it later.&lt;/p&gt;\n\n&lt;p&gt;Make up a story - break the information down and make up a story linking together each piece of information.&lt;/p&gt;\n\n&lt;p&gt;Ask yourself Who, What, When, Why, Where, How.&lt;/p&gt;\n\n&lt;p&gt;Create mind maps, colour-code your notes, rely on written instructions for assignments and projects, use visual aids such as flashcards, diagrams, charts and pictures, watch a short clip or video that explains your topic.&lt;/p&gt;\n\n&lt;p&gt;Record your lectures and class notes, attend group study sessions, participate in class discussions, read the material out loud, make up a rhyme or song about the topic, use word association.&lt;/p&gt;\n\n&lt;p&gt;Take detailed notes in class, rewrite your notes, turn diagrams and charts into words, ask your teacher for handouts, write regular summaries.&lt;/p&gt;\n\n&lt;p&gt;Add a physical activity while you study, act out concepts and theories, build 3D models to apply complex information, study in short blocks, study with others, including plenty of examples in your notes, attend field trips to make information more memorable.&lt;/p&gt;\n\n&lt;p&gt;Avoid over-studying and filling your brain with useless information. Many people have a tendency to focus on unnecessary facts&lt;/p&gt;\n\n&lt;p&gt;To remember any piece of information, begin the review/rehearsal process as soon as possible after the information enters your short-term memory.&lt;/p&gt;\n\n&lt;p&gt;It is easier to remember information that has been grouped, organized, or clustered than information that has not.&lt;/p&gt;\n\n&lt;p&gt;Individuals remember more of what was learned at the beginning and end of a learning session.&lt;/p&gt;\n\n&lt;p&gt;make a conscious decision and effort to remember something.&lt;/p&gt;\n\n&lt;p&gt;Avoid heavy meals before exams. Drink water earlier in the day to avoid dehydration.&lt;/p&gt;\n\n&lt;p&gt;In order to remember something thoroughly, we must be interested in it.&lt;/p&gt;\n\n&lt;p&gt;Be positive about remembering -&lt;/p&gt;\n\n&lt;p&gt;Most academic subjects can be classified into 3 categories,&lt;/p&gt;\n\n&lt;p&gt;a) problem solving;&lt;/p&gt;\n\n&lt;p&gt;b) concept-driven&lt;/p&gt;\n\n&lt;p&gt;c) interpretation&lt;/p&gt;\n\n&lt;p&gt;Segregate &amp;quot;core material&amp;quot; from &amp;quot;elaborative material&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Distil notes, repeat and write. Read ideas out loud in a dramatic manner. Re-organize ideas (think)&lt;/p&gt;\n\n&lt;p&gt;Get an overview. Limit the amount to learn. Visualise, or draw an image. Link concepts. Test yourself repeatedly. Explain ideas to someone. Develop or use photographic memory. Move while repeating the main points, walk, dance, kick a ball, then write them. Develop a mnemonic for the most important points.&lt;/p&gt;\n\n&lt;p&gt;We cannot write our notes down logical because we tend to forget logical stuff more than creative stuff. Jianfa Thesis: Typing your notes are fast, but you should turn your typed notes in your own words into questions and answers. Take the Q&amp;amp;A into Anki free flashcards app, as you test yourself with the flashcards, handwrite the answer down with pen and paper and draw simple images to illustrate the answer. Voice record the Q&amp;amp;A into a voice memo app and listen to your recording while travelling on public transport.&lt;/p&gt;\n\n&lt;p&gt;What feelings must you employ to improve your memory?&lt;/p&gt;\n\n&lt;p&gt;- Emotion - (If you love what you do, you will remember what you love) &amp;gt; Be Present - (Center yourself in the present and be aware) &amp;gt; Pay Attention - (Not paying attention leads to costly mistakes)&amp;gt; Do 1 thing at a time (Unless you are breathing and talking, humans can realistically do 1 thing well at a time.) &amp;gt; Interesting - (If you love your job, you will find even the most mundane of tasks interesting?) &amp;gt; Attractive - (Your mind remembers beautiful people? Capitalise that strength) &amp;gt; Ridiculous - (What was your first memory of a silly thing that you or someone else did decades ago?&lt;/p&gt;\n\n&lt;p&gt;How come you still remember that without initial effortful thinking?&lt;/p&gt;\n\n&lt;p&gt;What are the Associative Techniques to improve your memory?&lt;/p&gt;\n\n&lt;p&gt;Remembering what you read&lt;/p&gt;\n\n&lt;p&gt;Draw diagrams to see the structure of key ideas.â€&lt;/p&gt;\n\n&lt;p&gt;Ingrid Spielman recommends interacting with the text by asking yourself questions as you go along.&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€&lt;/p&gt;\n\n&lt;p&gt;skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.&lt;/p&gt;\n\n&lt;p&gt;The first part is the impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.&lt;/p&gt;\n\n&lt;p&gt;The second part is an association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.&lt;/p&gt;\n\n&lt;p&gt;The third part is the repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.&lt;/p&gt;\n\n&lt;p&gt;Experts say that, if you want to remember what you experience, itâ€™s important to do something with that information.&lt;/p&gt;\n\n&lt;p&gt;Two Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.&lt;/p&gt;\n\n&lt;p&gt;Plus, if you find that you canâ€™t explain it, you might want to go back and reread.&lt;/p&gt;\n\n&lt;p&gt;Readout loud to myself most of the time to understand and remember what I just read.â€&lt;/p&gt;\n\n&lt;p&gt;Start by reading a Wikipedia article on the subject as preparation&lt;/p&gt;\n\n&lt;p&gt;The key is to see, connect, and then repeat.&lt;/p&gt;\n\n&lt;p&gt;The more you actively engage with the content that you are consuming, the more readily youâ€™ll remember it.&lt;/p&gt;\n\n&lt;p&gt;To remember something, you need to apply it. Instead of passively taking in information or actively trying to memorize it by rote, itâ€™s important to make connections. If you can apply what youâ€™ve learned, get feedback, and re-apply a concept with feedback, itâ€™s much more likely to stick.&lt;/p&gt;\n\n&lt;p&gt;For example, reading a recipe alone wonâ€™t help you learn to cook. Cooking a meal and having the combined feedback of your taste-buds and the comments of others will stand out in your mind. Watching someone do an exercise never has the same impact as doing it yourself. A framework is all but useless unless you apply it.&lt;/p&gt;\n\n&lt;p&gt;When you apply a concept or practise to your life, it becomes easier to internalize the information.&lt;/p&gt;\n\n&lt;p&gt;When you pick up a book or sit down to watch a movie, have a purpose in mind.&lt;/p&gt;\n\n&lt;p&gt;When you spot related chapters or ideas in books, find ways to connect them. Highlight them, write notes, or clip the sections that are related. Taking notes by hand is an especially valuable way to help you remember important concepts.&lt;/p&gt;\n\n&lt;p&gt;People who watch lots of movies or read lots of books, but canâ€™t remember them, waste a lot of time. They havenâ€™t taken in any information that will actually help them. To avoid forgetting everything you see, apply it immediately after you see it, and revisit the concepts often.&lt;/p&gt;\n\n&lt;p&gt;Watching movies and reading mindlessly is a waste of time. Make the most of everything that you see and read by finding ways to engage with the content. Think of what youâ€™ll be missing if you allow these learning opportunities to pass you by.&lt;/p&gt;\n\n&lt;p&gt;What are the mental traps that lead to learning difficulties? Avoid being duped that it is easy to remember so you donâ€™t put the effort in repetitive recall thus you quickly forget.&lt;/p&gt;\n\n&lt;p&gt;Think in both images and words.&lt;/p&gt;\n\n&lt;p&gt;How do u recall information u have learnt? Give your mind a command to recall&lt;/p&gt;\n\n&lt;p&gt;How do you store information that you have read into your long term memory? Taking notes by hand is more effective than typing for memory retention&lt;/p&gt;\n\n&lt;p&gt;Pause after each page of reading, close your eyes to rest it and reflect on what you have read&lt;/p&gt;\n\n&lt;p&gt;Donâ€™t read over and over, write a one-page summary. Recall information from memory&lt;/p&gt;\n\n&lt;p&gt;Self-test using flashcards using spaced repetition. Reflect &amp;amp; repeat over &amp;amp; over.&lt;/p&gt;\n\n&lt;p&gt;Draw it out. Learn the difficult stuff in the morning&lt;/p&gt;\n\n&lt;p&gt;Identify the order in which to remember&lt;/p&gt;\n\n&lt;p&gt;Use Colour, Sexuality, Humour, Outrageous Images, Exaggeration, Size Manipulation, Quantity Manipulation, 3D, time and space, to lock information into your long term memory - draw it out to play with images from Google.  &lt;/p&gt;\n\n&lt;p&gt;8 tricks for remembering everything you read&lt;/p&gt;\n\n&lt;p&gt;Shana Lebowitz, Business Insider US&lt;/p&gt;\n\n&lt;p&gt;Reference: Public Accessed: 09. May 2020 &lt;a href=""https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;amp;IR=T""&gt;https://www.businessinsider.sg/how-to-remember-everything-you-read-2015-9?r=US&amp;amp;IR=T&lt;/a&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Take notes on the page.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;â€œNever read without a pencil,â€ says Quora user Deniz AteÅŸ. â€œUnderline sentences you find confusing, interesting, or important. Draw lines along the side of important paragraphs. Draw diagrams to see the structure of key ideas.â€&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ask yourself questions about the material.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ingrid Spielman recommends interacting with the text by asking yourself questions as you go along.&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading a textbook, the question can be as simple as, â€œWhat is the main idea of this section?â€&lt;/p&gt;\n\n&lt;p&gt;If youâ€™re reading fiction, you can ask, â€œWhat are the motives of the character?â€ and â€œIf you could rewrite this reading, what would your version be like?â€&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Skim the text first.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;An anonymous user cites an article by Bill Klemm, Ph.D., a professor of neuroscience, which highlights skimming as a key strategy for retaining information.&lt;/p&gt;\n\n&lt;p&gt;The idea here isnâ€™t to skip the whole reading process. Instead, youâ€™ll want to skim the text for important topics and keywords beforehand so you know what to expect when you actually dig into the material. Being familiar with the general themes, Klemm says, will help you remember the particulars.&lt;/p&gt;\n\n&lt;p&gt;Make associations between the information youâ€™re reading and facts you already know. Francisco Osorio/Flickr&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Impress, associate, repeat.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Stack Exchange user TRdH says that memory is a three-pronged process. (His answer was reproduced on Lifehacker.)&lt;/p&gt;\n\n&lt;p&gt;The first part is impression. You can increase the strength of the impression the text makes on you by picturing the situation in your mind or envisioning yourself participating in the events described.&lt;/p&gt;\n\n&lt;p&gt;The second part is association, or linking the material to something you already know. For example, maybe one of the characterâ€™s names sounds like your friendâ€™s name.&lt;/p&gt;\n\n&lt;p&gt;The third part is repetition. The more you read the material, the stronger your memory will be. If you donâ€™t want to reread a whole book, try highlighting some parts of the text that you can go back to.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Introduce the information to others.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Experts say that, if you want to remember what you experience, itâ€™s important to do something with that information.&lt;/p&gt;\n\n&lt;p&gt;Two Quora users listed talking about what you read as a useful means of processing new material. Venkatesh Rao suggests blogging, or otherwise trying to explain to others what you think youâ€™ve learned.&lt;/p&gt;\n\n&lt;p&gt;Plus, if you find that you canâ€™t explain it, you might want to go back and reread.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read out loud.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Another anonymous Quora user says, â€œI actually have to read out loud to myself most of the time to understand and remember what I just read.â€&lt;/p&gt;\n\n&lt;p&gt;Writing in Psychology Today, psychologist Art Markman, Ph.D., says this strategy might work best when there are a few key items you need to remember. Thatâ€™s because the sentences you speak (or even whisper) out loud take on a distinctiveness. You remember producing and hearing the items and so your memory for them is different from the memory of the words you read silently.&lt;/p&gt;\n\n&lt;p&gt;Research suggests reading on Kindle, instead of on paper, hurts your ability to remember a storyâ€™s plot. Flickr/Rich Mitchell&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read on paper.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;E-readers are convenient tools for when you want to bring a ton of books on vacation and for downloading stories in an instant.&lt;/p&gt;\n\n&lt;p&gt;But research suggests that they could also undermine the strength of your memories. One study found that, when people read the same short story in a paperback or on a Kindle, the paperback readers were better able to remember the storyâ€™s chronology.&lt;/p&gt;\n\n&lt;p&gt;Lead study author Anne Mangen, Ph.D., says thatâ€™s possibly because the piles of pages in your hands creates a â€œtactile sense of progressâ€ that you donâ€™t get from a Kindle. (Of course, itâ€™s possible that people who are more accustomed to reading online may not have this problem.)&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, Mangenâ€™s other research found that high-school students performed better on a test of reading comprehension when they read a text in print instead of on a computer screen.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Become familiar with the topic first.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Blogger Ryan Battles recommends gaining some background knowledge before you dive into a particular text.&lt;/p&gt;\n\n&lt;p&gt;â€œThe more you understand about a particular subject,â€ he writes, â€œthe more â€˜hooksâ€™ keep the facts in there.â€ Presumably, thatâ€™s because youâ€™re able to make more associations between the new information and what you already know.&lt;/p&gt;\n\n&lt;p&gt;You can even start by reading a Wikipedia article on the subject as preparation.&lt;/p&gt;\n\n&lt;p&gt;Information + Emotion = Long-Term Memory&lt;/p&gt;\n\n&lt;p&gt;Memory is the power of association.&lt;/p&gt;\n\n&lt;p&gt;Your memory is not fixed. You donâ€™t have a memory, you do a memory.&lt;/p&gt;\n\n&lt;p&gt;There is a learning curve, but also a forgetting curve. Within 48 hours of learning something new, 80% of it can be gone.&lt;/p&gt;\n\n&lt;p&gt;The art of memory is the art of attention.&lt;/p&gt;\n\n&lt;p&gt;An incredible memory and a powerful presence come from being powerfully present.&lt;/p&gt;\n\n&lt;p&gt;Use Spaced Repetition as a way to review the information to consolidate it from short to long-term memory.&lt;/p&gt;\n\n&lt;p&gt;Make the information memorable. Make it silly, shocking or different.&lt;/p&gt;\n\n&lt;p&gt;Use visualization and emotion when remembering things. What do you see and how it makes you feel.&lt;/p&gt;\n\n&lt;p&gt;Remember: Information + Emotion = Long-Term Memory.&lt;/p&gt;\n\n&lt;p&gt;Proper sleep is very important for your memory and your brain.&lt;/p&gt;\n\n&lt;p&gt;It consolidates your short to long-term memory.&lt;/p&gt;\n\n&lt;p&gt;During sleep, you clear the metabolic waste in your brain that leads to dementia and Alzheimer.&lt;/p&gt;\n\n&lt;p&gt;When you are dreaming is when you come up with new solutions and ideas.&lt;/p&gt;\n\n&lt;p&gt;Memory Principles:&lt;/p&gt;\n\n&lt;p&gt;First or Primacy&lt;/p&gt;\n\n&lt;p&gt;Last or Recency&lt;/p&gt;\n\n&lt;p&gt;Organized or Chunked&lt;/p&gt;\n\n&lt;p&gt;Emotional&lt;/p&gt;\n\n&lt;p&gt;Different or Unique&lt;/p&gt;\n\n&lt;p&gt;Familiar&lt;/p&gt;\n\n&lt;p&gt;We can visualize&lt;/p&gt;\n\n&lt;p&gt;Connected&lt;/p&gt;\n\n&lt;p&gt;Associated&lt;/p&gt;\n\n&lt;p&gt;Pictures are a universal language.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn each element Into a Picture.&lt;/p&gt;\n\n&lt;p&gt;Connect each picture to the next.&lt;/p&gt;\n\n&lt;p&gt;Use emotion, visualization, action, and exaggeration.&lt;/p&gt;\n\n&lt;p&gt;Use how your memory works, so that you can work your memory.&lt;/p&gt;\n\n&lt;p&gt;Brain Bites from this lesson:&lt;/p&gt;\n\n&lt;p&gt;The Sun List provides you with 20 pegs. Use them!&lt;/p&gt;\n\n&lt;p&gt;The right answer is whatever works for you.&lt;/p&gt;\n\n&lt;p&gt;The best practice is teaching someone.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;The challenge is not your retention, itâ€™s your attention.&lt;/p&gt;\n\n&lt;p&gt;M.O.M. - Motivation, Observation, and Mechanics.&lt;/p&gt;\n\n&lt;p&gt;BE SUAVE - Believe, Exercise, Say, Use, Ask, Visualize, and End.&lt;/p&gt;\n\n&lt;p&gt;Practice makes permanent.&lt;/p&gt;\n\n&lt;p&gt;Making pictures helps you remember better.&lt;/p&gt;\n\n&lt;p&gt;PIE: Place, Imagine, and Entwine.&lt;/p&gt;\n\n&lt;p&gt;The PIE Method works like this: Find a place on the person that pops out, imagine the personâ€™s name turned into a picture, then entwine or link the place and the image.&lt;/p&gt;\n\n&lt;p&gt;Genius leaves clues. There is always a method behind what appears to be magical.&lt;/p&gt;\n\n&lt;p&gt;Visual - Write the name on the person&amp;#39;s forehead. You can use your favorite colour.&lt;/p&gt;\n\n&lt;p&gt;Auditory - Repeat the name 2-3 times. [Remember also the S in BE SUAVE]&lt;/p&gt;\n\n&lt;p&gt;Kinaesthetic - Use micro-movements to write the name with your hand on the side of your body.&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;Link images using the Vowels.&lt;/p&gt;\n\n&lt;p&gt;The Vowels: Action, Emotion, Illogical, Outstanding, Unusual.&lt;/p&gt;\n\n&lt;p&gt;Your memory has three parts: Encode, Store, Retrieve.&lt;/p&gt;\n\n&lt;p&gt;Take a picture and substitute it for the word.&lt;/p&gt;\n\n&lt;p&gt;Turn the ordinary into extraordinary using intensifiers. [imagination, visualization, emotion, association, etc.]&lt;/p&gt;\n\n&lt;p&gt;You can learn using frequency, duration, or (the best way) intensity.&lt;/p&gt;\n\n&lt;p&gt;Turn the words you want to memorize into pictures and link them via intensifiers. [imagination, visualization, emotion, association, etc.]&lt;/p&gt;\n\n&lt;p&gt;Memory has 3 parts: Encode, Store, Retrieve.&lt;/p&gt;\n\n&lt;p&gt;P.I.E. - Place, Imagine, Entwine.&lt;/p&gt;\n\n&lt;p&gt;The Location Method&lt;/p&gt;\n\n&lt;p&gt;Find 5 places&lt;/p&gt;\n\n&lt;p&gt;Go clockwise&lt;/p&gt;\n\n&lt;p&gt;Pick unique items&lt;/p&gt;\n\n&lt;p&gt;Pick large items&lt;/p&gt;\n\n&lt;p&gt;No empty spaces&lt;/p&gt;\n\n&lt;p&gt;Jimâ€™s Morning Routine&lt;/p&gt;\n\n&lt;p&gt;Remember dreams&lt;/p&gt;\n\n&lt;p&gt;Make the bed&lt;/p&gt;\n\n&lt;p&gt;Drink water&lt;/p&gt;\n\n&lt;p&gt;Physical exercise&lt;/p&gt;\n\n&lt;p&gt;Breathing techniques&lt;/p&gt;\n\n&lt;p&gt;Cold shower&lt;/p&gt;\n\n&lt;p&gt;Brush teeth with the opposite hand&lt;/p&gt;\n\n&lt;p&gt;Superbrain Smoothie&lt;/p&gt;\n\n&lt;p&gt;Journaling&lt;/p&gt;\n\n&lt;p&gt;New learnings&lt;/p&gt;\n\n&lt;p&gt;T.I.P. - Turn Into Picture.&lt;/p&gt;\n\n&lt;p&gt;If you can clearly imagine it, you&amp;#39;ll clearly remember it.&lt;/p&gt;\n\n&lt;p&gt;Take the Sun List images and use Chain Linking.&lt;/p&gt;\n\n&lt;p&gt;Use visual or auditory Basic Association to build your own lists.&lt;/p&gt;\n\n&lt;p&gt;Auditory Basic Association&lt;/p&gt;\n\n&lt;p&gt;1 - Bun&lt;/p&gt;\n\n&lt;p&gt;2 - Shoe&lt;/p&gt;\n\n&lt;p&gt;3 - Tree&lt;/p&gt;\n\n&lt;p&gt;4 - Door&lt;/p&gt;\n\n&lt;p&gt;5 - Hive&lt;/p&gt;\n\n&lt;p&gt;6 - Sticks&lt;/p&gt;\n\n&lt;p&gt;7 - Heaven&lt;/p&gt;\n\n&lt;p&gt;8 - Gate&lt;/p&gt;\n\n&lt;p&gt;9 - Wine&lt;/p&gt;\n\n&lt;p&gt;10 - Zen&lt;/p&gt;\n\n&lt;p&gt;Numbers are abstract, words are easier to remember.&lt;/p&gt;\n\n&lt;p&gt;Use the numbers 0 to 9 and assign a consonant sound to each number.&lt;/p&gt;\n\n&lt;p&gt;Alphanumeric Code Of Memory&lt;/p&gt;\n\n&lt;p&gt;1 = T, D, Th (strokes)&lt;/p&gt;\n\n&lt;p&gt;2 = N (upside down N)&lt;/p&gt;\n\n&lt;p&gt;3 = M (3M)&lt;/p&gt;\n\n&lt;p&gt;4 = R (Four)&lt;/p&gt;\n\n&lt;p&gt;5 = L (hand)&lt;/p&gt;\n\n&lt;p&gt;6 = J, G (soft), Sh, Ch (Mirror Image)&lt;/p&gt;\n\n&lt;p&gt;7 = C (hard), K, G (hard) (top bottom K)&lt;/p&gt;\n\n&lt;p&gt;8 = F, V (V8)&lt;/p&gt;\n\n&lt;p&gt;9 = B, P (Mirror Image)&lt;/p&gt;\n\n&lt;p&gt;0 = S, C (soft), Z (Zorro)&lt;/p&gt;\n\n&lt;p&gt;Rules&lt;/p&gt;\n\n&lt;p&gt;Vowels have no value.&lt;/p&gt;\n\n&lt;p&gt;Silent letters have no value.&lt;/p&gt;\n\n&lt;p&gt;W, H, Y have no value.&lt;/p&gt;\n\n&lt;p&gt;Double letters count once.&lt;/p&gt;\n\n&lt;p&gt;Numbers are abstract, words are easier to remember.&lt;/p&gt;\n\n&lt;p&gt;Pick the word that is the easiest to picture.&lt;/p&gt;\n\n&lt;p&gt;Example of 1 to 10&lt;/p&gt;\n\n&lt;p&gt;1 = T = Tie&lt;/p&gt;\n\n&lt;p&gt;2 = N = Noah&lt;/p&gt;\n\n&lt;p&gt;3 = M = Ma (Mother)&lt;/p&gt;\n\n&lt;p&gt;4 = R = Rye&lt;/p&gt;\n\n&lt;p&gt;5 = L = Law&lt;/p&gt;\n\n&lt;p&gt;6 = Sh = Shoe&lt;/p&gt;\n\n&lt;p&gt;7 = K = Key&lt;/p&gt;\n\n&lt;p&gt;8 = V = Ivy&lt;/p&gt;\n\n&lt;p&gt;9 = B = Bee&lt;/p&gt;\n\n&lt;p&gt;10 = T, S = Toes&lt;/p&gt;\n\n&lt;p&gt;A single number can create multiple words&lt;/p&gt;\n\n&lt;p&gt;72 = K, N = Can | Cone | Gone&lt;/p&gt;\n\n&lt;p&gt;72 = G (hard), N = Gone | Gun | Goon&lt;/p&gt;\n\n&lt;p&gt;72 = C (hard), N = Cane&lt;/p&gt;\n\n&lt;p&gt;Other examples&lt;/p&gt;\n\n&lt;p&gt;33 = M, M = Mummy&lt;/p&gt;\n\n&lt;p&gt;47 = R, K = Rock&lt;/p&gt;\n\n&lt;p&gt;49 = R, P = Rope&lt;/p&gt;\n\n&lt;p&gt;51 = L, T = Lite&lt;/p&gt;\n\n&lt;p&gt;60 = Ch, S = Cheese&lt;/p&gt;\n\n&lt;p&gt;80 = F, C = Face&lt;/p&gt;\n\n&lt;p&gt;97 = B, K = Book&lt;/p&gt;\n\n&lt;p&gt;Words transformed into numbers&lt;/p&gt;\n\n&lt;p&gt;Table = T, B, L = 195&lt;/p&gt;\n\n&lt;p&gt;Cat = K, T = 71&lt;/p&gt;\n\n&lt;p&gt;Carpet = K, R, P, T = 7491&lt;/p&gt;\n\n&lt;p&gt;Butter = B, T, R = 914&lt;/p&gt;\n\n&lt;p&gt;Body Folders in Numeric Code&lt;/p&gt;\n\n&lt;p&gt;***We use just the 1st sound of each element in the Body Folders.&lt;/p&gt;\n\n&lt;p&gt;Top = T = 1&lt;/p&gt;\n\n&lt;p&gt;Nose = N = 2&lt;/p&gt;\n\n&lt;p&gt;Mouth = M = 3&lt;/p&gt;\n\n&lt;p&gt;Ears = R = 4&lt;/p&gt;\n\n&lt;p&gt;Larynx = L = 5&lt;/p&gt;\n\n&lt;p&gt;Shoulders = Sh = 6&lt;/p&gt;\n\n&lt;p&gt;Collar = C = 7&lt;/p&gt;\n\n&lt;p&gt;Fingers = F = 8&lt;/p&gt;\n\n&lt;p&gt;Belly = B = 9&lt;/p&gt;\n\n&lt;p&gt;Seat = S = 10&lt;/p&gt;\n\n&lt;p&gt;There is no learning without memory.&lt;/p&gt;\n\n&lt;p&gt;The champion pushes past the pain period.&lt;/p&gt;\n\n&lt;p&gt;All behaviour is belief-driven.&lt;/p&gt;\n\n&lt;p&gt;The key to better comprehension is by asking better questions.&lt;/p&gt;\n\n&lt;p&gt;Behaviour what, Capability how, beliefs and values why, identity who, environment where &amp;amp; when.&lt;/p&gt;\n\n&lt;p&gt;Ask a question, reticular activation system.&lt;/p&gt;\n\n&lt;p&gt;The 8 C&amp;#39;s To Muscle Memory&lt;/p&gt;\n\n&lt;p&gt;Competency&lt;/p&gt;\n\n&lt;p&gt;Chunking&lt;/p&gt;\n\n&lt;p&gt;Combining&lt;/p&gt;\n\n&lt;p&gt;Consequences&lt;/p&gt;\n\n&lt;p&gt;Character&lt;/p&gt;\n\n&lt;p&gt;Consistency&lt;/p&gt;\n\n&lt;p&gt;Commit by burning the bridges. Make a decision by cutting from other possibilities.&lt;/p&gt;\n\n&lt;p&gt;Have a Coach that challenge you.&lt;/p&gt;\n\n&lt;p&gt;Close my eyes and use my imagination and imagine I am that person.&lt;/p&gt;\n\n&lt;p&gt;You will never need to memorise a book word for word. &lt;/p&gt;\n\n&lt;p&gt;Take your textbook, and take a good look at it:&lt;/p&gt;\n\n&lt;p&gt;Look at the front cover.&lt;/p&gt;\n\n&lt;p&gt;Look at the back cover.&lt;/p&gt;\n\n&lt;p&gt;Look over the introduction.&lt;/p&gt;\n\n&lt;p&gt;Read the conclusion, and&lt;/p&gt;\n\n&lt;p&gt;Be sure to scan through the index, if your book has one.&lt;/p&gt;\n\n&lt;p&gt;And read information about the bookâ€™s publication, like the place of publication, the publisher, and the publication date&lt;/p&gt;\n\n&lt;p&gt;Read the table of contents, introduction and conclusion. &lt;/p&gt;\n\n&lt;p&gt;Turn the memorised information into knowledge that you can use over and over â€” not just for this single test or exam.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?auto=webp&amp;s=66e486c81869feaad60df238edaa128388f3110a', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e642309c726659985affc46be1040421897ce8d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9453b1ab1344ef827e5e7bc71f6e87fc973649b6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dcded9a7211251b822a8a446cbaf8c647b0f52a', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ef3d77ac395bf51c23ea92070a0e6a5320802e4e', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=666bfe10bbef0f8584a838c844d4c1c5c9d639ef', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/jYhE1XbrHmFn0vr_RBHdCvHmd85yKtxiqC82wMuwZbk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a77cb1ce3d021ab0e49f1235bbe929c8254ab0a6', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'JhDjM1_cc6F88dvY5zq9T1V-OXjf34-ySM2rRo5JObg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gg7vxh', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gg7vxh/studying_how_a_good_memory_makes_you_wealthy/', 'subreddit_subscribers': 33, 'created_utc': 1588994818.0, 'num_crossposts': 27, 'media': None, 'is_video': False}]",t3_gg7vxh,,
,learnmachinelearning,Iâ€™m new to python and Iâ€™m wondering if thereâ€™s a place where people created models to work with different genres. Is there a place to find them?,t2_x05qd,False,,0,False,Spleeter - where would I find and download other pre-made models?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8tpz,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1589027533.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Iâ€™m new to python and Iâ€™m wondering if thereâ€™s a place where people created models to work with different genres. Is there a place to find them?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg8tpz,True,,WAFFLED_II,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8tpz/spleeter_where_would_i_find_and_download_other/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8tpz/spleeter_where_would_i_find_and_download_other/,155203,1588998733.0,0,,False,,,,
,learnmachinelearning,"Hello as part of my subjects assignment, my task is to work on a sentiment analysis related project on Yelp review data. Iâ€™ve never done sentiment analysis before, would anyone here whoâ€™s had a go at it be able to guide me to some useful resources for a first timer? 

Thank you very much x",t2_120a94wr,False,,0,False,Sentiment analysis resources request,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg8tor,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1589027530.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello as part of my subjects assignment, my task is to work on a sentiment analysis related project on Yelp review data. Iâ€™ve never done sentiment analysis before, would anyone here whoâ€™s had a go at it be able to guide me to some useful resources for a first timer? &lt;/p&gt;

&lt;p&gt;Thank you very much x&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg8tor,True,,Khoobsuratt,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8tor/sentiment_analysis_resources_request/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg8tor/sentiment_analysis_resources_request/,155203,1588998730.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: High-Fidelity 3D Face Reconstruction,[],r/learnmachinelearning,False,6,,0,55.0,False,t3_gg8rvn,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg,False,,[],{},link,,False,,1589027327.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg8rvn,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg8rvn/from_cvpr_20_highfidelity_3d_face_reconstruction/,all_ads,False,/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/,155203,1588998527.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'For project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI)\n\nhttps://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e\n\nmain idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: High-Fidelity 3D Face Reconstruction"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 55, 'hide_score': False, 'media_metadata': {'81ta0o3c2ox41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55c32367cf95f7e6fa654cbf786e32d01ecd2dcc'}, {'y': 86, 'x': 216, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f885fc37a7b8df2c81d03bc56b51edb2a591a7ab'}, {'y': 127, 'x': 320, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7b2695c338c17666bf413cccda5684a106c7efa'}, {'y': 255, 'x': 640, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46fba7b645462a6a2fcbd3bd624b5df9e9aefdec'}, {'y': 383, 'x': 960, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b8d855cfc2c0ccc5d012b462f5e9d1d7fe03c2c'}, {'y': 431, 'x': 1080, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21026136b7f8cca8faf9058e0a868ef6c6a1143d'}], 's': {'y': 758, 'x': 1898, 'u': 'https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;format=png&amp;auto=webp&amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e'}, 'id': '81ta0o3c2ox41'}}, 'name': 't3_gg8qay', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/X812G8Juom6hky5SWddMRxxU2OXoaa7oOmn42ar7Ky4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589027133.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05653?fbclid=IwAR1JquJ4bBWX7VfUGm78NJqOPKAyF1O7XOkU_l0k1xT7rg6nm06VueJVoGI""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e""&gt;https://preview.redd.it/81ta0o3c2ox41.png?width=1898&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35dbcac798fe7ec62fd8f82e13b826f0081c7d2e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;main idea is to refine the initial texture generated by a 3DMM based method with facial details from the input image&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?auto=webp&amp;s=6225767bb348a765a9bf7d2773be244a42a8fc78', 'width': 594, 'height': 338}, 'resolutions': [{'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0dd3ce83c20ba2090d9b6f7d70601f5dd8b077b', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5228bdf69e06e26b78ceae9944e24b6a372e2cad', 'width': 216, 'height': 122}, {'url': 'https://external-preview.redd.it/a_HX7N5KFqa1zLYMOhe2lguyfkPtkAJt19ijnw8kvDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60e25d55d7e1dd48a99d40907ff417d0c0f8a7f8', 'width': 320, 'height': 182}], 'variants': {}, 'id': 'KVXM33i4nWnl2TFfCz8N6JSXlCNimw6gLao5I5QC0gc'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg8qay', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg8qay/from_cvpr_20_highfidelity_3d_face_reconstruction/', 'subreddit_subscribers': 3386, 'created_utc': 1588998333.0, 'num_crossposts': 12, 'media': None, 'is_video': False}]",t3_gg8qay,,
,learnmachinelearning,"I'm a newbie at ml, never finished any course that I have ever started. 
I was doing fastai's deep learning course and in that, the instructor recommended that we check out Andrew Ng's course on Coursera. 
Now, that course looks old and I found a new version on YouTube (from 2018) [here](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) 

Which one should I do? The Coursera one or this one? And should I continue fastai's course side by side?


Edit: Thank you everyone for your answers. I have started the ML course on Coursera today!",t2_qhggx,False,,0,False,Andrew Ng Coursera or CS229 YouTube 2018?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg2zh7,False,dark,0.81,,public,3,0,{},,,False,[],,False,False,,{},Question,False,3,,False,self,1589088323.0,,[],{},self,,True,,1589005047.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m a newbie at ml, never finished any course that I have ever started. 
I was doing fastai&amp;#39;s deep learning course and in that, the instructor recommended that we check out Andrew Ng&amp;#39;s course on Coursera. 
Now, that course looks old and I found a new version on YouTube (from 2018) &lt;a href=""https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU""&gt;here&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Which one should I do? The Coursera one or this one? And should I continue fastai&amp;#39;s course side by side?&lt;/p&gt;

&lt;p&gt;Edit: Thank you everyone for your answers. I have started the ML course on Coursera today!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?auto=webp&amp;s=7dad7e69084fe17dc70d1948b6f58fc13d288f0c', 'width': 168, 'height': 94}, 'resolutions': [{'url': 'https://external-preview.redd.it/1nUPsBj-8rC-W2T0bXPFWrytVDrE3EPgDKhXPlDXHxI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54a29aa3b0894b9f5f031e6ec93d6896ab4450', 'width': 108, 'height': 60}], 'variants': {}, 'id': 'WdNtCcCE5kkWeUtaFzPEmvl-T9SFUg0GJCbpJncoGqQ'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg2zh7,True,,RKRohk,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg2zh7/andrew_ng_coursera_or_cs229_youtube_2018/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg2zh7/andrew_ng_coursera_or_cs229_youtube_2018/,155203,1588976247.0,0,,False,,,,
,learnmachinelearning,"Could anyone help to derive the following [CIoU loss equations](https://arxiv.org/abs/1911.08287) ?

Note: I understood the concept behind *RDIoU*, but not *RCIoU*  


https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1",t2_bpftl,False,,0,False,Deriving CIoU equations,[],r/learnmachinelearning,False,6,,0,80.0,False,t3_gg84t2,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://a.thumbs.redditmedia.com/RdmkT3sxrnN6_60s1ykCRtj16zGr7o9ZIihH2jcPvQ0.jpg,False,,[],{},,,True,,1589024632.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Could anyone help to derive the following &lt;a href=""https://arxiv.org/abs/1911.08287""&gt;CIoU loss equations&lt;/a&gt; ?&lt;/p&gt;

&lt;p&gt;Note: I understood the concept behind &lt;em&gt;RDIoU&lt;/em&gt;, but not &lt;em&gt;RCIoU&lt;/em&gt;  &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1""&gt;https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg84t2,True,,promach,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg84t2/deriving_ciou_equations/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg84t2/deriving_ciou_equations/,155203,1588995832.0,0,,False,,,"{'7yvmjetkunx41': {'status': 'valid', 'e': 'Image', 'm': 'image/jpg', 'p': [{'y': 61, 'x': 108, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ed3a6dd261d1734d87c20561bb50141115febbd'}, {'y': 123, 'x': 216, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75b5fe0b87153e939c64385e46f776b5cd97e530'}, {'y': 182, 'x': 320, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8026a93a220ac791f86a1346a3deb9ec1a7bb231'}, {'y': 365, 'x': 640, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb38f866a9a8faeb079e133ca3d45902c868b4f7'}, {'y': 548, 'x': 960, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=36f79913a2e7858856997fde74e450703facaeb7'}, {'y': 617, 'x': 1080, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90bb0632f84a1f16c411a6a6e3595772ae263e7'}], 's': {'y': 800, 'x': 1400, 'u': 'https://preview.redd.it/7yvmjetkunx41.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=4e74c0cf381baa91d59b72aeb23bfa022986fee1'}, 'id': '7yvmjetkunx41'}}",
,learnmachinelearning,"Has anyone worked on PU-GAN or PU-Net? I was trying to give colour point cloud as input. I am not getting idea, how that can be done. If someone knows, give suggestions.",t2_3zkrpw14,False,,0,False,How can PU-GAN be changed to take input of xyzrgb,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg81w5,False,dark,0.66,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1589024303.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Has anyone worked on PU-GAN or PU-Net? I was trying to give colour point cloud as input. I am not getting idea, how that can be done. If someone knows, give suggestions.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gg81w5,True,,Shutthefrontdoooor,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg81w5/how_can_pugan_be_changed_to_take_input_of_xyzrgb/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg81w5/how_can_pugan_be_changed_to_take_input_of_xyzrgb/,155203,1588995503.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying - How to learn effectively 09. May 2020,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg7wxq,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},,,False,,1589023730.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg7wxq,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg7wxq/studying_how_to_learn_effectively_09_may_2020/,all_ads,False,/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/,155203,1588994930.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': '**SELF**\n\nIdentify where are your weaknesses are in learning and capitalise on your strengths by being honest with yourself.\n\nDevelop a love of learning\n\nSpend 20% time to chat with others and 80% time to think deeply\n\nAn expert is someone who can practically apply the books knowledge to the real-life situation.  \n\nThe practice is not how long you have studied or worked at a job, but your performance within a year. After the performance review, what you have done the previous year is erased and forgotten.\n\nIf you need to choose between learning technical skills or soft skills, what should you do?\n\nReduce entertainment hours, and focus 80% time and energy to master technical course content and selectively find people and resources to practice your soft skills with 20% of your time\n\nTell me and I forget\n\nTeach me &amp; I remember\n\nInvolve me &amp; I will learn\n\nThe secret to choosing the most suitable self-improvement book for you is to understand your current situation â€“ and to have a clear vision of what you hope to achieve in the future. \n\nThe ideal self-improvement book will be one that fits your current needs and will be easy and enjoyable for you to read from start to finish. \n\nAsk yourself at the end of every day, what did you learn today\n\nSeek progress, not perfection\n\nTake a step a day\n\nWhen you meet with hard stuff to learn, what must you think? This is fun - enjoy the process\n\nWhen you fail to understand a piece of information, what must you do? Tell yourself not to blame others - be grateful for the little things in life and things that you have learnt\n\nTake ownership of your mistakes but donâ€™t be a scapegoat. \n\n**CHALLENGE** \n\nIf the challenge is too hard, we give up. If the challenge is too easy, we get bored. \n\nAlways seek feedback loop on your progress from people around you. \n\nIf you want to improve yourself, seek out opponents that are better than you.\n\nPeople often lose concentration resulting in demotivation and giving up. \n\nTrain yourself to relax by taking multiple short breaks between intense work during each day.\n\n**BUSINESS IDEA**\n\nFrom an entrepreneurship point of view, why not hire humans with a sexy and husky voice as a selling point to read aloud and record the study notes of lonely men and women (customers) as a paid service to help enhance the memory retention of their notes? This helps to reduce crime and empower sex workers out of poverty. This saves time for time poor corporate executives to learn new knowledge which becomes a win-win solution.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying - How to learn effectively 09. May 2020', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg6spe', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589019086.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;strong&gt;SELF&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Identify where are your weaknesses are in learning and capitalise on your strengths by being honest with yourself.&lt;/p&gt;\n\n&lt;p&gt;Develop a love of learning&lt;/p&gt;\n\n&lt;p&gt;Spend 20% time to chat with others and 80% time to think deeply&lt;/p&gt;\n\n&lt;p&gt;An expert is someone who can practically apply the books knowledge to the real-life situation.  &lt;/p&gt;\n\n&lt;p&gt;The practice is not how long you have studied or worked at a job, but your performance within a year. After the performance review, what you have done the previous year is erased and forgotten.&lt;/p&gt;\n\n&lt;p&gt;If you need to choose between learning technical skills or soft skills, what should you do?&lt;/p&gt;\n\n&lt;p&gt;Reduce entertainment hours, and focus 80% time and energy to master technical course content and selectively find people and resources to practice your soft skills with 20% of your time&lt;/p&gt;\n\n&lt;p&gt;Tell me and I forget&lt;/p&gt;\n\n&lt;p&gt;Teach me &amp;amp; I remember&lt;/p&gt;\n\n&lt;p&gt;Involve me &amp;amp; I will learn&lt;/p&gt;\n\n&lt;p&gt;The secret to choosing the most suitable self-improvement book for you is to understand your current situation â€“ and to have a clear vision of what you hope to achieve in the future. &lt;/p&gt;\n\n&lt;p&gt;The ideal self-improvement book will be one that fits your current needs and will be easy and enjoyable for you to read from start to finish. &lt;/p&gt;\n\n&lt;p&gt;Ask yourself at the end of every day, what did you learn today&lt;/p&gt;\n\n&lt;p&gt;Seek progress, not perfection&lt;/p&gt;\n\n&lt;p&gt;Take a step a day&lt;/p&gt;\n\n&lt;p&gt;When you meet with hard stuff to learn, what must you think? This is fun - enjoy the process&lt;/p&gt;\n\n&lt;p&gt;When you fail to understand a piece of information, what must you do? Tell yourself not to blame others - be grateful for the little things in life and things that you have learnt&lt;/p&gt;\n\n&lt;p&gt;Take ownership of your mistakes but donâ€™t be a scapegoat. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CHALLENGE&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;If the challenge is too hard, we give up. If the challenge is too easy, we get bored. &lt;/p&gt;\n\n&lt;p&gt;Always seek feedback loop on your progress from people around you. &lt;/p&gt;\n\n&lt;p&gt;If you want to improve yourself, seek out opponents that are better than you.&lt;/p&gt;\n\n&lt;p&gt;People often lose concentration resulting in demotivation and giving up. &lt;/p&gt;\n\n&lt;p&gt;Train yourself to relax by taking multiple short breaks between intense work during each day.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;BUSINESS IDEA&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;From an entrepreneurship point of view, why not hire humans with a sexy and husky voice as a selling point to read aloud and record the study notes of lonely men and women (customers) as a paid service to help enhance the memory retention of their notes? This helps to reduce crime and empower sex workers out of poverty. This saves time for time poor corporate executives to learn new knowledge which becomes a win-win solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gg6spe', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gg6spe/studying_how_to_learn_effectively_09_may_2020/', 'subreddit_subscribers': 33, 'created_utc': 1588990286.0, 'num_crossposts': 29, 'media': None, 'is_video': False}]",t3_gg6spe,,
,learnmachinelearning,,t2_17rz8f84,False,,0,False,LSTM stock market prediction exercise. Some help needed please,[],r/learnmachinelearning,False,6,,0,,False,t3_gg79vh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1589021059.0,text,6,,,text,self.NeuralNetwork,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg79vh,True,,edenmannh,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg79vh/lstm_stock_market_prediction_exercise_some_help/,all_ads,False,/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/,155203,1588992259.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'NeuralNetwork', 'selftext': ""I have made a model which attempts to predict the next five days of closing price for a given stock (KMD.NZ for example). The training data is fetched from Yahoo Finance.  Depending on whether I download 10 years or 10.3 years, the 2 month trend completely changes (like from positive 30% to -5%). I'm programming in python using keras. Is there any way to place more weighting on recent data than data 10yrs ago? Any other reason why this small snippet of data would completely change the prediction?\n\nThanks."", 'author_fullname': 't2_17rz8f84', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'LSTM stock market prediction exercise. Some help needed please', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/NeuralNetwork', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gg76yr', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 4, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': 1588993522.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589020727.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.NeuralNetwork', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have made a model which attempts to predict the next five days of closing price for a given stock (KMD.NZ for example). The training data is fetched from Yahoo Finance.  Depending on whether I download 10 years or 10.3 years, the 2 month trend completely changes (like from positive 30% to -5%). I&amp;#39;m programming in python using keras. Is there any way to place more weighting on recent data than data 10yrs ago? Any other reason why this small snippet of data would completely change the prediction?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2s3sq', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg76yr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'edenmannh', 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/NeuralNetwork/comments/gg76yr/lstm_stock_market_prediction_exercise_some_help/', 'subreddit_subscribers': 2651, 'created_utc': 1588991927.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_gg76yr,,
,learnmachinelearning,,t2_5lflrdo8,False,,0,False,AI basketball analysis web App and API,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,46.0,False,t3_gf6zl8,False,light,1.0,,public,806,0,{},140.0,,False,[],,True,False,,{},Project,False,806,,False,https://a.thumbs.redditmedia.com/ivNOFyoEI-Mr2MSlv5hZlAPjua8n4B8x7aIqAB2gNr8.jpg,False,,[],{},image,,False,,1588888539.0,richtext,6,,,text,i.redd.it,True,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?format=png8&amp;s=3ebce6dd0db0d14061b50af6df99d478955b0686', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3d16e2d3a7d07ca46522f1d592d3b2f1e6487dc2', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=f905d9c859ed4af2c01f74ff6ccd046c79bdc3c8', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=7744aacfa1e46ec86d4ff396e8eee310fee5c5be', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=388e231701849acee50ba5accde0e13d1654f892', 'width': 640, 'height': 212}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?s=88c4904a289fdff3e755a6c41b167453976a3287', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;crop=smart&amp;s=d190fcc9ea29b77f782e29781a3d4a5e9c9a26b6', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;crop=smart&amp;s=79acfd768b2776afad15db91827819e719038053', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;crop=smart&amp;s=fa7947c378eb72a749934b5e76e50c2fb780ec61', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;crop=smart&amp;s=ea1faf5da973c35fb72d1d8a853a96778ec2594e', 'width': 640, 'height': 212}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/ddymeguslcx41.gif?format=mp4&amp;s=bb5ef88a30acb882c8efb78276d90df60225e742', 'width': 800, 'height': 266}, 'resolutions': [{'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=108&amp;format=mp4&amp;s=0479dda0d419f23dc563b55f4bdf4a427f4d9cf4', 'width': 108, 'height': 35}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=216&amp;format=mp4&amp;s=c0af8dc4f5ed6b19fbd937ff4085f75a3d652621', 'width': 216, 'height': 71}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=320&amp;format=mp4&amp;s=a45cab424209d45d5e13096fcd19954c18305eeb', 'width': 320, 'height': 106}, {'url': 'https://preview.redd.it/ddymeguslcx41.gif?width=640&amp;format=mp4&amp;s=22c50849835802c26e3a2ba27499975aa28aa568', 'width': 640, 'height': 212}]}}, 'id': 'ZP0v8R5rkp0Vai3Cr-z4BXKLCDv9sg9mwNzj5oVYffw'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf6zl8,True,,chonyyy,,42,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6zl8/ai_basketball_analysis_web_app_and_api/,all_ads,False,https://i.redd.it/ddymeguslcx41.gif,155203,1588859739.0,0,,False,,,,
,learnmachinelearning,,t2_34e5wzr6,False,,0,False,"Phd students and Drs, how much did you know about AI/ML before embarking on your PhD?",[],r/learnmachinelearning,False,6,,0,,False,t3_gfstro,False,dark,1.0,,public,11,0,{},,,False,[],,False,False,,{},,False,11,,False,self,False,,[],{},,,True,,1588971798.0,text,6,,,text,self.learnmachinelearning,False,,,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfstro,True,,AssumeSmallAngle,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfstro/phd_students_and_drs_how_much_did_you_know_about/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfstro/phd_students_and_drs_how_much_did_you_know_about/,155203,1588942998.0,0,,False,,,,
,learnmachinelearning,"A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)
 
Haven't taken the GRE yet. 
GPA 9.36/10

So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)",t2_4ih25pre,False,,0,False,How do I go about learning TensorFlow?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfo7cw,False,light,0.96,,public,25,0,{},,,False,[],,False,False,,{},HELP,False,25,,False,self,False,,[],{},,,True,,1588950050.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)&lt;/p&gt;

&lt;p&gt;Haven&amp;#39;t taken the GRE yet. 
GPA 9.36/10&lt;/p&gt;

&lt;p&gt;So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfo7cw,True,,SuccMyStrangerThings,,11,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo7cw/how_do_i_go_about_learning_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo7cw/how_do_i_go_about_learning_tensorflow/,155203,1588921250.0,0,,False,,,,
,learnmachinelearning,"I am looking to further build my mathematical foundation for machine learning, and I came across this book at https://mml-book.github.io. I am currently a software engineer who uses python as my main language, and mostly focus on data engineering work. My company would like me to move more into a data scientist role and wants me to delve into machine learning. I have a decent math background (could certainly be better). I took the full calc series, linear algebra, and the upper division courses such as real analysis and several other proof based classes. I would say probability and statistics are my weakest areas. So, is this book a good catch-all? Or should I focus on specific books for specific subjects? Thanks!",t2_mhvcd,False,,0,False,How is the book Mathematics for Machine Learning? Are there better resources for building a solid mathematical foundation?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfvjpg,False,dark,1.0,,public,4,0,{},,,False,[],,False,False,,{},Question,False,4,,False,self,False,,[],{},self,,True,,1588981590.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am looking to further build my mathematical foundation for machine learning, and I came across this book at &lt;a href=""https://mml-book.github.io""&gt;https://mml-book.github.io&lt;/a&gt;. I am currently a software engineer who uses python as my main language, and mostly focus on data engineering work. My company would like me to move more into a data scientist role and wants me to delve into machine learning. I have a decent math background (could certainly be better). I took the full calc series, linear algebra, and the upper division courses such as real analysis and several other proof based classes. I would say probability and statistics are my weakest areas. So, is this book a good catch-all? Or should I focus on specific books for specific subjects? Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?auto=webp&amp;s=a5a066e892cd1a887ba10e324183d3020e0906fe', 'width': 180, 'height': 261}, 'resolutions': [{'url': 'https://external-preview.redd.it/pg4jzhisIPYeNJIxUkkSShEjRZvuT9o_HLgRd5maTic.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=324d98b2afe974c9923a90b7fece00bbb8c2357f', 'width': 108, 'height': 156}], 'variants': {}, 'id': 'XzFfeCJvwXSOhVXOWBpN_GqQftnCRE5pJg-f7VJIAm8'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfvjpg,True,,RawCS,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfvjpg/how_is_the_book_mathematics_for_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfvjpg/how_is_the_book_mathematics_for_machine_learning/,155203,1588952790.0,0,,False,,,,
,learnmachinelearning,"Hey there, I decided to take CS230, and my AI isnâ€™t working properly, Itâ€™s supposed to track faces because it would be easy for me to make videos, can anyone help with this? Thanks.",t2_3kvgiyr1,False,,0,False,First ML Project,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg3zp9,False,light,0.33,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1589008529.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey there, I decided to take CS230, and my AI isnâ€™t working properly, Itâ€™s supposed to track faces because it would be easy for me to make videos, can anyone help with this? Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gg3zp9,True,,silverfoxreddits,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3zp9/first_ml_project/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3zp9/first_ml_project/,155203,1588979729.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,Bring Old Photos Back to Life,[],r/learnmachinelearning,False,6,,0,57.0,False,t3_gg3z53,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg,False,,[],{},link,,False,,1589008475.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3z53,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3z53/bring_old_photos_back_to_life/,all_ads,False,/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/,155203,1588979675.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': 'From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:\n\n**Bring Old Photos Back to Life**       \nFor project and code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2004.09484)\n\nhttps://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b\n\nThis new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos', 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Bring Old Photos Back to Life', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 57, 'hide_score': False, 'media_metadata': {'q9cz19sjgmx41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 44, 'x': 108, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5a0301ab1954fe92d14162ef010d5594c2163ac'}, {'y': 88, 'x': 216, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15b420753d4d84a5c4adede4fabd0ca0ee41f8b'}, {'y': 130, 'x': 320, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=571432baf4e3158e45084882f8b87bc44d31b982'}, {'y': 261, 'x': 640, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56dce9805fac99c032f0a6dcd382a6575a08ee5b'}, {'y': 391, 'x': 960, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c3f3cd35f64b66f1c381fe8a349e4ba144e069f'}, {'y': 440, 'x': 1080, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f9e19d9f978923cc491fa3295836356d1888206'}], 's': {'y': 552, 'x': 1352, 'u': 'https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;format=png&amp;auto=webp&amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b'}, 'id': 'q9cz19sjgmx41'}}, 'name': 't3_gg3rac', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 36, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 36, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/l_xQrQGuMxwpN5G0aUWM9-OGZoi0zRHs6y5KQn9qXps.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1589007729.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From researchers at Microsoft, City University of Hong Kong, University of Science and Technology of China:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bring Old Photos Back to Life&lt;/strong&gt;&lt;br/&gt;\nFor project and code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2004.09484""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b""&gt;https://preview.redd.it/q9cz19sjgmx41.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9b8f4cac19e2c8a424927e566ccbd942ed6a4b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This new method can handle the complex degradation mixed by both unstructured and structured defects in real old photos&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?auto=webp&amp;s=3ee6961d88445c2d066121eaff6b23559a952474', 'width': 654, 'height': 348}, 'resolutions': [{'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f3851843805287ba92edaff8f11f3ce2b26ffa4e', 'width': 108, 'height': 57}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=568754b724e494b26903388e154799a13b2fae1c', 'width': 216, 'height': 114}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4829d78277bf3af52eafcf8db8ccab736992f513', 'width': 320, 'height': 170}, {'url': 'https://external-preview.redd.it/b9u87JLFi_h5r56z0kHVMSllE3IpIxQW0-OsiMygJ7k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db342fc2adc7956d97654fcfe9fb6945789d5447', 'width': 640, 'height': 340}], 'variants': {}, 'id': 'JLDhPQFAIizsdivC3iTOBXsQt1gQDs0ndz-oTKqUcI0'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gg3rac', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/gg3rac/bring_old_photos_back_to_life/', 'subreddit_subscribers': 3386, 'created_utc': 1588978929.0, 'num_crossposts': 16, 'media': None, 'is_video': False}]",t3_gg3rac,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#learning #studying Quick Study Guides - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg3fmn,False,light,0.4,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},,,False,,1589006602.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg3fmn,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3fmn/learning_studying_quick_study_guides_a_university/,all_ads,False,/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/,155203,1588977802.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Public Accessed: 07. May 2020  [https://www.monash.edu/rlo/quick-study-guides](https://www.monash.edu/rlo/quick-study-guides)  Monash University, RLO Study Skills, 2020\n\nAssignment direction words\n\nBelow is an explanation of the keywords commonly used in assignment questions.\n\nThese explanations are intended as a guide only. There are not always hard distinctions between the words, and different academics may use them to mean slightly different things.\n\n&amp;#x200B;\n\nANALYSE. Identify the elements of an argument, text, issue, process or event and show how they are related.ARGUE. Present the case for and/or against a particular proposition. \n\n&amp;#x200B;\n\nCOMMENT ON. Point out the important features, Criticise. \n\n&amp;#x200B;\n\nCOMPARE. Identify and explain similarities and differences. \n\n&amp;#x200B;\n\nCONTRAST. Stress the differences between two or more things. \n\n&amp;#x200B;\n\nCRITICISE. Judge the merit or truth of the factors or views mentioned, including both strengths and weaknesses.CRITICALLYâ€¦Used to add direction to another direction word (e.g. â€œcritically analyseâ€), this means approaching the task in a **questioning** way: â€œHow does this work?â€ â€œWhy is it like this?â€ â€œWhat are its strengths and weaknesses?â€\n\n \n\nDEFINE. Provide concise, clear, and authoritative meanings. Give the limits of the definition, but omit detailed explanations. Show how the item defined differs from items in other classes. \n\n&amp;#x200B;\n\nDESCRIBE. Recount, characterise, outline, and relate in sequence.\n\n&amp;#x200B;\n\nDIAGRAM. A drawing, chart, plan, or graph. Diagrams should be labelled and there should be an accompanying explanation.\n\n&amp;#x200B;\n\nDISCUSS. Examine, analyse carefully and give reasons for and against. Be complete and give details, usually with a view to assessing how satisfactory something is.\n\n&amp;#x200B;\n\nEVALUATE. Appraise in relation to some standard, referring to advantages, limitations, and costs and benefits as Appropriate.\n\n&amp;#x200B;\n\nEXAMINE. Investigate critically appraises a subject in detail.\n\n&amp;#x200B;\n\nEXPLAIN. Clarify, interpret and elaborate on the material presented. Give reasons for differences of opinion or results, and try to analyse causes.\n\n&amp;#x200B;\n\nILLUSTRATE. Use a concrete example, diagram, or figure to explain or clarify a problem.\n\n&amp;#x200B;\n\nINDICATE. Identify, then focuses attention so as to clarify.\n\n&amp;#x200B;\n\nJUSTIFY. Prove or give reasons for conclusions or decisions.\n\n&amp;#x200B;\n\nOUTLINE. Present the essential features, showing the main points and subordinate points. Omit minor details.\n\n&amp;#x200B;\n\nREVIEW. Examine a subject critically, analysing and commenting on the important or controversial statements.\n\n&amp;#x200B;\n\nSTATE. Present the main points in a brief and clear sequence, usually omitting details or examples.\n\n&amp;#x200B;\n\nSUMMARISE. Give the main points or facts in condensed form. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nBrainstorming: Mind mapping\n\nWhy mind map?\n\nOne effective form of brainstorming is mind mapping. A mind map is a visual representation of your ideas, consisting of words, images and colours, and can help you to:\n\nfocus on your research topic/question\n\nstructure and plan your assignment\n\ncombine one or more types of major thought relationships\n\nidentify relationships between ideas/concepts.\n\nStage 1\n\nYou can create a mind map on the paper, whiteboard or digitally, using visual mapping software such as [FreeMind](http://freemind.sourceforge.net/wiki/index.php/Main_Page). To begin:\n\nwrite your topic in the centre of a blank page\n\nassociate your ideas freely anywhere on the page and do not filter out ideas.\n\nStage 2\n\nWhen you have run out of ideas:\n\nconsider each item and determine how this point is related to other points and to your topic\n\nmap relationships with lines, arrows, colours, images and bold type.\n\nStage 3\n\nUse the relationships you have identified to reorganise your ideas.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSignposting\n\nWhy signpost?\n\nSignposting helps you to:\n\ncreate a navigation path to guide your reader,\n\nindicate to your reader the direction your writing will take, and\n\nremember your key points.\n\nMajor signposts\n\nMajor signposts indicate to the reader the important elements of your writing such as the purpose, connection between points, and the conclusion.\n\nExamples:\n\n&amp;#x200B;\n\nThis study argues that â€¦\n\nThis paper examinesâ€¦\n\nThis essay begins byâ€¦ it goes on to â€¦\n\nIn conclusion â€¦\n\nTransition sentences\n\nTransition sentences inform the reader when your writing is moving from one idea to another, and how those ideas are connected.\n\n&amp;#x200B;\n\nExamples:\n\n&amp;#x200B;\n\nHaving examined; It is important to\n\nThe discussion highlights; It is also essential, however, to explore;\n\nA significant factor inâ€¦ isâ€¦\n\nLinking words\n\nLinking words signify to the reader the connections between your ideas. Linking words convey what you will be discussing next, and how the reader should interpret it.\n\n&amp;#x200B;\n\nSome types of linking words\n\n&amp;#x200B;\n\nExamples\n\nAddition\tin addition; also\n\nCause and effect\tdue to; as a result\n\nComparison\tsimilarly; likewise\n\nContrast\talternatively; in contrast\n\nExamples\tfor instance; to show this\n\nException\thowever; although\n\nTime and sequence\tinitially; to being; previously; finally\n\nReminders\n\nReminders can help the reader recall what you have previously discussed.\n\n&amp;#x200B;\n\nExamples:\n\n&amp;#x200B;\n\nAs discussed aboveâ€¦\n\nSection 2 outlinesâ€¦\n\nSo farâ€¦\n\nIn conclusionâ€¦\n\nIn briefâ€¦\n\nThusâ€¦\n\n&amp;#x200B;\n\nWriting essays\n\nAnalysing essay topics\n\nUnderstand the essay question\n\nMake sure you know the precise meaning of every word in the essay question. Use:\n\n&amp;#x200B;\n\na) your general dictionary for unfamiliar words such as intrinsic, core values\n\n&amp;#x200B;\n\nb) a subject-specific dictionary, for example, the APA Dictionary of Psychology, for academic words such as proof, random sample, significance level\n\n&amp;#x200B;\n\nThink about the different parts of the question\n\nDecide how many parts the question has.\n\n&amp;#x200B;\n\nList areas you will probably have to research. It may help to write yourself a brief task description: â€œFirst find out what a market niche is, then see what significance this has for marketing. Nextâ€¦â€\n\n&amp;#x200B;\n\nNote any direction words\n\nThese might include:\n\n&amp;#x200B;\n\ndiscuss, discuss critically, discuss the importance of, assess, justify, evaluate, analyse.\n\n&amp;#x200B;\n\nThe structure of an essay\n\nIntroduction\n\nAbout 10% of the total length. May be one paragraph or several, depending on essay length.\n\n&amp;#x200B;\n\nIntroduce the topic.\n\nProvide background information.\n\nLimit the scope of the discussion.\n\nDefine or state the topic.\n\nPresent the plan of coverage including your viewpoint and line of reasoning.\n\nMove from general background information to your specific topic. You can set your own agenda to avoid too broad a focus.\n\n&amp;#x200B;\n\nBody\n\nThis is a series of linked paragraphs.\n\n&amp;#x200B;\n\nEach paragraph should have one main point.\n\nThe topic sentence of each paragraph carries the theme or argument.\n\nConclusion\n\nHere you are moving from the specifics of your essay to the more general background of the topic.\n\n&amp;#x200B;\n\nSum up your argument and information with reference to the essay question.\n\nPerhaps mention wider implications or future directions.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nFeatures of a critical review\n\nA critical review requires you to evaluate an academic text and make your own judgement about it based on various criteria.\n\n&amp;#x200B;\n\nThis guide points out the common features of critical reviews. Always check with your lecturer to confirm the exact requirements of your assignment.\n\n&amp;#x200B;\n\nCritical reviews can present positive and/or negative judgements.\n\n&amp;#x200B;\n\nCritical reviews involve two main tasks:\n\n&amp;#x200B;\n\nsummary\n\nevaluation.\n\nThe evaluation criteria can vary depending on the discipline.\n\n&amp;#x200B;\n\nThe aim of a critical review is to evaluate the text. This means you must be very familiar with the text, and your writing needs to clearly present your judgement.\n\n&amp;#x200B;\n\nTitle\n\nUsually looks like an entry in a bibliography, and includes full bibliographic details of the text.\n\nIntroduction\n\nGives an overview of the text including the importance of the topic or question.\n\nBriefly states your evaluation of the merits of the text.\n\nOutlines your reviewâ€™s approach and structure.\n\nSummary (may be combined with evaluation)\n\nDescribes the key points from the text, including the authorâ€™s intentions and findings.\n\nEvaluation (may be combined with summary)\n\nPresents strengths and weaknesses.\n\nFocuses on the evaluation criteria to present your judgement of the text.\n\nConclusion\n\nUsually quite short, so can be included at the end of the evaluation or as a separate section.\n\nRestates your overall evaluation.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nFeatures of reports\n\nReferencing and quotations in reports follow the same guidelines required for essays.\n\nA system of\xa0numbered sections with headings is typically used.\n\nJust as in the essay, a good report will describe, analyse and evaluate a problem or issue. Unlike an essay, it will describe the method used to investigate the problem and formulate a set of recommendations based on the findings of the report.\n\nReport\n\nVS\n\nEssay\n\nOften a problem or case study which sets up a hypothetical situation\n\nBased on the reading, fieldwork or practical work\n\nTopic\n\nResponds to a question or a proposition\n\nIs based on research\n\nTo investigate, analyse and present information\n\nUsually to make recommendations to solve a problem\n\nPurpose\n\nTo articulate a well-argued response to a question or proposition\n\nEstablished in the topic and is often a client or manager\n\nAudience\n\nAn academic audience\n\nContains an executive summary or abstract\n\nComprises sections with headings\n\nMay use bullet points, tables, graphs to convey information\n\nFormat\n\nDoes not typically include sections or headings\n\nDoes not typically include use bullet points, tables, graphs\n\nThird-person\n\nFormal language\n\nStyle\n\nThird or first person\n\nFormal language\n\nSuccess depends on:\n\nthe demonstration of good research skills\n\nthe quality of the recommendations to respond to an issue\n\nthe presentation and analysis of relevant information\n\nAssessment\n\nSuccess depends on:\n\nthe demonstration of good research skills\n\nthe identification of a cogent argument\n\nthe quality of reasoning and evidence\n\nhow well it analyses and evaluates the issue\n\nDifferent types of reports typically include different sections.  \nFor the requirements for reports in Business and Economics, see the Q Manual. For all other disciplines, look at the Faculty examples in\xa0[Assignment Structures and Samples](https://www.monash.edu/rlo/assignment-samples).\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nWriting a case study\n\nThere are two different approaches to case studies. This guide focuses on the problem-oriented method. Always check with your lecturer to confirm if this is the type required.\n\nA successful case study analyses a real-life situation where existing problems need to be solved. It should relate the theory to a practical situation; for example, apply the ideas and knowledge discussed in the coursework to the practical situation at hand in the case study.\n\nIdentify the problems.\n\nSelect the major problems in the case.\n\nSuggest solutions to these major problems.\n\nRecommend the best solution to be implemented.\n\nDetail how this solution should be implemented.\n\nThere are usually eight sections in a case study:\n\n1. Synopsis/Executive Summary\n\nOutline the purpose of the case study.\n\nDescribe the field of research.\n\nOutline the issues and findings of the case study without specific details.\n\nIdentify the theory that will be used.\n\nNote any assumptions made (you may not have all the information you\'d like so some assumptions may be necessary e.g.: ""It has been assumed thatâ€¦"", ""Assuming that it takes half an hour to read one documentâ€¦"").\n\n2. Findings\n\nIdentify the problems found in the case by:\n\nanalysing the problem, supporting your findings with facts given in the case, the relevant theory and course concepts.\n\nsearching for the underlying problems\n\nThis section is often divided into sub-sections.\n\n3. Discussion\n\nSummarise the major problem/s.\n\nIdentify alternative solutions to this/these major problem/s.\n\nBriefly outline each alternative solution and evaluate its advantages and disadvantages.\n\nThere is no need to refer to theory or coursework here.\n\n4. Conclusion\n\nSum up the main points from the findings and discussion.\n\n5. Recommendations\n\nChoose which of the alternative solutions should be adopted.\n\nBriefly justify your choice explaining how it will solve the major problem/s.\n\nThis should be written in a forceful style as this section is intended to be persuasive.\n\nHere integration of theory and coursework is appropriate.\n\n6. Implementation\n\nExplain what should be done, by whom and by when.\n\nIf appropriate include a rough estimate of costs (both financial and time).\n\n7. References\n\nMake sure all references are cited correctly.\n\n8. Appendices (if any)\n\nAttach any original data that relates to the study but which would have interrupted the flow of the main body\n\n&amp;#x200B;\n\nWriting a critical review\n\nCritical reviews require careful planning and drafting just like any other assignment. This guide suggests what to focus on at each stage of the process.\n\n&amp;#x200B;\n\n1. Plan your approach\n\nRead your assignment instructions carefully in order to:\n\n&amp;#x200B;\n\ndetermine your exact criteria;\n\nfind out what proportion of your review you should dedicate to summary and evaluation; and\n\nknow whether the summary and evaluation should be presented as separate sections or a combined section.\n\n2. Make notes\n\nSkim read the text and make notes about:\n\n&amp;#x200B;\n\nthe main question or questions;\n\nthe authorâ€™s aim;\n\nthe methods used;\n\nthe evidence provided;\n\nthe key findings or answers; and\n\nthe implications and significance of the findings.\n\n3. Evaluate the text\n\nJudge the quality or value of the text (for other researchers, or to practitioners in the field, or to students).\n\nConsider the merits of the text in the short term as well as the long term.\n\nConsider the merits of the text in comparison to other related text.\n\nWhen evaluating the text you could answer some of the following questions:\n\n&amp;#x200B;\n\nIs the question the text tries to answer relevant, interesting, new, or useful? To who, and why?\n\nDoes the text give new answers or interpretations to an old question?\n\nIs the text detailed, or brief? Simple or complex?\n\nIs the evidence presented to support the answer extensive? Strong? Weak? Relevant? Persuasive? Contradictory?\n\nAre the conclusions reached final, limited, qualified or preliminary?\n\n4. Write it up\n\nWhen writing and proofreading your critical review:\n\n&amp;#x200B;\n\nStay focused on your evaluation criteria.\n\nRead the text you are reviewing again to check that you have covered everything.\n\n&amp;#x200B;\n\n \n\nEditing and proofreading your work\n\nRefining your own work is an essential skill, and an excellent way to continually improve your writing.\n\nThis guide covers the differences between editing and proofreading and provides checklists you can use to review your work before submission.\n\nWhat is the difference?\n\nEditing focuses on improving the \'big picture\' of your assignment. It is how you ensure you have fully addressed the task requirements, and involves making structural changes to your writing and checking the logic and flow.\n\nProofreading focuses on specific details like spelling, sentence structure, and referencing.\n\nStep 1: Reread your instructions, question and rubric, so you can approach the task with clarity about your aims and purpose.\n\nStep 2: Use the checklists below as a starting point to refine your work.\n\nTip: To spot errors more easily, read your text aloud, and take breaks between writing, editing, and proofreading.\n\nEditing\n\nStructural aspects\n\nThe introduction clearly states the topic and how it will be covered.\n\nParagraphs have clear topic sentences and present information in a logical order.\n\nThe conclusion sums up the main points and has a takeaway message.\n\nTopic coverage\n\nAll aspects of the question are answered.\n\nAll key terms and concepts are defined.\n\nEvery point on the rubric is fully addressed.\n\nAnalysis and argument\n\nThe analysis presents an evaluation (not just description).\n\nThe argument is supported by sufficient evidence and a range of sources.\n\nQuoting, paraphrasing\n\nQuotations are applied to your specific context, and their significance is clearly discussed.\n\nParaphrased content retains the same meaning as the original.\n\nProofreading\n\nFormatting\n\nCheck for consistency of:\n\nHeading levels\n\nDiagrams and tables\n\nMargins and indentation\n\nFootnotes and block quotes (if used)\n\nReferencing, citations\n\nAll quotes and paraphrases are cited.\n\nAll sources are in the reference list.\n\nAll reference details are complete.\n\nAll in-text citations and reference list entries are in the required style.\n\nLanguage use\n\nSentences are complete and separated by appropriate punctuation.\n\nSpelling is accurate and consistent, in Australian English.\n\nThe academic tone used throughout (formal, objective, impersonal, concise and precise).\n\nPersonal checklist\n\nAdd your own items to each of these checklists based on the feedback you have received in the past.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nA guide to oral presentations\n\nThe ability to undertake an oral presentation is a valuable skill for assessment tasks, interviews and your future career. This skill can be developed by everyone and is not reserved for those who are ""naturally"" confident at public speaking. This guide will provide you with some tips and techniques for ensuring your presentation is well planned, structured and delivered.\n\n1.\xa0\xa0Plan\n\nAnalyse your audience\n\nSome questions to consider include:\n\nWho is your audience?\n\nWhat do they know about the subject? What terminology will they know?\n\nWhat do they want to know?\n\nHow can you engage this audience? What matters to them?\n\nDetermine the purpose\n\nThe purpose of a presentation may be to provide information, persuade the audience to accept a point of view, or encourage them to take action. Knowing your purpose will help you decide what to include and how to structure your presentation.\n\nSelect effective information\n\nWhat kind of information will best support the presentation?\n\nWhat kind of information will appeal to the audience?\n\nAre there some useful examples or case studies to illustrate an idea?\n\n2.\xa0\xa0Prepare\n\nThe structure provides a framework for your presentation.\n\nIntroduction - an overview of the issue and the main ideas to be considered. Explain the issue, the background and key terms.\n\nBody\xa0- the main ideas, reasoning, evidence and explanation provided. Avoid overloading your audience with too much information. Categorise your information into key ideas.\n\nConclusion - a summary of what you have considered with repetition of key ideas.\n\nConsider how long you can spend on each section, given the time available.\n\nSelect appropriate visual aids\n\nRemember that the visuals are not the presentation. Their purpose is to enhance what you are saying by providing a visual link.\n\n3.\xa0\xa0Practise and present\n\nThe key to a good delivery is to practise your speech and your body language. Here are some tips to assist you:\n\nPractise your presentation several times, aloud and standing up.\n\nTime the presentation. If it is too long, remove and/or simplify information, rather than speaking more quickly.\n\nStand straight with your feet ""planted"" in the ground. This will eliminate swaying and nervous movements in the legs. You can move but do so with purpose.\n\nEstablish a ""resting place"" for your hands at the front of your body, such as cupped at waist level.\n\nEye contact is a powerful means to engage your audience so look at your audience when you speak.\n\nSpeak more slowly and clearly than you normally would. Provide emphasis through voice intonation, volume and pausing.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n \n\nPlanning your assignment\n\nAnalyse the topic\n\nWhat is the purpose of the task/topic?\n\nWhat is it asking?\n\nBrainstorm the topic\n\nWhat do I already know?\n\nWhat have I read?\n\nWhat ideas/questions do I want to research?\n\nWho are the key authors on my topic?\n\nPlan your time\n\nWork back from the due date to plan time for research, drafts and editing.\n\nTake time to account for other work due\n\nConstruct a rough plan\n\nWhat are my main points?\n\nWhat other ideas are worth including?\n\nPreliminary research\n\nRevise with lecture notes and prescribed or recommended textbooks.\n\nBegin researching\n\nKeep your topic in mind.\n\nFocus your research on relevant journal articles and texts.\n\nRecord your findings.\n\nRevise your plan\n\nConsider the relationships between your ideas.\n\nSelect\xa0key points.\n\nStart writing\n\nKeep your topic in mind when writing.\n\nFollow formatting requirements.\n\nCheck for grammar, coherence, flow.\n\nFollow relevant citation style.\n\nSubmit\n\nSubmit your assignment.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#learning #studying Quick Study Guides - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf1nhp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588863496.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Public Accessed: 07. May 2020  &lt;a href=""https://www.monash.edu/rlo/quick-study-guides""&gt;https://www.monash.edu/rlo/quick-study-guides&lt;/a&gt;  Monash University, RLO Study Skills, 2020&lt;/p&gt;\n\n&lt;p&gt;Assignment direction words&lt;/p&gt;\n\n&lt;p&gt;Below is an explanation of the keywords commonly used in assignment questions.&lt;/p&gt;\n\n&lt;p&gt;These explanations are intended as a guide only. There are not always hard distinctions between the words, and different academics may use them to mean slightly different things.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ANALYSE. Identify the elements of an argument, text, issue, process or event and show how they are related.ARGUE. Present the case for and/or against a particular proposition. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;COMMENT ON. Point out the important features, Criticise. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;COMPARE. Identify and explain similarities and differences. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;CONTRAST. Stress the differences between two or more things. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;CRITICISE. Judge the merit or truth of the factors or views mentioned, including both strengths and weaknesses.CRITICALLYâ€¦Used to add direction to another direction word (e.g. â€œcritically analyseâ€), this means approaching the task in a &lt;strong&gt;questioning&lt;/strong&gt; way: â€œHow does this work?â€ â€œWhy is it like this?â€ â€œWhat are its strengths and weaknesses?â€&lt;/p&gt;\n\n&lt;p&gt;DEFINE. Provide concise, clear, and authoritative meanings. Give the limits of the definition, but omit detailed explanations. Show how the item defined differs from items in other classes. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DESCRIBE. Recount, characterise, outline, and relate in sequence.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DIAGRAM. A drawing, chart, plan, or graph. Diagrams should be labelled and there should be an accompanying explanation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;DISCUSS. Examine, analyse carefully and give reasons for and against. Be complete and give details, usually with a view to assessing how satisfactory something is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EVALUATE. Appraise in relation to some standard, referring to advantages, limitations, and costs and benefits as Appropriate.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EXAMINE. Investigate critically appraises a subject in detail.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;EXPLAIN. Clarify, interpret and elaborate on the material presented. Give reasons for differences of opinion or results, and try to analyse causes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ILLUSTRATE. Use a concrete example, diagram, or figure to explain or clarify a problem.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;INDICATE. Identify, then focuses attention so as to clarify.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;JUSTIFY. Prove or give reasons for conclusions or decisions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;OUTLINE. Present the essential features, showing the main points and subordinate points. Omit minor details.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;REVIEW. Examine a subject critically, analysing and commenting on the important or controversial statements.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;STATE. Present the main points in a brief and clear sequence, usually omitting details or examples.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SUMMARISE. Give the main points or facts in condensed form. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Brainstorming: Mind mapping&lt;/p&gt;\n\n&lt;p&gt;Why mind map?&lt;/p&gt;\n\n&lt;p&gt;One effective form of brainstorming is mind mapping. A mind map is a visual representation of your ideas, consisting of words, images and colours, and can help you to:&lt;/p&gt;\n\n&lt;p&gt;focus on your research topic/question&lt;/p&gt;\n\n&lt;p&gt;structure and plan your assignment&lt;/p&gt;\n\n&lt;p&gt;combine one or more types of major thought relationships&lt;/p&gt;\n\n&lt;p&gt;identify relationships between ideas/concepts.&lt;/p&gt;\n\n&lt;p&gt;Stage 1&lt;/p&gt;\n\n&lt;p&gt;You can create a mind map on the paper, whiteboard or digitally, using visual mapping software such as &lt;a href=""http://freemind.sourceforge.net/wiki/index.php/Main_Page""&gt;FreeMind&lt;/a&gt;. To begin:&lt;/p&gt;\n\n&lt;p&gt;write your topic in the centre of a blank page&lt;/p&gt;\n\n&lt;p&gt;associate your ideas freely anywhere on the page and do not filter out ideas.&lt;/p&gt;\n\n&lt;p&gt;Stage 2&lt;/p&gt;\n\n&lt;p&gt;When you have run out of ideas:&lt;/p&gt;\n\n&lt;p&gt;consider each item and determine how this point is related to other points and to your topic&lt;/p&gt;\n\n&lt;p&gt;map relationships with lines, arrows, colours, images and bold type.&lt;/p&gt;\n\n&lt;p&gt;Stage 3&lt;/p&gt;\n\n&lt;p&gt;Use the relationships you have identified to reorganise your ideas.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Signposting&lt;/p&gt;\n\n&lt;p&gt;Why signpost?&lt;/p&gt;\n\n&lt;p&gt;Signposting helps you to:&lt;/p&gt;\n\n&lt;p&gt;create a navigation path to guide your reader,&lt;/p&gt;\n\n&lt;p&gt;indicate to your reader the direction your writing will take, and&lt;/p&gt;\n\n&lt;p&gt;remember your key points.&lt;/p&gt;\n\n&lt;p&gt;Major signposts&lt;/p&gt;\n\n&lt;p&gt;Major signposts indicate to the reader the important elements of your writing such as the purpose, connection between points, and the conclusion.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This study argues that â€¦&lt;/p&gt;\n\n&lt;p&gt;This paper examinesâ€¦&lt;/p&gt;\n\n&lt;p&gt;This essay begins byâ€¦ it goes on to â€¦&lt;/p&gt;\n\n&lt;p&gt;In conclusion â€¦&lt;/p&gt;\n\n&lt;p&gt;Transition sentences&lt;/p&gt;\n\n&lt;p&gt;Transition sentences inform the reader when your writing is moving from one idea to another, and how those ideas are connected.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Having examined; It is important to&lt;/p&gt;\n\n&lt;p&gt;The discussion highlights; It is also essential, however, to explore;&lt;/p&gt;\n\n&lt;p&gt;A significant factor inâ€¦ isâ€¦&lt;/p&gt;\n\n&lt;p&gt;Linking words&lt;/p&gt;\n\n&lt;p&gt;Linking words signify to the reader the connections between your ideas. Linking words convey what you will be discussing next, and how the reader should interpret it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some types of linking words&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples&lt;/p&gt;\n\n&lt;p&gt;Addition    in addition; also&lt;/p&gt;\n\n&lt;p&gt;Cause and effect    due to; as a result&lt;/p&gt;\n\n&lt;p&gt;Comparison  similarly; likewise&lt;/p&gt;\n\n&lt;p&gt;Contrast    alternatively; in contrast&lt;/p&gt;\n\n&lt;p&gt;Examples    for instance; to show this&lt;/p&gt;\n\n&lt;p&gt;Exception   however; although&lt;/p&gt;\n\n&lt;p&gt;Time and sequence   initially; to being; previously; finally&lt;/p&gt;\n\n&lt;p&gt;Reminders&lt;/p&gt;\n\n&lt;p&gt;Reminders can help the reader recall what you have previously discussed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As discussed aboveâ€¦&lt;/p&gt;\n\n&lt;p&gt;Section 2 outlinesâ€¦&lt;/p&gt;\n\n&lt;p&gt;So farâ€¦&lt;/p&gt;\n\n&lt;p&gt;In conclusionâ€¦&lt;/p&gt;\n\n&lt;p&gt;In briefâ€¦&lt;/p&gt;\n\n&lt;p&gt;Thusâ€¦&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing essays&lt;/p&gt;\n\n&lt;p&gt;Analysing essay topics&lt;/p&gt;\n\n&lt;p&gt;Understand the essay question&lt;/p&gt;\n\n&lt;p&gt;Make sure you know the precise meaning of every word in the essay question. Use:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;a) your general dictionary for unfamiliar words such as intrinsic, core values&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;b) a subject-specific dictionary, for example, the APA Dictionary of Psychology, for academic words such as proof, random sample, significance level&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Think about the different parts of the question&lt;/p&gt;\n\n&lt;p&gt;Decide how many parts the question has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;List areas you will probably have to research. It may help to write yourself a brief task description: â€œFirst find out what a market niche is, then see what significance this has for marketing. Nextâ€¦â€&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note any direction words&lt;/p&gt;\n\n&lt;p&gt;These might include:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;discuss, discuss critically, discuss the importance of, assess, justify, evaluate, analyse.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The structure of an essay&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;About 10% of the total length. May be one paragraph or several, depending on essay length.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Introduce the topic.&lt;/p&gt;\n\n&lt;p&gt;Provide background information.&lt;/p&gt;\n\n&lt;p&gt;Limit the scope of the discussion.&lt;/p&gt;\n\n&lt;p&gt;Define or state the topic.&lt;/p&gt;\n\n&lt;p&gt;Present the plan of coverage including your viewpoint and line of reasoning.&lt;/p&gt;\n\n&lt;p&gt;Move from general background information to your specific topic. You can set your own agenda to avoid too broad a focus.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Body&lt;/p&gt;\n\n&lt;p&gt;This is a series of linked paragraphs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each paragraph should have one main point.&lt;/p&gt;\n\n&lt;p&gt;The topic sentence of each paragraph carries the theme or argument.&lt;/p&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Here you are moving from the specifics of your essay to the more general background of the topic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sum up your argument and information with reference to the essay question.&lt;/p&gt;\n\n&lt;p&gt;Perhaps mention wider implications or future directions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Features of a critical review&lt;/p&gt;\n\n&lt;p&gt;A critical review requires you to evaluate an academic text and make your own judgement about it based on various criteria.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This guide points out the common features of critical reviews. Always check with your lecturer to confirm the exact requirements of your assignment.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Critical reviews can present positive and/or negative judgements.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Critical reviews involve two main tasks:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;summary&lt;/p&gt;\n\n&lt;p&gt;evaluation.&lt;/p&gt;\n\n&lt;p&gt;The evaluation criteria can vary depending on the discipline.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The aim of a critical review is to evaluate the text. This means you must be very familiar with the text, and your writing needs to clearly present your judgement.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;Usually looks like an entry in a bibliography, and includes full bibliographic details of the text.&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;Gives an overview of the text including the importance of the topic or question.&lt;/p&gt;\n\n&lt;p&gt;Briefly states your evaluation of the merits of the text.&lt;/p&gt;\n\n&lt;p&gt;Outlines your reviewâ€™s approach and structure.&lt;/p&gt;\n\n&lt;p&gt;Summary (may be combined with evaluation)&lt;/p&gt;\n\n&lt;p&gt;Describes the key points from the text, including the authorâ€™s intentions and findings.&lt;/p&gt;\n\n&lt;p&gt;Evaluation (may be combined with summary)&lt;/p&gt;\n\n&lt;p&gt;Presents strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;Focuses on the evaluation criteria to present your judgement of the text.&lt;/p&gt;\n\n&lt;p&gt;Conclusion&lt;/p&gt;\n\n&lt;p&gt;Usually quite short, so can be included at the end of the evaluation or as a separate section.&lt;/p&gt;\n\n&lt;p&gt;Restates your overall evaluation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Features of reports&lt;/p&gt;\n\n&lt;p&gt;Referencing and quotations in reports follow the same guidelines required for essays.&lt;/p&gt;\n\n&lt;p&gt;A system of\xa0numbered sections with headings is typically used.&lt;/p&gt;\n\n&lt;p&gt;Just as in the essay, a good report will describe, analyse and evaluate a problem or issue. Unlike an essay, it will describe the method used to investigate the problem and formulate a set of recommendations based on the findings of the report.&lt;/p&gt;\n\n&lt;p&gt;Report&lt;/p&gt;\n\n&lt;p&gt;VS&lt;/p&gt;\n\n&lt;p&gt;Essay&lt;/p&gt;\n\n&lt;p&gt;Often a problem or case study which sets up a hypothetical situation&lt;/p&gt;\n\n&lt;p&gt;Based on the reading, fieldwork or practical work&lt;/p&gt;\n\n&lt;p&gt;Topic&lt;/p&gt;\n\n&lt;p&gt;Responds to a question or a proposition&lt;/p&gt;\n\n&lt;p&gt;Is based on research&lt;/p&gt;\n\n&lt;p&gt;To investigate, analyse and present information&lt;/p&gt;\n\n&lt;p&gt;Usually to make recommendations to solve a problem&lt;/p&gt;\n\n&lt;p&gt;Purpose&lt;/p&gt;\n\n&lt;p&gt;To articulate a well-argued response to a question or proposition&lt;/p&gt;\n\n&lt;p&gt;Established in the topic and is often a client or manager&lt;/p&gt;\n\n&lt;p&gt;Audience&lt;/p&gt;\n\n&lt;p&gt;An academic audience&lt;/p&gt;\n\n&lt;p&gt;Contains an executive summary or abstract&lt;/p&gt;\n\n&lt;p&gt;Comprises sections with headings&lt;/p&gt;\n\n&lt;p&gt;May use bullet points, tables, graphs to convey information&lt;/p&gt;\n\n&lt;p&gt;Format&lt;/p&gt;\n\n&lt;p&gt;Does not typically include sections or headings&lt;/p&gt;\n\n&lt;p&gt;Does not typically include use bullet points, tables, graphs&lt;/p&gt;\n\n&lt;p&gt;Third-person&lt;/p&gt;\n\n&lt;p&gt;Formal language&lt;/p&gt;\n\n&lt;p&gt;Style&lt;/p&gt;\n\n&lt;p&gt;Third or first person&lt;/p&gt;\n\n&lt;p&gt;Formal language&lt;/p&gt;\n\n&lt;p&gt;Success depends on:&lt;/p&gt;\n\n&lt;p&gt;the demonstration of good research skills&lt;/p&gt;\n\n&lt;p&gt;the quality of the recommendations to respond to an issue&lt;/p&gt;\n\n&lt;p&gt;the presentation and analysis of relevant information&lt;/p&gt;\n\n&lt;p&gt;Assessment&lt;/p&gt;\n\n&lt;p&gt;Success depends on:&lt;/p&gt;\n\n&lt;p&gt;the demonstration of good research skills&lt;/p&gt;\n\n&lt;p&gt;the identification of a cogent argument&lt;/p&gt;\n\n&lt;p&gt;the quality of reasoning and evidence&lt;/p&gt;\n\n&lt;p&gt;how well it analyses and evaluates the issue&lt;/p&gt;\n\n&lt;p&gt;Different types of reports typically include different sections.&lt;br/&gt;\nFor the requirements for reports in Business and Economics, see the Q Manual. For all other disciplines, look at the Faculty examples in\xa0&lt;a href=""https://www.monash.edu/rlo/assignment-samples""&gt;Assignment Structures and Samples&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing a case study&lt;/p&gt;\n\n&lt;p&gt;There are two different approaches to case studies. This guide focuses on the problem-oriented method. Always check with your lecturer to confirm if this is the type required.&lt;/p&gt;\n\n&lt;p&gt;A successful case study analyses a real-life situation where existing problems need to be solved. It should relate the theory to a practical situation; for example, apply the ideas and knowledge discussed in the coursework to the practical situation at hand in the case study.&lt;/p&gt;\n\n&lt;p&gt;Identify the problems.&lt;/p&gt;\n\n&lt;p&gt;Select the major problems in the case.&lt;/p&gt;\n\n&lt;p&gt;Suggest solutions to these major problems.&lt;/p&gt;\n\n&lt;p&gt;Recommend the best solution to be implemented.&lt;/p&gt;\n\n&lt;p&gt;Detail how this solution should be implemented.&lt;/p&gt;\n\n&lt;p&gt;There are usually eight sections in a case study:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Synopsis/Executive Summary&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Outline the purpose of the case study.&lt;/p&gt;\n\n&lt;p&gt;Describe the field of research.&lt;/p&gt;\n\n&lt;p&gt;Outline the issues and findings of the case study without specific details.&lt;/p&gt;\n\n&lt;p&gt;Identify the theory that will be used.&lt;/p&gt;\n\n&lt;p&gt;Note any assumptions made (you may not have all the information you&amp;#39;d like so some assumptions may be necessary e.g.: &amp;quot;It has been assumed thatâ€¦&amp;quot;, &amp;quot;Assuming that it takes half an hour to read one documentâ€¦&amp;quot;).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Findings&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Identify the problems found in the case by:&lt;/p&gt;\n\n&lt;p&gt;analysing the problem, supporting your findings with facts given in the case, the relevant theory and course concepts.&lt;/p&gt;\n\n&lt;p&gt;searching for the underlying problems&lt;/p&gt;\n\n&lt;p&gt;This section is often divided into sub-sections.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Discussion&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Summarise the major problem/s.&lt;/p&gt;\n\n&lt;p&gt;Identify alternative solutions to this/these major problem/s.&lt;/p&gt;\n\n&lt;p&gt;Briefly outline each alternative solution and evaluate its advantages and disadvantages.&lt;/p&gt;\n\n&lt;p&gt;There is no need to refer to theory or coursework here.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Conclusion&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Sum up the main points from the findings and discussion.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Recommendations&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Choose which of the alternative solutions should be adopted.&lt;/p&gt;\n\n&lt;p&gt;Briefly justify your choice explaining how it will solve the major problem/s.&lt;/p&gt;\n\n&lt;p&gt;This should be written in a forceful style as this section is intended to be persuasive.&lt;/p&gt;\n\n&lt;p&gt;Here integration of theory and coursework is appropriate.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Implementation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Explain what should be done, by whom and by when.&lt;/p&gt;\n\n&lt;p&gt;If appropriate include a rough estimate of costs (both financial and time).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;References&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Make sure all references are cited correctly.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Appendices (if any)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Attach any original data that relates to the study but which would have interrupted the flow of the main body&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Writing a critical review&lt;/p&gt;\n\n&lt;p&gt;Critical reviews require careful planning and drafting just like any other assignment. This guide suggests what to focus on at each stage of the process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Plan your approach&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Read your assignment instructions carefully in order to:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;determine your exact criteria;&lt;/p&gt;\n\n&lt;p&gt;find out what proportion of your review you should dedicate to summary and evaluation; and&lt;/p&gt;\n\n&lt;p&gt;know whether the summary and evaluation should be presented as separate sections or a combined section.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Make notes&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Skim read the text and make notes about:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;the main question or questions;&lt;/p&gt;\n\n&lt;p&gt;the authorâ€™s aim;&lt;/p&gt;\n\n&lt;p&gt;the methods used;&lt;/p&gt;\n\n&lt;p&gt;the evidence provided;&lt;/p&gt;\n\n&lt;p&gt;the key findings or answers; and&lt;/p&gt;\n\n&lt;p&gt;the implications and significance of the findings.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Evaluate the text&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Judge the quality or value of the text (for other researchers, or to practitioners in the field, or to students).&lt;/p&gt;\n\n&lt;p&gt;Consider the merits of the text in the short term as well as the long term.&lt;/p&gt;\n\n&lt;p&gt;Consider the merits of the text in comparison to other related text.&lt;/p&gt;\n\n&lt;p&gt;When evaluating the text you could answer some of the following questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is the question the text tries to answer relevant, interesting, new, or useful? To who, and why?&lt;/p&gt;\n\n&lt;p&gt;Does the text give new answers or interpretations to an old question?&lt;/p&gt;\n\n&lt;p&gt;Is the text detailed, or brief? Simple or complex?&lt;/p&gt;\n\n&lt;p&gt;Is the evidence presented to support the answer extensive? Strong? Weak? Relevant? Persuasive? Contradictory?&lt;/p&gt;\n\n&lt;p&gt;Are the conclusions reached final, limited, qualified or preliminary?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write it up&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;When writing and proofreading your critical review:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Stay focused on your evaluation criteria.&lt;/p&gt;\n\n&lt;p&gt;Read the text you are reviewing again to check that you have covered everything.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Editing and proofreading your work&lt;/p&gt;\n\n&lt;p&gt;Refining your own work is an essential skill, and an excellent way to continually improve your writing.&lt;/p&gt;\n\n&lt;p&gt;This guide covers the differences between editing and proofreading and provides checklists you can use to review your work before submission.&lt;/p&gt;\n\n&lt;p&gt;What is the difference?&lt;/p&gt;\n\n&lt;p&gt;Editing focuses on improving the &amp;#39;big picture&amp;#39; of your assignment. It is how you ensure you have fully addressed the task requirements, and involves making structural changes to your writing and checking the logic and flow.&lt;/p&gt;\n\n&lt;p&gt;Proofreading focuses on specific details like spelling, sentence structure, and referencing.&lt;/p&gt;\n\n&lt;p&gt;Step 1: Reread your instructions, question and rubric, so you can approach the task with clarity about your aims and purpose.&lt;/p&gt;\n\n&lt;p&gt;Step 2: Use the checklists below as a starting point to refine your work.&lt;/p&gt;\n\n&lt;p&gt;Tip: To spot errors more easily, read your text aloud, and take breaks between writing, editing, and proofreading.&lt;/p&gt;\n\n&lt;p&gt;Editing&lt;/p&gt;\n\n&lt;p&gt;Structural aspects&lt;/p&gt;\n\n&lt;p&gt;The introduction clearly states the topic and how it will be covered.&lt;/p&gt;\n\n&lt;p&gt;Paragraphs have clear topic sentences and present information in a logical order.&lt;/p&gt;\n\n&lt;p&gt;The conclusion sums up the main points and has a takeaway message.&lt;/p&gt;\n\n&lt;p&gt;Topic coverage&lt;/p&gt;\n\n&lt;p&gt;All aspects of the question are answered.&lt;/p&gt;\n\n&lt;p&gt;All key terms and concepts are defined.&lt;/p&gt;\n\n&lt;p&gt;Every point on the rubric is fully addressed.&lt;/p&gt;\n\n&lt;p&gt;Analysis and argument&lt;/p&gt;\n\n&lt;p&gt;The analysis presents an evaluation (not just description).&lt;/p&gt;\n\n&lt;p&gt;The argument is supported by sufficient evidence and a range of sources.&lt;/p&gt;\n\n&lt;p&gt;Quoting, paraphrasing&lt;/p&gt;\n\n&lt;p&gt;Quotations are applied to your specific context, and their significance is clearly discussed.&lt;/p&gt;\n\n&lt;p&gt;Paraphrased content retains the same meaning as the original.&lt;/p&gt;\n\n&lt;p&gt;Proofreading&lt;/p&gt;\n\n&lt;p&gt;Formatting&lt;/p&gt;\n\n&lt;p&gt;Check for consistency of:&lt;/p&gt;\n\n&lt;p&gt;Heading levels&lt;/p&gt;\n\n&lt;p&gt;Diagrams and tables&lt;/p&gt;\n\n&lt;p&gt;Margins and indentation&lt;/p&gt;\n\n&lt;p&gt;Footnotes and block quotes (if used)&lt;/p&gt;\n\n&lt;p&gt;Referencing, citations&lt;/p&gt;\n\n&lt;p&gt;All quotes and paraphrases are cited.&lt;/p&gt;\n\n&lt;p&gt;All sources are in the reference list.&lt;/p&gt;\n\n&lt;p&gt;All reference details are complete.&lt;/p&gt;\n\n&lt;p&gt;All in-text citations and reference list entries are in the required style.&lt;/p&gt;\n\n&lt;p&gt;Language use&lt;/p&gt;\n\n&lt;p&gt;Sentences are complete and separated by appropriate punctuation.&lt;/p&gt;\n\n&lt;p&gt;Spelling is accurate and consistent, in Australian English.&lt;/p&gt;\n\n&lt;p&gt;The academic tone used throughout (formal, objective, impersonal, concise and precise).&lt;/p&gt;\n\n&lt;p&gt;Personal checklist&lt;/p&gt;\n\n&lt;p&gt;Add your own items to each of these checklists based on the feedback you have received in the past.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A guide to oral presentations&lt;/p&gt;\n\n&lt;p&gt;The ability to undertake an oral presentation is a valuable skill for assessment tasks, interviews and your future career. This skill can be developed by everyone and is not reserved for those who are &amp;quot;naturally&amp;quot; confident at public speaking. This guide will provide you with some tips and techniques for ensuring your presentation is well planned, structured and delivered.&lt;/p&gt;\n\n&lt;p&gt;1.\xa0\xa0Plan&lt;/p&gt;\n\n&lt;p&gt;Analyse your audience&lt;/p&gt;\n\n&lt;p&gt;Some questions to consider include:&lt;/p&gt;\n\n&lt;p&gt;Who is your audience?&lt;/p&gt;\n\n&lt;p&gt;What do they know about the subject? What terminology will they know?&lt;/p&gt;\n\n&lt;p&gt;What do they want to know?&lt;/p&gt;\n\n&lt;p&gt;How can you engage this audience? What matters to them?&lt;/p&gt;\n\n&lt;p&gt;Determine the purpose&lt;/p&gt;\n\n&lt;p&gt;The purpose of a presentation may be to provide information, persuade the audience to accept a point of view, or encourage them to take action. Knowing your purpose will help you decide what to include and how to structure your presentation.&lt;/p&gt;\n\n&lt;p&gt;Select effective information&lt;/p&gt;\n\n&lt;p&gt;What kind of information will best support the presentation?&lt;/p&gt;\n\n&lt;p&gt;What kind of information will appeal to the audience?&lt;/p&gt;\n\n&lt;p&gt;Are there some useful examples or case studies to illustrate an idea?&lt;/p&gt;\n\n&lt;p&gt;2.\xa0\xa0Prepare&lt;/p&gt;\n\n&lt;p&gt;The structure provides a framework for your presentation.&lt;/p&gt;\n\n&lt;p&gt;Introduction - an overview of the issue and the main ideas to be considered. Explain the issue, the background and key terms.&lt;/p&gt;\n\n&lt;p&gt;Body\xa0- the main ideas, reasoning, evidence and explanation provided. Avoid overloading your audience with too much information. Categorise your information into key ideas.&lt;/p&gt;\n\n&lt;p&gt;Conclusion - a summary of what you have considered with repetition of key ideas.&lt;/p&gt;\n\n&lt;p&gt;Consider how long you can spend on each section, given the time available.&lt;/p&gt;\n\n&lt;p&gt;Select appropriate visual aids&lt;/p&gt;\n\n&lt;p&gt;Remember that the visuals are not the presentation. Their purpose is to enhance what you are saying by providing a visual link.&lt;/p&gt;\n\n&lt;p&gt;3.\xa0\xa0Practise and present&lt;/p&gt;\n\n&lt;p&gt;The key to a good delivery is to practise your speech and your body language. Here are some tips to assist you:&lt;/p&gt;\n\n&lt;p&gt;Practise your presentation several times, aloud and standing up.&lt;/p&gt;\n\n&lt;p&gt;Time the presentation. If it is too long, remove and/or simplify information, rather than speaking more quickly.&lt;/p&gt;\n\n&lt;p&gt;Stand straight with your feet &amp;quot;planted&amp;quot; in the ground. This will eliminate swaying and nervous movements in the legs. You can move but do so with purpose.&lt;/p&gt;\n\n&lt;p&gt;Establish a &amp;quot;resting place&amp;quot; for your hands at the front of your body, such as cupped at waist level.&lt;/p&gt;\n\n&lt;p&gt;Eye contact is a powerful means to engage your audience so look at your audience when you speak.&lt;/p&gt;\n\n&lt;p&gt;Speak more slowly and clearly than you normally would. Provide emphasis through voice intonation, volume and pausing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Planning your assignment&lt;/p&gt;\n\n&lt;p&gt;Analyse the topic&lt;/p&gt;\n\n&lt;p&gt;What is the purpose of the task/topic?&lt;/p&gt;\n\n&lt;p&gt;What is it asking?&lt;/p&gt;\n\n&lt;p&gt;Brainstorm the topic&lt;/p&gt;\n\n&lt;p&gt;What do I already know?&lt;/p&gt;\n\n&lt;p&gt;What have I read?&lt;/p&gt;\n\n&lt;p&gt;What ideas/questions do I want to research?&lt;/p&gt;\n\n&lt;p&gt;Who are the key authors on my topic?&lt;/p&gt;\n\n&lt;p&gt;Plan your time&lt;/p&gt;\n\n&lt;p&gt;Work back from the due date to plan time for research, drafts and editing.&lt;/p&gt;\n\n&lt;p&gt;Take time to account for other work due&lt;/p&gt;\n\n&lt;p&gt;Construct a rough plan&lt;/p&gt;\n\n&lt;p&gt;What are my main points?&lt;/p&gt;\n\n&lt;p&gt;What other ideas are worth including?&lt;/p&gt;\n\n&lt;p&gt;Preliminary research&lt;/p&gt;\n\n&lt;p&gt;Revise with lecture notes and prescribed or recommended textbooks.&lt;/p&gt;\n\n&lt;p&gt;Begin researching&lt;/p&gt;\n\n&lt;p&gt;Keep your topic in mind.&lt;/p&gt;\n\n&lt;p&gt;Focus your research on relevant journal articles and texts.&lt;/p&gt;\n\n&lt;p&gt;Record your findings.&lt;/p&gt;\n\n&lt;p&gt;Revise your plan&lt;/p&gt;\n\n&lt;p&gt;Consider the relationships between your ideas.&lt;/p&gt;\n\n&lt;p&gt;Select\xa0key points.&lt;/p&gt;\n\n&lt;p&gt;Start writing&lt;/p&gt;\n\n&lt;p&gt;Keep your topic in mind when writing.&lt;/p&gt;\n\n&lt;p&gt;Follow formatting requirements.&lt;/p&gt;\n\n&lt;p&gt;Check for grammar, coherence, flow.&lt;/p&gt;\n\n&lt;p&gt;Follow relevant citation style.&lt;/p&gt;\n\n&lt;p&gt;Submit&lt;/p&gt;\n\n&lt;p&gt;Submit your assignment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf1nhp', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf1nhp/learning_studying_quick_study_guides_a_university/', 'subreddit_subscribers': 33, 'created_utc': 1588834696.0, 'num_crossposts': 30, 'media': None, 'is_video': False}]",t3_gf1nhp,,
,learnmachinelearning,"How do people find the github implementation of different models?

How do you even know some approach exists?

Are people finding new approaches through searching for papers? How do you find those papers?",t2_b3oz3,False,,0,False,How do you find the right method for a particular application?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg3f3s,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589006552.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How do people find the github implementation of different models?&lt;/p&gt;

&lt;p&gt;How do you even know some approach exists?&lt;/p&gt;

&lt;p&gt;Are people finding new approaches through searching for papers? How do you find those papers?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3f3s,True,,sinefine,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3f3s/how_do_you_find_the_right_method_for_a_particular/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3f3s/how_do_you_find_the_right_method_for_a_particular/,155203,1588977752.0,0,,False,,,,
,learnmachinelearning,"I'm working my way through Andrew Ng's ML class, but I have a question regarding theta in linear regression. From what I understand, the theta value is the weight associated with the X values in a machine learning problem, which are often set arbitrarily when the model is created. The purpose is to find the best theta value that will produce the best result on unknown data, right? Once a model is created and we test it, how do we find the actual theta value? For example, the model below is using the body mass index to predict whether someone is obese. A simple example, but how can we determine the theta value? 

&amp;#x200B;

    X = df['bmi']
    y = ['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
    
    reg = LinearRegression()
    reg.fit(X_train, y_train)
    y_pred = reg.predict(y_test)

&amp;#x200B;

From here how can we find the theta value? Is it just under the hood, or is there a way to find the actual theta value? I might be misunderstanding what exactly theta is.",t2_3pnizflv,False,,0,False,How to get the theta value for Linear Regression?,[],r/learnmachinelearning,False,6,,0,,False,t3_gg3bu5,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1589006244.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working my way through Andrew Ng&amp;#39;s ML class, but I have a question regarding theta in linear regression. From what I understand, the theta value is the weight associated with the X values in a machine learning problem, which are often set arbitrarily when the model is created. The purpose is to find the best theta value that will produce the best result on unknown data, right? Once a model is created and we test it, how do we find the actual theta value? For example, the model below is using the body mass index to predict whether someone is obese. A simple example, but how can we determine the theta value? &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X = df[&amp;#39;bmi&amp;#39;]
y = [&amp;#39;target&amp;#39;]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

reg = LinearRegression()
reg.fit(X_train, y_train)
y_pred = reg.predict(y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;From here how can we find the theta value? Is it just under the hood, or is there a way to find the actual theta value? I might be misunderstanding what exactly theta is.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg3bu5,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg3bu5/how_to_get_the_theta_value_for_linear_regression/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg3bu5/how_to_get_the_theta_value_for_linear_regression/,155203,1588977444.0,0,,False,,,,
,learnmachinelearning,"Greetings,

I'm working on uploading a few machine learning projects to GitHub as a way to build a portfolio of my skills. Recently I completed a machine learning project predicting heart disease; however, a few sample projects on Github look very similar to the one I worked on independently. Is this a strike against my project? I don't want someone to look at my project and think I just copied the one on GitHub. Is this a common issue with ML projects? Granted, my project isn't new, but it was a topic that's hits close to home, so I felt compelled to study it. Should I still add this project to my portfolio on GutHub?",t2_3pnizflv,False,,0,False,Project on Github similar to one I completed. Should I still use my project as a professional example?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfyyom,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588992375.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Greetings,&lt;/p&gt;

&lt;p&gt;I&amp;#39;m working on uploading a few machine learning projects to GitHub as a way to build a portfolio of my skills. Recently I completed a machine learning project predicting heart disease; however, a few sample projects on Github look very similar to the one I worked on independently. Is this a strike against my project? I don&amp;#39;t want someone to look at my project and think I just copied the one on GitHub. Is this a common issue with ML projects? Granted, my project isn&amp;#39;t new, but it was a topic that&amp;#39;s hits close to home, so I felt compelled to study it. Should I still add this project to my portfolio on GutHub?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfyyom,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfyyom/project_on_github_similar_to_one_i_completed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfyyom/project_on_github_similar_to_one_i_completed/,155203,1588963575.0,0,,False,,,,
,learnmachinelearning,"Hi Folks, a few months ago I taught an intro to Python course for data science students (my first teaching experience). I wrote a little Jupyter magic command which posts code executed in that cell to a server where I could see, in real-time what students were submitting. Once everyone submitted marked exercises, I would share the results with the class so students could see how their classmates solved the same problem. We would have a short discussion about the various solutions.

I found this real-time feedback to be pretty productive and students seemed to have liked it as well. It kept students engaged, provided them with immediate feedback and helped them see that their classmates were struggling with same issues (and how to solve them).

Iâ€™ve worked on the app for the past few weeks and converted it to something people other than myself can use. Please keep in mind that this is very basic and I have spent very little time around user interface niceties. I still need to implement some important features, but will wait until I get some feedback. Please consider this a â€œbetaâ€ (perhaps even a functional â€œalphaâ€) version.

Please check it out. The main url is:
https://postcell.io/

Iâ€™ve written a basic help page at:
https://postcell.io/help.html

There is a tiny skeleton project at:
https://github.com/falconair/postcell_example

TLDR:
I'd love to get feedback from instructors on my jupyter extension at https://postcell.io",t2_rep0j,False,,0,False,My quarantine project for instructors who use Jupyter to teach: real-time feedback,[],r/learnmachinelearning,False,6,,0,,False,t3_gfyvzp,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588992137.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi Folks, a few months ago I taught an intro to Python course for data science students (my first teaching experience). I wrote a little Jupyter magic command which posts code executed in that cell to a server where I could see, in real-time what students were submitting. Once everyone submitted marked exercises, I would share the results with the class so students could see how their classmates solved the same problem. We would have a short discussion about the various solutions.&lt;/p&gt;

&lt;p&gt;I found this real-time feedback to be pretty productive and students seemed to have liked it as well. It kept students engaged, provided them with immediate feedback and helped them see that their classmates were struggling with same issues (and how to solve them).&lt;/p&gt;

&lt;p&gt;Iâ€™ve worked on the app for the past few weeks and converted it to something people other than myself can use. Please keep in mind that this is very basic and I have spent very little time around user interface niceties. I still need to implement some important features, but will wait until I get some feedback. Please consider this a â€œbetaâ€ (perhaps even a functional â€œalphaâ€) version.&lt;/p&gt;

&lt;p&gt;Please check it out. The main url is:
&lt;a href=""https://postcell.io/""&gt;https://postcell.io/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Iâ€™ve written a basic help page at:
&lt;a href=""https://postcell.io/help.html""&gt;https://postcell.io/help.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is a tiny skeleton project at:
&lt;a href=""https://github.com/falconair/postcell_example""&gt;https://github.com/falconair/postcell_example&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TLDR:
I&amp;#39;d love to get feedback from instructors on my jupyter extension at &lt;a href=""https://postcell.io""&gt;https://postcell.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?auto=webp&amp;s=8759f660f293b041e021b14bd931cb50f5b81785', 'width': 2143, 'height': 1576}, 'resolutions': [{'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f06824a4e8347aa01feb74b7348b4225755a68b9', 'width': 108, 'height': 79}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=efb1b865c3fc7e1453ee0f1a5e5d99b600ddfe1e', 'width': 216, 'height': 158}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3498879e4ef836de3e7c6205bf989a8d905a5142', 'width': 320, 'height': 235}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=35c171ce19ed33dcad0369d350f9d1790b2c38c4', 'width': 640, 'height': 470}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55b18cc3dbff4c4e4e926ff3beb4da252438a0fb', 'width': 960, 'height': 706}, {'url': 'https://external-preview.redd.it/qCxRQQKAP6l2wDTMApkUCBED0UUqXfSjWfo2uFAxoX8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50e0f7d240a3de3cbe824d0c098116474c57824f', 'width': 1080, 'height': 794}], 'variants': {}, 'id': 'sJA7EnzpmJkrkhC5jLhRyqgDTTwvpwOm7dSJLkkFPgE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfyvzp,True,,shahbazac,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfyvzp/my_quarantine_project_for_instructors_who_use/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfyvzp/my_quarantine_project_for_instructors_who_use/,155203,1588963337.0,0,,False,,,,
,learnmachinelearning,"Hello,

Do it unusual &amp; pandemic situations, I got let go of my last job as a UI Tools Engineer and landed a *temporary* job at an old company dealing with machine learning. Right now I'm improving their data preparation for machine learning, and various other things as it's a small company. This is my first time dealing with ML professionally; I got a BS in CS but as for ML I'm self taught.

My question is how many years of experience does one need in order to make a jump to machine learning as a career? Many job listings don't seem to specify years of experience exactly. Technically I only have 0.5 years of experience, yet I'm in the industry now so I can't tell if I would be able to make the jump to another company (any size) any time soon; I got this role because of connections.",t2_fwcx8,False,,0,False,Typical years of experience needed?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfykh7,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588991146.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;Do it unusual &amp;amp; pandemic situations, I got let go of my last job as a UI Tools Engineer and landed a &lt;em&gt;temporary&lt;/em&gt; job at an old company dealing with machine learning. Right now I&amp;#39;m improving their data preparation for machine learning, and various other things as it&amp;#39;s a small company. This is my first time dealing with ML professionally; I got a BS in CS but as for ML I&amp;#39;m self taught.&lt;/p&gt;

&lt;p&gt;My question is how many years of experience does one need in order to make a jump to machine learning as a career? Many job listings don&amp;#39;t seem to specify years of experience exactly. Technically I only have 0.5 years of experience, yet I&amp;#39;m in the industry now so I can&amp;#39;t tell if I would be able to make the jump to another company (any size) any time soon; I got this role because of connections.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfykh7,True,,TIL_this_shit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfykh7/typical_years_of_experience_needed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfykh7/typical_years_of_experience_needed/,155203,1588962346.0,0,,False,,,,
,learnmachinelearning,,t2_ipugc,False,,0,False,Becoming a full-stack data scientist,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfy33s,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/6G_zB9Uxd1-vfL9ZxYm-dyA9Q7jor8VRyW9tHtftVWM.jpg,False,,[],{},link,,False,,1588989679.0,text,6,,,text,medium.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?auto=webp&amp;s=b0f6b3d205fde07ec0665abeb23c5d444e88f77c', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aab802192379e5f1e9ad830693840133c038d273', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77c2844e7d41513083d8b9254057e2dffd84b477', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=827ecbf79d80f037ea7a87892a558e43ed060326', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9eca57892e195e21d82fb4e05dfdc4e8aad9514c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=413034b19f1f54d396eb310263cad5c6b6764601', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/f6rHhkBzwT9Nk0Kcv76OSvvmt-A0ShwpW_VJuCpSOkQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b04c8ac29c62fe283ce2d5ead0fc1e6a1b5a440', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'y4E11f4IhVqXKgt5Hmw5FvvjfZru1XjOaXMkeTrW9Po'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfy33s,True,,stolzen,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfy33s/becoming_a_fullstack_data_scientist/,all_ads,False,https://medium.com/data-science-insider/becoming-a-full-stack-data-scientist-d6514ee2e34a,155203,1588960879.0,0,,False,,,,
,learnmachinelearning,"I'm trying to work with IIC by [u/xuj1](https://www.reddit.com/u/xuj1/):

[Invariant Information Clustering for Unsupervised Image Classification and Segmentation](https://arxiv.org/abs/1807.06653)

I've tried 3 different GitHub repositories and none work.  The main one isn't runnable at all and is hard to follow:

xu-ji/IIC

This one runs but it's only showing losses and the accuracy isn't working :

RuABraun/phone-clustering

And this one after a bunch of tweaks I got to run but it doesn't get higher than 30% with a full VGG net after 200 iterations and stays random with a smaller net:

DuaneNielsen/iic

Trying all with MNIST.

Thanks to anyone that can help.",t2_5mz3he12,False,,0,False,Invariant Information Clustering - has anyone actually gotten this to work?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfxa4y,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588987182.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to work with IIC by &lt;a href=""https://www.reddit.com/u/xuj1/""&gt;u/xuj1&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://arxiv.org/abs/1807.06653""&gt;Invariant Information Clustering for Unsupervised Image Classification and Segmentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve tried 3 different GitHub repositories and none work.  The main one isn&amp;#39;t runnable at all and is hard to follow:&lt;/p&gt;

&lt;p&gt;xu-ji/IIC&lt;/p&gt;

&lt;p&gt;This one runs but it&amp;#39;s only showing losses and the accuracy isn&amp;#39;t working :&lt;/p&gt;

&lt;p&gt;RuABraun/phone-clustering&lt;/p&gt;

&lt;p&gt;And this one after a bunch of tweaks I got to run but it doesn&amp;#39;t get higher than 30% with a full VGG net after 200 iterations and stays random with a smaller net:&lt;/p&gt;

&lt;p&gt;DuaneNielsen/iic&lt;/p&gt;

&lt;p&gt;Trying all with MNIST.&lt;/p&gt;

&lt;p&gt;Thanks to anyone that can help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfxa4y,True,,mustgoplay,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfxa4y/invariant_information_clustering_has_anyone/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfxa4y/invariant_information_clustering_has_anyone/,155203,1588958382.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,Improve your #listening skills by anonymous - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gg17bc,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1588999275.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?auto=webp&amp;s=228f714a4a874584faa511221d802f251ab528c4', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49315ce2396d3bd5881f9d8ab90637e2b2287b77', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005821dd56c5a268ef920f17f78ffca32344a95f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2224c7f2357d31b2898207bd5706effe92ee46b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2e4b198bf81b99a9a8a17c62554e72f54649d45', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b0964a7af8ddbef28c2be4929b17c53c07f9621', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=489247a3390e8d864a3539cba508e0f347a03851', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'CpHs7vQlxKRbpx3n1C8dTgV4xAyoMHI4qj_xxMUYdFk'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gg17bc,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg17bc/improve_your_listening_skills_by_anonymous_a/,all_ads,False,/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/,155203,1588970475.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': ""Reference: Public Accessed: 07. May 2020  [http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo](http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo)  Monash University, RLO, Study Skills, 2020\n\n*Lectures can be overwhelming no matter what you're studying. There's so much content! Do you need to write everything down? Romany Manuell, a subject librarian at Monash, offers a few tips to help you with your listening and note-taking skills.*  \nLectures- with a chalkboard!  \n\n\nWe all wish we had a photographic memory - with an audio component - so we can capture everything our lecturer says... These days, that wish is a reality! Many lecturers at Monash Uni make use of Learning Capture to record lectures, and then make the content available on Moodle through your unit site. But whether you're attending the lecture in person, or reviewing the lecture via Learning Capture, listening just isn't enough. You'll remember much more if you adopt some of these approaches:\n\n**1. Prepare to listen with purpose**  \n\n\nA good way to prepare for lectures is to try to read relevant readings beforehand and come to the lecture with a series of questions youâ€™d like the answers to. Listen out for the answers, and youâ€™ll be listening with a purpose! You donâ€™t actually have to ask the questions out loud, but if they arenâ€™t answered during the lecture, look for opportunities to discuss your questions with your lecturer, tutor or fellow students.\n\n**2. Practice your handwriting**  \n\n\nYes, itâ€™s old school, but according to studies such as [this one](http://journals.sagepub.com/doi/abs/10.1177/0956797614524581), writing by hand can actually help you remember. Researchers believe thereâ€™s something about handwriting that helps you to reframe content in your own words. So leave that laptop at home (it might help you stay off Facebook tooâ€¦ gulp!).\n\n**3. Listen out for signalling words**\n\nYou may find that words such as *first, second, also, furthermore, moreover, therefore* and *finally* indicate stages in the lecturer's argument. Listen out for those words in order to grab the main points. There are more useful signalling words and other tips available on [Research and Learning Online](https://www.monash.edu/rlo/study-skills/learning-in-university-classes/listening-and-notetaking-in-lectures).\n\nAs you can see, listening and note-taking really work hand-in-hand. So if you need to brush up on your note-taking skills, watch the video:  [**https://youtu.be/XELOxGx\\_ZZg**](https://youtu.be/XELOxGx_ZZg)"", 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Improve your #listening skills by anonymous - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf0q4i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588858834.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Public Accessed: 07. May 2020  &lt;a href=""http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo""&gt;http://librarymonash.blogspot.com/2017/09/sorry-what-improve-your-listening-skills.html#.WgjZkluCypo&lt;/a&gt;  Monash University, RLO, Study Skills, 2020&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Lectures can be overwhelming no matter what you&amp;#39;re studying. There&amp;#39;s so much content! Do you need to write everything down? Romany Manuell, a subject librarian at Monash, offers a few tips to help you with your listening and note-taking skills.&lt;/em&gt;&lt;br/&gt;\nLectures- with a chalkboard!  &lt;/p&gt;\n\n&lt;p&gt;We all wish we had a photographic memory - with an audio component - so we can capture everything our lecturer says... These days, that wish is a reality! Many lecturers at Monash Uni make use of Learning Capture to record lectures, and then make the content available on Moodle through your unit site. But whether you&amp;#39;re attending the lecture in person, or reviewing the lecture via Learning Capture, listening just isn&amp;#39;t enough. You&amp;#39;ll remember much more if you adopt some of these approaches:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Prepare to listen with purpose&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;A good way to prepare for lectures is to try to read relevant readings beforehand and come to the lecture with a series of questions youâ€™d like the answers to. Listen out for the answers, and youâ€™ll be listening with a purpose! You donâ€™t actually have to ask the questions out loud, but if they arenâ€™t answered during the lecture, look for opportunities to discuss your questions with your lecturer, tutor or fellow students.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Practice your handwriting&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;Yes, itâ€™s old school, but according to studies such as &lt;a href=""http://journals.sagepub.com/doi/abs/10.1177/0956797614524581""&gt;this one&lt;/a&gt;, writing by hand can actually help you remember. Researchers believe thereâ€™s something about handwriting that helps you to reframe content in your own words. So leave that laptop at home (it might help you stay off Facebook tooâ€¦ gulp!).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Listen out for signalling words&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You may find that words such as &lt;em&gt;first, second, also, furthermore, moreover, therefore&lt;/em&gt; and &lt;em&gt;finally&lt;/em&gt; indicate stages in the lecturer&amp;#39;s argument. Listen out for those words in order to grab the main points. There are more useful signalling words and other tips available on &lt;a href=""https://www.monash.edu/rlo/study-skills/learning-in-university-classes/listening-and-notetaking-in-lectures""&gt;Research and Learning Online&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;As you can see, listening and note-taking really work hand-in-hand. So if you need to brush up on your note-taking skills, watch the video:  &lt;a href=""https://youtu.be/XELOxGx_ZZg""&gt;&lt;strong&gt;https://youtu.be/XELOxGx_ZZg&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?auto=webp&amp;s=228f714a4a874584faa511221d802f251ab528c4', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49315ce2396d3bd5881f9d8ab90637e2b2287b77', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=005821dd56c5a268ef920f17f78ffca32344a95f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2224c7f2357d31b2898207bd5706effe92ee46b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2e4b198bf81b99a9a8a17c62554e72f54649d45', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b0964a7af8ddbef28c2be4929b17c53c07f9621', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/oE8MUFbPL4Im3lf-wWdyaddPbB-rJDRqFVt_WTIY5fA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=489247a3390e8d864a3539cba508e0f347a03851', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'CpHs7vQlxKRbpx3n1C8dTgV4xAyoMHI4qj_xxMUYdFk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf0q4i', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf0q4i/improve_your_listening_skills_by_anonymous_a/', 'subreddit_subscribers': 33, 'created_utc': 1588830034.0, 'num_crossposts': 31, 'media': None, 'is_video': False}]",t3_gf0q4i,,
,learnmachinelearning,"Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups.

I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I'm unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I'm trying to see how this would be changed to go to 25 groups.

    model = Sequential()
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(100, 100, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))

I tried to change the input scale to input\_shape=(200, 200, 1) to match my input image size, and I also changed the Final Dense layer from 10 to 25, however, I am still getting 0% accuracy. Any advice is appreciated!

EDIT: My [model.fit](https://model.fit) is and model.compile is: 

    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',     
    metrics=['accuracy']) 
    
    model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, validation_data=(X_test, y_test))

&amp;#x200B;",t2_7ajy1,False,,0,False,Looking for advice on how to set up parameters for CNN (sequential),[],r/learnmachinelearning,False,6,,0,,False,t3_gg0m3x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588970659.0,,[],{},,,True,,1588997436.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups.&lt;/p&gt;

&lt;p&gt;I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I&amp;#39;m unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I&amp;#39;m trying to see how this would be changed to go to 25 groups.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (5, 5), activation=&amp;#39;relu&amp;#39;, input_shape=(100, 100, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(10, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried to change the input scale to input_shape=(200, 200, 1) to match my input image size, and I also changed the Final Dense layer from 10 to 25, however, I am still getting 0% accuracy. Any advice is appreciated!&lt;/p&gt;

&lt;p&gt;EDIT: My &lt;a href=""https://model.fit""&gt;model.fit&lt;/a&gt; is and model.compile is: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.compile(optimizer=&amp;#39;adam&amp;#39;,loss=&amp;#39;sparse_categorical_crossentropy&amp;#39;,     
metrics=[&amp;#39;accuracy&amp;#39;]) 

model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=1, validation_data=(X_test, y_test))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg0m3x,True,,MrMegaGamerz,,8,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg0m3x/looking_for_advice_on_how_to_set_up_parameters/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg0m3x/looking_for_advice_on_how_to_set_up_parameters/,155203,1588968636.0,0,,False,,,,
,learnmachinelearning,"I've been wanting to try making a model that would be able to tell if an object is of a specific class or not (for ex. Dog and non-dogs). When, it came to thinking of implementation, I wasn't particularly sure what would be the right way of going about this problem.  


If I were to think of it as a multi-class classification, the number of classes would be limited to the number of classes under non-dog objects. On the other hand if I'll limit it to one class, when I think of compiling everything into a single class for non-dog objects, the objects in question are to varied to be classified as such.  


Since essentially we're checking the likelihood if it's a Dog or not, it's only reliant on the class of the dog? So is it really just a neural network trained under a single class then??  


Am i overthinking this?",t2_6cw729z7,False,,0,False,A question about the implementation of object discrimination,[],r/learnmachinelearning,False,6,,0,,False,t3_gfwk0z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588984862.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve been wanting to try making a model that would be able to tell if an object is of a specific class or not (for ex. Dog and non-dogs). When, it came to thinking of implementation, I wasn&amp;#39;t particularly sure what would be the right way of going about this problem.  &lt;/p&gt;

&lt;p&gt;If I were to think of it as a multi-class classification, the number of classes would be limited to the number of classes under non-dog objects. On the other hand if I&amp;#39;ll limit it to one class, when I think of compiling everything into a single class for non-dog objects, the objects in question are to varied to be classified as such.  &lt;/p&gt;

&lt;p&gt;Since essentially we&amp;#39;re checking the likelihood if it&amp;#39;s a Dog or not, it&amp;#39;s only reliant on the class of the dog? So is it really just a neural network trained under a single class then??  &lt;/p&gt;

&lt;p&gt;Am i overthinking this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfwk0z,True,,iloveuandre3000,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfwk0z/a_question_about_the_implementation_of_object/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfwk0z/a_question_about_the_implementation_of_object/,155203,1588956062.0,0,,False,,,,
,learnmachinelearning,"Hi, everyone I have been trying to train a custom a custom object detector. So far the in the official documentation I have found how to use preexisting models to do object detection and online there is a convoluted way of doing it with the object detection API but there is no depth in the tutorials concerning the API  it and it doesn't as fa as I know work with tf2. Where can I find a good resource for object detection?",t2_4q4a2isq,False,,0,False,The documentation on Object Detection with TensorFlow is Confusing.,[],r/learnmachinelearning,False,6,,0,,False,t3_gfw7l7,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588983740.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, everyone I have been trying to train a custom a custom object detector. So far the in the official documentation I have found how to use preexisting models to do object detection and online there is a convoluted way of doing it with the object detection API but there is no depth in the tutorials concerning the API  it and it doesn&amp;#39;t as fa as I know work with tf2. Where can I find a good resource for object detection?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfw7l7,True,,hassankamran689,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfw7l7/the_documentation_on_object_detection_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfw7l7/the_documentation_on_object_detection_with/,155203,1588954940.0,0,,False,,,,
,learnmachinelearning,"Hi!

My project basically involves predicting the relative ratios of Reactions (Love 0.3, Haha 0.4 etc. summing to 1) for a Facebook post.

I'm confused as to what loss function to use - Categorical Cross Entropy or MSE loss. I originally trained all my models using MSE, but am wondering if Categorical Cross Entropy would be more appropriate, as it seems to deal directly with probability distributions. However in most articles online, it is only used for classification, and where labels are encoded as one-hot vectors (as opposed to a distribution). I'm therefore concerned as to what the implications of using Categorical Cross Entropy over MSE are, and whether there is a 'right' choice.

Hope what I'm asking is clear - thanks!",t2_6d2jn3fg,False,,0,False,Predicting Facebook Reaction Ratios - MSE or Categorical Cross Entropy,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfsdsk,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588970058.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi!&lt;/p&gt;

&lt;p&gt;My project basically involves predicting the relative ratios of Reactions (Love 0.3, Haha 0.4 etc. summing to 1) for a Facebook post.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m confused as to what loss function to use - Categorical Cross Entropy or MSE loss. I originally trained all my models using MSE, but am wondering if Categorical Cross Entropy would be more appropriate, as it seems to deal directly with probability distributions. However in most articles online, it is only used for classification, and where labels are encoded as one-hot vectors (as opposed to a distribution). I&amp;#39;m therefore concerned as to what the implications of using Categorical Cross Entropy over MSE are, and whether there is a &amp;#39;right&amp;#39; choice.&lt;/p&gt;

&lt;p&gt;Hope what I&amp;#39;m asking is clear - thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfsdsk,True,,sbh116,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfsdsk/predicting_facebook_reaction_ratios_mse_or/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfsdsk/predicting_facebook_reaction_ratios_mse_or/,155203,1588941258.0,0,,False,,,,
,learnmachinelearning,"We were learning recommender systems in class this week, and we got into distance measures like cosine similarity.  It reminded me a lot like our lectures on KNN where it uses Euclidean distance to find its neighbors. So are recommender systems just another form of KNN? From what we were taught, the whole concept of KNN is just a wrapper for different algorithms like KDtree and Balltree.",t2_5xavqe1z,False,,0,False,Difference between KNN and recommender systems?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfv5ka,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588980273.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;We were learning recommender systems in class this week, and we got into distance measures like cosine similarity.  It reminded me a lot like our lectures on KNN where it uses Euclidean distance to find its neighbors. So are recommender systems just another form of KNN? From what we were taught, the whole concept of KNN is just a wrapper for different algorithms like KDtree and Balltree.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfv5ka,True,,phi_beta_kappa,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfv5ka/difference_between_knn_and_recommender_systems/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfv5ka/difference_between_knn_and_recommender_systems/,155203,1588951473.0,0,,False,,,,
,learnmachinelearning," Hi,

I had gotten the yearly membership but I am not using it anymore. Let me know if anyone wants it.

Thanks",t2_10vapm,False,,0,False,Selling my Dataquest account valid till Jan 26th. $130,[],r/learnmachinelearning,False,6,,0,,False,t3_gg4q7j,False,dark,0.22,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1589011149.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I had gotten the yearly membership but I am not using it anymore. Let me know if anyone wants it.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gg4q7j,True,,stupidarg,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gg4q7j/selling_my_dataquest_account_valid_till_jan_26th/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gg4q7j/selling_my_dataquest_account_valid_till_jan_26th/,155203,1588982349.0,0,,False,,,,
,learnmachinelearning,,t2_42dg2rb1,False,,0,False,COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfuoh3,False,dark,0.33,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases | DS Meetup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Oc4NB1qpPZE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfuoh3', 'height': 338}",,False,0,,False,https://b.thumbs.redditmedia.com/LjdfqmcozyAh4jTjvBP3Z3klGC74i5UAtfO2jNVnubw.jpg,False,,[],{},rich:video,,False,,1588978652.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?auto=webp&amp;s=6b63e72ee7efc07aa876eeda3846e62d47e26d2f', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e216bd855eb0e80860fe1153b68cd557deb67ae', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e04f5b822bd9e3b28a4a61ffa201e2562fdda33', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/kEklpVI_quJw312Gv9oGVhFjhZ-0YdjI5fKVePWtq7w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413c1f5302f343054a35238209dcc3f319dbcac4', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'EMtppusbcMDi1FLQh6piI8f_JAU-PUB6KD_D9bnd1rs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfuoh3,True,,reddit_data_guy,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfuoh3/covid19_ultrasound_detection_impact_of/,all_ads,False,https://youtu.be/Oc4NB1qpPZE?t=1826,155203,1588949852.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'COVID-19 Ultrasound Detection &amp; Impact of Non-Pharmaceutical Interventions on Cases | DS Meetup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Oc4NB1qpPZE?start=1826&amp;feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Analytics Club at ETH', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Oc4NB1qpPZE/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCLk8ET7GNIYOTCuD1XFJjtA'}}",False,,,,
,learnmachinelearning,"You can google similar images by uploading an image, I want to replicate this technology for my own project. Maybe somebody knows where to find a course or an article on how to do that?  
Thanks in advance.",t2_1bodl8op,False,,0,False,ASK: Maybe anyone knows how to implement a search by an image algorithm?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfuaik,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588977160.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;You can google similar images by uploading an image, I want to replicate this technology for my own project. Maybe somebody knows where to find a course or an article on how to do that?&lt;br/&gt;
Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfuaik,True,,lekorotkov,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfuaik/ask_maybe_anyone_knows_how_to_implement_a_search/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfuaik/ask_maybe_anyone_knows_how_to_implement_a_search/,155203,1588948360.0,0,,False,,,,
,learnmachinelearning,,t2_5jtt9adc,False,,0,False,How to handle clients which can be pain sometimes,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfqjwq,False,light,0.75,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,default,False,,[],{},,,False,,1588961710.0,richtext,6,,,text,link.medium.com,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfqjwq,True,,kaputasf,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqjwq/how_to_handle_clients_which_can_be_pain_sometimes/,all_ads,False,https://link.medium.com/3BNLkotLj6,155203,1588932910.0,0,,False,,,,
,learnmachinelearning,"I dont like following video courses much and I also get bored reading long books. Any short book/online resource which is fairly practical, but also involves  coneptual understanding and some mathematics!

I am a beginner!",t2_5n3pjk3w,False,,0,False,Resource suggestions for beginner,[],r/learnmachinelearning,False,6,,0,,False,t3_gfwt5x,False,dark,0.25,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588985680.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I dont like following video courses much and I also get bored reading long books. Any short book/online resource which is fairly practical, but also involves  coneptual understanding and some mathematics!&lt;/p&gt;

&lt;p&gt;I am a beginner!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfwt5x,True,,utm99,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfwt5x/resource_suggestions_for_beginner/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfwt5x/resource_suggestions_for_beginner/,155203,1588956880.0,0,,False,,,,
,learnmachinelearning,"A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)
 
Haven't taken the GRE yet. 
GPA 9.36/10

So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)",t2_4ih25pre,False,,0,False,How do I go about learning TensorFlow?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfo7cu,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588950049.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;A little background: 
I am a Third Year CS undergrad considering to pursue MS CS in AI (Mostly the T10s and T20s)&lt;/p&gt;

&lt;p&gt;Haven&amp;#39;t taken the GRE yet. 
GPA 9.36/10&lt;/p&gt;

&lt;p&gt;So I want to do some cool projects which I could talk about in SOP. But I want to be thorough with TensorFlow. I have some knowledge. However, I wanna finesse TF along with opencv. Where do I begin with? Which projects to consider as a newbie in both (TF and CV)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfo7cu,True,,SuccMyStrangerThings,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo7cu/how_do_i_go_about_learning_tensorflow/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo7cu/how_do_i_go_about_learning_tensorflow/,155203,1588921249.0,0,,False,,,,
,learnmachinelearning,,t2_jqdv3,False,,0,False,What are Concept Drifts in Time Series Data?,[],r/learnmachinelearning,False,6,,0,61.0,False,t3_gfs9bk,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Wr1SW3lqLCHepDiIGnXTq7b45RrfRFr7npnkJ47zTUs.jpg,False,,[],{},link,,False,,1588969546.0,text,6,,,text,iunera.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?auto=webp&amp;s=aed92538a0b3812d688044e55aea1b1c589ea66d', 'width': 1280, 'height': 558}, 'resolutions': [{'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b9b35c2b419189706c5ea2b8ad3c163aeed1e5d', 'width': 108, 'height': 47}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2566f9f254dd3139bb9e60d2e32fffc8694dc427', 'width': 216, 'height': 94}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78792c89d249fe5c3122e10fcfcb5f38d6ab6cba', 'width': 320, 'height': 139}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a99c62beb811f107e7cfcb9f37f5b70ee6753a36', 'width': 640, 'height': 279}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc8386edace9513aeaa3e409a4220811217e22a3', 'width': 960, 'height': 418}, {'url': 'https://external-preview.redd.it/f1GaC1Wr1fRqSQ6f0roc2S9YTqQQHyNuSW-7wuC-bfU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=09db4dcc8bd6f32a1c67f880ffcadd6c8b5ea6a0', 'width': 1080, 'height': 470}], 'variants': {}, 'id': '9IikYfauu55Db_94Pp22GuabwDNjWai_xXq3St9JLb0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfs9bk,True,,Timbo2020,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfs9bk/what_are_concept_drifts_in_time_series_data/,all_ads,False,https://www.iunera.com/kraken/fabric/concept-drifts/,155203,1588940746.0,0,,False,,,,
,learnmachinelearning,,t2_ftjjtyo,False,,0,False,Anybody knows how to solve this?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfs23x,False,dark,0.33,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,default,False,,[],{},,,False,,1588968689.0,text,6,,,text,self.ArtificialInteligence,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfs23x,True,,AdaptiveNarc,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfs23x/anybody_knows_how_to_solve_this/,all_ads,False,/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/,155203,1588939889.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ArtificialInteligence', 'selftext': 'I have this exercise:\n\nYou have a training set composed of grades from 6 classes for a population of 100 students from 20 countries. Can you use a K-nearest neighbor classifier to determine most likely nationality for an unknown student from their grades?  If it IS possible give the formula for P( X(vector) |Ï‰k ) . If it is not possible, explain why.', 'author_fullname': 't2_6et4j6li', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'K-nearest neighbor', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ArtificialInteligence', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gfcimg', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 0, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588905955.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ArtificialInteligence', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have this exercise:&lt;/p&gt;\n\n&lt;p&gt;You have a training set composed of grades from 6 classes for a population of 100 students from 20 countries. Can you use a K-nearest neighbor classifier to determine most likely nationality for an unknown student from their grades?  If it IS possible give the formula for P( X(vector) |Ï‰k ) . If it is not possible, explain why.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3crzr', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfcimg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'tasian123', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ArtificialInteligence/comments/gfcimg/knearest_neighbor/', 'subreddit_subscribers': 34102, 'created_utc': 1588877155.0, 'num_crossposts': 3, 'media': None, 'is_video': False}]",t3_gfcimg,,
,learnmachinelearning,"Hi everyone!

Currently, I'm working on my first Convolutional Neural Network for a project in university. I have to create a model that can recognize if a cable has a defect by only using images of ""good"" and ""defect"" cables.

Right now, I'm struggling with the quantity of my data:

I have 224 images of ""good"" cables in my ""train""-folder.

My ""test""-folder consists of 58 ""good""-cable images and 92 ""defect""-cable images. So my test-data is unbalanced. Our professor advised us to transfer 34 of the train images into the ""good""-cable class of the test folder so that we'll have a ratio of 92/92.

I wanted to load the data with the ""flow\_from\_directory""-function of keras. I already know how to only load the images from the ""good""-cable class of the test-folder. But now I'm drawing a blank about how to transfer the 34 pictures from the training folder into the test folder so that the folder has a balanced distribution of data.  I need to do it via code but can't come up with an idea. Can somebody maybe help me or give me a hint?

I would appreciate every input. :)",t2_1dzrzji,False,,0,False,[CNN] Loading and separating images with Keras and/or Tensorflow,[],r/learnmachinelearning,False,6,,0,,False,t3_gfrtkb,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588967655.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;

&lt;p&gt;Currently, I&amp;#39;m working on my first Convolutional Neural Network for a project in university. I have to create a model that can recognize if a cable has a defect by only using images of &amp;quot;good&amp;quot; and &amp;quot;defect&amp;quot; cables.&lt;/p&gt;

&lt;p&gt;Right now, I&amp;#39;m struggling with the quantity of my data:&lt;/p&gt;

&lt;p&gt;I have 224 images of &amp;quot;good&amp;quot; cables in my &amp;quot;train&amp;quot;-folder.&lt;/p&gt;

&lt;p&gt;My &amp;quot;test&amp;quot;-folder consists of 58 &amp;quot;good&amp;quot;-cable images and 92 &amp;quot;defect&amp;quot;-cable images. So my test-data is unbalanced. Our professor advised us to transfer 34 of the train images into the &amp;quot;good&amp;quot;-cable class of the test folder so that we&amp;#39;ll have a ratio of 92/92.&lt;/p&gt;

&lt;p&gt;I wanted to load the data with the &amp;quot;flow_from_directory&amp;quot;-function of keras. I already know how to only load the images from the &amp;quot;good&amp;quot;-cable class of the test-folder. But now I&amp;#39;m drawing a blank about how to transfer the 34 pictures from the training folder into the test folder so that the folder has a balanced distribution of data.  I need to do it via code but can&amp;#39;t come up with an idea. Can somebody maybe help me or give me a hint?&lt;/p&gt;

&lt;p&gt;I would appreciate every input. :)&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfrtkb,True,,Ruffybeo,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrtkb/cnn_loading_and_separating_images_with_keras/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfrtkb/cnn_loading_and_separating_images_with_keras/,155203,1588938855.0,0,,False,,,,
,learnmachinelearning,"I'm trying to create a network that sorts through data and classifies lists as either a sine or cosine. I've made the data and added noise to the sines and cosines and trained the network. I used an MNIST tutorial as a base and I was expecting my network to give an integer output based on whether the signal is a sine (0) or cosine (1) but instead I get a float.

    i = Input(shape=(100,))
    x = Dense(128, activation='relu')(i)
    x = Dropout(0.2)(x)
    x = Dense(1)(x)
    
    model = Model(i, x)
    model.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])
    r = model.fit(X, Y, epochs=50)

This is my model. Can anyone tell me if a part of this is the problem? I've tried changing the loss to 'sparse\_categorical\_crossentropy' but this gives the following error

    InvalidArgumentError:  Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0

This goes away if I change the last Dense layer to Dense(2) but I still don't get an integer.",t2_4di36zz,False,,0,False,Create classification network with integer output,[],r/learnmachinelearning,False,6,,0,,False,t3_gfrgms,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588966018.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to create a network that sorts through data and classifies lists as either a sine or cosine. I&amp;#39;ve made the data and added noise to the sines and cosines and trained the network. I used an MNIST tutorial as a base and I was expecting my network to give an integer output based on whether the signal is a sine (0) or cosine (1) but instead I get a float.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;i = Input(shape=(100,))
x = Dense(128, activation=&amp;#39;relu&amp;#39;)(i)
x = Dropout(0.2)(x)
x = Dense(1)(x)

model = Model(i, x)
model.compile(loss=&amp;#39;mse&amp;#39;, optimizer=&amp;#39;Adam&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])
r = model.fit(X, Y, epochs=50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is my model. Can anyone tell me if a part of this is the problem? I&amp;#39;ve tried changing the loss to &amp;#39;sparse_categorical_crossentropy&amp;#39; but this gives the following error&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;InvalidArgumentError:  Received a label value of 1 which is outside the valid range of [0, 1).  Label values: 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This goes away if I change the last Dense layer to Dense(2) but I still don&amp;#39;t get an integer.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfrgms,True,,Competitive_Mongoose,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfrgms/create_classification_network_with_integer_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfrgms/create_classification_network_with_integer_output/,155203,1588937218.0,0,,False,,,,
,learnmachinelearning,Is writing a function correct way to do ?. Or is there any alternative way to do that,t2_59797uro,False,,0,False,I have built a model after a lot of data cleaning. Now how do I do the same data cleaning steps for unseen data?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfmfc6,False,light,1.0,,public,3,0,{},,,False,[],,False,False,,{},HELP,False,3,,False,self,False,,[],{},,,True,,1588941363.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Is writing a function correct way to do ?. Or is there any alternative way to do that&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfmfc6,True,,FoolishlyPainful,,10,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfmfc6/i_have_built_a_model_after_a_lot_of_data_cleaning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfmfc6/i_have_built_a_model_after_a_lot_of_data_cleaning/,155203,1588912563.0,0,,False,,,,
,learnmachinelearning,"I'm currently an academic at a UK university. I primarily teach web and mobile development to undergraduate students. I'd like to pursue learning ML. However, I'm find it particularly overwhelming. I have a good understanding of Python and OOP, advanced knowledge of SQL and I'm  pretty good with AWS and GCP. So I think as far programming goes I'm okay. It's learning the ML theory that I'm finding overwhelming.

Any tips? I'd appreciate hearing how others have managed to overcome a similar situation.

Thanks in adv!",t2_6ewfalng,False,,0,False,Any advice.. beginning to feel somewhat overwhelmed,[],r/learnmachinelearning,False,6,,0,,False,t3_gfj3km,False,dark,0.89,,public,7,0,{},,,False,[],,False,False,,{},,False,7,,False,self,False,,[],{},,,True,,1588927685.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m currently an academic at a UK university. I primarily teach web and mobile development to undergraduate students. I&amp;#39;d like to pursue learning ML. However, I&amp;#39;m find it particularly overwhelming. I have a good understanding of Python and OOP, advanced knowledge of SQL and I&amp;#39;m  pretty good with AWS and GCP. So I think as far programming goes I&amp;#39;m okay. It&amp;#39;s learning the ML theory that I&amp;#39;m finding overwhelming.&lt;/p&gt;

&lt;p&gt;Any tips? I&amp;#39;d appreciate hearing how others have managed to overcome a similar situation.&lt;/p&gt;

&lt;p&gt;Thanks in adv!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfj3km,True,,datadroiduk,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfj3km/any_advice_beginning_to_feel_somewhat_overwhelmed/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfj3km/any_advice_beginning_to_feel_somewhat_overwhelmed/,155203,1588898885.0,0,,False,,,,
,learnmachinelearning,,t2_1k4nqnsa,False,,0,False,"[GitHub] a simple text autoencoder in Jupyter Notebook. As simple as it can be, so it is good for beginners.","[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_gfnyvw,False,light,0.67,,public,2,0,{},140.0,,False,[],,False,False,,{},Project,False,2,,False,https://b.thumbs.redditmedia.com/0dSqFVTaNBElSulyRQ7ULSdZdWViRmONWnwTl37fyJk.jpg,False,,[],{},link,,False,,1588948849.0,richtext,6,,,text,github.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/SpSOngL8ZZ2R4phXD-49_WuN8ccjeYgcII0LzrKbtoM.jpg?auto=webp&amp;s=96a67decf19f6bea2685973d7e241fb32a10373f', 'width': 215, 'height': 215}, 'resolutions': [{'url': 'https://external-preview.redd.it/SpSOngL8ZZ2R4phXD-49_WuN8ccjeYgcII0LzrKbtoM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b88cb1857a765fdec41923ee862b0c11fbcd26b3', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'zGIrxBhitFmy_opFh2espjqeKyetnqWE6n_OyIYfLWY'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gfnyvw,True,,kiasari,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfnyvw/github_a_simple_text_autoencoder_in_jupyter/,all_ads,False,https://github.com/kiasar/text_autoencoder/blob/master/Simple_Autoencoder_for_text.ipynb,155203,1588920049.0,0,,False,,,,
,learnmachinelearning,"Hey guys, I'm new to machine learning and have completed a udemy course talking about the basics and stuff. It would be great if someone suggested more material or sources that I can keep learning from. And any more suggestions that I need to be aware of.",t2_28m0i5zb,False,,0,False,Need help in learning ML,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfqji5,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588961653.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys, I&amp;#39;m new to machine learning and have completed a udemy course talking about the basics and stuff. It would be great if someone suggested more material or sources that I can keep learning from. And any more suggestions that I need to be aware of.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfqji5,True,,_skullcrusher1_,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqji5/need_help_in_learning_ml/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfqji5/need_help_in_learning_ml/,155203,1588932853.0,0,,False,,,,
,learnmachinelearning," 

Hi.

I am exploring Sutton and Barto's textbook on reinforcement learning. I think I need to learn some more of the underlying maths first.

I have a high-school level understanding of calculus, probability and statistics. I have taken a college course on linear algebra.

Real analysis, \_serious\_ probability and statistics are a weak spot. On the bright side, I am more interested in (as of now) implementing the key algorithms and techniques, more than proofs (though I will definitely revisit RL in a more rigorous way at a later time).

What all mathematics should I learn, and how (textbooks, courses etc)?

Thanks,

Pakodanomics",t2_zpl4l3o,False,,0,False,Maths behind Sutton and Barto's textbook?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfqf7h,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588961078.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi.&lt;/p&gt;

&lt;p&gt;I am exploring Sutton and Barto&amp;#39;s textbook on reinforcement learning. I think I need to learn some more of the underlying maths first.&lt;/p&gt;

&lt;p&gt;I have a high-school level understanding of calculus, probability and statistics. I have taken a college course on linear algebra.&lt;/p&gt;

&lt;p&gt;Real analysis, _serious_ probability and statistics are a weak spot. On the bright side, I am more interested in (as of now) implementing the key algorithms and techniques, more than proofs (though I will definitely revisit RL in a more rigorous way at a later time).&lt;/p&gt;

&lt;p&gt;What all mathematics should I learn, and how (textbooks, courses etc)?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Pakodanomics&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfqf7h,True,,pakodanomics,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfqf7h/maths_behind_sutton_and_bartos_textbook/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfqf7h/maths_behind_sutton_and_bartos_textbook/,155203,1588932278.0,0,,False,,,,
,learnmachinelearning,,t2_6bo5rgdf,False,,0,False,Intro to machine learning and data science with Python using the Iris dataset. 7 video course on YouTube.,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfn0lo,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Intro to Machine Learning with Python 1: Welcome and Project Setup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Project Data Science', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rdaG53khzv0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCM7_uh02Xqv4PFKbyIasP4g'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfn0lo', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/V190C7orjwFldbSPPS4S-Ga7ldr3wHhsdC3nG0H9TiA.jpg,False,,[],{},rich:video,,False,,1588944092.0,text,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?auto=webp&amp;s=cdc7f84812a2900a8da97ba4c38ffa52b2711e55', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=867b27a273dacb6c04953f7933b69ad11bb264ce', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e01515d7b43ae45a570b24c02f086fd8a7582c81', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/64BczoVeuB2LkvqhFbXHukuXyYAcXQsYTv5Qnu5w6EY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=33bd815a00a2703543c19824dd53d671d889a7f2', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'GQTyjtkHObDFTs_Juf_KzBRV40TJJVE7QJECPt_XhEQ'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn0lo,True,,projectdatascience,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn0lo/intro_to_machine_learning_and_data_science_with/,all_ads,False,https://www.youtube.com/watch?v=rdaG53khzv0&amp;list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr,155203,1588915292.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Intro to Machine Learning with Python 1: Welcome and Project Setup', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLMAyPTgGwv2DUV6DZib9eMetsTTX87JNr"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Project Data Science', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/rdaG53khzv0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCM7_uh02Xqv4PFKbyIasP4g'}}",False,,,,
,learnmachinelearning,"I'm trying to use some python 3D visualization tools to visualize 3D meshes, but non of the python libraries support ray-tracing.

However, all these cool videos that you can see online have really nice renderings with ray-tracing and everything with animation.

What kind of tools to graphics researchers use? and is there a good tutorial on learning how to make one?",t2_6ag000ku,False,,0,False,How do Siggraph authors make such a cool visualization and what tools do they use?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfppfp,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588957569.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m trying to use some python 3D visualization tools to visualize 3D meshes, but non of the python libraries support ray-tracing.&lt;/p&gt;

&lt;p&gt;However, all these cool videos that you can see online have really nice renderings with ray-tracing and everything with animation.&lt;/p&gt;

&lt;p&gt;What kind of tools to graphics researchers use? and is there a good tutorial on learning how to make one?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfppfp,True,,MiniMongMari,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfppfp/how_do_siggraph_authors_make_such_a_cool/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfppfp/how_do_siggraph_authors_make_such_a_cool/,155203,1588928769.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#TimeManagement tips: How to get organised by Rosemary Miller - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfpnna,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},Discussion,False,0,,True,default,False,,[],{},link,,False,,1588957330.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?auto=webp&amp;s=26b0feeed9db05e72415f41913a12bd8507b380a', 'width': 900, 'height': 473}, 'resolutions': [{'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bdb51f655c42d9bead3fe93ac306fdb2d72c770', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8be55f1ce4ded6d3dd9871571b3f1ce4e0dde0f9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2e5e47ae0ed547c115e33adaea67bc210d55a56', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e1b8e64f48e7304c6e0c0e10529dbc66ddb697d', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'vo8-QmQle6mXAT3LtitgtE8QPRljYtZGXBPMwS4RLnA'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfpnna,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfpnna/timemanagement_tips_how_to_get_organised_by/,all_ads,False,/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/,155203,1588928530.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University RLO Study Skills - Time Management, Publicly Accessed 07. May 2020,  [http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo](http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo) \n\n *Juggling readings, assignments and revision can be one of the most challenging parts of the university. Hereâ€™s how to get organised and make the most out of your time! By Clinton Bell*  \n\n\nYou probably already know procrastination is a bad idea. If you put off doing assignments or donâ€™t revise regularly, itâ€™s easy to fall behind and end up with way too much stuff to catch up on. Unfortunately, even if you know you should study, it can be difficult to make yourself do it - especially if youâ€™re busy with other things.  \n\n\nIf you find yourself struggling to make time for study, or you feel like you just have way too much going on, try planning your time with a study schedule! Thereâ€™s [an example of how to make one](http://www.monash.edu/rlo/study-skills/studying-effectively/managing-your-time) on the library website.  \n\n\nMaking a schedule has several benefits:\n\n* It helps you work out how much time you have, and plan your study around your work, social life, and other commitments\n* Itâ€™s easier to keep track of tasks and due dates if you have them all written down in one place\n* Youâ€™re less likely to procrastinate if the study is a regular part of your routine. Scheduling study in advance can also make you feel more committed to actually doing it\n* Having a plan can help you feel less stressed and more in control of your study.\n\nWhen making your schedule itâ€™s important to prioritise. Consider how important things are as well as when theyâ€™re due - if an assignment is worth a lot of marks youâ€™ll probably need to spend more time on it. If you need to do something which requires other people, special facilities or equipment, you may also need to work around when those things are available.  \n\n\nFor large assignments, it can be helpful to split the task into smaller goals. For example, you might aim to write one paragraph of an essay each night. Splitting the task into chunks can make it less intimidating to get started, and can also help you stress less - if youâ€™re meeting your goals you know youâ€™re on track to get the assignment done.  \n\n\nAs well as planning your time, itâ€™s important to use it effectively. Using good study methods and improving your skills can give you better results in less time:  \n\n\n* [Listen carefully in lectures, and take good notes](https://www.monash.edu/rlo/quick-study-guides?a=388983). This will reduce the time you need to spend revising later\n* [Read assignment instructions carefully](http://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/analysing-the-task-requirements), and if youâ€™re not clear on something ask your lecturer or tutor. If you donâ€™t understand the task, you can waste time and lose marks by doing the wrong thing\n* [Learn strategies that help you read more efficiently](http://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies).\n* Keep track of what youâ€™ve read. Youâ€™ll need [to reference your sources later](http://www.monash.edu/rlo/research-writing-assignments/referencing-and-academic-integrity), and if you didnâ€™t make a note of the information you need for referencing, youâ€™ll have to go back and find it\n* Get advice from the library. We can help you improve your study skills, and learn how to find the resources you need for your assignments more quickly. If you canâ€™t see us in person, [we also have a lot of helpful information online](http://www.monash.edu/rlo)!\n* [Get help from English Connect](http://www.monash.edu/english-connect) if you have difficulty with English language skills.\n* Get some sleep! You donâ€™t work or learn as efficiently if you donâ€™t get enough sleep, so staying up too late to study can be counterproductive.\n\nTime management can be challenging, but with good planning and study skills, you can get everything done on time. So best of luck with your study this semester - and remember, [come see us](http://www.monash.edu/library/skills/resources/programs/drop-in)\xa0at a drop-in session\xa0if you need help!', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#TimeManagement tips: How to get organised by Rosemary Miller - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf0oal', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588858591.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University RLO Study Skills - Time Management, Publicly Accessed 07. May 2020,  &lt;a href=""http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo""&gt;http://librarymonash.blogspot.com/2017/07/time-management-tips-how-to-get.html#.WgjlOVuCypo&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Juggling readings, assignments and revision can be one of the most challenging parts of the university. Hereâ€™s how to get organised and make the most out of your time! By Clinton Bell&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;You probably already know procrastination is a bad idea. If you put off doing assignments or donâ€™t revise regularly, itâ€™s easy to fall behind and end up with way too much stuff to catch up on. Unfortunately, even if you know you should study, it can be difficult to make yourself do it - especially if youâ€™re busy with other things.  &lt;/p&gt;\n\n&lt;p&gt;If you find yourself struggling to make time for study, or you feel like you just have way too much going on, try planning your time with a study schedule! Thereâ€™s &lt;a href=""http://www.monash.edu/rlo/study-skills/studying-effectively/managing-your-time""&gt;an example of how to make one&lt;/a&gt; on the library website.  &lt;/p&gt;\n\n&lt;p&gt;Making a schedule has several benefits:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It helps you work out how much time you have, and plan your study around your work, social life, and other commitments&lt;/li&gt;\n&lt;li&gt;Itâ€™s easier to keep track of tasks and due dates if you have them all written down in one place&lt;/li&gt;\n&lt;li&gt;Youâ€™re less likely to procrastinate if the study is a regular part of your routine. Scheduling study in advance can also make you feel more committed to actually doing it&lt;/li&gt;\n&lt;li&gt;Having a plan can help you feel less stressed and more in control of your study.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When making your schedule itâ€™s important to prioritise. Consider how important things are as well as when theyâ€™re due - if an assignment is worth a lot of marks youâ€™ll probably need to spend more time on it. If you need to do something which requires other people, special facilities or equipment, you may also need to work around when those things are available.  &lt;/p&gt;\n\n&lt;p&gt;For large assignments, it can be helpful to split the task into smaller goals. For example, you might aim to write one paragraph of an essay each night. Splitting the task into chunks can make it less intimidating to get started, and can also help you stress less - if youâ€™re meeting your goals you know youâ€™re on track to get the assignment done.  &lt;/p&gt;\n\n&lt;p&gt;As well as planning your time, itâ€™s important to use it effectively. Using good study methods and improving your skills can give you better results in less time:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=""https://www.monash.edu/rlo/quick-study-guides?a=388983""&gt;Listen carefully in lectures, and take good notes&lt;/a&gt;. This will reduce the time you need to spend revising later&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/analysing-the-task-requirements""&gt;Read assignment instructions carefully&lt;/a&gt;, and if youâ€™re not clear on something ask your lecturer or tutor. If you donâ€™t understand the task, you can waste time and lose marks by doing the wrong thing&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies""&gt;Learn strategies that help you read more efficiently&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;Keep track of what youâ€™ve read. Youâ€™ll need &lt;a href=""http://www.monash.edu/rlo/research-writing-assignments/referencing-and-academic-integrity""&gt;to reference your sources later&lt;/a&gt;, and if you didnâ€™t make a note of the information you need for referencing, youâ€™ll have to go back and find it&lt;/li&gt;\n&lt;li&gt;Get advice from the library. We can help you improve your study skills, and learn how to find the resources you need for your assignments more quickly. If you canâ€™t see us in person, &lt;a href=""http://www.monash.edu/rlo""&gt;we also have a lot of helpful information online&lt;/a&gt;!&lt;/li&gt;\n&lt;li&gt;&lt;a href=""http://www.monash.edu/english-connect""&gt;Get help from English Connect&lt;/a&gt; if you have difficulty with English language skills.&lt;/li&gt;\n&lt;li&gt;Get some sleep! You donâ€™t work or learn as efficiently if you donâ€™t get enough sleep, so staying up too late to study can be counterproductive.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Time management can be challenging, but with good planning and study skills, you can get everything done on time. So best of luck with your study this semester - and remember, &lt;a href=""http://www.monash.edu/library/skills/resources/programs/drop-in""&gt;come see us&lt;/a&gt;\xa0at a drop-in session\xa0if you need help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?auto=webp&amp;s=26b0feeed9db05e72415f41913a12bd8507b380a', 'width': 900, 'height': 473}, 'resolutions': [{'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bdb51f655c42d9bead3fe93ac306fdb2d72c770', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8be55f1ce4ded6d3dd9871571b3f1ce4e0dde0f9', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2e5e47ae0ed547c115e33adaea67bc210d55a56', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/4WfswWk1EO8jU42E08902Az9Pk4KR3eCeKM43BHZvew.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0e1b8e64f48e7304c6e0c0e10529dbc66ddb697d', 'width': 640, 'height': 336}], 'variants': {}, 'id': 'vo8-QmQle6mXAT3LtitgtE8QPRljYtZGXBPMwS4RLnA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gf0oal', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gf0oal/timemanagement_tips_how_to_get_organised_by/', 'subreddit_subscribers': 33, 'created_utc': 1588829791.0, 'num_crossposts': 28, 'media': None, 'is_video': False}]",t3_gf0oal,,
,learnmachinelearning,"I am going through some of the tensorflow tutorials here: 

[https://www.tensorflow.org/tutorials/images/cnn](https://www.tensorflow.org/tutorials/images/cnn) 

&amp;#x200B;

After the dataset is downloaded, the dataset is then divided by 255 so that we may normalize the values to be between 0 and 1

As seen here:  [https://i.imgur.com/nwbqDcT.png](https://i.imgur.com/nwbqDcT.png) 

This step does not seem to be necessary, as we can still train and validate the model without doing this part. But it does improve the accuracy of the model. Can anyone explain as to why this is?",t2_o9tnr,False,,0,False,Going through Tensorflow tutorials. How and why does normalizing data help this model?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfkmfd,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,False,,[],{},self,,True,,1588933625.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am going through some of the tensorflow tutorials here: &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.tensorflow.org/tutorials/images/cnn""&gt;https://www.tensorflow.org/tutorials/images/cnn&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;After the dataset is downloaded, the dataset is then divided by 255 so that we may normalize the values to be between 0 and 1&lt;/p&gt;

&lt;p&gt;As seen here:  &lt;a href=""https://i.imgur.com/nwbqDcT.png""&gt;https://i.imgur.com/nwbqDcT.png&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;This step does not seem to be necessary, as we can still train and validate the model without doing this part. But it does improve the accuracy of the model. Can anyone explain as to why this is?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?auto=webp&amp;s=a597266c297a2130a3826c64bb3418582ff46ad2', 'width': 769, 'height': 119}, 'resolutions': [{'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f5407c8f959e5304ae46b92e96c1902294a9785', 'width': 108, 'height': 16}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3d847552013d0c7aaa9c8d5c9ff85d6b663c0b8', 'width': 216, 'height': 33}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=34c120ef08662aed1c9a0632ae0b1fca4e274720', 'width': 320, 'height': 49}, {'url': 'https://external-preview.redd.it/Xn7hsB7E-Sb-nJE32oSOIJxgJ-f70T1ROFd0A-kCEGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b8c4c295444eba260414e78fcead30689943f61', 'width': 640, 'height': 99}], 'variants': {}, 'id': 't3zr2gBQMCJosgxrDFP6LhG4I2ZsodxSNnRgtP1iUQ4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfkmfd,True,,Moo3247,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkmfd/going_through_tensorflow_tutorials_how_and_why/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfkmfd/going_through_tensorflow_tutorials_how_and_why/,155203,1588904825.0,0,,False,,,,
,learnmachinelearning,,t2_2o7eaff,False,,0,False,This Week in AI - Issue #17 | Rubik's Code,[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gfot5l,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/q40TYZVgvDuVIhiCMY2LGrrwPV64Jh7cItGv36aVymY.jpg,False,,[],{},link,,False,,1588953119.0,text,6,,,text,rubikscode.net,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?auto=webp&amp;s=aba15a8e08da24b0f366ff7e94f7f49c652884e3', 'width': 1200, 'height': 675}, 'resolutions': [{'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19f794df176d742445e70a834f02073618a034a8', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbc2caf7263a43928c2f96facf6709c61c560994', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba4603038973138eb62c7d4c3e483b55662d27a9', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c5118555c9f824b549676e01773b5ef1341a3b0c', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bdb1652f7804251c04d9290f584d7a00cd93d38', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/vosFFqR89uhYYG6bIfS3WXBPOz0lIscwrACJBF5fl9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=238aef893d35150be90c75468911d2d706a4e494', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'A1f3svXfhPHnYTvqZdu1PaAFyL18pTPA-Iq6T1_JU48'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfot5l,True,,RubiksCodeNMZ,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfot5l/this_week_in_ai_issue_17_rubiks_code/,all_ads,False,https://rubikscode.net/2020/05/08/this-week-in-ai-issue-17/,155203,1588924319.0,0,,False,,,,
,learnmachinelearning,"Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.",t2_6l4z3,False,,0,False,Weekly Show-off!,[],r/learnmachinelearning,False,6,,0,,False,t3_gfo90z,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,True,self,False,,[],{},,,True,,1588950286.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Show off the machine learning projects that you have been working on! Whether big or small, any projects that you have built could be shared here.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gfo90z,True,,AutoModerator,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfo90z/weekly_showoff/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfo90z/weekly_showoff/,155203,1588921486.0,0,,False,,,,
,learnmachinelearning,"Hi everyone, my nameâ€™s Chris! Iâ€™m a research software engineer with a focus on applying machine learning techniques to a variety of new inventions. Iâ€™m also the co-founder and current co-lead of my companyâ€™s machine learning and artificial intelligence (ML/AI) community of practice.

In my free time, I apply ML to games in many different ways. My projects so far mainly have to do with Old School Runescape (examples below but thatâ€™s not what Iâ€™m here for today). Iâ€™m here today because a few people have reached out to me recently about starting their own projects applying ML to various games and I think thatâ€™s a great idea!! After all, the best way to learn is through implementation and why not implement something you care about? Not to mention, personal projects look great on any resume!

Throughout my journey, Iâ€™ve found that Iâ€™m happiest when helping people out by thinking through their process with them and figuring out what comes next. I would really like to share my experience and expertise gained with anyone who is interested in making their own projects and learning much more about ML and its applications in the process. I also have a bunch of ideas for ML-game projects that Iâ€™ll never have the time to implement myself which someone else might find invigorating.

That said, Iâ€™m thinking of starting what Iâ€™ll call the Chronic Coder Academy. Iâ€™ll start with a few students who I personally call on a weekly basis for about 30 minutes to an hour to discuss their new or ongoing ML projects - weekly todos, potential resources and next steps. I personally think itâ€™d help keep people accountable on their personal journey learning ML and also keep them motivated if I direct them well instead of being lost in the rough sea of ML/AI.

This is targeted at people who care about games and also want to learn more about ML through application. Your age, race, gender, ethnicity, etc mean nothing to me but you have to be passionate about what you do and hopefully converse well enough in English. Everything you make is completely your own, no matter how much advice or guidance I end up giving. Oh and also, itâ€™s completely free (before anyone asks). Iâ€™m really in this to better expose myself to different applications and ideas plus I truly enjoy watching people learn especially from interesting use cases like these. If they will allow me, I might use their projects as examples for teaching people more about specific ML concepts in the future. All that said, please fill out this survey if youâ€™re interested: [https://forms.gle/W37tzNFsTCFXGEwMA](https://forms.gle/W37tzNFsTCFXGEwMA)

If this isnâ€™t for you at this point in your life, feel free to join our discord! This applies regardless of who you are, what you do or what your experience level is. Itâ€™s an amazing community of ML experts and curious students alike who also happen to share a love for games. We could always use more experts to help anyone in need but also itâ€™s a place to learn if youâ€™re doing your own applied ML project and arenâ€™t sure who to go to. Join us here: [https://discord.gg/ZummSXK](https://discord.gg/ZummSXK)

Oh and if youâ€™re looking for a job sometime soon and worried about a project involving games not being a series enough topic to get hired, I previously did some asking around: [https://docs.google.com/document/d/1tU2GxQ3SZJVjV8\_A\_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing](https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing). Looks like most professionals agree that ML applied to games can be as serious a topic as any other within the ML field.

Whew that was a long piece... Thank you for reading if you've made it this far!

or **TL;DR** \- I want to help people learn ML by applying it to games. I intend to personally call them on a weekly basis to discuss progress and next steps. Fill out the form above if youâ€™re interested!

My Personal Projects:

**Bot Classification** \- currently in progress, classification techniques for the automated detection and reporting of bots: [https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp](https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp)

**Grand Exchange Prediction** \- time series analysis to predict the future prices of items in the grand exchange: [https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45](https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45)

**Cow Compliments** \- computer vision to detect cows on the screen in real time, walk over to them and compliment them for the lulz: [https://www.youtube.com/watch?v=7oW7jDyIufE](https://www.youtube.com/watch?v=7oW7jDyIufE)

&amp;#x200B;

**EDIT**: Firstly, THANK YOU FOR MY FIRST GOLD.

On the topic of when you can expect a response, I'll read through every submission by the end of the week and I'll let you guys know who I decide to start off working with by this weekend (05/10/2020). Probably a max of 5 individuals who I will reach out to once chosen.

The goal is to have a variety of different projects each from different games the individuals are passionate about. This is to hopefully allow for a broader scope of projects that ALL of you can contribute to as well. I plan to make a discord text channel for each game that we're going to be working on. There, we can all discuss ideas, issues and breakthroughs of applying ML to the chosen game. To be clear, these channels are not just for the main project that we'll be working on with the CCA Fellows (which you can contribute as well) but also for your own project ideas that you can work through alongside other ongoing projects that are different but still focused on that specific game.  So please join the discord if you're interested with working on these projects!

I know I will not be able to reach all of you due to the overwhelming response (once again, thank you so much!). However, I still definitely believe in the power of learning through application. Therefore, even if I can't get to you all personally, I hope you still get started on your own by either contributing to the main projects or with your own ML+Game projects. If you do, please keep us up to date as well on the channels we'll create for the games. We'll make sure to check in and, even if it's not a long personal call, that'll definitely still be a way to keep you accountable and continuously learning!",t2_3tqetf9o,False,,1,False,Want to learn ML by applying it to Games? I'll personally call you weekly and make sure you do it!,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_geuukb,False,light,0.96,,public,325,1,{},,,False,[],,False,False,,{},Project,False,325,,True,self,1588864050.0,,[],{'gid_2': 1},self,,True,,1588835404.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi everyone, my nameâ€™s Chris! Iâ€™m a research software engineer with a focus on applying machine learning techniques to a variety of new inventions. Iâ€™m also the co-founder and current co-lead of my companyâ€™s machine learning and artificial intelligence (ML/AI) community of practice.&lt;/p&gt;

&lt;p&gt;In my free time, I apply ML to games in many different ways. My projects so far mainly have to do with Old School Runescape (examples below but thatâ€™s not what Iâ€™m here for today). Iâ€™m here today because a few people have reached out to me recently about starting their own projects applying ML to various games and I think thatâ€™s a great idea!! After all, the best way to learn is through implementation and why not implement something you care about? Not to mention, personal projects look great on any resume!&lt;/p&gt;

&lt;p&gt;Throughout my journey, Iâ€™ve found that Iâ€™m happiest when helping people out by thinking through their process with them and figuring out what comes next. I would really like to share my experience and expertise gained with anyone who is interested in making their own projects and learning much more about ML and its applications in the process. I also have a bunch of ideas for ML-game projects that Iâ€™ll never have the time to implement myself which someone else might find invigorating.&lt;/p&gt;

&lt;p&gt;That said, Iâ€™m thinking of starting what Iâ€™ll call the Chronic Coder Academy. Iâ€™ll start with a few students who I personally call on a weekly basis for about 30 minutes to an hour to discuss their new or ongoing ML projects - weekly todos, potential resources and next steps. I personally think itâ€™d help keep people accountable on their personal journey learning ML and also keep them motivated if I direct them well instead of being lost in the rough sea of ML/AI.&lt;/p&gt;

&lt;p&gt;This is targeted at people who care about games and also want to learn more about ML through application. Your age, race, gender, ethnicity, etc mean nothing to me but you have to be passionate about what you do and hopefully converse well enough in English. Everything you make is completely your own, no matter how much advice or guidance I end up giving. Oh and also, itâ€™s completely free (before anyone asks). Iâ€™m really in this to better expose myself to different applications and ideas plus I truly enjoy watching people learn especially from interesting use cases like these. If they will allow me, I might use their projects as examples for teaching people more about specific ML concepts in the future. All that said, please fill out this survey if youâ€™re interested: &lt;a href=""https://forms.gle/W37tzNFsTCFXGEwMA""&gt;https://forms.gle/W37tzNFsTCFXGEwMA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If this isnâ€™t for you at this point in your life, feel free to join our discord! This applies regardless of who you are, what you do or what your experience level is. Itâ€™s an amazing community of ML experts and curious students alike who also happen to share a love for games. We could always use more experts to help anyone in need but also itâ€™s a place to learn if youâ€™re doing your own applied ML project and arenâ€™t sure who to go to. Join us here: &lt;a href=""https://discord.gg/ZummSXK""&gt;https://discord.gg/ZummSXK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Oh and if youâ€™re looking for a job sometime soon and worried about a project involving games not being a series enough topic to get hired, I previously did some asking around: &lt;a href=""https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing""&gt;https://docs.google.com/document/d/1tU2GxQ3SZJVjV8_A_PwJBlrQf1tGO1mN0n9YIYLnlSg/edit?usp=sharing&lt;/a&gt;. Looks like most professionals agree that ML applied to games can be as serious a topic as any other within the ML field.&lt;/p&gt;

&lt;p&gt;Whew that was a long piece... Thank you for reading if you&amp;#39;ve made it this far!&lt;/p&gt;

&lt;p&gt;or &lt;strong&gt;TL;DR&lt;/strong&gt; - I want to help people learn ML by applying it to games. I intend to personally call them on a weekly basis to discuss progress and next steps. Fill out the form above if youâ€™re interested!&lt;/p&gt;

&lt;p&gt;My Personal Projects:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bot Classification&lt;/strong&gt; - currently in progress, classification techniques for the automated detection and reporting of bots: &lt;a href=""https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp""&gt;https://www.youtube.com/watch?v=Dk4Yahv2lek&amp;amp;list=PLX9loFun2zNkqwEk3abeMzZnVlT0YPxkp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Grand Exchange Prediction&lt;/strong&gt; - time series analysis to predict the future prices of items in the grand exchange: &lt;a href=""https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45""&gt;https://www.youtube.com/watch?v=D5TmBcpgm7k&amp;amp;list=PLX9loFun2zNmri7jHhLs7NV76wcGRzI45&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cow Compliments&lt;/strong&gt; - computer vision to detect cows on the screen in real time, walk over to them and compliment them for the lulz: &lt;a href=""https://www.youtube.com/watch?v=7oW7jDyIufE""&gt;https://www.youtube.com/watch?v=7oW7jDyIufE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Firstly, THANK YOU FOR MY FIRST GOLD.&lt;/p&gt;

&lt;p&gt;On the topic of when you can expect a response, I&amp;#39;ll read through every submission by the end of the week and I&amp;#39;ll let you guys know who I decide to start off working with by this weekend (05/10/2020). Probably a max of 5 individuals who I will reach out to once chosen.&lt;/p&gt;

&lt;p&gt;The goal is to have a variety of different projects each from different games the individuals are passionate about. This is to hopefully allow for a broader scope of projects that ALL of you can contribute to as well. I plan to make a discord text channel for each game that we&amp;#39;re going to be working on. There, we can all discuss ideas, issues and breakthroughs of applying ML to the chosen game. To be clear, these channels are not just for the main project that we&amp;#39;ll be working on with the CCA Fellows (which you can contribute as well) but also for your own project ideas that you can work through alongside other ongoing projects that are different but still focused on that specific game.  So please join the discord if you&amp;#39;re interested with working on these projects!&lt;/p&gt;

&lt;p&gt;I know I will not be able to reach all of you due to the overwhelming response (once again, thank you so much!). However, I still definitely believe in the power of learning through application. Therefore, even if I can&amp;#39;t get to you all personally, I hope you still get started on your own by either contributing to the main projects or with your own ML+Game projects. If you do, please keep us up to date as well on the channels we&amp;#39;ll create for the games. We&amp;#39;ll make sure to check in and, even if it&amp;#39;s not a long personal call, that&amp;#39;ll definitely still be a way to keep you accountable and continuously learning!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?auto=webp&amp;s=ceeb466dab89c19b28d7c1a705c120ab083492c7', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63c56cac71cca6eb0ed38496f3be93af748929f9', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f51db1d17542846e83cc1ab922f1cb853d3a084', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7c65bdec2e163e3ecb46b11cc9164edba8c7a36', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a126016c4897032f5ee69cda7573a9fcfa044088', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ac65fb8c06828aba0b8ea3ccca6671fef5b4cf21', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/rSupq0XVK8r_3TgBPgITaEkzcpE84UnHnN9WTqzi-cI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d00c2b2a25880883bb0dd5fa4c0e6e523d27a59f', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'u5mV1tvG7-5CDubQL7omAhHKFY2GHhm2sfnyhKj4sMA'}], 'enabled': False}","[{'giver_coin_reward': None, 'subreddit_id': None, 'is_new': False, 'days_of_drip_extension': 0, 'coin_price': 500, 'id': 'gid_2', 'penny_donate': None, 'coin_reward': 100, 'icon_url': 'https://www.redditstatic.com/gold/awards/icon/gold_512.png', 'days_of_premium': 7, 'icon_height': 512, 'resized_icons': [{'url': 'https://www.redditstatic.com/gold/awards/icon/gold_16.png', 'width': 16, 'height': 16}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_32.png', 'width': 32, 'height': 32}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_48.png', 'width': 48, 'height': 48}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_64.png', 'width': 64, 'height': 64}, {'url': 'https://www.redditstatic.com/gold/awards/icon/gold_128.png', 'width': 128, 'height': 128}], 'icon_width': 512, 'start_date': None, 'is_enabled': True, 'description': 'Gives the author a week of Reddit Premium, %{coin_symbol}100 Coins to do with as they please, and shows a Gold Award.', 'end_date': None, 'subreddit_coin_reward': 0, 'count': 1, 'name': 'Gold', 'icon_format': None, 'award_sub_type': 'GLOBAL', 'penny_price': None, 'award_type': 'global'}]",[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geuukb,True,,chriskok1337,,27,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuukb/want_to_learn_ml_by_applying_it_to_games_ill/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geuukb/want_to_learn_ml_by_applying_it_to_games_ill/,155203,1588806604.0,0,,False,,,,
,learnmachinelearning,"I have a set of data in a csv file, roughly 1700 rows by 35 columns, that gets added to every day and my goal is to take that data and use it to predict one number per row. The problem that I am running into is that I don't know where go from here. All of the machine learning software I have looked at is focused on image recognition and not text data problems. If I do find a guide for one it's super basic and uses downloaded data that I can't easily replace with a csv import. So I was wondering if you guys have any recommendations for software I could use to solve this problem and if you know any tutorials that go along with them.",t2_57emyy01,False,,0,False,What to use to run a regression problem in python?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfnujz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Request,False,1,,False,self,False,,[],{},,,True,,1588948254.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a set of data in a csv file, roughly 1700 rows by 35 columns, that gets added to every day and my goal is to take that data and use it to predict one number per row. The problem that I am running into is that I don&amp;#39;t know where go from here. All of the machine learning software I have looked at is focused on image recognition and not text data problems. If I do find a guide for one it&amp;#39;s super basic and uses downloaded data that I can&amp;#39;t easily replace with a csv import. So I was wondering if you guys have any recommendations for software I could use to solve this problem and if you know any tutorials that go along with them.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gfnujz,True,,Void-Nut,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfnujz/what_to_use_to_run_a_regression_problem_in_python/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfnujz/what_to_use_to_run_a_regression_problem_in_python/,155203,1588919454.0,0,,False,,,,
,learnmachinelearning,,t2_o0pbd,False,,0,False,"22 worked examples in machine learning (energy, medicine, banking, retail...)",[],r/learnmachinelearning,False,6,,0,73.0,False,t3_gf2coi,False,dark,0.89,,public,62,0,{},140.0,,False,[],,False,False,,{},,False,62,,False,https://b.thumbs.redditmedia.com/AtNCeUtZZotZem202EB0jbArWcXkaKK6S5i3Sa9o6iE.jpg,False,,[],{},link,,False,,1588867158.0,text,6,,,text,neuraldesigner.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?auto=webp&amp;s=d100079d234b6b84dba6f38614d575af623b1d3e', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8109e511182067447f3ff3b0fc99d8c07f798df', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=39122191eb7fb9bed7ca1924828fb21e7cb6ab3f', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acbb3f60a8963b1b096f874c06457e6dded7e691', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66874fccb85f1c12b0b0dbaf3ddaf3957fb21ef2', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=34a2144239c51c4bed947b72af8f19c88655a513', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/Ww3v2kakiBs0xUoXvnKEO4iKUyCmRXll_nzBTBLVl_8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=faf980c3422664e2d56565b03e1416d12b54bd17', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'lDWE1FUKuv1DyfNkkPXhMigVGDiektzN0SUFmFD3RTw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf2coi,True,,datapablo,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2coi/22_worked_examples_in_machine_learning_energy/,all_ads,False,https://www.neuraldesigner.com/learning/examples,155203,1588838358.0,0,,False,,,,
,learnmachinelearning,"Hello.  I would like to create a custom real-time object detection app, preferably in Pytorch, that people can download to Mac, PC, or Linux, without having to install Python or anything else except for that one app.  I would like this to work on very old and slow computers too, so I'm hoping for a program that is fast but also small on disk space and easy to use.  Any advice for me on which ML packages I should use?  

Thank you in advance for your help.",t2_57i0bime,False,,0,False,Advice on standalone object detection app please!,[],r/learnmachinelearning,False,6,,0,,False,t3_gfn2mu,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588944364.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello.  I would like to create a custom real-time object detection app, preferably in Pytorch, that people can download to Mac, PC, or Linux, without having to install Python or anything else except for that one app.  I would like this to work on very old and slow computers too, so I&amp;#39;m hoping for a program that is fast but also small on disk space and easy to use.  Any advice for me on which ML packages I should use?  &lt;/p&gt;

&lt;p&gt;Thank you in advance for your help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn2mu,True,,tylersuard,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn2mu/advice_on_standalone_object_detection_app_please/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfn2mu/advice_on_standalone_object_detection_app_please/,155203,1588915564.0,0,,False,,,,
,learnmachinelearning,"I have a decent understanding of Python (OOP, Pandas, NumPy) and a solid foundation in Statistics and Linear Algebra. I want to learn predictive modeling and machine learning. As I have a decent understanding of the pre-requisites, what resources do you recommend I learn?",t2_6dx8f99v,False,,0,False,"I know most of the pre-reqs for Machine Learning, where to start?",[],r/learnmachinelearning,False,6,,0,,False,t3_gfn0c9,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588944055.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a decent understanding of Python (OOP, Pandas, NumPy) and a solid foundation in Statistics and Linear Algebra. I want to learn predictive modeling and machine learning. As I have a decent understanding of the pre-requisites, what resources do you recommend I learn?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfn0c9,True,,whyamisosmart,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfn0c9/i_know_most_of_the_prereqs_for_machine_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfn0c9/i_know_most_of_the_prereqs_for_machine_learning/,155203,1588915255.0,0,,False,,,,
,learnmachinelearning,,t2_4th82f0f,False,,0,False,Funny tweet,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_gfmtsf,False,dark,1.0,,public,1,0,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'height': 796}",140.0,,False,[],"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/richard_droste/status/1258096722657579008', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Richard Droste', 'height': 796, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/richard_droste', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,False,,"{'content': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'width': 350, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfmtsf', 'height': 796}",,False,1,,False,https://b.thumbs.redditmedia.com/wtOPds_RRS9_Q5rz0QI7h5_OpMJC34_rXqfaeCFKyOw.jpg,False,,[],{},link,,False,,1588943212.0,text,6,,,text,mobile.twitter.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?auto=webp&amp;s=a200776f1543e11890960170ad1b09247277d60a', 'width': 616, 'height': 1272}, 'resolutions': [{'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=909b8a123d9b6f8d1a69c873cf1ce907c725366a', 'width': 108, 'height': 216}, {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dbdcae36b58ab83f6a0f585e79d105dc56a66a39', 'width': 216, 'height': 432}, {'url': 'https://external-preview.redd.it/5zfA8zJo6_FvrKhu6L6Cfg93mcRMl6ay-AS-HDJahaE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99bf40978b5349d442655598351d3a5095194339', 'width': 320, 'height': 640}], 'variants': {}, 'id': 'ShGOari78UprytI0ZwV-IMroCLmhfmrrzgfTJKp91TE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfmtsf,True,,Jolly-Theory,,0,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfmtsf/funny_tweet/,all_ads,False,https://mobile.twitter.com/richard_droste/status/1258096722657579008,155203,1588914412.0,0,"{'oembed': {'provider_url': 'https://twitter.com', 'url': 'https://twitter.com/richard_droste/status/1258096722657579008', 'html': '&lt;blockquote class=""twitter-video""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Fixed it for AI &lt;a href=""https://t.co/N7NeQ43D3q""&gt;pic.twitter.com/N7NeQ43D3q&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard Droste (@richard_droste) &lt;a href=""https://twitter.com/richard_droste/status/1258096722657579008?ref_src=twsrc%5Etfw""&gt;May 6, 2020&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;\n', 'author_name': 'Richard Droste', 'height': 796, 'width': 350, 'version': '1.0', 'author_url': 'https://twitter.com/richard_droste', 'provider_name': 'Twitter', 'cache_age': 3153600000, 'type': 'rich'}, 'type': 'twitter.com'}",False,,,,
,learnmachinelearning,,t2_50tfqv5x,False,,0,False,Top Machine Learning Companies in India - 2020,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,93.0,False,t3_gft84z,False,light,0.14,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/OELzze_HNOwQrm2KcM_Lje29mcy0cYZfruX2IO9LbgU.jpg,False,,[],{},link,,False,,1588973285.0,richtext,6,,,text,mygreatlearning.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?auto=webp&amp;s=5b626a5b6565a84ef8db6cdebe0737409e6d9571', 'width': 1254, 'height': 837}, 'resolutions': [{'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=315dbf5e94e11aa8d0eff69b0d3ed384226085b7', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c07a8aa61da54a837ae1ab49737f85fa3d36c1b', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4a65015fe36f76b8066e9217db0d06dc11e8aceb', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=89f078bb5a516bb3d7fdf7ba5200c311f1c6aa9a', 'width': 640, 'height': 427}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=21c8109a5ea1d4b60a307f2282c82fb3cc01609a', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/fjULuOVqXjMzMQPeZ8ooWDXtEpdFChdgv7wynVpBN8s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96ceba3b0bccc7226332a715672a55980d15d767', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 't0IxogoYjl-0vChEZJ0MWwVXNi1hmRlisO8c6eb_y8U'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gft84z,True,,agarwalsimran,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gft84z/top_machine_learning_companies_in_india_2020/,all_ads,False,https://www.mygreatlearning.com/blog/top-machine-learning-companies-in-india/,155203,1588944485.0,0,,False,,,,
,learnmachinelearning,"Hello! 

Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups. 

I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I'm unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I'm trying to see how this would be changed to go to 25 groups. Any help or advice on where to look would be appreciated! Thanks 

    model = Sequential()
    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1))) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu')) 
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(10, activation='softmax'))",t2_7ajy1,False,,0,False,Looking for advice on how to set up parameters for CNN (sequential),"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfm7bn,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Question,False,0,,False,self,False,,[],{},,,True,,1588940306.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello! &lt;/p&gt;

&lt;p&gt;Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups. &lt;/p&gt;

&lt;p&gt;I understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I&amp;#39;m unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I&amp;#39;m trying to see how this would be changed to go to 25 groups. Any help or advice on where to look would be appreciated! Thanks &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (5, 5), activation=&amp;#39;relu&amp;#39;, input_shape=(120, 320, 1))) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;)) 
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation=&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation=&amp;#39;relu&amp;#39;))
model.add(Dense(10, activation=&amp;#39;softmax&amp;#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfm7bn,True,,MrMegaGamerz,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfm7bn/looking_for_advice_on_how_to_set_up_parameters/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfm7bn/looking_for_advice_on_how_to_set_up_parameters/,155203,1588911506.0,0,,False,,,,
,learnmachinelearning,"I'm working on a project to predict the likelihood of having heart disease based on 8 features from a dataset found on kaggle. The results are binary, between 0 and 1. After trying logistic regression and SVM, the best model has been random forest classifier, but I get suspiciously good results--model round 98% accuray. I added a cross\_val\_score, which I thought would give me more accurate results, but I still get in the high 90s. There's no way this can be true, as the dataset is very small, around 1025 rows. Is this high because the dataset is so small, or am I making an error in any of the steps that would be give suspicious results? Is overfitting a potention issue with random forest?

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
    
    from sklearn.ensemble import RandomForestClassifier
    classifier2 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')
    classifier2.fit(X_train, y_train)
    
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(classifier2, X, y, cv=5)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    
    ypredd = classifier2.predict(X_test)
    
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, ypredd)
    
    from sklearn.metrics import classification_report
    
    print(classification_report(y_test,ypredd))",t2_3pnizflv,False,,0,False,Random Forest Classifier too good--I'm suspicious,[],r/learnmachinelearning,False,6,,0,,False,t3_gflxig,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,1588911462.0,,[],{},,,True,,1588939100.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m working on a project to predict the likelihood of having heart disease based on 8 features from a dataset found on kaggle. The results are binary, between 0 and 1. After trying logistic regression and SVM, the best model has been random forest classifier, but I get suspiciously good results--model round 98% accuray. I added a cross_val_score, which I thought would give me more accurate results, but I still get in the high 90s. There&amp;#39;s no way this can be true, as the dataset is very small, around 1025 rows. Is this high because the dataset is so small, or am I making an error in any of the steps that would be give suspicious results? Is overfitting a potention issue with random forest?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

from sklearn.ensemble import RandomForestClassifier
classifier2 = RandomForestClassifier(n_estimators = 10, criterion = &amp;#39;entropy&amp;#39;)
classifier2.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(classifier2, X, y, cv=5)
print(&amp;quot;Accuracy: %0.2f (+/- %0.2f)&amp;quot; % (scores.mean(), scores.std() * 2))

ypredd = classifier2.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, ypredd)

from sklearn.metrics import classification_report

print(classification_report(y_test,ypredd))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gflxig,True,,Tyron_Slothrop,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gflxig/random_forest_classifier_too_goodim_suspicious/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gflxig/random_forest_classifier_too_goodim_suspicious/,155203,1588910300.0,0,,False,,,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#Studying and #Learning Effectively - a university guide,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,110.0,False,t3_gfkuq9,False,light,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Discussion,False,1,,True,https://b.thumbs.redditmedia.com/pgwfxutWY-G_dXN34KAKVpuEzp1Y6ibnE32S8enW7Tg.jpg,False,,[],{},,,False,,1588934561.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfkuq9,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkuq9/studying_and_learning_effectively_a_university/,all_ads,False,/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/,155203,1588905761.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University, Studying Effectively, 2020  [https://www.monash.edu/rlo/study-skills/studying-effectively](https://www.monash.edu/rlo/study-skills/studying-effectively) \n\nNew study patterns\n\nLectures and tutorials take up part of the day. You must plan your own long and short term timetables.\n\nFor every one-hour lecture or tutorial, about two hours of private study will be necessary.\n\nAssignments tend to be long and not frequent. They are usually set many weeks ahead.\n\nWide reading is essential. You may be given a reading list, but you should read other sources as well.\n\nLecture groups may be large. It is up to you to approach your lecturer or tutor if you are having difficulties.\n\nYou will have to identify and make notes on the main points in lectures and texts.\n\nYou must acknowledge all your sources. To avoid plagiarism, you will need to learn referencing skills.\n\nYou are expected to develop independent thinking by:\n\nmemorising information,\n\nasking questions,\n\nexamining evidence, and\n\nthinking critically.\n\n&amp;#x200B;\n\n \n\nManaging your time\n\nPlanning your semester\n\nIt is not uncommon to have several assignments due at the same time, and it is easy to run out of time without careful planning. All assignments due dates are listed in your unit guides, so you can make at least a draft version of this activity at the end of week 1.\n\nIt will help you plan ahead realistically so that you don\'t miss any deadlines.\n\n1. Write the name or code of each unit in the spaces across the top of the planner.\n\n2. For each unit, fill in the due dates of all tests and assignments in the appropriate week.\n\n3. For each assignment or test:  \n\n\na. Break down the preparation into a series of steps.  \nb. Consider how much time will be needed for each step and record your time estimates next to the tasks.  \nc. Working backwards from the due date, distribute the tasks in a logical order through the weeks available.\n\n4. Calculate the average number of hours you need to allocate each week to prepare for assignments and tests:\n\na. Add up the total number of hours needed in the first 6 weeks of the semester.  \nb. Divide the sum by the number of weeks.\n\nFor example:\n\n6 + 16 + 16 = 38 hours needed for assignments and test\n\n38 hours / 6 weeks = 6 hours and 20 min per week\n\nPlanning your week\n\nThis weekly schedule should be used flexibly; every week will be different. However, this example will give you an idea of what your \'average\' week should look like. If you can\'t fit everything in, you may need to consider reducing your extra-curricular activities during the semester.\n\n1. Enter all your classes, recording the unit code or name and class type.\n\n2. Enter any other commitments with set hours, e.g. club meetings, team sports practice, work.\n\n3. Block in at least 30 minutes preparation time for each class, recording the unit code or name and class type. Some units will need longer. Classes should be reviewed on the same day as much as possible.\n\n4. Allocate time to work on assignments or to study for tests. Consider your preferences: short bursts or long stretches?\n\n5. Block in your travel time (to and from uni, sports practice, work, etc.) and meal times. Allow time for cooking and cleaning up if necessary. Give yourself a lunch break even on busy days.\n\n6. Block in time for regular chores: shopping, washing, cleaning, etc. Be realistic: they take time.\n\n7. Now add time to relax at the end of the day and socialise with family and friends.\n\n8. Finally, make sure you have some free \'catch-up\' hours to allow for the unexpected.\n\n&amp;#x200B;\n\n Reference: Monash University, RLO Study Skills, 2020  [https://www.monash.edu/rlo/study-skills/reading-and-note-taking](https://www.monash.edu/rlo/study-skills/reading-and-note-taking) \n\nReading and note-taking\n\n Getting started\n\nThe amount of reading that you are expected to do at university can be daunting.\n\nHowever, with some preparation and adjustment to your reading techniques, you will soon be able to read effectively and efficiently.\n\n \n\nTop tip\n\nThe key to managing your reading load is to become an active reader - that is, you need to:\n\nask questions about what you need to know,\n\nselect readings that relate to your questions and tasks,\n\nand read them efficiently.\n\n \n\nActivity: Academic Reading\n\nTake this quiz to recognise your current level of interpreting academic reading.\n\n I have to read every book or article on my subject reading list. \n\nOption 1: True\n\nWrong answer.\n\n \n\nOption 2:  False\n\nCorrect answer.\n\nLarge reading lists can be very intimidating. You are not expected to read absolutely everything on your subject. University study requires a lot of reading within a limited time, so it is important to be selective about what you read. You need to make decisions about what is essential.\n\n&amp;#x200B;\n\n You can spend many hours reading, and feel as if you are being productive, but actually not get very far with learning, or achieving your study goal. \n\n Four essential pre-reading tips:  \n\n1. Know your purpose\n\nAlways read with a purpose in mind.\n\nFor example, are you reading to:\n\ngain an overview of the area of study?\n\nlocate specific information?\n\nunderstand difficult concepts?\n\nrevise for exams?\n\ncomplete a specific assignment?\n\nWhen you know why you are reading, you will be better equipped to decide how you are going to read (ie. Which reading strategies you need to use â€“ more on that later!)\n\n&amp;#x200B;\n\n \n\n2. Be selective with what you read and focus on the question or task\n\nOnce you know what you are looking for (i.e. have a purpose), you can start making some decisions about what is essential reading, and what can be saved for later.\n\nAsk yourself what you need to find out. Identify:\n\nKey topic words or phrases\n\nQuestions that you want to answer\n\nThen actively look for answers and evidence to support these answers.\n\nTip:\xa0\n\nhave a copy of your assignment question with you and read with it nearby, so you donâ€™t waste time reading irrelevant material.\n\n&amp;#x200B;\n\n \n\n3. Break reading into segments\n\nBreak things down to manageable chunks (e.g. number of pages, articles or chapters). Identify how much time you have and aim to read a certain number of pages, or for a set length of time.\n\n&amp;#x200B;\n\n \n\n4. Keep track of what you have read\n\nAlways note down where information and ideas have come from. This will also help when you need to reference the text. \xa0\n\nKeep track of details such as: \xa0\xa0\n\nAuthor\n\nTitle\n\nPlace of Publication\n\nDate\n\nPage numbers\n\n&amp;#x200B;\n\nReference: Monash University, RLO Study Skill, 2020,  [https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text) \n\n&amp;#x200B;\n\n \n\nApproaching and evaluating a text\n\n \n\nAs you become more familiar with the text of your specific discipline area (economics, history, engineering, etc.), you will become better at predicting the kinds of questions you might find answers to in the text.\n\nBeing able to make predictions is important. Creating certain expectations will increase your alertness to the text, and you will be better at noticing whether or not your expectations are met.\n\nThe accuracy of your prediction is not usually the most important thing. What is important is recognising what the text does or does not deal with.\n\n&amp;#x200B;\n\n \n\nActivity: Predicting\n\nSelect any of the subject areas below by clicking on the ðŸ“·.\n\nLook at the title of the book. Then consider what you already know about the topic. Think of some questions to which the book might supply answers.\n\n Agriculture in semi-arid environments\n\n... I don\'t have any specialised knowledge about agriculture, and especially agriculture in semi-arid environments, but I do have everyday knowledge. I know agriculture needs water, and very often lots of it. Knowing that leads me to ask the following questions:\n\nHow can agriculture survive in such dry places?\n\nIs it possible to develop agricultural practices that use less water, or is it possible to recycle water, or do farmers in such environments have to pipe in large amounts of water?\n\nThis leads to a further question:\n\nIf farmers have to pipe in water, and perhaps have to use lots of fertilisers to make the semi-arid soils nutritious and agriculturally productive, how can such agriculture be economically viable?\n\nThat makes me wonder:\n\nAre there other ways of naturally enriching soils through changing farming practices?\n\nNB. It is possible that none of these questions helps to focus the reader\'s attention and increase his/her ability to make sense of the text. As the reader looks for answers, more precise ideas will come to his/her attention, and as a result, more specific questions can then be asked...\n\n&amp;#x200B;\n\nCivil Engineering - Soil plasticity\n\n""I know nothing about soil plasticity, and therefore my questions are very basic. I need to know:\n\nIn what ways are soils \'plastic\'? What does plasticity mean here?\n\nWhat is the significance of soil plasticity?\n\nHow does soil plasticity affect engineering design or practices?\n\nIf I were knowledgeable about the subject, I could ask more specific questions based on that knowledge.""\n\n&amp;#x200B;\n\nLiterature -  Madness and sexual politics in the feminist novel\n\n... I have some background knowledge on this topic. I know for instance that sexual politics is about power relations between the two sexes. But I also know there are different theories about these power relations. So my first question might be:\n\nWhich theorists and theoretical approaches do this text draw on?\n\nI also know that there is no clear definition of what we mean by a feminist novel. So I might ask:\n\nHow does this writer define a feminist novel? What novels does s/he discuss?\n\nI know madness is usually the concern of medicine, but I also know that madness is of interest in humanities subjects. Some people argue that those who are mad do not necessarily have anything wrong with them, but that they are categorised as mad by society because they do not conform in certain ways. I suspect the writer of this article takes up this view of madness. I predict therefore that s/he will look at how women in novels become mad, and that s/he will link this to their resistance or nonconformity to struggle with male-dominated practices. Therefore I might ask:\n\nWhat is meant by madness in this text?\n\nIs female madness the result of male-oriented social relations? ...\n\n&amp;#x200B;\n\nEducation -  Gender issues in physics education\n\n""I know there has been a lot of concern about girls taking an interest in physics at school. I also know that while some people think this is because girls are naturally not interested, others argue it is because physics is seen as a masculine subject, and so girls do not become interested, even though they could actually do as well as boys. I suspect this article adopts this last view and therefore sees the way we create masculinity and femininity in our society (gender issues) as a central problem in physics education. Therefore I might ask:\n\nIs the gender issue discussed here the one I have predicted - that girls do not take up physics because it is not feminine? Or is it another issue?\n\nWhat exactly are the issues to be discussed? (Is there a list of them?)\n\nDoes the author only describe the issues, or does s/he also suggest ways of overcoming the gender problems?\n\nIf s/he offers solutions, do these lie in changing teaching methodologies, or changing the physics curriculum, or something else?""\n\n&amp;#x200B;\n\nPhilosophy -  Is it good to make people happy?\n\n... This looks like a strange topic. Everybody knows it is good to make people happy! But I know that philosophy wants to understand precisely why it might be good, and ethics wants to distinguish between good and bad actions. So a definition of good seems essential. Therefore, my first questions might be:\n\nWhat is meant by good in this context? Can I quickly find a definition?\n\nWhy is it good to make people happy, and not merely nice?\n\nI might also want to know about the link between goodness and happiness. If happiness is good, then presumably it should always be an aim of ours. But what happens if happiness is in conflict with justice? Pursuing justice can make some people very unhappy. So I might also ask:\n\nWhat happens if the principle of happiness conflicts with another principle, such as justice? If happiness gives way to justice, does this mean it is not good, but merely desirable?\n\nThe more we think about this topic, the more questions will come to mind, and so we can get deeper and deeper into the text ...\n\n&amp;#x200B;\n\nPsychology -  A theory of cultural values and some implications for work\n\nWhat is the theory of cultural values?\n\nHow can theory have implications for work?\n\nThese questions immediately come to my mind. A further question follows:\n\nWhat kind of theory does this article present?\n\nIt seems to me that cultural values include many things. Another question comes to mind:\n\nWhat cultural values exactly will this text discuss?\n\nThis leads to further questions:\n\nIs this text talking about cultural values that have a direct bearing on work?\n\nIf so, what aspects of work? Does it discuss the ways cultural values affect attitudes to work, or work efficiency, or the kinds of works that people will want to do?\n\nI find very quickly that I have a lot of questions. This title seems so general it is hard to predict what it is likely to be about. Therefore my questions are more to do with trying to define the scope of this text. Once I have some sense of its scope, then I might be able to make predictions and ask questions about the content ...\n\n&amp;#x200B;\n\n \n\nStrategies for effective reading\n\n \n\nThere are different strategies you can apply to your reading, depending on your goal. You might need to choose one or a few of these strategies for each text that you read.\n\nIn this section:\n\n[Previewing](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#previewing)\n\n[Skimming](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#skimming)\n\n[Scanning](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#scanning)\n\n[Detailed reading](https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#detailed_reading)\n\nPreviewing\n\nWhat is previewing?\n\nPreviewing is getting a sense of what\'s in a given piece of work without reading the body of the text\n\nWhen should you preview?\n\nPreviewing helps you decide whether a book or article is useful for your purpose. \xa0It gives you a general sense of the content so you can see if you want to read in more detail, and it helps you locate sections that you need to read, and sections you don\'t.\n\nHow to preview:\n\nRead the title and author details\n\nRead the abstract (if available)\n\nRead main headings, chapter summaries, and anything that \'jumps out\' at you\n\nLook at any diagrams, graphs, tables. \xa0These usually summarise the content of large written paragraphs.\n\n&amp;#x200B;\n\n \n\nWhat is skimming?\n\nSkimming is reading small amounts from throughout the text.\xa0It is different from previewing because you\'re reading the body of the text. The chief benefit of skimming is in being able to pick up the key ideas quickly. \xa0\n\nHow to skim\n\nIf the introduction is short read it in full. If long, read the first sentence of each paragraph. Then read the first sentence of each subsequent paragraph, or until you find the topic sentence (usually the first or second sentence).\xa0This will give you an overview of the content of the passage.\xa0It can also be useful to read the concluding paragraph in full.\n\nFor a report or research paper,\xa0first of all, read the Abstract. Then look over the section headings and subheadings and any figures or tables before skimming the text. It may also be useful to read the Conclusion.\n\nDon\'t get bogged down. \xa0This is a fast process.\n\n&amp;#x200B;\n\n \n\nWhen should you skim?\n\nWhen you want to get an overview or the gist of a text. This can help you decide whether or not to read the full text.\n\nSkimming adds to the information that you picked up in previewing.\n\n&amp;#x200B;\n\n \n\nActivity: Test your skimming skills\n\nIn the excerpt from a book chapter below, the first paragraph is presented in full. In the following paragraphs (2-6), only the topic sentences appear. You should still be able to get the gist of the full passage.\n\nRead the first paragraph and the topic sentence of each subsequent paragraph.\n\nYou should find you can easily answer the three questions which follow.\n\nUse the blue arrows to move through the quiz.\n\nChapter Twelve: Trade routes and rituals  \n1. Trade between distant people is often seen as a mark of a more advanced economic life. If this insight is valid, many groups of aboriginals must have been far from backward because their raw materials and manufactures were traded to people hundreds of miles away. It is probable that every tribe in Australia traded with its neighbours, and a few commodities were involved in such a sequence of transactions that they crossed from the tropical coast almost to the Southern Ocean.  \n2. Pearl shell travelled further perhaps than any other Item. (13 lines deleted)  \n3. In eastern Australia, the axe-stone also moved over a wide area. (6 lines deleted)  \n4. A quarry which provided stone fit for stronger, sharper axes was likely to supply trade routes stretching in every direction. (6 lines deleted)  \n5. As the written records were thin in tracing the trade-in stone axes from the Tamworth district; other ways of reconstructing the extent of the trade were needed. The petrological analysis was one promising technique.  \n6. This kind of archaeological jigsaw - the exact matching of axe and quarry - can be solved only when every likely source of stone has been discovered and described. (3 lines deleted) â€¦ axes had gone overland through a chain of tribal territories to Cobar, Bourke, Wilcannia, and other points on the plains as remote as 500 miles from the home quarries. (2 lines deleted)  \n(Blainey, G. (1975). Triumph of the nomads: A History of Ancient Australia. South Melbourne, Vic.: Macmillan. p. 203-204.)\n\n&amp;#x200B;\n\n What is the topic of the passage? \n\n \n\n1. The distribution of axe stone in eastern Australia.\n\nWrong answer.\n\nThis topic was used to illustrate the topic.\n\n&amp;#x200B;\n\n \n\n2. Aboriginal trade routes.\n\nWrong answer.\n\nThe focus is not so much on the routes themselves as their far-reaching range.\n\n&amp;#x200B;\n\n3.  Trade among indigenous populations.\n\nWrong answer.\n\nThis is true, but the scope in the passage is limited to Australia.\n\n&amp;#x200B;\n\n \n\n4.  The extent of trade among Australian aborigines \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n What trade items were discussed?  Axe Stone\n\n&amp;#x200B;\n\n \n\nWhat is scanning?\n\nYou skim-read material to get the general picture. \xa0\n\nYou scan when looking for specific information.\n\n&amp;#x200B;\n\n \n\nWhen should you scan?\n\nYou may need to find specific details on a topic for an assignment or a task that your lecturer has set. \xa0There is little point in skimming a whole book for this purpose. \xa0You should scan the text for words related to the topic. \xa0You can run your eyes down the page looking for these expressions - in chapter headings or sub-headings, or in the text itself.\n\n&amp;#x200B;\n\n \n\nActivity: Test your scanning skills\n\nYou need to find the definition of â€˜postmodernityâ€™ for an assignment.\n\nYour first step might be to look in the index for the word, and see if it can direct you to some definitions. However, if the whole book is about postmodernism, then you might have too many references to check. Another obvious place to look is in the Introduction.\n\nBelow is a section from the Introduction to Intimations of postmodernity.\n\nScan through to see if you can locate where in the text the author defines what he means by the term.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[ Adapted from Bauman, Z. \\(1992\\). Intimations of postmodernity. London: Routledge ](https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;format=png&amp;auto=webp&amp;s=bd823975e2c04aa78391017c3ab7eff261046b70)\n\n \n\nWhich sentence in the text does the author define postmodernity?\n\n7 Wrong answer.Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(\'perhaps more than anything else.\')\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n11 Correct answer. RIGHT ANSWER! In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(\'perhaps more than anything else.\')\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n3 Wrong answer. Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a \'state of mind\'\xa0(perhaps more than anything else)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of \'state of mind\'\xa0we may, as we skim on, notice comments such as - \'This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness\'. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.\n\n&amp;#x200B;\n\n \n\nWhat is a detailed reading?\n\nDetailed reading is intensive reading that focuses on the important parts - key chapters, books, poems, pages. This is when you are likely to take detailed notes.\n\n&amp;#x200B;\n\n \n\nWhen should you read in detail?\n\nIntensive reading is usually the final step - after you\'ve previewed, scanned and skimmed when you know that it is worth reading the full text.\xa0\n\nIn every unit of study, there will be key sources or texts that you will need to read carefully. \xa0\n\nExpect to read these more than once and to take notes on important material.\n\n You will often need to take notes while reading. This video on [Efficient note taking strategies](https://www.youtube.com/watch?v=XELOxGx_ZZg) will help you take better notes. \n\n&amp;#x200B;\n\n \n\nDeveloping your critical reading skills\n\n \n\nIn university study you will frequently hear your lecturer or tutor encouraging you to take a critical approach to reading and researching. A critical approach to reading does not mean criticising or \'finding fault\'. It means asking yourself why this particular author has written this particular text, and evaluating their strengths and weaknesses.\n\nYou can ask yourself many questions about the text - the more the better. These are shaped by what you already understand about the text, and what you need to get from it.\n\nðŸ“·\n\nConsider the following\n\nThere are several questions to consider when approaching your reading. The following examples will give you a few ideas on the types of questions you should be asking when reading:\n\nThe author:\n\n \n\nWhat is the author\'s main theme or point?\n\nIs the author making any particular assumptions? On what authority?\n\nWhich aspects does the author focus on and why? Does he/she omit any important points?\n\nAre there additional clues about the author\'s attitude or stance (e.g. from his/her position/qualifications, country of origin,\xa0the text\'s date of publication or publisher,\xa0the type of text)?\n\nWhat theoretical perspective has the author taken (e.g. which writers does she/he cite most often or most approvingly)?\n\nWhat basis or criteria is the author using to make judgements?\n\n&amp;#x200B;\n\nThe content:\n\n \n\nWhat is the main point, thesis or argument?\n\nWhat is the text really about (i.e. special agenda, underlying themes)?\n\nWhat explanations or supporting evidence are drawn on? Do they seem adequate, completely relevant?\n\nIs all the factual information correct as far as you know?\n\nWhat aspect of the topic has the author chosen to focus on? What has s/he omitted?\n\nWhat are the authorâ€™s assumptions? Are they explicitly or implicitly stated?\n\nIs there any evidence of deliberate bias?\n\nIs there any particular philosophy that influence the authorâ€™s view?\n\nDoes any idea/information interest, confuse or intrigue you?\n\n&amp;#x200B;\n\nThe structure:\n\n \n\nWhat is the structure of the text? What does the structure of the text reveal?\n\nIs the framework clear (e.g. different theories compared with a preferred theory)?\n\nHow is the content developed? Is the material developed historically, in order of importance, in terms of a debate?\n\nHow does the conclusion relate to the rest of the material? Does the conclusion work logically, and is it representative of the findings?\n\n&amp;#x200B;\n\nThe style:\n\n \n\nIn what style has the material been written? Eg. Is it formal, informal, analytical, narrative, persuasive, argumentative, or didactic?\n\nHow do the style and format influence your reaction to the material?\n\n&amp;#x200B;\n\n \n\nReading difficult material\n\n In the course of your studies, it is inevitable that you will come across dense and difficult reading material.\xa0\xa0It is easy to feel overwhelmed. But do not give up!\xa0\xa0The ability to unpack complex and \'dry\' material is an essential skill for academic work. \n\n&amp;#x200B;\n\nBreak your reading into portions:  Donâ€™t feel overwhelmed on the size or complexity of the reading material. Set yourself a goal of reading a section and work hard at just understanding that section. \n\n&amp;#x200B;\n\nSkim the text and get an overview:  Run your eyes over the titles, headings and abstract. Examine any graphs, diagrams, charts. Highlight and read the topic sentence (usually first sentence) of each paragraph. Look for any key words/phrases that are relevant to your topic or what you are wanting to find out. \n\n&amp;#x200B;\n\nFlag what you do not understand to re-read later.  Donâ€™t worry about the parts that you do not understand. A difficult text will always require more than one reading, and a partial understanding will make it easier when you revisit the material a second or third time. \n\n&amp;#x200B;\n\nFind some resources to help you understand.  You might need to find some other material to help you with the background to the reading. If you are having trouble with the vocabulary, find a subject-specific dictionary so you can understand the key words. If the difficult text youâ€™re reading is a seminal one for your topic, you might be able to find some reviews or critical articles analysing your text. \n\n&amp;#x200B;\n\nMake notes while you read.  It often helps to write while you are reading. Writing ideas down in your own words and organising information in a structure, summary or diagram that works for you will help you grasp the material. \n\n&amp;#x200B;\n\nTalk to others.  Talk through difficult material with your fellow students, lecturer or tutor. \n\n&amp;#x200B;\n\nDo not panic!  Put the book or article aside, and read it again the next day. This gives your brain a chance to process. You will be surprised by how much you can pick up a second time around! \n\n&amp;#x200B;\n\nEfficient note-taking strategies.  [https://www.youtube.com/watch?v=XELOxGx\\_ZZg](https://www.youtube.com/watch?v=XELOxGx_ZZg)', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#Studying and #Learning Effectively - a university guide', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 110, 'hide_score': False, 'media_metadata': {'lm9io3ptz8x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 85, 'x': 108, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a690ccf5a5f542b42df4179730935de73b5e186'}, {'y': 170, 'x': 216, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82d24e253426c7d434c77cede485f5f1d2e54341'}, {'y': 252, 'x': 320, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a0849ab09d8b68b80148c39d6b4e463837ac14e'}, {'y': 505, 'x': 640, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=876b545faf12a7079e3a6cb8fbf771c0b0bf6516'}], 's': {'y': 607, 'x': 768, 'u': 'https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;format=png&amp;auto=webp&amp;s=bd823975e2c04aa78391017c3ab7eff261046b70'}, 'id': 'lm9io3ptz8x41'}}, 'name': 't3_geyx3w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 10, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 10, 'approved_by': None, 'author_premium': True, 'thumbnail': 'https://b.thumbs.redditmedia.com/pgwfxutWY-G_dXN34KAKVpuEzp1Y6ibnE32S8enW7Tg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588850577.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University, Studying Effectively, 2020  &lt;a href=""https://www.monash.edu/rlo/study-skills/studying-effectively""&gt;https://www.monash.edu/rlo/study-skills/studying-effectively&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;New study patterns&lt;/p&gt;\n\n&lt;p&gt;Lectures and tutorials take up part of the day. You must plan your own long and short term timetables.&lt;/p&gt;\n\n&lt;p&gt;For every one-hour lecture or tutorial, about two hours of private study will be necessary.&lt;/p&gt;\n\n&lt;p&gt;Assignments tend to be long and not frequent. They are usually set many weeks ahead.&lt;/p&gt;\n\n&lt;p&gt;Wide reading is essential. You may be given a reading list, but you should read other sources as well.&lt;/p&gt;\n\n&lt;p&gt;Lecture groups may be large. It is up to you to approach your lecturer or tutor if you are having difficulties.&lt;/p&gt;\n\n&lt;p&gt;You will have to identify and make notes on the main points in lectures and texts.&lt;/p&gt;\n\n&lt;p&gt;You must acknowledge all your sources. To avoid plagiarism, you will need to learn referencing skills.&lt;/p&gt;\n\n&lt;p&gt;You are expected to develop independent thinking by:&lt;/p&gt;\n\n&lt;p&gt;memorising information,&lt;/p&gt;\n\n&lt;p&gt;asking questions,&lt;/p&gt;\n\n&lt;p&gt;examining evidence, and&lt;/p&gt;\n\n&lt;p&gt;thinking critically.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Managing your time&lt;/p&gt;\n\n&lt;p&gt;Planning your semester&lt;/p&gt;\n\n&lt;p&gt;It is not uncommon to have several assignments due at the same time, and it is easy to run out of time without careful planning. All assignments due dates are listed in your unit guides, so you can make at least a draft version of this activity at the end of week 1.&lt;/p&gt;\n\n&lt;p&gt;It will help you plan ahead realistically so that you don&amp;#39;t miss any deadlines.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Write the name or code of each unit in the spaces across the top of the planner.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For each unit, fill in the due dates of all tests and assignments in the appropriate week.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For each assignment or test:  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Break down the preparation into a series of steps.&lt;br/&gt;\nb. Consider how much time will be needed for each step and record your time estimates next to the tasks.&lt;br/&gt;\nc. Working backwards from the due date, distribute the tasks in a logical order through the weeks available.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Calculate the average number of hours you need to allocate each week to prepare for assignments and tests:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Add up the total number of hours needed in the first 6 weeks of the semester.&lt;br/&gt;\nb. Divide the sum by the number of weeks.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;p&gt;6 + 16 + 16 = 38 hours needed for assignments and test&lt;/p&gt;\n\n&lt;p&gt;38 hours / 6 weeks = 6 hours and 20 min per week&lt;/p&gt;\n\n&lt;p&gt;Planning your week&lt;/p&gt;\n\n&lt;p&gt;This weekly schedule should be used flexibly; every week will be different. However, this example will give you an idea of what your &amp;#39;average&amp;#39; week should look like. If you can&amp;#39;t fit everything in, you may need to consider reducing your extra-curricular activities during the semester.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Enter all your classes, recording the unit code or name and class type.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Enter any other commitments with set hours, e.g. club meetings, team sports practice, work.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in at least 30 minutes preparation time for each class, recording the unit code or name and class type. Some units will need longer. Classes should be reviewed on the same day as much as possible.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Allocate time to work on assignments or to study for tests. Consider your preferences: short bursts or long stretches?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in your travel time (to and from uni, sports practice, work, etc.) and meal times. Allow time for cooking and cleaning up if necessary. Give yourself a lunch break even on busy days.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Block in time for regular chores: shopping, washing, cleaning, etc. Be realistic: they take time.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Now add time to relax at the end of the day and socialise with family and friends.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finally, make sure you have some free &amp;#39;catch-up&amp;#39; hours to allow for the unexpected.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reference: Monash University, RLO Study Skills, 2020  &lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking""&gt;https://www.monash.edu/rlo/study-skills/reading-and-note-taking&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Reading and note-taking&lt;/p&gt;\n\n&lt;p&gt;Getting started&lt;/p&gt;\n\n&lt;p&gt;The amount of reading that you are expected to do at university can be daunting.&lt;/p&gt;\n\n&lt;p&gt;However, with some preparation and adjustment to your reading techniques, you will soon be able to read effectively and efficiently.&lt;/p&gt;\n\n&lt;p&gt;Top tip&lt;/p&gt;\n\n&lt;p&gt;The key to managing your reading load is to become an active reader - that is, you need to:&lt;/p&gt;\n\n&lt;p&gt;ask questions about what you need to know,&lt;/p&gt;\n\n&lt;p&gt;select readings that relate to your questions and tasks,&lt;/p&gt;\n\n&lt;p&gt;and read them efficiently.&lt;/p&gt;\n\n&lt;p&gt;Activity: Academic Reading&lt;/p&gt;\n\n&lt;p&gt;Take this quiz to recognise your current level of interpreting academic reading.&lt;/p&gt;\n\n&lt;p&gt;I have to read every book or article on my subject reading list. &lt;/p&gt;\n\n&lt;p&gt;Option 1: True&lt;/p&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;Option 2:  False&lt;/p&gt;\n\n&lt;p&gt;Correct answer.&lt;/p&gt;\n\n&lt;p&gt;Large reading lists can be very intimidating. You are not expected to read absolutely everything on your subject. University study requires a lot of reading within a limited time, so it is important to be selective about what you read. You need to make decisions about what is essential.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;You can spend many hours reading, and feel as if you are being productive, but actually not get very far with learning, or achieving your study goal. &lt;/p&gt;\n\n&lt;p&gt;Four essential pre-reading tips:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Know your purpose&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Always read with a purpose in mind.&lt;/p&gt;\n\n&lt;p&gt;For example, are you reading to:&lt;/p&gt;\n\n&lt;p&gt;gain an overview of the area of study?&lt;/p&gt;\n\n&lt;p&gt;locate specific information?&lt;/p&gt;\n\n&lt;p&gt;understand difficult concepts?&lt;/p&gt;\n\n&lt;p&gt;revise for exams?&lt;/p&gt;\n\n&lt;p&gt;complete a specific assignment?&lt;/p&gt;\n\n&lt;p&gt;When you know why you are reading, you will be better equipped to decide how you are going to read (ie. Which reading strategies you need to use â€“ more on that later!)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Be selective with what you read and focus on the question or task&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you know what you are looking for (i.e. have a purpose), you can start making some decisions about what is essential reading, and what can be saved for later.&lt;/p&gt;\n\n&lt;p&gt;Ask yourself what you need to find out. Identify:&lt;/p&gt;\n\n&lt;p&gt;Key topic words or phrases&lt;/p&gt;\n\n&lt;p&gt;Questions that you want to answer&lt;/p&gt;\n\n&lt;p&gt;Then actively look for answers and evidence to support these answers.&lt;/p&gt;\n\n&lt;p&gt;Tip:\xa0&lt;/p&gt;\n\n&lt;p&gt;have a copy of your assignment question with you and read with it nearby, so you donâ€™t waste time reading irrelevant material.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Break reading into segments&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Break things down to manageable chunks (e.g. number of pages, articles or chapters). Identify how much time you have and aim to read a certain number of pages, or for a set length of time.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Keep track of what you have read&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Always note down where information and ideas have come from. This will also help when you need to reference the text. \xa0&lt;/p&gt;\n\n&lt;p&gt;Keep track of details such as: \xa0\xa0&lt;/p&gt;\n\n&lt;p&gt;Author&lt;/p&gt;\n\n&lt;p&gt;Title&lt;/p&gt;\n\n&lt;p&gt;Place of Publication&lt;/p&gt;\n\n&lt;p&gt;Date&lt;/p&gt;\n\n&lt;p&gt;Page numbers&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reference: Monash University, RLO Study Skill, 2020,  &lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text""&gt;https://www.monash.edu/rlo/study-skills/reading-and-note-taking/approaching-and-evaluating-a-text&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Approaching and evaluating a text&lt;/p&gt;\n\n&lt;p&gt;As you become more familiar with the text of your specific discipline area (economics, history, engineering, etc.), you will become better at predicting the kinds of questions you might find answers to in the text.&lt;/p&gt;\n\n&lt;p&gt;Being able to make predictions is important. Creating certain expectations will increase your alertness to the text, and you will be better at noticing whether or not your expectations are met.&lt;/p&gt;\n\n&lt;p&gt;The accuracy of your prediction is not usually the most important thing. What is important is recognising what the text does or does not deal with.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Predicting&lt;/p&gt;\n\n&lt;p&gt;Select any of the subject areas below by clicking on the ðŸ“·.&lt;/p&gt;\n\n&lt;p&gt;Look at the title of the book. Then consider what you already know about the topic. Think of some questions to which the book might supply answers.&lt;/p&gt;\n\n&lt;p&gt;Agriculture in semi-arid environments&lt;/p&gt;\n\n&lt;p&gt;... I don&amp;#39;t have any specialised knowledge about agriculture, and especially agriculture in semi-arid environments, but I do have everyday knowledge. I know agriculture needs water, and very often lots of it. Knowing that leads me to ask the following questions:&lt;/p&gt;\n\n&lt;p&gt;How can agriculture survive in such dry places?&lt;/p&gt;\n\n&lt;p&gt;Is it possible to develop agricultural practices that use less water, or is it possible to recycle water, or do farmers in such environments have to pipe in large amounts of water?&lt;/p&gt;\n\n&lt;p&gt;This leads to a further question:&lt;/p&gt;\n\n&lt;p&gt;If farmers have to pipe in water, and perhaps have to use lots of fertilisers to make the semi-arid soils nutritious and agriculturally productive, how can such agriculture be economically viable?&lt;/p&gt;\n\n&lt;p&gt;That makes me wonder:&lt;/p&gt;\n\n&lt;p&gt;Are there other ways of naturally enriching soils through changing farming practices?&lt;/p&gt;\n\n&lt;p&gt;NB. It is possible that none of these questions helps to focus the reader&amp;#39;s attention and increase his/her ability to make sense of the text. As the reader looks for answers, more precise ideas will come to his/her attention, and as a result, more specific questions can then be asked...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Civil Engineering - Soil plasticity&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I know nothing about soil plasticity, and therefore my questions are very basic. I need to know:&lt;/p&gt;\n\n&lt;p&gt;In what ways are soils &amp;#39;plastic&amp;#39;? What does plasticity mean here?&lt;/p&gt;\n\n&lt;p&gt;What is the significance of soil plasticity?&lt;/p&gt;\n\n&lt;p&gt;How does soil plasticity affect engineering design or practices?&lt;/p&gt;\n\n&lt;p&gt;If I were knowledgeable about the subject, I could ask more specific questions based on that knowledge.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Literature -  Madness and sexual politics in the feminist novel&lt;/p&gt;\n\n&lt;p&gt;... I have some background knowledge on this topic. I know for instance that sexual politics is about power relations between the two sexes. But I also know there are different theories about these power relations. So my first question might be:&lt;/p&gt;\n\n&lt;p&gt;Which theorists and theoretical approaches do this text draw on?&lt;/p&gt;\n\n&lt;p&gt;I also know that there is no clear definition of what we mean by a feminist novel. So I might ask:&lt;/p&gt;\n\n&lt;p&gt;How does this writer define a feminist novel? What novels does s/he discuss?&lt;/p&gt;\n\n&lt;p&gt;I know madness is usually the concern of medicine, but I also know that madness is of interest in humanities subjects. Some people argue that those who are mad do not necessarily have anything wrong with them, but that they are categorised as mad by society because they do not conform in certain ways. I suspect the writer of this article takes up this view of madness. I predict therefore that s/he will look at how women in novels become mad, and that s/he will link this to their resistance or nonconformity to struggle with male-dominated practices. Therefore I might ask:&lt;/p&gt;\n\n&lt;p&gt;What is meant by madness in this text?&lt;/p&gt;\n\n&lt;p&gt;Is female madness the result of male-oriented social relations? ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Education -  Gender issues in physics education&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I know there has been a lot of concern about girls taking an interest in physics at school. I also know that while some people think this is because girls are naturally not interested, others argue it is because physics is seen as a masculine subject, and so girls do not become interested, even though they could actually do as well as boys. I suspect this article adopts this last view and therefore sees the way we create masculinity and femininity in our society (gender issues) as a central problem in physics education. Therefore I might ask:&lt;/p&gt;\n\n&lt;p&gt;Is the gender issue discussed here the one I have predicted - that girls do not take up physics because it is not feminine? Or is it another issue?&lt;/p&gt;\n\n&lt;p&gt;What exactly are the issues to be discussed? (Is there a list of them?)&lt;/p&gt;\n\n&lt;p&gt;Does the author only describe the issues, or does s/he also suggest ways of overcoming the gender problems?&lt;/p&gt;\n\n&lt;p&gt;If s/he offers solutions, do these lie in changing teaching methodologies, or changing the physics curriculum, or something else?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Philosophy -  Is it good to make people happy?&lt;/p&gt;\n\n&lt;p&gt;... This looks like a strange topic. Everybody knows it is good to make people happy! But I know that philosophy wants to understand precisely why it might be good, and ethics wants to distinguish between good and bad actions. So a definition of good seems essential. Therefore, my first questions might be:&lt;/p&gt;\n\n&lt;p&gt;What is meant by good in this context? Can I quickly find a definition?&lt;/p&gt;\n\n&lt;p&gt;Why is it good to make people happy, and not merely nice?&lt;/p&gt;\n\n&lt;p&gt;I might also want to know about the link between goodness and happiness. If happiness is good, then presumably it should always be an aim of ours. But what happens if happiness is in conflict with justice? Pursuing justice can make some people very unhappy. So I might also ask:&lt;/p&gt;\n\n&lt;p&gt;What happens if the principle of happiness conflicts with another principle, such as justice? If happiness gives way to justice, does this mean it is not good, but merely desirable?&lt;/p&gt;\n\n&lt;p&gt;The more we think about this topic, the more questions will come to mind, and so we can get deeper and deeper into the text ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Psychology -  A theory of cultural values and some implications for work&lt;/p&gt;\n\n&lt;p&gt;What is the theory of cultural values?&lt;/p&gt;\n\n&lt;p&gt;How can theory have implications for work?&lt;/p&gt;\n\n&lt;p&gt;These questions immediately come to my mind. A further question follows:&lt;/p&gt;\n\n&lt;p&gt;What kind of theory does this article present?&lt;/p&gt;\n\n&lt;p&gt;It seems to me that cultural values include many things. Another question comes to mind:&lt;/p&gt;\n\n&lt;p&gt;What cultural values exactly will this text discuss?&lt;/p&gt;\n\n&lt;p&gt;This leads to further questions:&lt;/p&gt;\n\n&lt;p&gt;Is this text talking about cultural values that have a direct bearing on work?&lt;/p&gt;\n\n&lt;p&gt;If so, what aspects of work? Does it discuss the ways cultural values affect attitudes to work, or work efficiency, or the kinds of works that people will want to do?&lt;/p&gt;\n\n&lt;p&gt;I find very quickly that I have a lot of questions. This title seems so general it is hard to predict what it is likely to be about. Therefore my questions are more to do with trying to define the scope of this text. Once I have some sense of its scope, then I might be able to make predictions and ask questions about the content ...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Strategies for effective reading&lt;/p&gt;\n\n&lt;p&gt;There are different strategies you can apply to your reading, depending on your goal. You might need to choose one or a few of these strategies for each text that you read.&lt;/p&gt;\n\n&lt;p&gt;In this section:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#previewing""&gt;Previewing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#skimming""&gt;Skimming&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#scanning""&gt;Scanning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/reading-and-note-taking/effective-reading-strategies#detailed_reading""&gt;Detailed reading&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Previewing&lt;/p&gt;\n\n&lt;p&gt;What is previewing?&lt;/p&gt;\n\n&lt;p&gt;Previewing is getting a sense of what&amp;#39;s in a given piece of work without reading the body of the text&lt;/p&gt;\n\n&lt;p&gt;When should you preview?&lt;/p&gt;\n\n&lt;p&gt;Previewing helps you decide whether a book or article is useful for your purpose. \xa0It gives you a general sense of the content so you can see if you want to read in more detail, and it helps you locate sections that you need to read, and sections you don&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;How to preview:&lt;/p&gt;\n\n&lt;p&gt;Read the title and author details&lt;/p&gt;\n\n&lt;p&gt;Read the abstract (if available)&lt;/p&gt;\n\n&lt;p&gt;Read main headings, chapter summaries, and anything that &amp;#39;jumps out&amp;#39; at you&lt;/p&gt;\n\n&lt;p&gt;Look at any diagrams, graphs, tables. \xa0These usually summarise the content of large written paragraphs.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is skimming?&lt;/p&gt;\n\n&lt;p&gt;Skimming is reading small amounts from throughout the text.\xa0It is different from previewing because you&amp;#39;re reading the body of the text. The chief benefit of skimming is in being able to pick up the key ideas quickly. \xa0&lt;/p&gt;\n\n&lt;p&gt;How to skim&lt;/p&gt;\n\n&lt;p&gt;If the introduction is short read it in full. If long, read the first sentence of each paragraph. Then read the first sentence of each subsequent paragraph, or until you find the topic sentence (usually the first or second sentence).\xa0This will give you an overview of the content of the passage.\xa0It can also be useful to read the concluding paragraph in full.&lt;/p&gt;\n\n&lt;p&gt;For a report or research paper,\xa0first of all, read the Abstract. Then look over the section headings and subheadings and any figures or tables before skimming the text. It may also be useful to read the Conclusion.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t get bogged down. \xa0This is a fast process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you skim?&lt;/p&gt;\n\n&lt;p&gt;When you want to get an overview or the gist of a text. This can help you decide whether or not to read the full text.&lt;/p&gt;\n\n&lt;p&gt;Skimming adds to the information that you picked up in previewing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Test your skimming skills&lt;/p&gt;\n\n&lt;p&gt;In the excerpt from a book chapter below, the first paragraph is presented in full. In the following paragraphs (2-6), only the topic sentences appear. You should still be able to get the gist of the full passage.&lt;/p&gt;\n\n&lt;p&gt;Read the first paragraph and the topic sentence of each subsequent paragraph.&lt;/p&gt;\n\n&lt;p&gt;You should find you can easily answer the three questions which follow.&lt;/p&gt;\n\n&lt;p&gt;Use the blue arrows to move through the quiz.&lt;/p&gt;\n\n&lt;p&gt;Chapter Twelve: Trade routes and rituals&lt;br/&gt;\n1. Trade between distant people is often seen as a mark of a more advanced economic life. If this insight is valid, many groups of aboriginals must have been far from backward because their raw materials and manufactures were traded to people hundreds of miles away. It is probable that every tribe in Australia traded with its neighbours, and a few commodities were involved in such a sequence of transactions that they crossed from the tropical coast almost to the Southern Ocean.&lt;br/&gt;\n2. Pearl shell travelled further perhaps than any other Item. (13 lines deleted)&lt;br/&gt;\n3. In eastern Australia, the axe-stone also moved over a wide area. (6 lines deleted)&lt;br/&gt;\n4. A quarry which provided stone fit for stronger, sharper axes was likely to supply trade routes stretching in every direction. (6 lines deleted)&lt;br/&gt;\n5. As the written records were thin in tracing the trade-in stone axes from the Tamworth district; other ways of reconstructing the extent of the trade were needed. The petrological analysis was one promising technique.&lt;br/&gt;\n6. This kind of archaeological jigsaw - the exact matching of axe and quarry - can be solved only when every likely source of stone has been discovered and described. (3 lines deleted) â€¦ axes had gone overland through a chain of tribal territories to Cobar, Bourke, Wilcannia, and other points on the plains as remote as 500 miles from the home quarries. (2 lines deleted)&lt;br/&gt;\n(Blainey, G. (1975). Triumph of the nomads: A History of Ancient Australia. South Melbourne, Vic.: Macmillan. p. 203-204.)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is the topic of the passage? &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The distribution of axe stone in eastern Australia.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;This topic was used to illustrate the topic.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Aboriginal trade routes.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;The focus is not so much on the routes themselves as their far-reaching range.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; Trade among indigenous populations.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Wrong answer.&lt;/p&gt;\n\n&lt;p&gt;This is true, but the scope in the passage is limited to Australia.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; The extent of trade among Australian aborigines &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What trade items were discussed?  Axe Stone&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is scanning?&lt;/p&gt;\n\n&lt;p&gt;You skim-read material to get the general picture. \xa0&lt;/p&gt;\n\n&lt;p&gt;You scan when looking for specific information.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you scan?&lt;/p&gt;\n\n&lt;p&gt;You may need to find specific details on a topic for an assignment or a task that your lecturer has set. \xa0There is little point in skimming a whole book for this purpose. \xa0You should scan the text for words related to the topic. \xa0You can run your eyes down the page looking for these expressions - in chapter headings or sub-headings, or in the text itself.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Activity: Test your scanning skills&lt;/p&gt;\n\n&lt;p&gt;You need to find the definition of â€˜postmodernityâ€™ for an assignment.&lt;/p&gt;\n\n&lt;p&gt;Your first step might be to look in the index for the word, and see if it can direct you to some definitions. However, if the whole book is about postmodernism, then you might have too many references to check. Another obvious place to look is in the Introduction.&lt;/p&gt;\n\n&lt;p&gt;Below is a section from the Introduction to Intimations of postmodernity.&lt;/p&gt;\n\n&lt;p&gt;Scan through to see if you can locate where in the text the author defines what he means by the term.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/lm9io3ptz8x41.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd823975e2c04aa78391017c3ab7eff261046b70""&gt; Adapted from Bauman, Z. (1992). Intimations of postmodernity. London: Routledge &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Which sentence in the text does the author define postmodernity?&lt;/p&gt;\n\n&lt;p&gt;7 Wrong answer.Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(&amp;#39;perhaps more than anything else.&amp;#39;)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;11 Correct answer. RIGHT ANSWER! In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(&amp;#39;perhaps more than anything else.&amp;#39;)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;3 Wrong answer. Incorrect -\xa0 In Sentence 11, the author suggests what he thinks is the most important meaning of postmodernity: It is a &amp;#39;state of mind&amp;#39;\xa0(perhaps more than anything else)\xa0- the state of those minds that think about what present life is like. The first paragraph you will have realised is where the author describes what other people say postmodernity means. Having recognised the importance of &amp;#39;state of mind&amp;#39;\xa0we may, as we skim on, notice comments such as - &amp;#39;This is a state of mind marked above all by its all-deriding, all-eroding, all-dissolving destructiveness&amp;#39;. As we continue we will recognise other relevant comments on what this author takes postmodernity to be.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What is a detailed reading?&lt;/p&gt;\n\n&lt;p&gt;Detailed reading is intensive reading that focuses on the important parts - key chapters, books, poems, pages. This is when you are likely to take detailed notes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When should you read in detail?&lt;/p&gt;\n\n&lt;p&gt;Intensive reading is usually the final step - after you&amp;#39;ve previewed, scanned and skimmed when you know that it is worth reading the full text.\xa0&lt;/p&gt;\n\n&lt;p&gt;In every unit of study, there will be key sources or texts that you will need to read carefully. \xa0&lt;/p&gt;\n\n&lt;p&gt;Expect to read these more than once and to take notes on important material.&lt;/p&gt;\n\n&lt;p&gt;You will often need to take notes while reading. This video on &lt;a href=""https://www.youtube.com/watch?v=XELOxGx_ZZg""&gt;Efficient note taking strategies&lt;/a&gt; will help you take better notes. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Developing your critical reading skills&lt;/p&gt;\n\n&lt;p&gt;In university study you will frequently hear your lecturer or tutor encouraging you to take a critical approach to reading and researching. A critical approach to reading does not mean criticising or &amp;#39;finding fault&amp;#39;. It means asking yourself why this particular author has written this particular text, and evaluating their strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;You can ask yourself many questions about the text - the more the better. These are shaped by what you already understand about the text, and what you need to get from it.&lt;/p&gt;\n\n&lt;p&gt;ðŸ“·&lt;/p&gt;\n\n&lt;p&gt;Consider the following&lt;/p&gt;\n\n&lt;p&gt;There are several questions to consider when approaching your reading. The following examples will give you a few ideas on the types of questions you should be asking when reading:&lt;/p&gt;\n\n&lt;p&gt;The author:&lt;/p&gt;\n\n&lt;p&gt;What is the author&amp;#39;s main theme or point?&lt;/p&gt;\n\n&lt;p&gt;Is the author making any particular assumptions? On what authority?&lt;/p&gt;\n\n&lt;p&gt;Which aspects does the author focus on and why? Does he/she omit any important points?&lt;/p&gt;\n\n&lt;p&gt;Are there additional clues about the author&amp;#39;s attitude or stance (e.g. from his/her position/qualifications, country of origin,\xa0the text&amp;#39;s date of publication or publisher,\xa0the type of text)?&lt;/p&gt;\n\n&lt;p&gt;What theoretical perspective has the author taken (e.g. which writers does she/he cite most often or most approvingly)?&lt;/p&gt;\n\n&lt;p&gt;What basis or criteria is the author using to make judgements?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The content:&lt;/p&gt;\n\n&lt;p&gt;What is the main point, thesis or argument?&lt;/p&gt;\n\n&lt;p&gt;What is the text really about (i.e. special agenda, underlying themes)?&lt;/p&gt;\n\n&lt;p&gt;What explanations or supporting evidence are drawn on? Do they seem adequate, completely relevant?&lt;/p&gt;\n\n&lt;p&gt;Is all the factual information correct as far as you know?&lt;/p&gt;\n\n&lt;p&gt;What aspect of the topic has the author chosen to focus on? What has s/he omitted?&lt;/p&gt;\n\n&lt;p&gt;What are the authorâ€™s assumptions? Are they explicitly or implicitly stated?&lt;/p&gt;\n\n&lt;p&gt;Is there any evidence of deliberate bias?&lt;/p&gt;\n\n&lt;p&gt;Is there any particular philosophy that influence the authorâ€™s view?&lt;/p&gt;\n\n&lt;p&gt;Does any idea/information interest, confuse or intrigue you?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The structure:&lt;/p&gt;\n\n&lt;p&gt;What is the structure of the text? What does the structure of the text reveal?&lt;/p&gt;\n\n&lt;p&gt;Is the framework clear (e.g. different theories compared with a preferred theory)?&lt;/p&gt;\n\n&lt;p&gt;How is the content developed? Is the material developed historically, in order of importance, in terms of a debate?&lt;/p&gt;\n\n&lt;p&gt;How does the conclusion relate to the rest of the material? Does the conclusion work logically, and is it representative of the findings?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The style:&lt;/p&gt;\n\n&lt;p&gt;In what style has the material been written? Eg. Is it formal, informal, analytical, narrative, persuasive, argumentative, or didactic?&lt;/p&gt;\n\n&lt;p&gt;How do the style and format influence your reaction to the material?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Reading difficult material&lt;/p&gt;\n\n&lt;p&gt;In the course of your studies, it is inevitable that you will come across dense and difficult reading material.\xa0\xa0It is easy to feel overwhelmed. But do not give up!\xa0\xa0The ability to unpack complex and &amp;#39;dry&amp;#39; material is an essential skill for academic work. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Break your reading into portions:  Donâ€™t feel overwhelmed on the size or complexity of the reading material. Set yourself a goal of reading a section and work hard at just understanding that section. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Skim the text and get an overview:  Run your eyes over the titles, headings and abstract. Examine any graphs, diagrams, charts. Highlight and read the topic sentence (usually first sentence) of each paragraph. Look for any key words/phrases that are relevant to your topic or what you are wanting to find out. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Flag what you do not understand to re-read later.  Donâ€™t worry about the parts that you do not understand. A difficult text will always require more than one reading, and a partial understanding will make it easier when you revisit the material a second or third time. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Find some resources to help you understand.  You might need to find some other material to help you with the background to the reading. If you are having trouble with the vocabulary, find a subject-specific dictionary so you can understand the key words. If the difficult text youâ€™re reading is a seminal one for your topic, you might be able to find some reviews or critical articles analysing your text. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Make notes while you read.  It often helps to write while you are reading. Writing ideas down in your own words and organising information in a structure, summary or diagram that works for you will help you grasp the material. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Talk to others.  Talk through difficult material with your fellow students, lecturer or tutor. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do not panic!  Put the book or article aside, and read it again the next day. This gives your brain a chance to process. You will be surprised by how much you can pick up a second time around! &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Efficient note-taking strategies.  &lt;a href=""https://www.youtube.com/watch?v=XELOxGx_ZZg""&gt;https://www.youtube.com/watch?v=XELOxGx_ZZg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'geyx3w', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/geyx3w/studying_and_learning_effectively_a_university/', 'subreddit_subscribers': 33, 'created_utc': 1588821777.0, 'num_crossposts': 43, 'media': None, 'is_video': False}]",t3_geyx3w,,
,learnmachinelearning,,t2_61mllz70,False,,0,False,#studying #learning - How to Learn Effectively at University,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfh2kh,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,True,default,False,,[],{},,,False,,1588920520.0,richtext,6,,,text,self.ideas_jianfa,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfh2kh,True,,jianfa-ben-tsai,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh2kh/studying_learning_how_to_learn_effectively_at/,all_ads,False,/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/,155203,1588891720.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ideas_jianfa', 'selftext': 'Reference: Monash University RLO Study Skills, 2020,  [https://www.monash.edu/rlo/study-skills/learning-at-university](https://www.monash.edu/rlo/study-skills/learning-at-university)\n\nMaking useful study notes\n\nWhy do you need to make notes?\n\nStudy notes are a way for you to summarise and synthesise the material that you are learning or collecting for a written assignment. A key point for making notes is that you need to re-write the material\xa0in your own words.\n\nTypes of notes: Flowcharts or process notes\n\nFlowcharts and similar process notes can be useful when making notes for certain processes or methods.\n\nExample 1: Historical timelines\n\nExample 2: Steps in a laboratory method\n\nExample 3: Mind Maps - For more information, visit [Brainstorming: Mind mapping](https://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/brainstorming-and-mind-mapping)\n\nTypes of notes: Cornell notes\n\nTo make Cornell notes, divide the page up into three areas:\n\nA larger notes column (on the right of the page)\n\nA smaller recall column (on the left of the page)\n\nA summary section (at the bottom of the page)\n\nNotes column: In the right-hand column, write down the bulk of the notes from your lectures or study materials as you are reading your textbook or online materials.\n\nRecall column: After you finish a chapter, section or study session, note down any questions, keywords and main ideas in the recall column.\n\nSummary section: At the bottom of the page, the summary section is used to summarise all the notes from the page, to make sense of the material and make revision easier.\n\nUsing your notes\n\nFor the study, revisit your notes:\n\nthe day after you wrote them,\n\nthe following week,\n\nat least once the following month.\n\nThis helps with your memory, meaning you will be better prepared for your exams!\n\nTip: If you are making research notes or using multiple different sources, also note down the reference details!\n\n&amp;#x200B;\n\nParticipating in tutorials\n\nTutorials are useful for:\n\nconsolidating your understanding of a topic/reading/lecture.\n\nExamining a topic critically by\n\nasking questions\n\nreflecting on the material\n\nmaking links to related topics/ideas.\n\nAttendance\n\nRegular attendance is advisable. Note that for some classes attendance is compulsory, while for others there can be a minimum number of tutorials that a student must attend. In some subjects, marks are awarded for student participation. Check with your tutor or in your unit guide.\n\nExpectations\n\nYou will be expected to:\n\nanswer questions from the lecturer, tutor and/or other students\n\ncontribute to the discussion by sharing and comparing ideas\n\ntake ownership of your learning by seeking clarification of any material you do not fully understanding.\n\nPreparation\n\nIt is essential to complete all set tasks, such as the required reading or set questions, before your tutorial so that you can participate fully in the discussion. Reflect on unit learning objectives and think of questions arising from the reading or the lecture that you can ask during the tutorial.\n\nTo contribute:\n\nThink of several questions/examples/comments you would like to make.\n\nLook for pauses during the tutorial session to enable you to enter the discussion.\n\nIndicate that you want to speak by making eye contact with the tutor or by taking a more alert body posture.\n\nYou can enter the discussion by:\n\nagreeing with what someone has said and adding your own thoughts\n\nE.g. ""I agree with what \\[NAME\\] said about \\_\\_\\_. In my viewâ€¦""\n\ndisagreeing with what someone has said and providing reasons for your position\n\nE.g. ""I think \\[NAME\\] made an interesting point; however, in my opinionâ€¦""\n\nraising new points/questions.\n\nE.g. ""I think that one aspect we have not considered isâ€¦\'\n\nlinking the discussion back to the weekly readings\n\nE.g. ""According to \\[AUTHOR\\]â€¦"" or ""In her article, \\[AUTHOR\\] argued thatâ€¦""\n\nAfter the tutorial:\n\nContinue the discussion with your classmates.\n\nContact the tutor to clarify any important points you didn\'t understand.\n\nFinish any unfinished work.\n\nRevise. Sit down and consolidate the concepts that you have learned.\n\nEnter questions in your notebook; write answers.\n\nCheck terms/ jargon.\n\n&amp;#x200B;\n\nLearning in practical environments\n\nThroughout your studies, you will perform exercises in practical environments such as laboratories, site visits or fieldwork.\n\nPractical exercises are designed to help you:\n\napply your theoretical understanding in a tactile way\n\nexpand your theoretical understanding\n\nuse techniques and equipment safely and correctly\n\ndevelop skills such as time management.\n\nMost practical exercises consist of three major parts.\n\nPreliminary work or â€˜pre-labsâ€™, which involve preparing for the exercise.\n\nThe exercise itself, where you will perform the activity under the guidance of an educator.\n\nA post-exercise submission to be completed within the session or at a later date. These can take the form of work such as reports or presentations.\n\nYou will be provided with detailed guidelines for each exercise you undertake. Below are some general tips on how to prepare for your exercise, minimise stress during your session, and complete any submissions.\n\nBefore the exercise:\n\nRead the materials and procedures for your exercise 2 to 3 days prior to your session (highlight key concepts, procedures and measurements).\n\nAsk yourself: what am I trying to determine in this exercise?\n\nIf a method is provided summarise the key steps in a dedicated notebook.\n\nComplete and pass any pre-exercise activities such as quizzes, risk assessments or additional reading.\n\nDuring the exercise:\n\nArrive on time! Educators generally provide context, useful tips and safety warnings at the start of the session.\n\nAsk your educator for clarification of the material and procedures.\n\nDiscuss your understanding and predictions with other students and the educator.\n\nTake detailed notes of procedures, observations and measurements in a dedicated notebook.\n\nAfter the exercise:\n\nConsult the assessment guide and/or marking rubric for any required submissions.\n\nClarify points of confusion with your educators and/or other students (unit forums are an ideal place).\n\nComplete any required submissions within the time allocated.\n\n&amp;#x200B;\n\nA guide to group work\n\nWhy work in a group or a team?\n\nDevelops excellent graduate attributes:\n\ndecision making and problem-solving skills\n\nproject management and organisational skills\n\ncommunication and conflict resolution skills.\n\nThe process\n\n1. Build a strong foundation\n\nGet to know your team.\n\nDiscuss strengths and weaknesses.\n\nMeet early and set rules.\n\nAgree on the aims, scope, and quality of the project.\n\n2. Get organised\n\nDecide on a method of group communication.\n\nAgree on who will do what.\n\nSet early deadlines.\n\nPlan on how to present the project as a unified work.\n\n3. Hold productive meetings\n\nMake sure everyone knows the goal of the meeting, and what to bring.\n\nHave a leader to keep the meeting on track.\n\nHave a note-taker to record decisions.\n\n4. Maintain relationships\n\nResolve problems as a group. Don\'t exclude others.\n\nAddress concerns as soon as they appear.\n\nRenegotiate as needed.\n\nUnderstand the other person\'s point of view.\n\nLeadership\n\nMeans\n\nhelping the group work efficiently,\n\nmonitoring progress,\n\nknowing when a team member needs help, and\n\nkeeping the group motivated.\n\nDoes NOT mean\n\ndoing all the work,\n\nmaking decisions and telling others what to do, or\n\nbeing solely responsible for the success or failure of the project.\n\nEveryone is responsible for the success or failure of the project, not individual team members.\n\nDifficulties\n\nYou can\'t work as quickly in a group as you can by yourself.\n\nGroup/team members may have conflicting ideas or viewpoints.\n\nGroup/team members may not contribute equally.\n\nFor successful group and teamwork\n\nBe patient, demonstrate good communication skills and be committed.\n\nFocus on the process rather than just the end product.\n\nCollaborating and communicating online\n\nIntroduction\n\nDuring your university studies:\n\nYou may need to complete group assessment tasks.\xa0Using\xa0online meeting and collaboration tools can make this process easier, particularly if you are not able to meet face-to-face.\n\nYou may be asked to comment on online discussion boards (including Moodle forums) as part of your unit assessment tasks.\n\nOnline communication and collaboration tools are useful during your studies as they can help you to work on group projects, share ideas and information, and provide encouragement and feedback on each otherâ€™s work.\n\nCommunicating and collaborating online are also important skills in the workplace, as many workplaces require staff to work in groups on shared tasks, often across multiple locations. The ability to effectively use online meeting and collaboration tools is a key employability skill.\n\nYou may already be using social networking tools such as Facebook to work on group projects.\xa0However, there are some key differences between using social networking tools for informal activities and using these tools informal study (e.g. assessment) or work activities.\xa0These differences include the language used and the structure of the interactions in an online meeting.\xa0For example, an online meeting for a group assessment task or a workplace shared task will often use an agenda and have a person designated as the meeting chair.\n\nThis module includes tips for effectively communicating and collaborating when:\n\n[participating in online meetings](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#meetings)\n\n[using online discussion boards](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#discussion), for example, Moodle forums\n\n[using online collaboration tools](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#tools).\n\nCommunicating online - key terms\n\n[Synchronous and asynchronous communication](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Synchronous_and_asynchronous_communication-1)\n\n[Verbal and non-verbal communication](https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Verbal_and_non-verbal_communication-2)\n\nWithout non-verbal communication cues, it can be very difficult for your readers to detect sarcasm and other hidden meanings, and your words may be interpreted as being more unfriendly or bossy than intended. Use the following activity to explore some ways of communicating online effectively.\n\n You disagree with the statements made in a discussion forum post from another student in your class. \xa0Which of the following responses might be better? \n\n1.  One point that you might like to consider is â€¦, which was included in this weekâ€™s reading.\n2. No, you are wrong. \xa0You need to read this weekâ€™s required reading to see why you are wrong..\n\nDiscussion on 1:  Good choice! By using this type of response, you are indicating that you do not fully agree with the post, but in a way that is not direct and bossy. \xa0You are also giving the other student some useful information by referring directly to the point raised in the reading. \n\nDiscussion on 2:  This is probably not the best choice. \xa0This response uses direct language and may be interpreted as unfriendly and bossy. \xa0The student who you are responding to may ignore your feedback as it doesnâ€™t give them much useful information (other than that they need to do the reading).\xa0 Your feedback is more likely to be heeded when you use cautious language, such as â€œOne point that you might like to consider is â€¦, which was included in this weekâ€™s readingâ€. \xa0 \n\nOnline symbols can be used to express emotion and can be particularly useful when working in less formal situations such as collaborating on online projects with other students. \xa0 \n\nEmoji icons can be useful in expressing emotions when communicating online. However, emoji may not be appropriate in more formal settings.\n\n&amp;#x200B;\n\nParticipating in online meetings\n\nOnline meetings and virtual team projects are a common industry practice.\xa0They are used to communicate information, collect data, generate ideas, build teams, solve problems and make decisions.\n\nIn a university setting, group projects are a common form of assessment in many course units.\xa0They are collaborative learning opportunities that require students to evaluate ideas, analyse and derive meaning from information, and produce work cooperatively.\xa0Effective online meetings can help you to share information, discuss ideas, solve problems and produce work during your studies, as well as in the workplace.\n\n&amp;#x200B;\n\nUsing online discussion boards\n\nA discussion board, such as a Moodle forum, is different to an online meeting in that it is a communication tool that enables participants to post messages and to reply to others\' messages asynchronously (i.e. not at the same time).\n\nThe discussions on online discussion boards typically last for a longer period of time (days, weeks or months) and allow participants time to think about what they are going to contribute.\xa0Some units will include discussion board contributions as assessment tasks.\xa0In these assessment tasks, your lecturer will expect you to provide reflective, detailed responses to the topic and to other students\' posts. Often, to obtain high marks for these assessment types, you need to demonstrate your engagement with and understanding of the unit materials such as readings and lecture content.\n\nThe purpose of this section is to explore some ways of effectively using online discussion boards:\n\n[https://youtu.be/gfS4xgGmzbE](https://youtu.be/gfS4xgGmzbE)\n\nUsing online collaboration tools\n\nDigital tools\n\nThere are a large number of digital tools that can be used during your time at university. There is software available to be purchased or downloaded, including:\n\npackages in the Microsoft suite for written communication\n\nSkype and Zoom for online face-to-face communication and collaboration\n\nplanning tools such as Catme, Dapulse.com and Microsoft Project\n\nsmartphone applications.\n\nSome tools are freely available while others must be purchased.\n\nMonash\xa0University provides digital resources using the Google platform to create, store, share, collaborate and present different kinds of documents. This section will discuss how you can use these tools for group work, planning and documenting your projects.\n\nUsing digital tools for online communication\n\nThe digital tools shown below enable various types of online communication and collaboration including the sharing of files, the creation of websites for group work, and online meetings.\n\nHow to access these Google digital tools at Monash University\n\nMonash students and staff can access these via [my.monash](https://my.monash/) website.\n\n1. Choose an internet browser.\n2. Type my.monash in the browser window or search for my.monash\n\n3.\xa0Type your Monash username and password to access content from my.monash website.\n\n4.\xa0Click to open either the Email or Calendar tiles.\n\n5.\xa0To access other Google products, click on the small boxes on the top right-hand side of the screen\n\n6.\xa0Other Google products will be displayed. Select the digital tool you wish to use. Click More to see more of the digital tools.', 'author_fullname': 't2_61mllz70', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '#studying #learning - How to Learn Effectively at University', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ideas_jianfa', 'collections': [{'permalink': 'https://www.reddit.com/r/ideas_jianfa/collection/35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'link_ids': ['t3_gcoc3k', 't3_gdtf52', 't3_gdveqf', 't3_gdvmlx', 't3_ge5rr0', 't3_ge8v8z', 't3_ge9lxm', 't3_gebc5q', 't3_gevwrs', 't3_geyx3w', 't3_gf04c6', 't3_gf05hi', 't3_gf0oal', 't3_gf0q4i', 't3_gf0w5u', 't3_gf1nhp', 't3_gg4ilp', 't3_gg6spe', 't3_gg7vxh', 't3_gg8a10'], 'description': '', 'title': 'Learn', 'created_at_utc': 1588501452.204, 'subreddit_id': 't5_2maa9f', 'author_name': 'jianfa-ben-tsai', 'collection_id': '35e9aeab-e939-43e5-8845-e1ab1bb3fd1d', 'author_id': 't2_61mllz70', 'last_update_utc': 1588996450.974, 'display_layout': None}], 'hidden': False, 'pwls': None, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gevwrs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'restricted', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Learn', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'author_premium': True, 'thumbnail': 'self', 'edited': 1588813442.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588839123.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ideas_jianfa', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Reference: Monash University RLO Study Skills, 2020,  &lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university""&gt;https://www.monash.edu/rlo/study-skills/learning-at-university&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Making useful study notes&lt;/p&gt;\n\n&lt;p&gt;Why do you need to make notes?&lt;/p&gt;\n\n&lt;p&gt;Study notes are a way for you to summarise and synthesise the material that you are learning or collecting for a written assignment. A key point for making notes is that you need to re-write the material\xa0in your own words.&lt;/p&gt;\n\n&lt;p&gt;Types of notes: Flowcharts or process notes&lt;/p&gt;\n\n&lt;p&gt;Flowcharts and similar process notes can be useful when making notes for certain processes or methods.&lt;/p&gt;\n\n&lt;p&gt;Example 1: Historical timelines&lt;/p&gt;\n\n&lt;p&gt;Example 2: Steps in a laboratory method&lt;/p&gt;\n\n&lt;p&gt;Example 3: Mind Maps - For more information, visit &lt;a href=""https://www.monash.edu/rlo/research-writing-assignments/understanding-the-assignment/brainstorming-and-mind-mapping""&gt;Brainstorming: Mind mapping&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Types of notes: Cornell notes&lt;/p&gt;\n\n&lt;p&gt;To make Cornell notes, divide the page up into three areas:&lt;/p&gt;\n\n&lt;p&gt;A larger notes column (on the right of the page)&lt;/p&gt;\n\n&lt;p&gt;A smaller recall column (on the left of the page)&lt;/p&gt;\n\n&lt;p&gt;A summary section (at the bottom of the page)&lt;/p&gt;\n\n&lt;p&gt;Notes column: In the right-hand column, write down the bulk of the notes from your lectures or study materials as you are reading your textbook or online materials.&lt;/p&gt;\n\n&lt;p&gt;Recall column: After you finish a chapter, section or study session, note down any questions, keywords and main ideas in the recall column.&lt;/p&gt;\n\n&lt;p&gt;Summary section: At the bottom of the page, the summary section is used to summarise all the notes from the page, to make sense of the material and make revision easier.&lt;/p&gt;\n\n&lt;p&gt;Using your notes&lt;/p&gt;\n\n&lt;p&gt;For the study, revisit your notes:&lt;/p&gt;\n\n&lt;p&gt;the day after you wrote them,&lt;/p&gt;\n\n&lt;p&gt;the following week,&lt;/p&gt;\n\n&lt;p&gt;at least once the following month.&lt;/p&gt;\n\n&lt;p&gt;This helps with your memory, meaning you will be better prepared for your exams!&lt;/p&gt;\n\n&lt;p&gt;Tip: If you are making research notes or using multiple different sources, also note down the reference details!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Participating in tutorials&lt;/p&gt;\n\n&lt;p&gt;Tutorials are useful for:&lt;/p&gt;\n\n&lt;p&gt;consolidating your understanding of a topic/reading/lecture.&lt;/p&gt;\n\n&lt;p&gt;Examining a topic critically by&lt;/p&gt;\n\n&lt;p&gt;asking questions&lt;/p&gt;\n\n&lt;p&gt;reflecting on the material&lt;/p&gt;\n\n&lt;p&gt;making links to related topics/ideas.&lt;/p&gt;\n\n&lt;p&gt;Attendance&lt;/p&gt;\n\n&lt;p&gt;Regular attendance is advisable. Note that for some classes attendance is compulsory, while for others there can be a minimum number of tutorials that a student must attend. In some subjects, marks are awarded for student participation. Check with your tutor or in your unit guide.&lt;/p&gt;\n\n&lt;p&gt;Expectations&lt;/p&gt;\n\n&lt;p&gt;You will be expected to:&lt;/p&gt;\n\n&lt;p&gt;answer questions from the lecturer, tutor and/or other students&lt;/p&gt;\n\n&lt;p&gt;contribute to the discussion by sharing and comparing ideas&lt;/p&gt;\n\n&lt;p&gt;take ownership of your learning by seeking clarification of any material you do not fully understanding.&lt;/p&gt;\n\n&lt;p&gt;Preparation&lt;/p&gt;\n\n&lt;p&gt;It is essential to complete all set tasks, such as the required reading or set questions, before your tutorial so that you can participate fully in the discussion. Reflect on unit learning objectives and think of questions arising from the reading or the lecture that you can ask during the tutorial.&lt;/p&gt;\n\n&lt;p&gt;To contribute:&lt;/p&gt;\n\n&lt;p&gt;Think of several questions/examples/comments you would like to make.&lt;/p&gt;\n\n&lt;p&gt;Look for pauses during the tutorial session to enable you to enter the discussion.&lt;/p&gt;\n\n&lt;p&gt;Indicate that you want to speak by making eye contact with the tutor or by taking a more alert body posture.&lt;/p&gt;\n\n&lt;p&gt;You can enter the discussion by:&lt;/p&gt;\n\n&lt;p&gt;agreeing with what someone has said and adding your own thoughts&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I agree with what [NAME] said about ___. In my viewâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;disagreeing with what someone has said and providing reasons for your position&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I think [NAME] made an interesting point; however, in my opinionâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;raising new points/questions.&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;I think that one aspect we have not considered isâ€¦&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;linking the discussion back to the weekly readings&lt;/p&gt;\n\n&lt;p&gt;E.g. &amp;quot;According to [AUTHOR]â€¦&amp;quot; or &amp;quot;In her article, [AUTHOR] argued thatâ€¦&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;After the tutorial:&lt;/p&gt;\n\n&lt;p&gt;Continue the discussion with your classmates.&lt;/p&gt;\n\n&lt;p&gt;Contact the tutor to clarify any important points you didn&amp;#39;t understand.&lt;/p&gt;\n\n&lt;p&gt;Finish any unfinished work.&lt;/p&gt;\n\n&lt;p&gt;Revise. Sit down and consolidate the concepts that you have learned.&lt;/p&gt;\n\n&lt;p&gt;Enter questions in your notebook; write answers.&lt;/p&gt;\n\n&lt;p&gt;Check terms/ jargon.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Learning in practical environments&lt;/p&gt;\n\n&lt;p&gt;Throughout your studies, you will perform exercises in practical environments such as laboratories, site visits or fieldwork.&lt;/p&gt;\n\n&lt;p&gt;Practical exercises are designed to help you:&lt;/p&gt;\n\n&lt;p&gt;apply your theoretical understanding in a tactile way&lt;/p&gt;\n\n&lt;p&gt;expand your theoretical understanding&lt;/p&gt;\n\n&lt;p&gt;use techniques and equipment safely and correctly&lt;/p&gt;\n\n&lt;p&gt;develop skills such as time management.&lt;/p&gt;\n\n&lt;p&gt;Most practical exercises consist of three major parts.&lt;/p&gt;\n\n&lt;p&gt;Preliminary work or â€˜pre-labsâ€™, which involve preparing for the exercise.&lt;/p&gt;\n\n&lt;p&gt;The exercise itself, where you will perform the activity under the guidance of an educator.&lt;/p&gt;\n\n&lt;p&gt;A post-exercise submission to be completed within the session or at a later date. These can take the form of work such as reports or presentations.&lt;/p&gt;\n\n&lt;p&gt;You will be provided with detailed guidelines for each exercise you undertake. Below are some general tips on how to prepare for your exercise, minimise stress during your session, and complete any submissions.&lt;/p&gt;\n\n&lt;p&gt;Before the exercise:&lt;/p&gt;\n\n&lt;p&gt;Read the materials and procedures for your exercise 2 to 3 days prior to your session (highlight key concepts, procedures and measurements).&lt;/p&gt;\n\n&lt;p&gt;Ask yourself: what am I trying to determine in this exercise?&lt;/p&gt;\n\n&lt;p&gt;If a method is provided summarise the key steps in a dedicated notebook.&lt;/p&gt;\n\n&lt;p&gt;Complete and pass any pre-exercise activities such as quizzes, risk assessments or additional reading.&lt;/p&gt;\n\n&lt;p&gt;During the exercise:&lt;/p&gt;\n\n&lt;p&gt;Arrive on time! Educators generally provide context, useful tips and safety warnings at the start of the session.&lt;/p&gt;\n\n&lt;p&gt;Ask your educator for clarification of the material and procedures.&lt;/p&gt;\n\n&lt;p&gt;Discuss your understanding and predictions with other students and the educator.&lt;/p&gt;\n\n&lt;p&gt;Take detailed notes of procedures, observations and measurements in a dedicated notebook.&lt;/p&gt;\n\n&lt;p&gt;After the exercise:&lt;/p&gt;\n\n&lt;p&gt;Consult the assessment guide and/or marking rubric for any required submissions.&lt;/p&gt;\n\n&lt;p&gt;Clarify points of confusion with your educators and/or other students (unit forums are an ideal place).&lt;/p&gt;\n\n&lt;p&gt;Complete any required submissions within the time allocated.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A guide to group work&lt;/p&gt;\n\n&lt;p&gt;Why work in a group or a team?&lt;/p&gt;\n\n&lt;p&gt;Develops excellent graduate attributes:&lt;/p&gt;\n\n&lt;p&gt;decision making and problem-solving skills&lt;/p&gt;\n\n&lt;p&gt;project management and organisational skills&lt;/p&gt;\n\n&lt;p&gt;communication and conflict resolution skills.&lt;/p&gt;\n\n&lt;p&gt;The process&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Build a strong foundation&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Get to know your team.&lt;/p&gt;\n\n&lt;p&gt;Discuss strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;Meet early and set rules.&lt;/p&gt;\n\n&lt;p&gt;Agree on the aims, scope, and quality of the project.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get organised&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Decide on a method of group communication.&lt;/p&gt;\n\n&lt;p&gt;Agree on who will do what.&lt;/p&gt;\n\n&lt;p&gt;Set early deadlines.&lt;/p&gt;\n\n&lt;p&gt;Plan on how to present the project as a unified work.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Hold productive meetings&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Make sure everyone knows the goal of the meeting, and what to bring.&lt;/p&gt;\n\n&lt;p&gt;Have a leader to keep the meeting on track.&lt;/p&gt;\n\n&lt;p&gt;Have a note-taker to record decisions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Maintain relationships&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Resolve problems as a group. Don&amp;#39;t exclude others.&lt;/p&gt;\n\n&lt;p&gt;Address concerns as soon as they appear.&lt;/p&gt;\n\n&lt;p&gt;Renegotiate as needed.&lt;/p&gt;\n\n&lt;p&gt;Understand the other person&amp;#39;s point of view.&lt;/p&gt;\n\n&lt;p&gt;Leadership&lt;/p&gt;\n\n&lt;p&gt;Means&lt;/p&gt;\n\n&lt;p&gt;helping the group work efficiently,&lt;/p&gt;\n\n&lt;p&gt;monitoring progress,&lt;/p&gt;\n\n&lt;p&gt;knowing when a team member needs help, and&lt;/p&gt;\n\n&lt;p&gt;keeping the group motivated.&lt;/p&gt;\n\n&lt;p&gt;Does NOT mean&lt;/p&gt;\n\n&lt;p&gt;doing all the work,&lt;/p&gt;\n\n&lt;p&gt;making decisions and telling others what to do, or&lt;/p&gt;\n\n&lt;p&gt;being solely responsible for the success or failure of the project.&lt;/p&gt;\n\n&lt;p&gt;Everyone is responsible for the success or failure of the project, not individual team members.&lt;/p&gt;\n\n&lt;p&gt;Difficulties&lt;/p&gt;\n\n&lt;p&gt;You can&amp;#39;t work as quickly in a group as you can by yourself.&lt;/p&gt;\n\n&lt;p&gt;Group/team members may have conflicting ideas or viewpoints.&lt;/p&gt;\n\n&lt;p&gt;Group/team members may not contribute equally.&lt;/p&gt;\n\n&lt;p&gt;For successful group and teamwork&lt;/p&gt;\n\n&lt;p&gt;Be patient, demonstrate good communication skills and be committed.&lt;/p&gt;\n\n&lt;p&gt;Focus on the process rather than just the end product.&lt;/p&gt;\n\n&lt;p&gt;Collaborating and communicating online&lt;/p&gt;\n\n&lt;p&gt;Introduction&lt;/p&gt;\n\n&lt;p&gt;During your university studies:&lt;/p&gt;\n\n&lt;p&gt;You may need to complete group assessment tasks.\xa0Using\xa0online meeting and collaboration tools can make this process easier, particularly if you are not able to meet face-to-face.&lt;/p&gt;\n\n&lt;p&gt;You may be asked to comment on online discussion boards (including Moodle forums) as part of your unit assessment tasks.&lt;/p&gt;\n\n&lt;p&gt;Online communication and collaboration tools are useful during your studies as they can help you to work on group projects, share ideas and information, and provide encouragement and feedback on each otherâ€™s work.&lt;/p&gt;\n\n&lt;p&gt;Communicating and collaborating online are also important skills in the workplace, as many workplaces require staff to work in groups on shared tasks, often across multiple locations. The ability to effectively use online meeting and collaboration tools is a key employability skill.&lt;/p&gt;\n\n&lt;p&gt;You may already be using social networking tools such as Facebook to work on group projects.\xa0However, there are some key differences between using social networking tools for informal activities and using these tools informal study (e.g. assessment) or work activities.\xa0These differences include the language used and the structure of the interactions in an online meeting.\xa0For example, an online meeting for a group assessment task or a workplace shared task will often use an agenda and have a person designated as the meeting chair.&lt;/p&gt;\n\n&lt;p&gt;This module includes tips for effectively communicating and collaborating when:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#meetings""&gt;participating in online meetings&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#discussion""&gt;using online discussion boards&lt;/a&gt;, for example, Moodle forums&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#tools""&gt;using online collaboration tools&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Communicating online - key terms&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Synchronous_and_asynchronous_communication-1""&gt;Synchronous and asynchronous communication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://www.monash.edu/rlo/study-skills/learning-at-university/collaborating-and-communicating-online#Verbal_and_non-verbal_communication-2""&gt;Verbal and non-verbal communication&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Without non-verbal communication cues, it can be very difficult for your readers to detect sarcasm and other hidden meanings, and your words may be interpreted as being more unfriendly or bossy than intended. Use the following activity to explore some ways of communicating online effectively.&lt;/p&gt;\n\n&lt;p&gt;You disagree with the statements made in a discussion forum post from another student in your class. \xa0Which of the following responses might be better? &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; One point that you might like to consider is â€¦, which was included in this weekâ€™s reading.&lt;/li&gt;\n&lt;li&gt;No, you are wrong. \xa0You need to read this weekâ€™s required reading to see why you are wrong..&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Discussion on 1:  Good choice! By using this type of response, you are indicating that you do not fully agree with the post, but in a way that is not direct and bossy. \xa0You are also giving the other student some useful information by referring directly to the point raised in the reading. &lt;/p&gt;\n\n&lt;p&gt;Discussion on 2:  This is probably not the best choice. \xa0This response uses direct language and may be interpreted as unfriendly and bossy. \xa0The student who you are responding to may ignore your feedback as it doesnâ€™t give them much useful information (other than that they need to do the reading).\xa0 Your feedback is more likely to be heeded when you use cautious language, such as â€œOne point that you might like to consider is â€¦, which was included in this weekâ€™s readingâ€. \xa0 &lt;/p&gt;\n\n&lt;p&gt;Online symbols can be used to express emotion and can be particularly useful when working in less formal situations such as collaborating on online projects with other students. \xa0 &lt;/p&gt;\n\n&lt;p&gt;Emoji icons can be useful in expressing emotions when communicating online. However, emoji may not be appropriate in more formal settings.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Participating in online meetings&lt;/p&gt;\n\n&lt;p&gt;Online meetings and virtual team projects are a common industry practice.\xa0They are used to communicate information, collect data, generate ideas, build teams, solve problems and make decisions.&lt;/p&gt;\n\n&lt;p&gt;In a university setting, group projects are a common form of assessment in many course units.\xa0They are collaborative learning opportunities that require students to evaluate ideas, analyse and derive meaning from information, and produce work cooperatively.\xa0Effective online meetings can help you to share information, discuss ideas, solve problems and produce work during your studies, as well as in the workplace.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Using online discussion boards&lt;/p&gt;\n\n&lt;p&gt;A discussion board, such as a Moodle forum, is different to an online meeting in that it is a communication tool that enables participants to post messages and to reply to others&amp;#39; messages asynchronously (i.e. not at the same time).&lt;/p&gt;\n\n&lt;p&gt;The discussions on online discussion boards typically last for a longer period of time (days, weeks or months) and allow participants time to think about what they are going to contribute.\xa0Some units will include discussion board contributions as assessment tasks.\xa0In these assessment tasks, your lecturer will expect you to provide reflective, detailed responses to the topic and to other students&amp;#39; posts. Often, to obtain high marks for these assessment types, you need to demonstrate your engagement with and understanding of the unit materials such as readings and lecture content.&lt;/p&gt;\n\n&lt;p&gt;The purpose of this section is to explore some ways of effectively using online discussion boards:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://youtu.be/gfS4xgGmzbE""&gt;https://youtu.be/gfS4xgGmzbE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Using online collaboration tools&lt;/p&gt;\n\n&lt;p&gt;Digital tools&lt;/p&gt;\n\n&lt;p&gt;There are a large number of digital tools that can be used during your time at university. There is software available to be purchased or downloaded, including:&lt;/p&gt;\n\n&lt;p&gt;packages in the Microsoft suite for written communication&lt;/p&gt;\n\n&lt;p&gt;Skype and Zoom for online face-to-face communication and collaboration&lt;/p&gt;\n\n&lt;p&gt;planning tools such as Catme, Dapulse.com and Microsoft Project&lt;/p&gt;\n\n&lt;p&gt;smartphone applications.&lt;/p&gt;\n\n&lt;p&gt;Some tools are freely available while others must be purchased.&lt;/p&gt;\n\n&lt;p&gt;Monash\xa0University provides digital resources using the Google platform to create, store, share, collaborate and present different kinds of documents. This section will discuss how you can use these tools for group work, planning and documenting your projects.&lt;/p&gt;\n\n&lt;p&gt;Using digital tools for online communication&lt;/p&gt;\n\n&lt;p&gt;The digital tools shown below enable various types of online communication and collaboration including the sharing of files, the creation of websites for group work, and online meetings.&lt;/p&gt;\n\n&lt;p&gt;How to access these Google digital tools at Monash University&lt;/p&gt;\n\n&lt;p&gt;Monash students and staff can access these via &lt;a href=""https://my.monash/""&gt;my.monash&lt;/a&gt; website.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Choose an internet browser.&lt;/li&gt;\n&lt;li&gt;Type my.monash in the browser window or search for my.monash&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;3.\xa0Type your Monash username and password to access content from my.monash website.&lt;/p&gt;\n\n&lt;p&gt;4.\xa0Click to open either the Email or Calendar tiles.&lt;/p&gt;\n\n&lt;p&gt;5.\xa0To access other Google products, click on the small boxes on the top right-hand side of the screen&lt;/p&gt;\n\n&lt;p&gt;6.\xa0Other Google products will be displayed. Select the digital tool you wish to use. Click More to see more of the digital tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6310d036-8b9f-11ea-9f86-0e5dc2371a7b', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2maa9f', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#373c3f', 'id': 'gevwrs', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'jianfa-ben-tsai', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ideas_jianfa/comments/gevwrs/studying_learning_how_to_learn_effectively_at/', 'subreddit_subscribers': 33, 'created_utc': 1588810323.0, 'num_crossposts': 68, 'media': None, 'is_video': False}]",t3_gevwrs,,
,learnmachinelearning,"Anyone with any experience taking Stanfordâ€™s cohort-based XCS229I?

Link: https://online.stanford.edu/courses/xcs229i-machine-learning

This is supposedly part of their AI Professional Certificate and a more rigorous treatment of the topics than the Coursera course. The 10 week course is $1,595 so not cheap by any means. I would not personally have to pay out of my own pocket to take the course but don it want to waste my employers money (or my time) if I can just watch the lectures for free in YouTube.",t2_5n5gtkbw,False,,0,False,Stanford XCS229I,[],r/learnmachinelearning,False,6,,0,,False,t3_gfkhrh,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588933120.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Anyone with any experience taking Stanfordâ€™s cohort-based XCS229I?&lt;/p&gt;

&lt;p&gt;Link: &lt;a href=""https://online.stanford.edu/courses/xcs229i-machine-learning""&gt;https://online.stanford.edu/courses/xcs229i-machine-learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is supposedly part of their AI Professional Certificate and a more rigorous treatment of the topics than the Coursera course. The 10 week course is $1,595 so not cheap by any means. I would not personally have to pay out of my own pocket to take the course but don it want to waste my employers money (or my time) if I can just watch the lectures for free in YouTube.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfkhrh,True,,throwawaymlquestion,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfkhrh/stanford_xcs229i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfkhrh/stanford_xcs229i/,155203,1588904320.0,0,,False,,,,
,learnmachinelearning,,t2_4h2fow53,False,,0,False,Graph Neural Network model calibration for trusted predictions,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_gfivdg,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/ReI7VjHLtOcuYnxcPZbxp0tt2knTa2GINsIZgMpQepA.jpg,False,,[],{},link,,False,,1588926876.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?auto=webp&amp;s=4ec6e4346f82a426a74a947a5cddafd8282a0df5', 'width': 1024, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4237449b2889082b54e82a800fbd28619b6b5ee', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3f71f677009d6092a0d56eb0cd3c415b82147ebb', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=608f23c0415ac964547f187b023df817ceeeda99', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e88d396b8faeaed1d4b0f0c3a177c4be70438deb', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/dVgfVingOXBCpfnizCqozUCoMoRPCklRWU004KS9wJE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dec2a7fb0d7b09e62496f958fef547109be57ea7', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'WsF9q0qWaDPohum0lvCnsKqgN9QPpiwsKa9hLCHalYY'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfivdg,True,,StellarGraphLibrary,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfivdg/graph_neural_network_model_calibration_for/,all_ads,False,https://medium.com/stellargraph/graph-neural-network-model-calibration-for-trusted-predictions-e49628487e7b?source=friends_link&amp;sk=1364a91c903fbb47e78830300e6b591e,155203,1588898076.0,0,,False,,,,
,learnmachinelearning,,t2_rlnpqcu,False,,0,False,System Requirements for *Learning* ML,[],r/learnmachinelearning,False,6,,0,,False,t3_gfip17,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,default,False,,[],{},,,False,,1588926228.0,text,6,,,text,self.MLQuestions,False,,,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfip17,True,,cosmictypist,,5,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfip17/system_requirements_for_learning_ml/,all_ads,False,/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/,155203,1588897428.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'MLQuestions', 'selftext': ""Hi. This question may have been asked before, but I couldn't find it and hence posting here. In case it has been answered before, I'll appreciate being pointed in the right direction.\n\nBasically, I wanted to know how fast/advanced my system needs to be if I want to start learning ML from scratch. I did some search on Google and it seems that a fairly fast CPU (Intel Core i7) along with a GPU are generally recommended - but are these necessary? Are there ways to make use of some sort of a remoteGPU server while having fairly low-average specs for your system?\n\nI have a fairly old Intel core i5 2.4 GHz CPU with 2GB RAM. Yeah I'll probably need to get a new one but don't want to splurge if it is not required. Thanks."", 'author_fullname': 't2_rlnpqcu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'System Requirements for *Learning* ML', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/MLQuestions', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gfin87', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 6, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588926055.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.MLQuestions', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi. This question may have been asked before, but I couldn&amp;#39;t find it and hence posting here. In case it has been answered before, I&amp;#39;ll appreciate being pointed in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Basically, I wanted to know how fast/advanced my system needs to be if I want to start learning ML from scratch. I did some search on Google and it seems that a fairly fast CPU (Intel Core i7) along with a GPU are generally recommended - but are these necessary? Are there ways to make use of some sort of a remoteGPU server while having fairly low-average specs for your system?&lt;/p&gt;\n\n&lt;p&gt;I have a fairly old Intel core i5 2.4 GHz CPU with 2GB RAM. Yeah I&amp;#39;ll probably need to get a new one but don&amp;#39;t want to splurge if it is not required. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_30rel', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gfin87', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'cosmictypist', 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/MLQuestions/comments/gfin87/system_requirements_for_learning_ml/', 'subreddit_subscribers': 27178, 'created_utc': 1588897255.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_gfin87,,
,learnmachinelearning,"I'm an absolute beginner to the field , I think my idea is supposed to be very simple and easy yet I'm feeling lost..

I want to train a model to localize only hand written brackets in pictures of a scanned textbook pages (written by me). 

So can I consider this as an object detection problem and proceed with that?  hand written Brackets are alot more simple than a real life object, however They are different too, there is a different type of noise in my case which is the tge text printed words which can be easily detected falsly as brackets as this noise is similar to the geometry of my brackets ...

And if that's the case, how am I supposed to label my hand written bracket with a rectangle?! , there is no way I can label it without including some of the surrounding text , will this affect the accuracy of my results?

I'm sorry to over explain my likely very simple problem, however this is my first project and I need to get it done, please help me ðŸ™

Thanks!",t2_15ee6s,False,,0,False,Lost with my first ML Project ðŸ˜­. I need HELP!,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfin5h,False,light,0.5,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,1588897550.0,,[],{},,,True,,1588926046.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m an absolute beginner to the field , I think my idea is supposed to be very simple and easy yet I&amp;#39;m feeling lost..&lt;/p&gt;

&lt;p&gt;I want to train a model to localize only hand written brackets in pictures of a scanned textbook pages (written by me). &lt;/p&gt;

&lt;p&gt;So can I consider this as an object detection problem and proceed with that?  hand written Brackets are alot more simple than a real life object, however They are different too, there is a different type of noise in my case which is the tge text printed words which can be easily detected falsly as brackets as this noise is similar to the geometry of my brackets ...&lt;/p&gt;

&lt;p&gt;And if that&amp;#39;s the case, how am I supposed to label my hand written bracket with a rectangle?! , there is no way I can label it without including some of the surrounding text , will this affect the accuracy of my results?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m sorry to over explain my likely very simple problem, however this is my first project and I need to get it done, please help me ðŸ™&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gfin5h,True,,homamoooo1234,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfin5h/lost_with_my_first_ml_project_i_need_help/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfin5h/lost_with_my_first_ml_project_i_need_help/,155203,1588897246.0,0,,False,,,,
,learnmachinelearning," 

Machine Learning today is one of the most sought-after skills in the market. A lot of Software Engineers are picking up ML, simply because it is a highly paid skill.

So, how do you learn Machine Learning?

* First things first â€“ the prerequisites:
   * *Basic calculus*. In Machine Learning, youâ€™d be working on a lot of optimizations that require knowledge of Calculus. It would be highly recommended that you are aware of functions, limits, differentiation, maxima, minima, etc.
   * *Linear Algebra*. When you talk about ML, you will be dealing with matrices and vectors every day. So, knowledge of Linear Algebra is a must. However, youâ€™d also be required to know about other important topics like Eigenvalues and Eigenvectors.
   * *Probability*. Most ML algorithms try to â€œmodelâ€ the underlying phenomena that generated the observed data. All of this modelling is probabilistic. It is therefore highly recommended that you are comfortable with the theory of Probability.
* Getting into actual ML:
   * Take a great online course on ML. The most well-known course is the one offered byÂ Andrew Ng (Coursera). It is a great course and it teaches you the basics of Machine Learning â€“ Regression, classification, various ML algorithms, etc. The course also requires you to build a digit recognition system.
   * Once you have the basics in place, it would be a great idea to practice some problems onÂ Kaggle. Kaggle is a well-known Machine Learning contest platform where you can compete with others in training ML models on various datasets.
   * Take up ML projects. This is the most important point. Ideally, youâ€™d want to have not only ML experience but also some great projects on your resume that you can showcase. These projects will help you distinguish yourself from other candidates. After searching a lot for courses that teach ML through projects, this [Eduonix](https://inr.deals/track?id=100613576719&amp;src=backend&amp;url=https%3A%2F%2Fwww.eduonix.com%2Flearn-machine-learning-by-building-projects%3F)Â quite relevant.

The best way to learn Machine Learning is to actually apply it to real datasets and solve real problems. Machine Learning is as much of an art as it is a science. You will learn it from experience. Your focus should be on attempting multiple ML projects so as to gain experience and build a strong profile",t2_2om0uq0e,False,,0,False,Should I follow This Guide?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5xzq,False,dark,0.88,,public,6,0,{},,,False,[],,False,False,,{},,False,6,,False,self,1588861265.0,,[],{},self,,True,,1588884660.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Machine Learning today is one of the most sought-after skills in the market. A lot of Software Engineers are picking up ML, simply because it is a highly paid skill.&lt;/p&gt;

&lt;p&gt;So, how do you learn Machine Learning?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First things first â€“ the prerequisites:

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Basic calculus&lt;/em&gt;. In Machine Learning, youâ€™d be working on a lot of optimizations that require knowledge of Calculus. It would be highly recommended that you are aware of functions, limits, differentiation, maxima, minima, etc.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Linear Algebra&lt;/em&gt;. When you talk about ML, you will be dealing with matrices and vectors every day. So, knowledge of Linear Algebra is a must. However, youâ€™d also be required to know about other important topics like Eigenvalues and Eigenvectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Probability&lt;/em&gt;. Most ML algorithms try to â€œmodelâ€ the underlying phenomena that generated the observed data. All of this modelling is probabilistic. It is therefore highly recommended that you are comfortable with the theory of Probability.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Getting into actual ML:

&lt;ul&gt;
&lt;li&gt;Take a great online course on ML. The most well-known course is the one offered byÂ Andrew Ng (Coursera). It is a great course and it teaches you the basics of Machine Learning â€“ Regression, classification, various ML algorithms, etc. The course also requires you to build a digit recognition system.&lt;/li&gt;
&lt;li&gt;Once you have the basics in place, it would be a great idea to practice some problems onÂ Kaggle. Kaggle is a well-known Machine Learning contest platform where you can compete with others in training ML models on various datasets.&lt;/li&gt;
&lt;li&gt;Take up ML projects. This is the most important point. Ideally, youâ€™d want to have not only ML experience but also some great projects on your resume that you can showcase. These projects will help you distinguish yourself from other candidates. After searching a lot for courses that teach ML through projects, this &lt;a href=""https://inr.deals/track?id=100613576719&amp;amp;src=backend&amp;amp;url=https%3A%2F%2Fwww.eduonix.com%2Flearn-machine-learning-by-building-projects%3F""&gt;Eduonix&lt;/a&gt;Â quite relevant.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The best way to learn Machine Learning is to actually apply it to real datasets and solve real problems. Machine Learning is as much of an art as it is a science. You will learn it from experience. Your focus should be on attempting multiple ML projects so as to gain experience and build a strong profile&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?auto=webp&amp;s=0f335926f0d3960f9882daf4a67f417a10df4d66', 'width': 647, 'height': 422}, 'resolutions': [{'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0c3c1af91133e4e1b526f4e838db4eb741e8ce6', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12fce343264e852d8f714c9ec9e96f1813a8681e', 'width': 216, 'height': 140}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8443b6d856dcf75ecc6d025d467997c9046e5805', 'width': 320, 'height': 208}, {'url': 'https://external-preview.redd.it/svYhfvPOkltWvTF6JQ_u1ZOlF3_4ftcTgtg-8BbD7OQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e5bd15b6f7995a4b278556ea23bb689c6a1609f', 'width': 640, 'height': 417}], 'variants': {}, 'id': 'jST3NfM-cC_ASNZRu30Adudq5cD6mRpGIhGvenQ514g'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5xzq,True,,Amitagarwal7021,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5xzq/should_i_follow_this_guide/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5xzq/should_i_follow_this_guide/,155203,1588855860.0,0,,False,,,,
,learnmachinelearning,"Hey all,

  
So I've just started my MSc research project on Graph Neural  Networks and ngl I'm finding this whole research thing super daunting   and not sure exactly what I should be doing most of the time...

  
Does  anyone know of any good resources that outline the basics of how to conduct/write up research for a thesis, preferably specific to  computer science or machine learning?

  
I'd love it if there was a book that I could read that would help put me on the right path.",t2_fblqx,False,,0,False,Any resources for writing better academic research?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf2vg3,False,light,1.0,,public,14,0,{},,,False,[],,False,False,,{},HELP,False,14,,False,self,False,,[],{},,,True,,1588869930.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey all,&lt;/p&gt;

&lt;p&gt;So I&amp;#39;ve just started my MSc research project on Graph Neural  Networks and ngl I&amp;#39;m finding this whole research thing super daunting   and not sure exactly what I should be doing most of the time...&lt;/p&gt;

&lt;p&gt;Does  anyone know of any good resources that outline the basics of how to conduct/write up research for a thesis, preferably specific to  computer science or machine learning?&lt;/p&gt;

&lt;p&gt;I&amp;#39;d love it if there was a book that I could read that would help put me on the right path.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gf2vg3,True,,ghoumrassi,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2vg3/any_resources_for_writing_better_academic_research/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf2vg3/any_resources_for_writing_better_academic_research/,155203,1588841130.0,0,,False,,,,
,learnmachinelearning,"I was learning the basics of data preprocessing and came across two ways to encode categorical data. I tried to find the difference between the two encoders, when to use which encoder and why one hot encoder is used but couldn't find any good resource.",t2_5t9rtf1h,False,,0,False,One Hot Encoder vs Label Encoder,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfhbg1,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588921351.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was learning the basics of data preprocessing and came across two ways to encode categorical data. I tried to find the difference between the two encoders, when to use which encoder and why one hot encoder is used but couldn&amp;#39;t find any good resource.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfhbg1,True,,Sid200026,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfhbg1/one_hot_encoder_vs_label_encoder/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfhbg1/one_hot_encoder_vs_label_encoder/,155203,1588892551.0,0,,False,,,,
,learnmachinelearning,"Hey everyone. I'm a researcher who's currently doing a lot of work with very large spatial files and deep learning models, I've reached the point where I simply can't load the data needed for training into memory. What recommendations would you have for cloud storage and GPU training, and what sort of cost should I be expecting?  

I've been looking into using AWS with S3 for storing the data and SageMaker for training the models but I've only come out more confused by the variety Amazon has on offer. I've previously done a fair bit of web dev and have them running live so I'm comfortable working in predominantly CLI environments.

Thanks for all help!",t2_4g6qbb8j,False,,0,False,Best Cloud Storage &amp; GPU Training,[],r/learnmachinelearning,False,6,,0,,False,t3_gfh8dw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588921065.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everyone. I&amp;#39;m a researcher who&amp;#39;s currently doing a lot of work with very large spatial files and deep learning models, I&amp;#39;ve reached the point where I simply can&amp;#39;t load the data needed for training into memory. What recommendations would you have for cloud storage and GPU training, and what sort of cost should I be expecting?  &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been looking into using AWS with S3 for storing the data and SageMaker for training the models but I&amp;#39;ve only come out more confused by the variety Amazon has on offer. I&amp;#39;ve previously done a fair bit of web dev and have them running live so I&amp;#39;m comfortable working in predominantly CLI environments.&lt;/p&gt;

&lt;p&gt;Thanks for all help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfh8dw,True,,EnergyVis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh8dw/best_cloud_storage_gpu_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfh8dw/best_cloud_storage_gpu_training/,155203,1588892265.0,0,,False,,,,
,learnmachinelearning,"For understanding which parts of the images to look into, we usually use GradCam or Cam. In CAM, we multiply the outermost conv features with the weights of the matrix pertaining to the class. Correct me if I am wrong though.
However, does anyone know which paper introduced this?",t2_1myrhoji,False,,0,False,Does anybody know the name of the paper where CAM was applied?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfh56o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588920758.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;For understanding which parts of the images to look into, we usually use GradCam or Cam. In CAM, we multiply the outermost conv features with the weights of the matrix pertaining to the class. Correct me if I am wrong though.
However, does anyone know which paper introduced this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfh56o,True,,thearkamitra,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfh56o/does_anybody_know_the_name_of_the_paper_where_cam/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfh56o/does_anybody_know_the_name_of_the_paper_where_cam/,155203,1588891958.0,0,,False,,,,
,learnmachinelearning,"I'm struggling to find a scalable solution that will allow me to make predictions for several hundred response variables using a common set of predictors using Keras in R. Since I'm pretty new to NN and DL, I simulated a small toy dataset that consists of five dependent response variables and a set of 60K predictors, and fit some very basic multi-output NN with Keras in R. The plan is to build on these NN and to apply them to a real dataset consisting of 60K predictors and \~200 response variables. The code for my very simple NN is:

    input &lt;- layer_input(shape = dim(trnX.RR)[2], name=""t_in"")
    
    layer_t &lt;- input %&gt;%
          layer_dense(units = units_M, activation='linear', 
                      kernel_regularizer = regularizer_l2(k))
        
    ## Output
    yhat_t1 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_t1"")
    yhat_t2 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_t2"")
    yhat_m1 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_m1"")
    yhat_m2 &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_m2"")
    yhat_y &lt;- layer_t %&gt;%
          layer_dense(units = 1, name=""yhat_y"")
        
    # build model
    model &lt;- keras_model(inputs = input,
                  outputs = c(yhat_t1,yhat_t2,yhat_m1,yhat_m2,yhat_y)) %&gt;%
          compile(optimizer = ""rmsprop"",
                  loss=""mse"",
                  metrics=""mae"")
    
    model_fit &lt;- model %&gt;%
          fit(x = trnX.RR,
              y = list(trnYs[,1], trnYs[,2], trnYs[,3], trnYs[,4], trnYs[,5]),
              epochs = epochs_M,
              batch_size = 50,
              verbose = 0, validation_split = 0.2)

Coding the output layer for each response variable is not great when you have 100+ response variables. Is there a better way to do this? I've seen multi-output code in Python that is much more efficient (See the first chunk of code under model building here [https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6](https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6)).",t2_ztlov,False,,0,False,Scalable multi-output NN with keras in R,[],r/learnmachinelearning,False,6,,0,,False,t3_gfgrjx,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588919439.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m struggling to find a scalable solution that will allow me to make predictions for several hundred response variables using a common set of predictors using Keras in R. Since I&amp;#39;m pretty new to NN and DL, I simulated a small toy dataset that consists of five dependent response variables and a set of 60K predictors, and fit some very basic multi-output NN with Keras in R. The plan is to build on these NN and to apply them to a real dataset consisting of 60K predictors and ~200 response variables. The code for my very simple NN is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input &amp;lt;- layer_input(shape = dim(trnX.RR)[2], name=&amp;quot;t_in&amp;quot;)

layer_t &amp;lt;- input %&amp;gt;%
      layer_dense(units = units_M, activation=&amp;#39;linear&amp;#39;, 
                  kernel_regularizer = regularizer_l2(k))

## Output
yhat_t1 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_t1&amp;quot;)
yhat_t2 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_t2&amp;quot;)
yhat_m1 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_m1&amp;quot;)
yhat_m2 &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_m2&amp;quot;)
yhat_y &amp;lt;- layer_t %&amp;gt;%
      layer_dense(units = 1, name=&amp;quot;yhat_y&amp;quot;)

# build model
model &amp;lt;- keras_model(inputs = input,
              outputs = c(yhat_t1,yhat_t2,yhat_m1,yhat_m2,yhat_y)) %&amp;gt;%
      compile(optimizer = &amp;quot;rmsprop&amp;quot;,
              loss=&amp;quot;mse&amp;quot;,
              metrics=&amp;quot;mae&amp;quot;)

model_fit &amp;lt;- model %&amp;gt;%
      fit(x = trnX.RR,
          y = list(trnYs[,1], trnYs[,2], trnYs[,3], trnYs[,4], trnYs[,5]),
          epochs = epochs_M,
          batch_size = 50,
          verbose = 0, validation_split = 0.2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Coding the output layer for each response variable is not great when you have 100+ response variables. Is there a better way to do this? I&amp;#39;ve seen multi-output code in Python that is much more efficient (See the first chunk of code under model building here &lt;a href=""https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6""&gt;https://towardsdatascience.com/bayesian-neural-networks-with-tensorflow-probability-fbce27d6ef6&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?auto=webp&amp;s=96e3097a6229b44ddb351653d2042328c5c1d77f', 'width': 1200, 'height': 632}, 'resolutions': [{'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efc7ce2ac39ae790c39989f75df706e8d7cd6d3f', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ced8175b8541647e09709fe336284a9ea9911fb2', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa7983c250103827ec6672c13e7857da7c68c14b', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=33ba343cd1e9885b783734b2c520b89af5cb59d2', 'width': 640, 'height': 337}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79901ec86563628848db82d59c469e21bc020f12', 'width': 960, 'height': 505}, {'url': 'https://external-preview.redd.it/vbEtulmVkoODkIK-rbVtCDgOpQ7vvD916m3sOvINnBI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86ae05e59c88f25df2053ab664daec88923afc29', 'width': 1080, 'height': 568}], 'variants': {}, 'id': 'TlWGZM09jhy8Xf2FKW_9uq_xRvOB6q5mKjx4IfxteZI'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfgrjx,True,,stinkyEyesMcGee,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfgrjx/scalable_multioutput_nn_with_keras_in_r/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfgrjx/scalable_multioutput_nn_with_keras_in_r/,155203,1588890639.0,0,,False,,,,
,learnmachinelearning,"if you have any starting projects recommendations that would be cool too. Assume that I have good enough base programing knowledge outside of deep learning.

Edit: Am instead of a in the title",t2_4dukeyr5,False,,0,False,"I want to get into deep learning (a currently playing around with TensorFlow), do you have any big recommendations? For a book or a lecture / tutorial that would help me get started and advance my through the subject?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfgjz6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,True,self,False,,[],{},,,True,,1588918724.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;if you have any starting projects recommendations that would be cool too. Assume that I have good enough base programing knowledge outside of deep learning.&lt;/p&gt;

&lt;p&gt;Edit: Am instead of a in the title&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfgjz6,True,,_AguruAguru,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfgjz6/i_want_to_get_into_deep_learning_a_currently/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfgjz6/i_want_to_get_into_deep_learning_a_currently/,155203,1588889924.0,0,,False,,,,
,learnmachinelearning," Tensorflow version - 1.14.0 Python version - 3.7.5 

This is the code I'm using. But when I try to save my model it says 

'Model object has no attribute '\_is\_graph\_network''

    from tensorflow 
    import keras 
    import tensorflow.python.keras.backend as K 
    from tensorflow.python.keras import callbacks 
    from tensorflow.python.keras 
    import Sequential from tensorflow.python.keras.models 
    import Model from tensorflow.python.keras.layers 
    import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D 
    from tensorflow.python.keras.preprocessing.image 
    import ImageDataGenerator 
    from tensorflow.python.keras.callbacks 
    import ModelCheckpoint, EarlyStopping 
    
    
    def create_model_v1():      
        model = keras.Sequential()       
        model.add(Conv2D(filters = 64, kernel_size = 3, padding='same', activation = 'relu',                                  input_shape=(img_rows, img_cols, color_type)))      
        model.add(MaxPooling2D(pool_size = 2))      
        model.add(Conv2D(filters = 128, padding='same', kernel_size = 3, activation = 'relu'))                                               model.add(MaxPooling2D(pool_size = 2))
    
    def create_model_v1():  
        model_v1 = create_model_v1()  
        history_v1 = model_v1.fit(x_train, y_train,validation_data=(x_test, y_test),callbacks=callbacks,epochs=nb_epoch, batch_size=batch_size, verbose=1)
    
    
    keras_file = 'saved_models/history1.h5'
    keras.models.save_model(history_v1, keras_file)
    
    converter = tf.lite.TocoConverter.from_keras_model_file(keras_file)
    tflite_model = converter.convert()
    open('linear.tflite', 'wb').write(tflite_model)",t2_2bqrjjyu,False,,0,False,"Model object has no attribute '_is_graph_network', when I try to save my model to tflite",[],r/learnmachinelearning,False,6,,0,,False,t3_gfga8y,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588917839.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Tensorflow version - 1.14.0 Python version - 3.7.5 &lt;/p&gt;

&lt;p&gt;This is the code I&amp;#39;m using. But when I try to save my model it says &lt;/p&gt;

&lt;p&gt;&amp;#39;Model object has no attribute &amp;#39;_is_graph_network&amp;#39;&amp;#39;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from tensorflow 
import keras 
import tensorflow.python.keras.backend as K 
from tensorflow.python.keras import callbacks 
from tensorflow.python.keras 
import Sequential from tensorflow.python.keras.models 
import Model from tensorflow.python.keras.layers 
import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D 
from tensorflow.python.keras.preprocessing.image 
import ImageDataGenerator 
from tensorflow.python.keras.callbacks 
import ModelCheckpoint, EarlyStopping 


def create_model_v1():      
    model = keras.Sequential()       
    model.add(Conv2D(filters = 64, kernel_size = 3, padding=&amp;#39;same&amp;#39;, activation = &amp;#39;relu&amp;#39;,                                  input_shape=(img_rows, img_cols, color_type)))      
    model.add(MaxPooling2D(pool_size = 2))      
    model.add(Conv2D(filters = 128, padding=&amp;#39;same&amp;#39;, kernel_size = 3, activation = &amp;#39;relu&amp;#39;))                                               model.add(MaxPooling2D(pool_size = 2))

def create_model_v1():  
    model_v1 = create_model_v1()  
    history_v1 = model_v1.fit(x_train, y_train,validation_data=(x_test, y_test),callbacks=callbacks,epochs=nb_epoch, batch_size=batch_size, verbose=1)


keras_file = &amp;#39;saved_models/history1.h5&amp;#39;
keras.models.save_model(history_v1, keras_file)

converter = tf.lite.TocoConverter.from_keras_model_file(keras_file)
tflite_model = converter.convert()
open(&amp;#39;linear.tflite&amp;#39;, &amp;#39;wb&amp;#39;).write(tflite_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfga8y,True,,Revanthmk23200,,12,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfga8y/model_object_has_no_attribute_is_graph_network/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfga8y/model_object_has_no_attribute_is_graph_network/,155203,1588889039.0,0,,False,,,,
,learnmachinelearning,"Hello,
Im really new to programming snd have a bg interest in AI and machine learning and their applications. Problem is I dont have much programming knowledge- I only just finished week 1 in Harvardâ€™s CS50 course.
So, whatâ€™s the best to start learning? 
Many thanks in advance!",t2_3femx8ef,False,,0,False,Best way to start learning?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfbg5o,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588902647.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello,
Im really new to programming snd have a bg interest in AI and machine learning and their applications. Problem is I dont have much programming knowledge- I only just finished week 1 in Harvardâ€™s CS50 course.
So, whatâ€™s the best to start learning? 
Many thanks in advance!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gfbg5o,True,,octopussssssssy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfbg5o/best_way_to_start_learning/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfbg5o/best_way_to_start_learning/,155203,1588873847.0,0,,False,,,,
,learnmachinelearning,"Most books rely on closed form functions or differential equations.  I'm not really looking for that.  Are there books that look at mathematical modeling from a different perspective, like stochastic modeling, or a more probabilistic view?  Thanks.",t2_r9kac,False,,0,False,Any recommendations for mathematical modeling?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfdtzc,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588910018.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Most books rely on closed form functions or differential equations.  I&amp;#39;m not really looking for that.  Are there books that look at mathematical modeling from a different perspective, like stochastic modeling, or a more probabilistic view?  Thanks.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfdtzc,True,,babbab55,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfdtzc/any_recommendations_for_mathematical_modeling/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfdtzc/any_recommendations_for_mathematical_modeling/,155203,1588881218.0,0,,False,,,,
,learnmachinelearning,,t2_1084g6,False,,0,False,Body Movement Correction Using Pose Estimation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gf5h3w,False,light,0.78,,public,5,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Body Movement Correction Using Pose Estimation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EpySUFqSO4c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gf5h3w', 'height': 338}",Project,False,5,,False,https://a.thumbs.redditmedia.com/qsm7JCT9udT66DaxswVTMbUJ-Tz_mnB8H4xP64ZYYP0.jpg,False,,[],{},rich:video,,False,,1588882778.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?auto=webp&amp;s=fa44800f3c70638e873929dbf9d4b4e0e663aca1', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23c042d7d7217224f5bfd0cb9f6da1687d539ea3', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5a38af28775399847f62fb7489d9b65346fcfa9', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/UxmXvAHvmUzhvGTOb5r6dENBXCVbC0leigtaLwEwCG0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7923b6c33b8c7f4df7ff394df265944c235367dd', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'fUivSBSCzTFa4-2HVW8LiKsDk3es9ZGWbcvpjIZkTIw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf5h3w,True,,rednivrug,,6,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5h3w/body_movement_correction_using_pose_estimation/,all_ads,False,https://www.youtube.com/watch?v=EpySUFqSO4c,155203,1588853978.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Body Movement Correction Using Pose Estimation', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/EpySUFqSO4c?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'Rednivrug', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/EpySUFqSO4c/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCniYYA7_Xh72b0ovYAAmYoQ'}}",False,,,,
,learnmachinelearning,"Imagine you have a picture of a dress and a picture of a person. You want to combine them into the picture of that person wearing that dress.

I want to ask if it is possible with deep learning? If it is possible, where can I start researching it? Thanks in advance.",t2_4bfdiwq7,False,,0,False,How to use machine learning to change clothes for people?,[],r/learnmachinelearning,False,6,,0,,False,t3_gfdi9x,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588909011.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Imagine you have a picture of a dress and a picture of a person. You want to combine them into the picture of that person wearing that dress.&lt;/p&gt;

&lt;p&gt;I want to ask if it is possible with deep learning? If it is possible, where can I start researching it? Thanks in advance.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfdi9x,True,,ConVit,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfdi9x/how_to_use_machine_learning_to_change_clothes_for/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfdi9x/how_to_use_machine_learning_to_change_clothes_for/,155203,1588880211.0,0,,False,,,,
,learnmachinelearning,,t2_c14wpji,False,,0,False,What is Self-Supervised Learning ? Will machines be able to learn like humans ?,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gfd4xf,False,dark,0.5,,public,0,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Self-Supervised Learning ? | Will machines be able to learn like humans ? 29', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lgVwtTof1ew/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gfd4xf', 'height': 338}",,False,0,,False,https://a.thumbs.redditmedia.com/tVkKXl4MkOJQAI2Hefrms3-xYCXb-t4kvWLvRyhMnQ4.jpg,False,,[],{},rich:video,,False,,1588907866.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?auto=webp&amp;s=d7c90919d5674edbf28dd99035850188f54c60b5', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efa96672a7b85989c503d0b81c154d865d794b92', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7d2c0e61b4b4378090aa6dbbaa9133c5ae24224', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/xIIuCucc4w-iF1_lX0vfRC969PGQZz1_jSJrVXx_Kjw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c12936ff6ce9472a6c7885a5d8fc4436de3d7e7', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ApmWhh_MYhLpQOTFLpJyvz6HrDHCk6PTEXWmBi2G55I'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfd4xf,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfd4xf/what_is_selfsupervised_learning_will_machines_be/,all_ads,False,https://youtu.be/lgVwtTof1ew,155203,1588879066.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'What is Self-Supervised Learning ? | Will machines be able to learn like humans ? 29', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/lgVwtTof1ew?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'What is Artificial Intelligence', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/lgVwtTof1ew/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_icu1syl,False,,0,False,What Is Machine Learning? - Visual Explanations | Data Revenue,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,73.0,False,t3_gfc9sv,False,light,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/zqyy8kZFfqXZKYBO09CzIqX6kE1_QFjE4Byx1uqwnqc.jpg,False,,[],{},link,,False,,1588905177.0,richtext,6,,,text,datarevenue.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?auto=webp&amp;s=a63181128d592b63016103297f71e24edcd97767', 'width': 1200, 'height': 627}, 'resolutions': [{'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c2135477b6c729dc1fb2061b79893156f521d0a', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=654d179674876650e27f1eb36bf5b9148502e02e', 'width': 216, 'height': 112}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b1ca145da0f020c5f7d46a6863f30e3f741f493', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=95ec6b6d6ec8a67bb5289f7ab1964cdc8dcc7ff8', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fea3d43b6c19b4f07514726c100af54ea7dfea15', 'width': 960, 'height': 501}, {'url': 'https://external-preview.redd.it/xkdzBXC-nmFuhh3c_nmiaeMdUWVUTkFFj_XF7nCv2Ug.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a180e8c2fb4548f46b1c56b3a68ec256cffba152', 'width': 1080, 'height': 564}], 'variants': {}, 'id': 'sOH89vvM6svgLL71vuQxsI9aIXz73Y6ha0hHxiNg8xY'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gfc9sv,True,,milosmudric,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfc9sv/what_is_machine_learning_visual_explanations_data/,all_ads,False,https://datarevenue.com/en-blog/what-is-machine-learning-a-visual-explanation?utm_source=Reddit&amp;utm_medium=learnMachineLearning,155203,1588876377.0,0,,False,,,,
,learnmachinelearning,"I've found AWS SageMaker very handy to jump on and run some ML code, but it's very annoying to have to upload and download files from UI, or copying the private GitHub key over.

I've set up SSH for myself months ago and finally decided to write a guide on it. Hope it's going to be helpful [https://biasandvariance.com/sagemaker-ssh-setup/](https://biasandvariance.com/sagemaker-ssh-setup/)",t2_2zijr2cb,False,,0,False,"SSHing to Sagemaker instance, step by step guide",[],r/learnmachinelearning,False,6,,0,,False,t3_gfc41a,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588904676.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve found AWS SageMaker very handy to jump on and run some ML code, but it&amp;#39;s very annoying to have to upload and download files from UI, or copying the private GitHub key over.&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve set up SSH for myself months ago and finally decided to write a guide on it. Hope it&amp;#39;s going to be helpful &lt;a href=""https://biasandvariance.com/sagemaker-ssh-setup/""&gt;https://biasandvariance.com/sagemaker-ssh-setup/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gfc41a,True,,derivablefunc,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfc41a/sshing_to_sagemaker_instance_step_by_step_guide/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfc41a/sshing_to_sagemaker_instance_step_by_step_guide/,155203,1588875876.0,0,,False,,,,
,learnmachinelearning,,t2_5iu2csdl,False,,0,False,Making David Dobrik Video Titles with Recurring Neural Network,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gf732i,False,dark,0.75,,public,2,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TEACHING A COMPUTER TO CREATE DAVID DOBRIK VIDEOS!?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Rishi Chillara', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BG2xaWl1wnw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCKS74Dpy07o4uroFinQH1_g'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gf732i', 'height': 338}",,False,2,,False,https://b.thumbs.redditmedia.com/oyudRI3ceEuMkWEJ4Hpu79rODdf-lc5_i4qk26sR2Bc.jpg,False,,[],{},rich:video,,False,,1588888863.0,text,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?auto=webp&amp;s=d59253b6b90b6ba2a717bf2fcb8c6e6ce3da9910', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47d86be9b21e04a7613c945621d9739e0c38dddd', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b645c936c7a671dd3130bda53014fbeb57e1b060', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/GOPFlBtqlScIAiqnkP8gqZXuoAlrbo7nTcIglyckgGw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a5aa2d694647e1f93ef33d76cd3afb1017b5b61', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ddJAQimHi_EkCTL1yACzhhg5MQNG-B8tVqPO6fXfxro'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf732i,True,,netflixandchillara,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf732i/making_david_dobrik_video_titles_with_recurring/,all_ads,False,https://youtu.be/BG2xaWl1wnw,155203,1588860063.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'TEACHING A COMPUTER TO CREATE DAVID DOBRIK VIDEOS!?', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/BG2xaWl1wnw?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Rishi Chillara', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/BG2xaWl1wnw/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCKS74Dpy07o4uroFinQH1_g'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hey guys
How do I start off with implementing papers?
Many sota ones have multiple modules and I honestly don't feel up for the task atm ;-;",t2_3mwi2rxe,False,,0,False,Paper implementation,[],r/learnmachinelearning,False,6,,0,,False,t3_gf4ys2,False,dark,0.83,,public,4,0,{},,,False,[],,False,False,,{},,False,4,,False,self,False,,[],{},,,True,,1588880590.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey guys
How do I start off with implementing papers?
Many sota ones have multiple modules and I honestly don&amp;#39;t feel up for the task atm ;-;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4ys2,True,,GrImPeAper23032000,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4ys2/paper_implementation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4ys2/paper_implementation/,155203,1588851790.0,0,,False,,,,
,learnmachinelearning,"At the early stages of planning to set up a model to do some prediction. The issue is lack of training data (a couple thousand rows). Have been thinking about different ways to generate more data (for training, validation, texting). 
  
There might be a little that can be done with text augmentation, creating fake data from the real data, pseudo labelling, resampling/oversampling/undersampling but I am curious how onerous it would be to set up a Generative Adversarial Network to create realistic but fake data. 
  
Keep in mind that all I'm doing is classifying based on a number of attributes based on a statistical model. Not trying to do visual detection in any way. But I do need statistically accurate data. 
  
So many of the models I can find that use GANs are visual detection....
  
Even if someone can point me to examples or papers where people are using a GAN to generate accurate ""text"" data would help.",t2_ffyi2,False,,0,False,"Generating training data, how practical is a GAN?",[],r/learnmachinelearning,False,6,,0,,False,t3_gf6q69,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588887592.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;At the early stages of planning to set up a model to do some prediction. The issue is lack of training data (a couple thousand rows). Have been thinking about different ways to generate more data (for training, validation, texting). &lt;/p&gt;

&lt;p&gt;There might be a little that can be done with text augmentation, creating fake data from the real data, pseudo labelling, resampling/oversampling/undersampling but I am curious how onerous it would be to set up a Generative Adversarial Network to create realistic but fake data. &lt;/p&gt;

&lt;p&gt;Keep in mind that all I&amp;#39;m doing is classifying based on a number of attributes based on a statistical model. Not trying to do visual detection in any way. But I do need statistically accurate data. &lt;/p&gt;

&lt;p&gt;So many of the models I can find that use GANs are visual detection....&lt;/p&gt;

&lt;p&gt;Even if someone can point me to examples or papers where people are using a GAN to generate accurate &amp;quot;text&amp;quot; data would help.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6q69,True,,apercu_consulting,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6q69/generating_training_data_how_practical_is_a_gan/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6q69/generating_training_data_how_practical_is_a_gan/,155203,1588858792.0,0,,False,,,,
,learnmachinelearning,"Hello all,

Recently I've been reading Bishop's excellent book. I already have a fair understanding of ML &amp; DL but I want to read it to acquire a deeper understanding of the core concepts described there.

However, I find the book quite challenging, even in the first chapter. For example, I am having a lot of trouble grasping equations 1.68 to 1.72 or the ""straightforward"" result in 1.58 (you can easily find the pdf online).

So, I would like to ask for two things:

1. Where should I go to refresh my math background needed for this? I wouldn't want to take a huge detour because I feel that this will distract me from my final goal and I already have some background in calculus, linear algebra and probabilities. Essentially, I would like to quickly go through some short theory and practical exercises that combine integrals, expected values and vectors/matrices.
2. Is there any in detail discussion group or video that deals with the book? For example, something that expands Bishop's ""straightforward"" to ""here is a detailed step-to-step calculation that leads us to this result""

Thank you very much for your time",t2_6eozcnvb,False,,0,False,"Regarding Bishop's ""Pattern Recognition and Machine Learning""",[],r/learnmachinelearning,False,6,,0,,False,t3_gf4qcu,False,dark,1.0,,public,3,0,{},,,False,[],,False,False,,{},,False,3,,False,self,1588851068.0,,[],{},,,True,,1588879473.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all,&lt;/p&gt;

&lt;p&gt;Recently I&amp;#39;ve been reading Bishop&amp;#39;s excellent book. I already have a fair understanding of ML &amp;amp; DL but I want to read it to acquire a deeper understanding of the core concepts described there.&lt;/p&gt;

&lt;p&gt;However, I find the book quite challenging, even in the first chapter. For example, I am having a lot of trouble grasping equations 1.68 to 1.72 or the &amp;quot;straightforward&amp;quot; result in 1.58 (you can easily find the pdf online).&lt;/p&gt;

&lt;p&gt;So, I would like to ask for two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Where should I go to refresh my math background needed for this? I wouldn&amp;#39;t want to take a huge detour because I feel that this will distract me from my final goal and I already have some background in calculus, linear algebra and probabilities. Essentially, I would like to quickly go through some short theory and practical exercises that combine integrals, expected values and vectors/matrices.&lt;/li&gt;
&lt;li&gt;Is there any in detail discussion group or video that deals with the book? For example, something that expands Bishop&amp;#39;s &amp;quot;straightforward&amp;quot; to &amp;quot;here is a detailed step-to-step calculation that leads us to this result&amp;quot;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you very much for your time&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4qcu,True,,P52-328,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4qcu/regarding_bishops_pattern_recognition_and_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4qcu/regarding_bishops_pattern_recognition_and_machine/,155203,1588850673.0,0,,False,,,,
,learnmachinelearning," Coursera is free for university students now and soon I'll be having an interview for Data Science Intern position. I have experience mostly with machine learning with Scikit-learn, also Numpy, Scipy and Pandas, but not very much. I want to pick up some courses and prepare, I have quite a bit of time. For my case Python is preferable to R. Which ones would you suggest?",t2_wounm,False,,0,False,What Coursera courses would you suggest to prepare for Data Science Intern interview?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_gfaguq,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},Request,False,0,,False,self,False,,[],{},,,True,,1588899755.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Coursera is free for university students now and soon I&amp;#39;ll be having an interview for Data Science Intern position. I have experience mostly with machine learning with Scikit-learn, also Numpy, Scipy and Pandas, but not very much. I want to pick up some courses and prepare, I have quite a bit of time. For my case Python is preferable to R. Which ones would you suggest?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,gfaguq,True,,qalis,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gfaguq/what_coursera_courses_would_you_suggest_to/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gfaguq/what_coursera_courses_would_you_suggest_to/,155203,1588870955.0,0,,False,,,,
,learnmachinelearning,,t2_63zbto72,False,,0,False,Implementation of 3D Photo inpainting,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_geg4zw,False,light,0.97,,public,391,0,{},140.0,,False,[],,True,False,,{},Project,False,391,,False,https://b.thumbs.redditmedia.com/EIp6H9fFk4O35-wQaMr_sLiRbMWNeCgCWjPO9dKZiro.jpg,False,,[],{},image,,False,,1588782832.0,richtext,6,,,text,i.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?format=png8&amp;s=a9ed8c98cb65e8bc0c748861c7210335ca6d2c6d', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=0a0a6d8d67284d81d2e7768a2d0971296faaf77e', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=9635eb5d622349a9f1e3e5b7db99e3851a116972', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=77fa81dea801f2b4b1afd5923c173a9f10321c1f', 'width': 320, 'height': 426}], 'variants': {'gif': {'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?s=3815b66431aa59597ecb25093cc7764dcadef142', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;crop=smart&amp;s=14ea85c8b61377d04b9696e61349d2874628794b', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;crop=smart&amp;s=f32d2c592959cbb6ad4c847faee8e002dfe8dfb6', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;crop=smart&amp;s=a5402ace5d65646084af38ba8ba99782b2830f71', 'width': 320, 'height': 426}]}, 'mp4': {'source': {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?format=mp4&amp;s=a9dc40864c4a37431eb2681075c75e9cc00eadff', 'width': 600, 'height': 800}, 'resolutions': [{'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=108&amp;format=mp4&amp;s=f259a806e3d4f3b18cbd4f3d7a541f0545d58237', 'width': 108, 'height': 144}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=216&amp;format=mp4&amp;s=0fa7553043bea1f02bc6ae55c4f5161a56e7748a', 'width': 216, 'height': 288}, {'url': 'https://preview.redd.it/apdpu72qv3x41.gif?width=320&amp;format=mp4&amp;s=d11efe4f201e8dd2c57cc25001392e2e4c4a2f03', 'width': 320, 'height': 426}]}}, 'id': 'PxEWqC7qZfOJSIho3t9QpH-26WNojRnqWCQaq702UA8'}], 'enabled': True}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geg4zw,True,,wordflowai,,14,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geg4zw/implementation_of_3d_photo_inpainting/,all_ads,False,https://i.redd.it/apdpu72qv3x41.gif,155203,1588754032.0,0,,False,,,,
,learnmachinelearning,,t2_24j8nwr,False,,0,False,"Hello, Iâ€™d like to make a neural network for getting points clouds to know what has been scanned or 3d scanners to know what has been scanned in order to automaticly process it in a drawn out plan. Is this possible?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf5w3k,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,default,False,,[],{},,,False,,1588884451.0,richtext,6,,,text,self.ArtificialInteligence,False,,,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf5w3k,True,,AVEdrums,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5w3k/hello_id_like_to_make_a_neural_network_for/,all_ads,False,/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/,155203,1588855651.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'ArtificialInteligence', 'selftext': 'So Iâ€™m a student land sureveying and Iâ€™m doing research on how ML and AI and DL interferes in our profession and Iâ€™d like to make a neural network on: \n\nSo when we scan the environment, all these points are measured and Iâ€™d like to get the scanner to know what it has measured so it directly puts the information in the CAD program. So when it scans a wall, when I upload the data it automaticly draws a wall out of the points. The same for other aspects of objects of the buidling or environment. Is this possible? I really would like some help,thanks!!', 'author_fullname': 't2_24j8nwr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Hello, Iâ€™d like to make a neural network for getting points clouds to know what has been scanned or 3d scanners to know what has been scanned in order to automaticly process it in a drawn out plan. Is this possible?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/ArtificialInteligence', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': None, 'hide_score': False, 'name': 't3_gf5vll', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 1, 'approved_by': None, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588884395.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.ArtificialInteligence', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So Iâ€™m a student land sureveying and Iâ€™m doing research on how ML and AI and DL interferes in our profession and Iâ€™d like to make a neural network on: &lt;/p&gt;\n\n&lt;p&gt;So when we scan the environment, all these points are measured and Iâ€™d like to get the scanner to know what it has measured so it directly puts the information in the CAD program. So when it scans a wall, when I upload the data it automaticly draws a wall out of the points. The same for other aspects of objects of the buidling or environment. Is this possible? I really would like some help,thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_3crzr', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gf5vll', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AVEdrums', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/ArtificialInteligence/comments/gf5vll/hello_id_like_to_make_a_neural_network_for/', 'subreddit_subscribers': 34102, 'created_utc': 1588855595.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_gf5vll,,
,learnmachinelearning,"Please give me a link or the name of the site. 

I think it can help noobs like us become a little better in ML",t2_5fi9eeym,False,,0,False,Is there any place where I can practice mathematical problems or solve theoretical problems on ML?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf9p4z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588897404.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Please give me a link or the name of the site. &lt;/p&gt;

&lt;p&gt;I think it can help noobs like us become a little better in ML&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf9p4z,True,,dhokna,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf9p4z/is_there_any_place_where_i_can_practice/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf9p4z/is_there_any_place_where_i_can_practice/,155203,1588868604.0,0,,False,,,,
,learnmachinelearning,"I'm processing 200k texts into embeddings of dimension 512, and Jupyter Notebook stops responding when it finishes.",t2_b6hjxvn,False,,0,False,"In Python, what can I use to process feature vectors and store them to clean away from memory?",[],r/learnmachinelearning,False,6,,0,,False,t3_gf5gol,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588882730.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m processing 200k texts into embeddings of dimension 512, and Jupyter Notebook stops responding when it finishes.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5gol,True,,R717159631668645,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5gol/in_python_what_can_i_use_to_process_feature/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5gol/in_python_what_can_i_use_to_process_feature/,155203,1588853930.0,0,,False,,,,
,learnmachinelearning,"In this tutorial, youâ€™ll learn how to encode melodies effectively to train a neural network with the aim to generate music. In the process, youâ€™ll also learn fundamental music theory concepts (e.g., key, time signature) that are important to understand the melody generation problem better.

This video is part of the series â€œGenerating melodies with LSTM netsâ€, a course thatâ€™ll teach you how to build RNN-LSTMs to generate effective melodies using TensorFlow.

Hereâ€™s the video:

[https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2](https://www.youtube.com/watch?v=LFDovU96EdY&amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;index=2)",t2_12ahau,False,,0,False,Music representation for melody generation,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf8qtc,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Project,False,1,,False,self,False,,[],{},self,,True,,1588894455.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;In this tutorial, youâ€™ll learn how to encode melodies effectively to train a neural network with the aim to generate music. In the process, youâ€™ll also learn fundamental music theory concepts (e.g., key, time signature) that are important to understand the melody generation problem better.&lt;/p&gt;

&lt;p&gt;This video is part of the series â€œGenerating melodies with LSTM netsâ€, a course thatâ€™ll teach you how to build RNN-LSTMs to generate effective melodies using TensorFlow.&lt;/p&gt;

&lt;p&gt;Hereâ€™s the video:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.youtube.com/watch?v=LFDovU96EdY&amp;amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;amp;index=2""&gt;https://www.youtube.com/watch?v=LFDovU96EdY&amp;amp;list=PL-wATfeyAMNr0KMutwtbeDCmpwvtul-Xz&amp;amp;index=2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?auto=webp&amp;s=73d6e77c8501a3243a64934d63e734949dce6744', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d4e09f088ad5c8f245449dfbbfad938054fc5d', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4372641e203f86eec7e32236b904dd4ead63fa6', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/lrkuYtso0q54YGD3unSlvNUYtm6np2KYPcvOC-NlrtU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=13f227e3e50ff569cdf927461ba607f1531e232c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'iaCrpzcwgXUcic4WH0ZKOYi64AWzv1iJSFC4xa5F0r4'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gf8qtc,True,,diabulusInMusica,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf8qtc/music_representation_for_melody_generation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf8qtc/music_representation_for_melody_generation/,155203,1588865655.0,0,,False,,,,
,learnmachinelearning,"in coursera,udemy or edx.

Thanks",t2_14sxpc49,False,,0,False,Any suggestions on Advanced Machine Learning MOOC's.,[],r/learnmachinelearning,False,6,,0,,False,t3_gewja9,False,dark,1.0,,public,16,0,{},,,False,[],,False,False,,{},,False,16,,False,self,False,,[],{},,,True,,1588841429.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;in coursera,udemy or edx.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gewja9,True,,esenthil,,23,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gewja9/any_suggestions_on_advanced_machine_learning_moocs/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gewja9/any_suggestions_on_advanced_machine_learning_moocs/,155203,1588812629.0,0,,False,,,,
,learnmachinelearning,,t2_329unddh,False,,0,False,Top 7 Frameworks That Have Enhanced Machine Learning,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,72.0,False,t3_gf3fcu,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},Discussion,False,3,,False,https://b.thumbs.redditmedia.com/Mg1gO0cutXWgjvPPRV2rPLK45VkKWnXMqlKdx-TV6gk.jpg,False,,[],{},link,,False,,1588872788.0,richtext,6,,,text,technostacks.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?auto=webp&amp;s=b20ad349d0d2a4e82e3b3e1bf6d5355f4f78e894', 'width': 1021, 'height': 529}, 'resolutions': [{'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f1b284895a959d0291fe835c7a88dfe044c07f6', 'width': 108, 'height': 55}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=396a3cc7afd29b7cc243c786a2bd8413f2ba0ebd', 'width': 216, 'height': 111}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1bfb13357ce09a7812760beeef9816b2cc422ae7', 'width': 320, 'height': 165}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=21843c0a2e83053cb7d6666a0eae7c14ecfcdf81', 'width': 640, 'height': 331}, {'url': 'https://external-preview.redd.it/cF8EjB49TITEm0NbkNKwFt1PS3rmYZBxjY7DGBI7x7o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=45c607ee7c925eda9196c1f4d9cef17f7fc8cb36', 'width': 960, 'height': 497}], 'variants': {}, 'id': 'L1eORhldnrXeb_c7TOriyOripKWWZdHgDDB6rDxb-e8'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf3fcu,True,,MichaelOconnor1,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3fcu/top_7_frameworks_that_have_enhanced_machine/,all_ads,False,https://technostacks.com/blog/machine-learning-frameworks/,155203,1588843988.0,0,,False,,,,
,learnmachinelearning,"Hi !

I'm looking for learning buddies interested in learning reinforcement learning. I've played around with RL before, but I want to get more in depth. 

&amp;#x200B;

I'm planning on going though the book Reinforcement Learning An Introduction Second Edition, but I don't think we need to go through the same material to help each other out, although I would be fun to !

&amp;#x200B;

Anyways, if you're interested in learning RL, hit me up !",t2_2gaeqekv,False,,0,False,Looking for learning buddies to learn reinforcement learning,[],r/learnmachinelearning,False,6,,0,,False,t3_gf87zh,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588892747.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi !&lt;/p&gt;

&lt;p&gt;I&amp;#39;m looking for learning buddies interested in learning reinforcement learning. I&amp;#39;ve played around with RL before, but I want to get more in depth. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I&amp;#39;m planning on going though the book Reinforcement Learning An Introduction Second Edition, but I don&amp;#39;t think we need to go through the same material to help each other out, although I would be fun to !&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Anyways, if you&amp;#39;re interested in learning RL, hit me up !&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf87zh,True,,simetin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf87zh/looking_for_learning_buddies_to_learn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf87zh/looking_for_learning_buddies_to_learn/,155203,1588863947.0,0,,False,,,,
,learnmachinelearning,,t2_3h885uzr,False,,0,False,The Extent of AI and ML Technologyâ€™s Impact on the Banking Sector: The Blueprint for Future,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,64.0,False,t3_gf4nfq,False,light,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},Discussion,False,2,,False,https://b.thumbs.redditmedia.com/3Sv2Lh4sPhSBInnFgz2A0g7SaxJRThSEzPCWos7vIZQ.jpg,False,,[],{},link,,False,,1588879063.0,richtext,6,,,text,artiba.org,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?auto=webp&amp;s=7e73463528b3f1ec00ddad4dfdcb7cb1aa4f20b5', 'width': 800, 'height': 370}, 'resolutions': [{'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14a963b77d7ecc2cf7cfcaeebfe366b018b0eafb', 'width': 108, 'height': 49}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0dd6760440080dbb52c54adf6f0f8464380560e4', 'width': 216, 'height': 99}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb7647d16dc22a65332159736bde6e33451b150d', 'width': 320, 'height': 148}, {'url': 'https://external-preview.redd.it/rUHeeehlntNMhdvqi7zOjL2L-wcjwT89gpLBoPDth7A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31f74e8279935c76a869952b43eff761b26e51ab', 'width': 640, 'height': 296}], 'variants': {}, 'id': 'GbV_qWNexnb-NbFFtwMTFXkR2MgYvIQLnafehtQpeCY'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf4nfq,True,,Albertchristopher,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4nfq/the_extent_of_ai_and_ml_technologys_impact_on_the/,all_ads,False,https://www.artiba.org/blog/ai-and-machine-learning-are-reshaping-the-global-banking-industry,155203,1588850263.0,0,,False,,,,
,learnmachinelearning,,t2_dpzgk,False,,0,False,Free Webinar on Introduction to Natural Language Processing,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_gf6wfk,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Un0eHYHmvVKy6ipnaOlVam6B-I6roaiEYrauR-jetlo.jpg,False,,[],{},link,,False,,1588888229.0,text,6,,,text,eventbrite.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?auto=webp&amp;s=76e72ca41b90717601e2641b902060778a0bc558', 'width': 1000, 'height': 500}, 'resolutions': [{'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=390d729b171af5dce45ff794338eed85ff77c32b', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e1b0ac00c1cc2618da260ae45982b34afb288b5', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9b0062658fb95b645cd09ad18bfcbab0867e76b', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ad44f249a984cba029eecad260e0931f576b3fa', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/mVJdAMEACKkI8GMd32QaG4IRgGgizVAgeB1AB5ocepc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=57c21d840eef27e3227d920511429e4e16631f82', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'zaiQsmrwpYwZxxNwh8-xb_WVPNqohHVWF54s80Or-Zw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6wfk,True,,Reginald_Martin,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6wfk/free_webinar_on_introduction_to_natural_language/,all_ads,False,https://www.eventbrite.com/e/free-webinar-on-introduction-to-natural-language-processing-tickets-104126148112,155203,1588859429.0,0,,False,,,,
,learnmachinelearning,"I am working with mapped cylindrical images - so the image wraps around 360 degrees, with the left side ""joined"" to the right side. As such, my objects are always the same width as the image. The height is variable.

Is it possible to inform a region proposal network of this fact? I looked through detectron2's modules, and found [this line](https://github.com/facebookresearch/detectron2/blob/c526a492e4eb51cf8ad08b8af8f076dab72697e7/detectron2/structures/boxes.py#L210) regarding height and width threshold values for dropping/keeping a proposal.

Is it as simple as editing that to reference my image width? Or will this have some unintended side effects / not improve accuracy/false positives?

I suspect tuning the RPN network proposal size has been done before - but I couldn't find any specific references to it. If anyone has any resources/papers, please let me know!",t2_5hswm,False,,0,False,"I want to improve bounding box proposals in detectron2's RPN, as I know my objects always have a specific width. Is this possible?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf6r8t,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},self,,True,,1588887701.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am working with mapped cylindrical images - so the image wraps around 360 degrees, with the left side &amp;quot;joined&amp;quot; to the right side. As such, my objects are always the same width as the image. The height is variable.&lt;/p&gt;

&lt;p&gt;Is it possible to inform a region proposal network of this fact? I looked through detectron2&amp;#39;s modules, and found &lt;a href=""https://github.com/facebookresearch/detectron2/blob/c526a492e4eb51cf8ad08b8af8f076dab72697e7/detectron2/structures/boxes.py#L210""&gt;this line&lt;/a&gt; regarding height and width threshold values for dropping/keeping a proposal.&lt;/p&gt;

&lt;p&gt;Is it as simple as editing that to reference my image width? Or will this have some unintended side effects / not improve accuracy/false positives?&lt;/p&gt;

&lt;p&gt;I suspect tuning the RPN network proposal size has been done before - but I couldn&amp;#39;t find any specific references to it. If anyone has any resources/papers, please let me know!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?auto=webp&amp;s=f680398e7963974ca0243b664917ac9952be3d5a', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=37c2c0153cd32a88568382d114cb65cee015284b', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d00b5d2870902fc94ba53b0d4fd13f417386d957', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/7zz8EiQYKifNt2Ljh9ukl_79rAb8Ht6ukYk7DWS3inA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdf76d779dfc79c842c98f44da755e0c5860751d', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'TSNP042Xulx8o04vL-EtljVjrl9Il3rg6hZjwvpza3E'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf6r8t,True,,Fenr-i-r,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6r8t/i_want_to_improve_bounding_box_proposals_in/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6r8t/i_want_to_improve_bounding_box_proposals_in/,155203,1588858901.0,0,,False,,,,
,learnmachinelearning,"Hi, I am a software engineer looking for a solution by machine learning to compare my graphs.

My graphs are directed and each node has multiple dimension weights. I want to generate whole graph embeddings to compare. (not node nor edge embeddings)
Do you know any good software or ML algorithm to geranate them?

I have tried benedekrozemberczki/karateclub but it does not support directed graphs and nodes with weight.
Please give me your idea.
TIA.",t2_xmoea,False,,0,False,Directed graph embedding with node weight,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf6qfh,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},HELP,False,1,,False,self,False,,[],{},,,True,,1588887620.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi, I am a software engineer looking for a solution by machine learning to compare my graphs.&lt;/p&gt;

&lt;p&gt;My graphs are directed and each node has multiple dimension weights. I want to generate whole graph embeddings to compare. (not node nor edge embeddings)
Do you know any good software or ML algorithm to geranate them?&lt;/p&gt;

&lt;p&gt;I have tried benedekrozemberczki/karateclub but it does not support directed graphs and nodes with weight.
Please give me your idea.
TIA.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gf6qfh,True,,metaphoricwords,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6qfh/directed_graph_embedding_with_node_weight/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf6qfh/directed_graph_embedding_with_node_weight/,155203,1588858820.0,0,,False,,,,
,learnmachinelearning,,t2_1myz87vv,False,,0,False,Cohen's kappa coefficient in Python,[],r/learnmachinelearning,False,6,,0,93.0,False,t3_gf6lha,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/nnrqMOV9NWrNuZaeZjdCiSZWkJDoEXofEY9uQRQXmuc.jpg,False,,[],{},link,,False,,1588887116.0,text,6,,,text,bush-dev.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?auto=webp&amp;s=c5601aefd19481d4fea7093158128ee8bc7779ee', 'width': 1000, 'height': 667}, 'resolutions': [{'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=833a0bc74755bf7b46adcafd7086c178b75d1138', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=88242a3100f1d216d5bb1d8413e8dae72f91b03c', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3a0d9463b348f5e188de85888d91c549dea6bbc', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d78fb7a33829fa598ec0176b8edb1948b60f14a3', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/8akhQuxG8sWgcSgqd_-ZVjaNXqUZohKI9p51yAfJXO0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84901931ba1cf1e6e285ee05e608d41d388afb32', 'width': 960, 'height': 640}], 'variants': {}, 'id': 'cZgTK8hmv5pC-zhmJ_Ih3wjEo1GyXpxQYLV3ccidUQE'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf6lha,True,,bush_dev,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf6lha/cohens_kappa_coefficient_in_python/,all_ads,False,http://bush-dev.com/cohens-kappa-coefficient-in-python,155203,1588858316.0,0,,False,,,,
,learnmachinelearning,"Hi,
I used Random Forest to predict what measures a government must take to achieve a Covid-19 growth rate of less than 5%. I got 99% accuracy but 61% specificity. Is it a good model?

https://www.kaggle.com/gianlab/government-measures-against-covid-2",t2_4dpo2m4y,False,,0,False,Government measures against Covid,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5dwq,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588882416.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi,
I used Random Forest to predict what measures a government must take to achieve a Covid-19 growth rate of less than 5%. I got 99% accuracy but 61% specificity. Is it a good model?&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://www.kaggle.com/gianlab/government-measures-against-covid-2""&gt;https://www.kaggle.com/gianlab/government-measures-against-covid-2&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/x4RZ-_GIkVwaUtmgleV05BCjOdsqUGfFaLyBlP72NUg.jpg?auto=webp&amp;s=6d3f76b84a633a2a41c9c2d39039f059c1f09974', 'width': 100, 'height': 100}, 'resolutions': [], 'variants': {}, 'id': 'eE0bEqjeT_FoRR2Lo7jX20-FVqPnvpBwCX-_MlcZFX8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5dwq,True,,lovepeacejoy4,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5dwq/government_measures_against_covid/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5dwq/government_measures_against_covid/,155203,1588853616.0,0,,False,,,,
,learnmachinelearning,"I am unable to understand the relationship between training and validation performance metrics for my NLP experiment (BERT-LSTM-Linear model). Are there any papers out there exploring what causes 

1. High training accuracy, low validation accuracy
2. Oscillating loss during training
3. A decrease in F1 scores (weight, micro, macro) during training

I understand that it must change from data to data, but reading through some papers describing relationships between the metrics or even trying to explore might help put things into some perspective.",t2_9ptho1m,False,,0,False,NLP papers exploring training and validation performance metrics?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf550p,False,dark,0.67,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588881397.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am unable to understand the relationship between training and validation performance metrics for my NLP experiment (BERT-LSTM-Linear model). Are there any papers out there exploring what causes &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;High training accuracy, low validation accuracy&lt;/li&gt;
&lt;li&gt;Oscillating loss during training&lt;/li&gt;
&lt;li&gt;A decrease in F1 scores (weight, micro, macro) during training&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I understand that it must change from data to data, but reading through some papers describing relationships between the metrics or even trying to explore might help put things into some perspective.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf550p,True,,freaky_eater,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf550p/nlp_papers_exploring_training_and_validation/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf550p/nlp_papers_exploring_training_and_validation/,155203,1588852597.0,0,,False,,,,
,learnmachinelearning,"Isn't there a term for a problem with no hidden information, e.g. the problem state is fully visible at each iteration?",t2_4we1a,False,,0,False,[Question] Name for the term of problems with complete information,[],r/learnmachinelearning,False,6,,0,,False,t3_gf5453,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588881289.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Isn&amp;#39;t there a term for a problem with no hidden information, e.g. the problem state is fully visible at each iteration?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf5453,True,,Stewie977,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf5453/question_name_for_the_term_of_problems_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf5453/question_name_for_the_term_of_problems_with/,155203,1588852489.0,0,,False,,,,
,learnmachinelearning,"Here is a [TF example](https://github.com/tensorflow/examples/blob/master/community/en/autoencoder.ipynb) for autoencoder and it passes a single vector of MINST dataset with fixed size  (784,) . 

If my input data is not fixed-size then what should I do? how can I change it to be flexible in input data?",t2_1k4nqnsa,False,,0,False,Autoencoder with not fix size,[],r/learnmachinelearning,False,6,,0,,False,t3_gf1qhl,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588863911.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Here is a &lt;a href=""https://github.com/tensorflow/examples/blob/master/community/en/autoencoder.ipynb""&gt;TF example&lt;/a&gt; for autoencoder and it passes a single vector of MINST dataset with fixed size  (784,) . &lt;/p&gt;

&lt;p&gt;If my input data is not fixed-size then what should I do? how can I change it to be flexible in input data?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1qhl,True,,kiasari,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1qhl/autoencoder_with_not_fix_size/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf1qhl/autoencoder_with_not_fix_size/,155203,1588835111.0,0,,False,,,,
,learnmachinelearning,"Hello!  


I'm writing this post because I need help with my college project - I am saying this because I have some strict requirements due to that. One of them is necessity of using model that will help with predicting certain values. I hope that you will be able to give me some advice.

My project in few words is tool for predicting your internet speed- based on measurements. The more measurements there are gathered, the better result.

The most important part of my project is the predictor. I have automated process of gathering data, now its time to use that data. And here comes my problem. I decided to use ARIMA model for time series forecasting. But it turned out to be complete disaster with my data. I don't know if arima in python is just not that good as it should be or it's just wrong model for my task. 

Should I maybe use R language for that? will it perform better? Which model should I use with my data? 

Here's sample of my data - it's not complete for now. It will be spectrum of full 24hours.

[https://i.imgur.com/xuyFiSx.png](https://i.imgur.com/xuyFiSx.png)

&amp;#x200B;

Thanks for any advice and have a nice day!",t2_nst5o,False,,0,False,Which model should I use for my predictor? (little data)(timeseriesforecasting),[],r/learnmachinelearning,False,6,,0,,False,t3_gf4b9d,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588877373.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello!  &lt;/p&gt;

&lt;p&gt;I&amp;#39;m writing this post because I need help with my college project - I am saying this because I have some strict requirements due to that. One of them is necessity of using model that will help with predicting certain values. I hope that you will be able to give me some advice.&lt;/p&gt;

&lt;p&gt;My project in few words is tool for predicting your internet speed- based on measurements. The more measurements there are gathered, the better result.&lt;/p&gt;

&lt;p&gt;The most important part of my project is the predictor. I have automated process of gathering data, now its time to use that data. And here comes my problem. I decided to use ARIMA model for time series forecasting. But it turned out to be complete disaster with my data. I don&amp;#39;t know if arima in python is just not that good as it should be or it&amp;#39;s just wrong model for my task. &lt;/p&gt;

&lt;p&gt;Should I maybe use R language for that? will it perform better? Which model should I use with my data? &lt;/p&gt;

&lt;p&gt;Here&amp;#39;s sample of my data - it&amp;#39;s not complete for now. It will be spectrum of full 24hours.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://i.imgur.com/xuyFiSx.png""&gt;https://i.imgur.com/xuyFiSx.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any advice and have a nice day!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/vuPSR-xlP0WJKC0qV4QDwV1q557vzZGwUsXTZZ5Ec1k.png?auto=webp&amp;s=b198bda107895ddd36a4e9f9dd4dde9f3d6cf6e3', 'width': 182, 'height': 865}, 'resolutions': [{'url': 'https://external-preview.redd.it/vuPSR-xlP0WJKC0qV4QDwV1q557vzZGwUsXTZZ5Ec1k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=16715678af7abbfa80d9a1562d16f49dc1981e6b', 'width': 108, 'height': 216}], 'variants': {}, 'id': 'DHgshvdwrgf92TAKD16PfKn4h_sIv5tujj0g5VZmnus'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf4b9d,True,,cl_m4ster,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf4b9d/which_model_should_i_use_for_my_predictor_little/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf4b9d/which_model_should_i_use_for_my_predictor_little/,155203,1588848573.0,0,,False,,,,
,learnmachinelearning,"Any good books for Tensorflow 2.0 and also any online course for all levels. Thanks for the help,",t2_14sxpc49,False,,0,False,Tensorflow 2.0 Books or courses,[],r/learnmachinelearning,False,6,,0,,False,t3_gewl58,False,dark,0.86,,public,5,0,{},,,False,[],,False,False,,{},,False,5,,False,self,False,,[],{},,,True,,1588841608.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Any good books for Tensorflow 2.0 and also any online course for all levels. Thanks for the help,&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gewl58,True,,esenthil,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gewl58/tensorflow_20_books_or_courses/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gewl58/tensorflow_20_books_or_courses/,155203,1588812808.0,0,,False,,,,
,learnmachinelearning,"I have total 3000 900x900 matrix( same as 1 band image) include SAR scatter values. (SAR scatter pixels give us geometry information where signals comes like ""34.515, 29.1618, 15.5489""). I want to use this matrix feature. My labels is total 3000 900x900 matrix also there are pixels only two values 0 and 1. 1 is building 0 is non-building. I'd like train my scatter values for labels 1 or 0 . After that i will use created model on my test SAR images.

1- Which losses, activations, optimazer etc. may i use ?

2- I tried several times in Python (Tensorflow) but there is a lot of features (3000x900x900) and labels same dimension as well. There is an error OOM. For this error can i use genaretor ? Is this make sense?

3- I am new to machine learning if there is a nonsense in my sentences sorry about it.
Thanks for your helps",t2_3dwu4yfm,False,,0,False,Suggestion machine learning model and tips multi dimensional input and output,[],r/learnmachinelearning,False,6,,0,,False,t3_gf3nv8,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588874033.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have total 3000 900x900 matrix( same as 1 band image) include SAR scatter values. (SAR scatter pixels give us geometry information where signals comes like &amp;quot;34.515, 29.1618, 15.5489&amp;quot;). I want to use this matrix feature. My labels is total 3000 900x900 matrix also there are pixels only two values 0 and 1. 1 is building 0 is non-building. I&amp;#39;d like train my scatter values for labels 1 or 0 . After that i will use created model on my test SAR images.&lt;/p&gt;

&lt;p&gt;1- Which losses, activations, optimazer etc. may i use ?&lt;/p&gt;

&lt;p&gt;2- I tried several times in Python (Tensorflow) but there is a lot of features (3000x900x900) and labels same dimension as well. There is an error OOM. For this error can i use genaretor ? Is this make sense?&lt;/p&gt;

&lt;p&gt;3- I am new to machine learning if there is a nonsense in my sentences sorry about it.
Thanks for your helps&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3nv8,True,,cartwhell07,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3nv8/suggestion_machine_learning_model_and_tips_multi/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf3nv8/suggestion_machine_learning_model_and_tips_multi/,155203,1588845233.0,0,,False,,,,
,learnmachinelearning,,t2_5pglpm9p,False,,0,False,5 Booming AI and ML Trends 2020,[],r/learnmachinelearning,False,6,,0,73.0,False,t3_gf3ngt,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/u1YhSofEcxxN8XbWeaejEGeQJSHaTmaJsJJw5CQzhbk.jpg,False,,[],{},link,,False,,1588873978.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?auto=webp&amp;s=125a2fc27267efe35cace13d9fa3170570be6ba8', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d70e961e7631d50c96c6ce6c92b46344c6f80e5', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e18ef5af193df2dc849587e6cebc07a4ac4c8eb3', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3226865fe5f3698229c923638f94f46fe7016c2', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d705dd59e553805311b3c8670705dbe8ba599f6c', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f7136325cbd84265c9e7c8f687aecbb9f6cdf8b', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/_YBv97eRcF-HtrTGIkqrMH-hbFWzQLfTWB8Gp5ckdGU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72634008d56f3f3a588db6ecadd5f2d7da3bffae', 'width': 1080, 'height': 565}], 'variants': {}, 'id': '3Cwo_31cs1xzlXXKDTbqT1iw_-TqVOL1iu_gR5cQWzw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3ngt,True,,Ramesh_Sethi,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3ngt/5_booming_ai_and_ml_trends_2020/,all_ads,False,https://medium.com/@Oodles_ai/5-booming-ai-and-ml-trends-2020-5f22589eea3e,155203,1588845178.0,0,,False,,,,
,learnmachinelearning,,t2_4hnnxddz,False,,0,False,Log transformation increased my data Skewness. Need help,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_geuwia,False,light,1.0,,public,8,0,{},140.0,,False,[],,True,False,,{},HELP,False,8,,False,https://b.thumbs.redditmedia.com/PsoxyfOIStJG2jHxu71wCL0vQkux6alytnQycK7NFOs.jpg,False,,[],{},image,,False,,1588835590.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?auto=webp&amp;s=2dbeac26cffa40bb02b1a9ee275839223ab9bbe0', 'width': 1920, 'height': 1920}, 'resolutions': [{'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4177ba99515adea53b2925703c4b6a914e6dfaf8', 'width': 108, 'height': 108}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b94f4336d487c8a4fe5219baa287bf5604aba3e', 'width': 216, 'height': 216}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cf0f6bedf127a0b68b18f1ab8bbff7e8367ece1', 'width': 320, 'height': 320}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d361256d81323d4d0819c0ccc43d15c396a97fa5', 'width': 640, 'height': 640}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=739d19b23fba63d57e025216d9bf927df73b1eeb', 'width': 960, 'height': 960}, {'url': 'https://preview.redd.it/87hd4e5s88x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd5efe99410cdd52bd7bb244e7877e9ec4ef403c', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'LkI25zMKYOb9GHdTNUFB9vXJtks65Oaiow6OBPeyF30'}], 'enabled': True}",[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geuwia,True,,hyper482,,18,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuwia/log_transformation_increased_my_data_skewness/,all_ads,False,https://i.redd.it/87hd4e5s88x41.jpg,155203,1588806790.0,0,,False,,,,
,learnmachinelearning,"The profile has a major shift in behavior (e.g. Didn't post frequently and suddenly posts frequently).

What are the possible ways to approach this problem???",t2_tabm4,False,,0,False,How to detect sudden user behavior change on twitter over time?,[],r/learnmachinelearning,False,6,,0,,False,t3_gf3j8o,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588873355.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;The profile has a major shift in behavior (e.g. Didn&amp;#39;t post frequently and suddenly posts frequently).&lt;/p&gt;

&lt;p&gt;What are the possible ways to approach this problem???&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf3j8o,True,,waheed0332,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf3j8o/how_to_detect_sudden_user_behavior_change_on/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf3j8o/how_to_detect_sudden_user_behavior_change_on/,155203,1588844555.0,0,,False,,,,
,learnmachinelearning,,t2_47jpmh5m,False,,0,False,Your complete learning path to start a career in machine learning.,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geq3ln,False,dark,0.82,,public,15,0,{},140.0,,False,[],,False,False,,{},,False,15,,False,https://b.thumbs.redditmedia.com/A0e0SeE-wdwF4-aKyyKQyR6MjETnTl0Ro_7PNy2QWMw.jpg,False,,[],{},link,,False,,1588820231.0,text,6,,,text,kdnuggets.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?auto=webp&amp;s=524afcdaf62407becb958f6dec41743d5cdbd5a9', 'width': 1400, 'height': 1866}, 'resolutions': [{'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3faf100be8c1920cd5b68579054b4d66cfa24bb5', 'width': 108, 'height': 143}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3ee3a09706729af760d552dfeac1c6c0375ead6', 'width': 216, 'height': 287}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84720d18e8b849e0a9a02247636a8b010ccda1f4', 'width': 320, 'height': 426}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1aa7b12162f81e806ad9828ca0b8bbded6ca4002', 'width': 640, 'height': 853}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7963756ad494dbcb0cd249e8ea0652a261077d06', 'width': 960, 'height': 1279}, {'url': 'https://external-preview.redd.it/UoGoTBPNqu2y-v1TGP0LsPyC48iybs-bOnNna6t0pTI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d349e383c3a5a881869a6f4b811cca114fe9cd4a', 'width': 1080, 'height': 1439}], 'variants': {}, 'id': 'eneKBvxPey6iAK0yJy6VELxOxNxBg5TW9S_sE2SNcT4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geq3ln,True,,ItisAhmad,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geq3ln/your_complete_learning_path_to_start_a_career_in/,all_ads,False,https://www.kdnuggets.com/2020/05/beginners-learning-path-machine-learning.html,155203,1588791431.0,0,,False,,,,
,learnmachinelearning,https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787,t2_45tb49o2,False,,0,False,Understanding the mathematics behind linear regression- part 1,[],r/learnmachinelearning,False,6,,0,,False,t3_gf26j6,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588866241.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787""&gt;https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf26j6,True,,codegeass30,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf26j6/understanding_the_mathematics_behind_linear/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf26j6/understanding_the_mathematics_behind_linear/,155203,1588837441.0,0,,False,,,,
,learnmachinelearning,"I'm doing multiclass classification in python. There are over 300 classes and 5 instances for each class. I'm currently using random forest classifier. After extensive hyperparameter tuning, the best accuracy performance is around 10% only. I've done other classification problems pretty well so I'm thinking what is causing such bad performance. 

I'd like to know if 5 instances for each class for training is too little and I'd like any suggestions on the problem. Thank you ðŸ˜Š",t2_5oiyst74,False,,0,False,5 samples per class for classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gf22mf,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588865672.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;m doing multiclass classification in python. There are over 300 classes and 5 instances for each class. I&amp;#39;m currently using random forest classifier. After extensive hyperparameter tuning, the best accuracy performance is around 10% only. I&amp;#39;ve done other classification problems pretty well so I&amp;#39;m thinking what is causing such bad performance. &lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to know if 5 instances for each class for training is too little and I&amp;#39;d like any suggestions on the problem. Thank you ðŸ˜Š&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf22mf,True,,xxare,,4,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf22mf/5_samples_per_class_for_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf22mf/5_samples_per_class_for_classification/,155203,1588836872.0,0,,False,,,,
,learnmachinelearning,,t2_1jddy6x5,False,,0,False,Holy smoke! Differentiation is now upstreamed to Swift. It appropriate to say ð›Swift. Yay! ðŸŽ‰ ðŸ¥³,[],r/learnmachinelearning,False,6,,0,138.0,False,t3_gf1upz,False,dark,0.67,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/Iohh_96XHituEFUpTbZD5UY8Z_ygTTb4bZ30cugxoqY.jpg,False,,[],{},link,,False,,1588864514.0,text,6,,,text,forums.swift.org,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?auto=webp&amp;s=f629c0a49e4aaa92449ba32794cf3e737717b518', 'width': 590, 'height': 585}, 'resolutions': [{'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d3658b06e9263859a81ad28fd0b4e39fa6ae4da', 'width': 108, 'height': 107}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6abf1209e759dbbd85985f6a6039c6c82d4403a', 'width': 216, 'height': 214}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8dceba83e7b210c9e71853c9314981e05e36e5d', 'width': 320, 'height': 317}], 'variants': {}, 'id': 'pEdzDuPhjQNkUbvjZWcOHMphCOfFW9cWD3aLjM-IrtU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1upz,True,,rahulbhalley,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1upz/holy_smoke_differentiation_is_now_upstreamed_to/,all_ads,False,https://forums.swift.org/t/trajectory-for-evaluating-adding-automatic-differentiation-to-swift/30048/7?u=dan-zheng,155203,1588835714.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'swift', 'selftext': '', 'author_fullname': 't2_1jddy6x5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Holy smoke! Differentiation is now upstreamed to Swift. It appropriate to say ð›Swift. Yay! ðŸŽ‰ ðŸ¥³', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/swift', 'hidden': False, 'pwls': 6, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 138, 'hide_score': False, 'name': 't3_gf1tie', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 148, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 148, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Iohh_96XHituEFUpTbZD5UY8Z_ygTTb4bZ30cugxoqY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588864341.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'forums.swift.org', 'allow_live_comments': True, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?auto=webp&amp;s=f629c0a49e4aaa92449ba32794cf3e737717b518', 'width': 590, 'height': 585}, 'resolutions': [{'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d3658b06e9263859a81ad28fd0b4e39fa6ae4da', 'width': 108, 'height': 107}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6abf1209e759dbbd85985f6a6039c6c82d4403a', 'width': 216, 'height': 214}, {'url': 'https://external-preview.redd.it/_QArKlECzETtvMpuvjYe6bCC9g4GDToA-GG48WeAW20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8dceba83e7b210c9e71853c9314981e05e36e5d', 'width': 320, 'height': 317}], 'variants': {}, 'id': 'pEdzDuPhjQNkUbvjZWcOHMphCOfFW9cWD3aLjM-IrtU'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2z6zi', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'gf1tie', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'rahulbhalley', 'discussion_type': None, 'num_comments': 23, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/swift/comments/gf1tie/holy_smoke_differentiation_is_now_upstreamed_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://forums.swift.org/t/trajectory-for-evaluating-adding-automatic-differentiation-to-swift/30048/7?u=dan-zheng', 'subreddit_subscribers': 62949, 'created_utc': 1588835541.0, 'num_crossposts': 7, 'media': None, 'is_video': False}]",t3_gf1tie,,
,learnmachinelearning,"I wrote a technical article on how to structure Juptyer notebooks for machine learning projects. Basically my workflow and tips on using Jupyter notebook for productive DS tasks. Let me know what you think, thanks!

https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb",t2_j9tvg,False,,0,False,I wrote a technical article on how to structure Juptyer notebooks for Machine Learning projects,[],r/learnmachinelearning,False,6,,0,,False,t3_gf1pdw,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588863764.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a technical article on how to structure Juptyer notebooks for machine learning projects. Basically my workflow and tips on using Jupyter notebook for productive DS tasks. Let me know what you think, thanks!&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb""&gt;https://medium.com/@desmondyeoh/structuring-jupyter-notebooks-for-fast-and-iterative-machine-learning-experiments-e09b56fa26bb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?auto=webp&amp;s=8a738956afb851262167a873d329931f52ee42b7', 'width': 1200, 'height': 752}, 'resolutions': [{'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5d61980aea9b9080ee7fa192883f3b9fcf385464', 'width': 108, 'height': 67}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d252fbdeae13283cefdf7b0a217567948888216', 'width': 216, 'height': 135}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1626610262de91c49473f71013df141f024fb5f', 'width': 320, 'height': 200}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5bfc54e9754b2e9d4fcf1bf59a13acf21d3840f', 'width': 640, 'height': 401}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfec06e571892c7851e78daf83e4482170893563', 'width': 960, 'height': 601}, {'url': 'https://external-preview.redd.it/4g-6KgQKoejximeD-HkR5Ww9pgfWMaNKCSpHp5_M154.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b7f15a3dd15ade69da3102188184df1d91d10581', 'width': 1080, 'height': 676}], 'variants': {}, 'id': '_BddIxP4XWrPlelML-pKNBjdx_-mOdsZuo7G7NjrzhU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf1pdw,True,,desmondyeoh,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf1pdw/i_wrote_a_technical_article_on_how_to_structure/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf1pdw/i_wrote_a_technical_article_on_how_to_structure/,155203,1588834964.0,0,,False,,,,
,learnmachinelearning,"So I know how to solve probability questions in combinatorics and about discrete/continuous CDF/PDFs / common distributions etc.

But everytime I read a book (e.g., ""Statistical Inference"" or ""All of Statistics: A Concise Course in Statistical Inference""), there's a moment where they introduce a marginal distribution or something (actually I feel like I understand the idea for a discrete case) and continuous priors and I'm kinda lost without seeing any simple examples.

I wanna learn the idea behind the methods of maximum likelihood and random buzzwords like Fisher information.",t2_l109wse,False,,0,False,Could you recommend me a book about statistics (the details inside)?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gerz9k,False,light,1.0,,public,6,0,{},,,False,[],,False,False,,{},HELP,False,6,,False,self,False,,[],{},,,True,,1588826080.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So I know how to solve probability questions in combinatorics and about discrete/continuous CDF/PDFs / common distributions etc.&lt;/p&gt;

&lt;p&gt;But everytime I read a book (e.g., &amp;quot;Statistical Inference&amp;quot; or &amp;quot;All of Statistics: A Concise Course in Statistical Inference&amp;quot;), there&amp;#39;s a moment where they introduce a marginal distribution or something (actually I feel like I understand the idea for a discrete case) and continuous priors and I&amp;#39;m kinda lost without seeing any simple examples.&lt;/p&gt;

&lt;p&gt;I wanna learn the idea behind the methods of maximum likelihood and random buzzwords like Fisher information.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gerz9k,True,,giannis_34,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gerz9k/could_you_recommend_me_a_book_about_statistics/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gerz9k/could_you_recommend_me_a_book_about_statistics/,155203,1588797280.0,0,,False,,,,
,learnmachinelearning,"How can I encourage a network's output to be sparse? 

I'm in an RL setting, creating trajectories by giving an input state, then sampling a continuous vector action. Then eventually I train on good trajectories. I know that sparse outputs will be better than denser ones by nature of the task. Should I just randomly mask some idxs of the output?",t2_12zuf2,False,,0,False,How to encourage sparse output?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf09yz,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588856671.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;How can I encourage a network&amp;#39;s output to be sparse? &lt;/p&gt;

&lt;p&gt;I&amp;#39;m in an RL setting, creating trajectories by giving an input state, then sampling a continuous vector action. Then eventually I train on good trajectories. I know that sparse outputs will be better than denser ones by nature of the task. Should I just randomly mask some idxs of the output?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gf09yz,True,,throwaway775849,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf09yz/how_to_encourage_sparse_output/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf09yz/how_to_encourage_sparse_output/,155203,1588827871.0,0,,False,,,,
,learnmachinelearning,,t2_60jokg20,False,,0,False,Guys please suggest me from where should I learn statistics for machine learning .,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gf06fg,False,light,1.0,,public,1,0,{},,,False,[],,False,False,,{},Discussion,False,1,,False,self,False,,[],{},,,True,,1588856231.0,richtext,6,,,text,self.learnmachinelearning,False,,,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf06fg,True,,omkar_00,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf06fg/guys_please_suggest_me_from_where_should_i_learn/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf06fg/guys_please_suggest_me_from_where_should_i_learn/,155203,1588827431.0,0,,False,,,,
,learnmachinelearning,"If articles like this are not allowed please remove.

All of us here would be familiar with jupyter notebooks, they are a great tool for learning and coding for data science and machine learning.

One constant annoyance for me is the fact that most of these notebooks are so un-intuitive and it takes forever to trace variables through the code to figure out what's going on. I'm writing a series of posts on how to improve user experience in jupyter notebooks; I've published my first one on handling user input. I welcome any feedback.

[https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d](https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d)",t2_faofu,False,,0,False,User experience in jypyter notebooks,[],r/learnmachinelearning,False,6,,0,,False,t3_gf05bu,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},self,,True,,1588856086.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;If articles like this are not allowed please remove.&lt;/p&gt;

&lt;p&gt;All of us here would be familiar with jupyter notebooks, they are a great tool for learning and coding for data science and machine learning.&lt;/p&gt;

&lt;p&gt;One constant annoyance for me is the fact that most of these notebooks are so un-intuitive and it takes forever to trace variables through the code to figure out what&amp;#39;s going on. I&amp;#39;m writing a series of posts on how to improve user experience in jupyter notebooks; I&amp;#39;ve published my first one on handling user input. I welcome any feedback.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d""&gt;https://medium.com/@zahid.p.akbar/ux-in-jupyter-user-input-essentials-779c9b449f2d&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?auto=webp&amp;s=db10c1c33dd1d24a61ed23c5df2cf024e4efce41', 'width': 1200, 'height': 794}, 'resolutions': [{'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4438df21957bb5013befe7b16bfec7b62c86f080', 'width': 108, 'height': 71}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=717928c91a188d140568d5c7c45ebe635e591b26', 'width': 216, 'height': 142}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c837096ca122a82ea01d6af89865f11561e4322d', 'width': 320, 'height': 211}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8383635d07da8ba5485b3224d89e021ab7878a4a', 'width': 640, 'height': 423}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f256c71068288d66a35ad0ceb167b126bda2182', 'width': 960, 'height': 635}, {'url': 'https://external-preview.redd.it/JM8HZqwWHtPIOM7mQhGT9T38nDz6GB0yLfsGG452z-A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15724ec04be6b189a3f673b123c34f3862db710', 'width': 1080, 'height': 714}], 'variants': {}, 'id': 'mqEwOWA7Q6KbXlsNkxzPMqAvcLEiKXFlt3pmCbGsevs'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gf05bu,True,,thezaza101,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf05bu/user_experience_in_jypyter_notebooks/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf05bu/user_experience_in_jypyter_notebooks/,155203,1588827286.0,0,,False,,,,
,learnmachinelearning,"Recently,Iâ€˜m trying to make my App more intelligent and close to the needs of users.Because most of my users use Huawei phones,I used HUAWEI ML Kit on my app.

Currently the experience is very good,Or do you have a better experience?

&amp;#x200B;

https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;format=png&amp;auto=webp&amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1",t2_67j6c1h9,False,,0,False,"In machine learning, I have made new discoveries","[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,55.0,False,t3_gf2h70,False,light,0.33,,public,0,0,{},140.0,,False,[],,False,False,,{},Discussion,False,0,,False,https://b.thumbs.redditmedia.com/3UtCSHUB7kiLhGriCG_aJ2rMpICKeXqL-LaxXoawdgU.jpg,False,,[],{},,,True,,1588867805.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Recently,Iâ€˜m trying to make my App more intelligent and close to the needs of users.Because most of my users use Huawei phones,I used HUAWEI ML Kit on my app.&lt;/p&gt;

&lt;p&gt;Currently the experience is very good,Or do you have a better experience?&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1""&gt;https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gf2h70,True,,Ivy_zhao,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gf2h70/in_machine_learning_i_have_made_new_discoveries/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gf2h70/in_machine_learning_i_have_made_new_discoveries/,155203,1588839005.0,0,,False,,,"{'d6oquz2kwax41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=14e043b7fefdc0d88d394ef28dc9dfc9e8a02322'}, {'y': 86, 'x': 216, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=665c9edb85886b9b06878f670450dfb99a355c25'}, {'y': 127, 'x': 320, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b0a7a3b8ed43d10443aee49b39ea9a9ee4e43c96'}], 's': {'y': 240, 'x': 602, 'u': 'https://preview.redd.it/d6oquz2kwax41.png?width=602&amp;format=png&amp;auto=webp&amp;s=4f97bd34e850064d702f9f1f99efdfb6578425c1'}, 'id': 'd6oquz2kwax41'}}",
,learnmachinelearning,,t2_695y5lr4,False,,0,False,Understanding the shape of large scale data,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,140.0,False,t3_getrpw,False,light,0.72,,public,3,0,{},140.0,,False,[],,False,False,,{},Discussion,False,3,,False,https://a.thumbs.redditmedia.com/ODx-Gi0xkgxOEKqbBtbJ-CUWtjyIiblb219uJo6dAC0.jpg,False,,[],{},link,,False,,1588831789.0,richtext,6,,,text,ai.googleblog.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?auto=webp&amp;s=d45552298a94c0bc0e771853afe179cbb0e3f951', 'width': 1200, 'height': 1200}, 'resolutions': [{'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=467975d187bd3f0e5cf8f0880665db7e4eca4fcb', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=121a848b6e9c40ce8f4c995b663108493b3b069d', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e62560ba614db17109e1924a187a6575b9f7399', 'width': 320, 'height': 320}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46bf2a0eba7661a832cec202bf2cec4660a37b4e', 'width': 640, 'height': 640}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73be8d146553ec5d17c995a6cc1c0f963532ae61', 'width': 960, 'height': 960}, {'url': 'https://external-preview.redd.it/WIrkmtU3_fcptRO4rAsUpS3dcvevn7W-qKDu5KWN0BM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af590ed730005b197bb9c10c9bc2c173078730ef', 'width': 1080, 'height': 1080}], 'variants': {}, 'id': 'q7V8BDv9opANwPukjjPK-R82fNE4Qq4vwXKRxiy1DHo'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,getrpw,True,,jsamwrites,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getrpw/understanding_the_shape_of_large_scale_data/,all_ads,False,https://ai.googleblog.com/2020/05/understanding-shape-of-large-scale-data.html,155203,1588802989.0,0,,False,,,,
,learnmachinelearning,"Hello all, I'm a 3rd year undergraduate. I love to contribute to AI research community. Bit how to start doing research in AI? 
In my opinion the order of doing research is as follows:
1. Find the problem to solve
2. Read many research papers that are already piblished in the related problem area
3. After knowing about the problem through papers, blogs, etc.. 'You will probably find an idea towards the solution  of your problem'

But, after reading those papers what if I can't find the solution? What if I don't get any idea to solve the problem? 

Should I change my area of research? Will research take a lot of time? To publishing a paper and to contribute to the community, will it take a lot of time?

I'm struck, How to get started in research? I'm still a 3rd year undergraduate. 

Thankyou",t2_6dn8cxrd,False,,0,False,Advice for research!,[],r/learnmachinelearning,False,6,,0,,False,t3_gezbji,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588852298.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello all, I&amp;#39;m a 3rd year undergraduate. I love to contribute to AI research community. Bit how to start doing research in AI? 
In my opinion the order of doing research is as follows:
1. Find the problem to solve
2. Read many research papers that are already piblished in the related problem area
3. After knowing about the problem through papers, blogs, etc.. &amp;#39;You will probably find an idea towards the solution  of your problem&amp;#39;&lt;/p&gt;

&lt;p&gt;But, after reading those papers what if I can&amp;#39;t find the solution? What if I don&amp;#39;t get any idea to solve the problem? &lt;/p&gt;

&lt;p&gt;Should I change my area of research? Will research take a lot of time? To publishing a paper and to contribute to the community, will it take a lot of time?&lt;/p&gt;

&lt;p&gt;I&amp;#39;m struck, How to get started in research? I&amp;#39;m still a 3rd year undergraduate. &lt;/p&gt;

&lt;p&gt;Thankyou&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gezbji,True,,saiyan6174,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gezbji/advice_for_research/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gezbji/advice_for_research/,155203,1588823498.0,0,,False,,,,
,learnmachinelearning,"I have a medium-sized project that's outgrown my local GPU and figured I'd give it it a try on google colab. I've looked at a few tutorials and I'm a bit confused about the general workflow, and session persistence. Namely, there's a fair number of steps involved in setting up the environment, pointing it to my large-ish file database, and of course, doing this all in jupyter notebook is all kind of tedious. Does the colab hosting ensure my setup efforts all ""persist"" or am I going to have to do this each and every time? Ideally, I'd like to just upload my local python script and run it but it seems it doesn't quite work that way.",t2_3hprhzvw,False,,0,False,Some beginner questions about google Colab,[],r/learnmachinelearning,False,6,,0,,False,t3_geytzr,False,dark,0.5,,public,0,0,{},,,False,[],,False,False,,{},,False,0,,False,self,False,,[],{},,,True,,1588850234.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a medium-sized project that&amp;#39;s outgrown my local GPU and figured I&amp;#39;d give it it a try on google colab. I&amp;#39;ve looked at a few tutorials and I&amp;#39;m a bit confused about the general workflow, and session persistence. Namely, there&amp;#39;s a fair number of steps involved in setting up the environment, pointing it to my large-ish file database, and of course, doing this all in jupyter notebook is all kind of tedious. Does the colab hosting ensure my setup efforts all &amp;quot;persist&amp;quot; or am I going to have to do this each and every time? Ideally, I&amp;#39;d like to just upload my local python script and run it but it seems it doesn&amp;#39;t quite work that way.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geytzr,True,,Theweekendstate,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geytzr/some_beginner_questions_about_google_colab/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geytzr/some_beginner_questions_about_google_colab/,155203,1588821434.0,0,,False,,,,
,learnmachinelearning,"I am trying to create a generative/discriminator network system. My goal is for a generator to create a pattern of pixels (28x28), overlay it on a background image, then have a discriminator try to locate the pattern in the background image. However, I am struggling to write the custom training loop. It has something to do with going back and forth between tensors and nd\_arrays, but I can't figure out how to solve my problem without doing so.

Here's a link to a notebook that has an example:

[https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV\_?usp=sharing](https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing)

Here is the training loop:

`#@tf.function`  
`#the example code had this annotation, but my code breaks with it`  
`def train_step():`  
 `#Generate noisy seeds`  
`noise = tf.random.normal([BATCH_SIZE, noise_dim])`  
 `with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:`  
`pattern = generator(noise, training=True)`  
`gen_tape.watch(pattern)`  
`imageDataset, lblDataset = getDatasetFromPattern(np.reshape(pattern, (28,28)), 1)`  
`discriminator_output = discriminator(imageDataset, training=True)`  
`gen_loss = generator_loss(lblDataset, discriminator_output)`  
`disc_loss = discriminator_loss(lblDataset, discriminator_output)`  
`gradients_of_generator = gen_tape.gradient(disc_loss, generator.trainable_variables, gen_tape)`  
`gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)`  
`generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))`  
`discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))`

Here s the getDatasetFromPattern pattern:

`def getDatasetFromPattern(nd_array, num_samples):`  
`height = nd_array.shape[0]`  
`width = nd_array.shape[1]`  
`backgrounds = [Image.fromarray(np.zeros((640, 480, CHANNELS), dtype=np.uint8))]`  
`patternImg = Image.fromarray(nd_array, 'L')`  
`imgArray = np.ndarray((num_samples * len(backgrounds),`  
`BACKGROUND_HEIGHT, BACKGROUND_WIDTH, 3))`  
   
`lblArray = np.ndarray((num_samples * len(backgrounds), 2))`  
 `for i in range(num_samples):`  
`x = random.randint(0, BACKGROUND_WIDTH - width)`  
`y = random.randint(0, BACKGROUND_HEIGHT - height)`  
 `for bg in backgrounds:`  
`bg.paste(patternImg, (x, y), patternImg.convert(""RGBA""))`  
`imgArray[i, :, :, :] = bg`  
`lblArray[i] = [x, y]`  
 `return tf.stack(imgArray), tf.stack(lblArray)`

Any help is greatly appreciated!",t2_48qz3zo4,False,,0,False,How to manipulate image tensors TF/custom training loops in TF,[],r/learnmachinelearning,False,6,,0,,False,t3_geyiwk,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588848969.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I am trying to create a generative/discriminator network system. My goal is for a generator to create a pattern of pixels (28x28), overlay it on a background image, then have a discriminator try to locate the pattern in the background image. However, I am struggling to write the custom training loop. It has something to do with going back and forth between tensors and nd_arrays, but I can&amp;#39;t figure out how to solve my problem without doing so.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a link to a notebook that has an example:&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing""&gt;https://colab.research.google.com/drive/1kYMk4vX7cjdM8pcNCeL93ELPHJGZqiV_?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is the training loop:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#@tf.function&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;#the example code had this annotation, but my code breaks with it&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;def train_step():&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;#Generate noisy seeds&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;noise = tf.random.normal([BATCH_SIZE, noise_dim])&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;pattern = generator(noise, training=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gen_tape.watch(pattern)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imageDataset, lblDataset = getDatasetFromPattern(np.reshape(pattern, (28,28)), 1)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;discriminator_output = discriminator(imageDataset, training=True)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gen_loss = generator_loss(lblDataset, discriminator_output)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;disc_loss = discriminator_loss(lblDataset, discriminator_output)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gradients_of_generator = gen_tape.gradient(disc_loss, generator.trainable_variables, gen_tape)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here s the getDatasetFromPattern pattern:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;def getDatasetFromPattern(nd_array, num_samples):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;height = nd_array.shape[0]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;width = nd_array.shape[1]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;backgrounds = [Image.fromarray(np.zeros((640, 480, CHANNELS), dtype=np.uint8))]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;patternImg = Image.fromarray(nd_array, &amp;#39;L&amp;#39;)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imgArray = np.ndarray((num_samples * len(backgrounds),&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;BACKGROUND_HEIGHT, BACKGROUND_WIDTH, 3))&lt;/code&gt;  &lt;/p&gt;

&lt;p&gt;&lt;code&gt;lblArray = np.ndarray((num_samples * len(backgrounds), 2))&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for i in range(num_samples):&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;x = random.randint(0, BACKGROUND_WIDTH - width)&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;y = random.randint(0, BACKGROUND_HEIGHT - height)&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;for bg in backgrounds:&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;bg.paste(patternImg, (x, y), patternImg.convert(&amp;quot;RGBA&amp;quot;))&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;imgArray[i, :, :, :] = bg&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;lblArray[i] = [x, y]&lt;/code&gt;&lt;br/&gt;
 &lt;code&gt;return tf.stack(imgArray), tf.stack(lblArray)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Any help is greatly appreciated!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?auto=webp&amp;s=ffc13b63083ce9180bdf2cf28705ee407b659975', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f53340195fae0f84ec3871625087bf02fd428ab', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/kfDh9FNsuWs5VThFyc16bdwBtJ99FmMWgOZYUyq7LPg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6722a649351a89204dede7d65ad009eea43566c', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'mAwt5FN14h93icEvsjkWqFiukRT9FqTVVq54ZHDgaos'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geyiwk,True,,fullyLethal,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geyiwk/how_to_manipulate_image_tensors_tfcustom_training/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geyiwk/how_to_manipulate_image_tensors_tfcustom_training/,155203,1588820169.0,0,,False,,,,
,learnmachinelearning,,t2_uh8z4m5,False,,0,False,Faster machine learning on larger graphs: how NumPy and Pandas slashed memory and time in StellarGraph,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_geyanz,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/mcl9tRjRqyffpXRnTzyhnKLIoQhcG3QnqMZBDsNmuIg.jpg,False,,[],{},link,,False,,1588848024.0,text,6,,,text,medium.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?auto=webp&amp;s=24c955dd2e72ed7a7f80de843ed052388bab4529', 'width': 1024, 'height': 512}, 'resolutions': [{'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d39df137a29f0ac8fd3197bb961832570482c23f', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=958ad8f7e2da6602fb3407ae3ab9434c2027654f', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c1ce7fabc376a384446bd92655d3e9d78c18dd54', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=159c76d1403f22c3fc5da08f5eb92e25433b788a', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/gouvlSwI9D74r0GTsQLpPf-8I8dXYg_bxmIYw6uyV8Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a16246ab3539e638686c7f3febf240c605d2a45', 'width': 960, 'height': 480}], 'variants': {}, 'id': 'RYjGRysZcCqaR-JRAX0qqwNsOehdIU5Oo2daJzePuiU'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geyanz,True,,huonw,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geyanz/faster_machine_learning_on_larger_graphs_how/,all_ads,False,https://medium.com/stellargraph/faster-machine-learning-on-larger-graphs-how-numpy-and-pandas-slashed-memory-and-time-in-79b6c63870ef,155203,1588819224.0,0,,False,,,,
,learnmachinelearning,"Hello everyone, it's my first time posting here so apologies if this is the wrong sub for this question...

I work for an advertising company that is trying to aggregate consumer commentary about a client's product. Scraping and extracting data on social media platforms is well documented BUT I was wondering if anyone had experience with mining/scraping/crawling (not sure the right word here) the internet as a whole to find more consumer commentary?

What I'm envisioning is a system where you can upload 30-40 relevant website URLs, some example text/commentary from consumers that we're looking for (we can give the system thousands of examples if it needs it), and let it loose to find more websites/text from OTHER sources than the 30-40 initial websites we gave it.

Does something like this exist? I've spoken to a few developer friends and they seem to think something like that is difficult since you have to somehow code the website layout for the scraper to understand where text is located on a page, let alone WHAT to scrape. But does anyone know of a company that can do this (maybe even self service?). It'd be great if we could get commentary from thousands of websites. Thank you ahead of time!",t2_mpw09,False,,0,False,Mining Public Text Data,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,,False,t3_geubsy,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Project,False,2,,False,self,False,,[],{},,,True,,1588833642.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello everyone, it&amp;#39;s my first time posting here so apologies if this is the wrong sub for this question...&lt;/p&gt;

&lt;p&gt;I work for an advertising company that is trying to aggregate consumer commentary about a client&amp;#39;s product. Scraping and extracting data on social media platforms is well documented BUT I was wondering if anyone had experience with mining/scraping/crawling (not sure the right word here) the internet as a whole to find more consumer commentary?&lt;/p&gt;

&lt;p&gt;What I&amp;#39;m envisioning is a system where you can upload 30-40 relevant website URLs, some example text/commentary from consumers that we&amp;#39;re looking for (we can give the system thousands of examples if it needs it), and let it loose to find more websites/text from OTHER sources than the 30-40 initial websites we gave it.&lt;/p&gt;

&lt;p&gt;Does something like this exist? I&amp;#39;ve spoken to a few developer friends and they seem to think something like that is difficult since you have to somehow code the website layout for the scraper to understand where text is located on a page, let alone WHAT to scrape. But does anyone know of a company that can do this (maybe even self service?). It&amp;#39;d be great if we could get commentary from thousands of websites. Thank you ahead of time!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geubsy,True,,whorehey19,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geubsy/mining_public_text_data/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geubsy/mining_public_text_data/,155203,1588804842.0,0,,False,,,,
,learnmachinelearning,"This is a Discord server where everyone wanting to learn machine learning is welcomed!

Share you projects, interesting research papers, courses, kaggle competitions, learn together ask any questions related to the field!

The more we are, the more we learn! Spread your knowledge and Learn ML Together!

Join us: [https://discord.gg/SVse4Sr](https://discord.gg/SVse4Sr)",t2_c14wpji,False,,0,False,"A discord server for everyone working / learning Al, ML &amp; DL. Share your project, papers, ask questions, learn together, create Kaggle competition teams and more!",[],r/learnmachinelearning,False,6,,0,,False,t3_getr3k,False,dark,0.63,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},self,,True,,1588831732.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This is a Discord server where everyone wanting to learn machine learning is welcomed!&lt;/p&gt;

&lt;p&gt;Share you projects, interesting research papers, courses, kaggle competitions, learn together ask any questions related to the field!&lt;/p&gt;

&lt;p&gt;The more we are, the more we learn! Spread your knowledge and Learn ML Together!&lt;/p&gt;

&lt;p&gt;Join us: &lt;a href=""https://discord.gg/SVse4Sr""&gt;https://discord.gg/SVse4Sr&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?auto=webp&amp;s=bbf5a5f8cdd32a365c20b807d6e558010bf21ad3', 'width': 256, 'height': 256}, 'resolutions': [{'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7e52d627b24baf552b6a16dd42fad4b83bfdd50', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/sFi6pinpZUiiKVkZTvMmGtINoLH5uFdQU3YSvC52Z4g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8796fb46cfe3d0305f26fc605b5c6505c2cd4ffe', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'MId8qCjBpXwiZK4_j8Xrn-37sQy_bUZ9YanGSEY9-S8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,getr3k,True,,OnlyProggingForFun,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getr3k/a_discord_server_for_everyone_working_learning_al/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/getr3k/a_discord_server_for_everyone_working_learning_al/,155203,1588802932.0,0,,False,,,,
,learnmachinelearning,I recently just finished Python Crash Course and did numerous projects of my own. Iâ€™m very curious about Machine Learning and would like more insight. Any recommendations?,t2_115nlpd7,False,,0,False,What books should I read to get started with Machine Learning using Python ?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_gexf4z,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588844658.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I recently just finished Python Crash Course and did numerous projects of my own. Iâ€™m very curious about Machine Learning and would like more insight. Any recommendations?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gexf4z,True,,FlySeddy,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gexf4z/what_books_should_i_read_to_get_started_with/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gexf4z/what_books_should_i_read_to_get_started_with/,155203,1588815858.0,0,,False,,,,
,learnmachinelearning,,t2_4xto167r,False,,0,False,"MIT-OCW: A 2020 Vision of Linear Algebra, Spring 2020 | Gilbert Strang | Brand new, intuitive, short videos on Linear Algebra",[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gdy9ve,False,dark,0.99,,public,715,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Intro: A New Way to Start Linear Algebra', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'MIT OpenCourseWare', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YrHlHbtiSM0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/MIT'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gdy9ve', 'height': 338}",,False,715,,False,https://b.thumbs.redditmedia.com/jV7_Z5kH2VFt_ROohoDrxjPv4U2lzDq7qov6DYd4OgE.jpg,False,,[],{},rich:video,,False,,1588716139.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?auto=webp&amp;s=00f256c18ac7eb362188fd81a37d990320424387', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3473d4ae632260c746e99e9c641b02f7db40497', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=48cc4db6a21a4c37f67e16d57fea4acea1a2e275', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/wW-aCG8JTXZlP_ZAqlewv29Fdf4F-q3urJ4eRsiRAMY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de56a18d15adacb01d4015b213dfef562ef3307a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'jazTf8nlqJzP_Ur50WSVB0-b5bQ77d0DGUwyO4tBEZw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gdy9ve,True,,samketa,,33,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gdy9ve/mitocw_a_2020_vision_of_linear_algebra_spring/,all_ads,False,https://www.youtube.com/watch?v=YrHlHbtiSM0&amp;list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek,155203,1588687339.0,3,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Intro: A New Way to Start Linear Algebra', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/videoseries?list=PLUl4u3cNGP61iQEFiWLE21EJCxwmWvvek"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'MIT OpenCourseWare', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/YrHlHbtiSM0/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/user/MIT'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_633vt2ak,False,,0,False,New algorithms help scientists connect data points from multiple sources to solve high-risk problems,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,103.0,False,t3_getdqk,False,light,1.0,,public,2,0,{},140.0,,False,[],,True,False,,{},Discussion,False,2,,False,https://b.thumbs.redditmedia.com/c2Odg0jY5B-hdWkBaB42GGgd5FEPPu8QnkVZgZ6J3-w.jpg,False,,[],{},image,,False,,1588830520.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?auto=webp&amp;s=7128b0a1f24aed9c8116a5e6f08e7c7155c798b5', 'width': 1280, 'height': 949}, 'resolutions': [{'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c5696868bd2a52cfc8dcac22d86fda4373b71d0', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deaed160345f2554fe75660117e6ba156f138da7', 'width': 216, 'height': 160}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ddc6f86f51707a440ffd86ee177bced326cc789', 'width': 320, 'height': 237}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb7e2304a2457f5986e19bf6445a0379f376ea2c', 'width': 640, 'height': 474}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a5f6abfacd1a0dcb5bdab68af7fc7d09ac6a7ee', 'width': 960, 'height': 711}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f35c323653f41054361d8122bdf892259d8c6bf', 'width': 1080, 'height': 800}], 'variants': {}, 'id': 'zrRxrlDYLKhTNj8clke9XLj95pXFRdbdgEr6ExfYXTk'}], 'enabled': True}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,getdqk,True,,GeaninaKera,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/getdqk/new_algorithms_help_scientists_connect_data/,all_ads,False,https://i.redd.it/kxi0qjqht7x41.jpg,155203,1588801720.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'VisualCoding', 'selftext': '', 'author_fullname': 't2_633vt2ak', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'New algorithms help scientists connect data points from multiple sources to solve high-risk problems', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/VisualCoding', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 103, 'hide_score': False, 'name': 't3_getd4a', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.92, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 9, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/c2Odg0jY5B-hdWkBaB42GGgd5FEPPu8QnkVZgZ6J3-w.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'mod_note': None, 'created': 1588830462.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?auto=webp&amp;s=7128b0a1f24aed9c8116a5e6f08e7c7155c798b5', 'width': 1280, 'height': 949}, 'resolutions': [{'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c5696868bd2a52cfc8dcac22d86fda4373b71d0', 'width': 108, 'height': 80}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=deaed160345f2554fe75660117e6ba156f138da7', 'width': 216, 'height': 160}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ddc6f86f51707a440ffd86ee177bced326cc789', 'width': 320, 'height': 237}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb7e2304a2457f5986e19bf6445a0379f376ea2c', 'width': 640, 'height': 474}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a5f6abfacd1a0dcb5bdab68af7fc7d09ac6a7ee', 'width': 960, 'height': 711}, {'url': 'https://preview.redd.it/kxi0qjqht7x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2f35c323653f41054361d8122bdf892259d8c6bf', 'width': 1080, 'height': 800}], 'variants': {}, 'id': 'zrRxrlDYLKhTNj8clke9XLj95pXFRdbdgEr6ExfYXTk'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2jgq2d', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'getd4a', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'GeaninaKera', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/VisualCoding/comments/getd4a/new_algorithms_help_scientists_connect_data/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://i.redd.it/kxi0qjqht7x41.jpg', 'subreddit_subscribers': 585, 'created_utc': 1588801662.0, 'num_crossposts': 1, 'media': None, 'is_video': False}]",t3_getd4a,,
,learnmachinelearning,,t2_3d8dg3uh,False,,0,False,AI Generates SharinGAN - Part 2 (Tried To Get Better Results),"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gepy8f,False,light,1.0,,public,3,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates SharinGAN - Part 2 (Tried To Get Better Results)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Mtb6MhGXrU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gepy8f', 'height': 338}",Project,False,3,,False,https://b.thumbs.redditmedia.com/D4UterRi_csn-u1rXh6Le4YtLmOYgblXexDYt_SUCdk.jpg,False,,[],{},rich:video,,False,,1588819781.0,richtext,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?auto=webp&amp;s=e7062f8c1bd7721d58050c90600a023ff4764d7c', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=796002cd86bff68f1f8ecaf15d29008512068121', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58ea39e444d5927020c111a01ca4a4944b2dfb51', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/c46du71Aw6lxSjt63Lzjlo2UjzCrGUnZRIVGtPjue1s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f0720ed55d89ec8be9a1887f965b47034970a33', 'width': 320, 'height': 240}], 'variants': {}, 'id': '4tBptj2vi4L8qdtI4TPnYnWDcd1Plwg4_QfbCgh2GUI'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,gepy8f,True,,oFlamingo,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gepy8f/ai_generates_sharingan_part_2_tried_to_get_better/,all_ads,False,https://www.youtube.com/watch?v=Mtb6MhGXrU4,155203,1588790981.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'AI Generates SharinGAN - Part 2 (Tried To Get Better Results)', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Mtb6MhGXrU4?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'AngryCoder', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Mtb6MhGXrU4/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCta6mmYG1NLeDeFFaLP2eug'}}",False,,,,
,learnmachinelearning,,t2_4eskgjow,False,,0,False,Great article about LSTM Networks,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geh67p,False,dark,0.94,,public,13,0,{},140.0,,False,[],,False,False,,{},,False,13,,False,https://a.thumbs.redditmedia.com/8Jl84Nbj4TDv6yYpzKjp2GVMBgOQbrKWW4TDdS04zy0.jpg,False,,[],{},link,,False,,1588788239.0,text,6,,,text,colah.github.io,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?auto=webp&amp;s=f6b09874635bb0653fd4d03a806ee1fb2ff66f89', 'width': 600, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2996612ff7a9fc61114d2c281f2ff7dcb7d732cf', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1a774718f27448361578b84d4d76fc7b649a710', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/uI1XMsl2KxMWVQhJJIWQ3gu3hxwxUDq0c5u5QN2HQ5o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=19cc02439bd6e94150f6efa421a142b6cae1c94a', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'SFKPM5-FCzRAy7BEoJqEnwOql0eWz7NWthvpDeCK8qA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geh67p,True,,jsanrom,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geh67p/great_article_about_lstm_networks/,all_ads,False,https://colah.github.io/posts/2015-08-Understanding-LSTMs/,155203,1588759439.0,0,,False,,,,
,learnmachinelearning,,t2_44mbtmjy,False,,0,False,From CVPR '20: Photo-Realistic Virtual Try-On,[],r/learnmachinelearning,False,6,,0,76.0,False,t3_geuvbf,False,dark,0.5,,public,0,0,{},140.0,,False,[],,False,False,,{},,False,0,,False,https://b.thumbs.redditmedia.com/kWlFZ245BAgE_fiEWUeXV5bEvd-MLEO2_9G_jmHMFTI.jpg,False,,[],{},link,,False,,1588835476.0,text,6,,,text,self.LatestInML,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?auto=webp&amp;s=44b2e5dec112285d7768028b1dce3792624fe8bf', 'width': 638, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d871a6546c4df9a28a6339e3f7d6d58c5cf777b', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13800f679f77a4253734b2371cff4545c07363dc', 'width': 216, 'height': 118}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349af67a18b5cbe022530a5957a92eab8ca47164', 'width': 320, 'height': 175}], 'variants': {}, 'id': 'B7sUwDQ3vFL22HPV8SBYSE_lUB3uQur9P24m0QIM_uM'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geuvbf,True,,MLtinkerer,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geuvbf/from_cvpr_20_photorealistic_virtual_tryon/,all_ads,False,/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/,155203,1588806676.0,0,,False,"[{'approved_at_utc': None, 'subreddit': 'LatestInML', 'selftext': ""From CVPR '20: Photo-Realistic Virtual Try-On\n\nFor project, code or API request: [click here](https://www.catalyzex.com/paper/arxiv:2003.05863)\n\nhttps://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;format=png&amp;auto=webp&amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48\n\nThey propose a novel visual try-on network which In comparison to the state-of-the-art methods can generate photo-realistic images with much better perceptual quality and richer fine-details."", 'author_fullname': 't2_44mbtmjy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': ""From CVPR '20: Photo-Realistic Virtual Try-On"", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/LatestInML', 'hidden': False, 'pwls': None, 'link_flair_css_class': None, 'downs': 0, 'thumbnail_height': 76, 'hide_score': False, 'media_metadata': {'1gwv97ax58x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 59, 'x': 108, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44881ccdb4abcb06bacaedc285001ea727b65322'}, {'y': 118, 'x': 216, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e595b0a3f833bfd5cb70974e448221fad0673fd8'}, {'y': 174, 'x': 320, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dce1aa730c16ac443f499002aaa993d0575a6ecd'}, {'y': 349, 'x': 640, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9ccb324a50d5f1f432e9066280e0ce0ec7f5e2d2'}, {'y': 524, 'x': 960, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=410c51c8b921b46854a3d22333954ada8c31aeaa'}, {'y': 590, 'x': 1080, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05d2cc24e24c45d72aabb7100ce1499a6c911585'}], 's': {'y': 1074, 'x': 1964, 'u': 'https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;format=png&amp;auto=webp&amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48'}, 'id': '1gwv97ax58x41'}}, 'name': 't3_geumem', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': None, 'can_mod_post': False, 'score': 18, 'approved_by': None, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/kWlFZ245BAgE_fiEWUeXV5bEvd-MLEO2_9G_jmHMFTI.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1588834640.0, 'link_flair_type': 'text', 'wls': None, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.LatestInML', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;From CVPR &amp;#39;20: Photo-Realistic Virtual Try-On&lt;/p&gt;\n\n&lt;p&gt;For project, code or API request: &lt;a href=""https://www.catalyzex.com/paper/arxiv:2003.05863""&gt;click here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=""https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48""&gt;https://preview.redd.it/1gwv97ax58x41.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=830f18dd50237c9422dbd11f0d16c6b7ea055e48&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They propose a novel visual try-on network which In comparison to the state-of-the-art methods can generate photo-realistic images with much better perceptual quality and richer fine-details.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?auto=webp&amp;s=44b2e5dec112285d7768028b1dce3792624fe8bf', 'width': 638, 'height': 350}, 'resolutions': [{'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d871a6546c4df9a28a6339e3f7d6d58c5cf777b', 'width': 108, 'height': 59}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=13800f679f77a4253734b2371cff4545c07363dc', 'width': 216, 'height': 118}, {'url': 'https://external-preview.redd.it/yxdkBW8lGVpt3sz90ZzPXncAAB7Y6u71b7UbhwjQDQs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349af67a18b5cbe022530a5957a92eab8ca47164', 'width': 320, 'height': 175}], 'variants': {}, 'id': 'B7sUwDQ3vFL22HPV8SBYSE_lUB3uQur9P24m0QIM_uM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2c0xdn', 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': 'geumem', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'MLtinkerer', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': None, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/', 'parent_whitelist_status': None, 'stickied': False, 'url': 'https://www.reddit.com/r/LatestInML/comments/geumem/from_cvpr_20_photorealistic_virtual_tryon/', 'subreddit_subscribers': 3386, 'created_utc': 1588805840.0, 'num_crossposts': 9, 'media': None, 'is_video': False}]",t3_geumem,,
,learnmachinelearning,"I was thinking of a program that could discern between quality of instrument tone (Or sound quality). Particularly for beginners, it would be helpful to have a tool that could rate your tone say from 1-10. As a trombone player I'd want to start with trombone tone, and if it works expand to many different instruments.

I'm not a programmer and I only know basic stuff about machine learning. How doable is this?",t2_ge5os,False,,0,False,I have an idea that uses machine learning but I don't know ho doable it is.,[],r/learnmachinelearning,False,6,,0,,False,t3_geq1av,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588820038.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I was thinking of a program that could discern between quality of instrument tone (Or sound quality). Particularly for beginners, it would be helpful to have a tool that could rate your tone say from 1-10. As a trombone player I&amp;#39;d want to start with trombone tone, and if it works expand to many different instruments.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not a programmer and I only know basic stuff about machine learning. How doable is this?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geq1av,True,,Floppy_Trombone,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geq1av/i_have_an_idea_that_uses_machine_learning_but_i/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geq1av/i_have_an_idea_that_uses_machine_learning_but_i/,155203,1588791238.0,0,,False,,,,
,learnmachinelearning," I've downloaded it from github. I also have python 3.8, but I can't seem to get it up and running. I instructions I have found online assumes I have prior knowledge, which I do not. Please EILI5.",t2_aual3,False,,0,False,How do I get Tacotron 2 up and running?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gepx0v,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588819672.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve downloaded it from github. I also have python 3.8, but I can&amp;#39;t seem to get it up and running. I instructions I have found online assumes I have prior knowledge, which I do not. Please EILI5.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gepx0v,True,,BobLordOfTheCows,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gepx0v/how_do_i_get_tacotron_2_up_and_running/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gepx0v/how_do_i_get_tacotron_2_up_and_running/,155203,1588790872.0,0,,False,,,,
,learnmachinelearning,"Hi all, so I come from a pretty heavy classical stats/probability background and I've been reading about machine learning, and I'm confused as to what machine learners are talking about when they talk about bias, especially with respect to the bias-variance tradeoff. To give you an example of what I'm confused about, suppose you have some data from a true model which is quadratic with an additive error:

y = x^2 + epsilon, epsilon ~ N(0, sigma^2)

The way I would describe bias and the bias-variance tradeoff from my probability background is: if you were to fit a quadratic regresion using ordinary least squares, your estimates would be unbiased. Alternatively, you could fit the same quadratic regression using a method like ridge regression to get estimates that add a little bit of bias but have a big reduction in variance, so you've traded some bias for some variance to reduce total MSE. 

But, I google ""bias-variance tradeoff"" and I get articles that basically explain it like this: if you fit a simple linear regression, it has high bias. If you fit a high-order polynomial regression, it has high variance. The goal is to find the best in-between model which would balance bias and variance to get the optimum MSE on a test set. This is usually indicated by something that looks like the OLS quadratic regression (though often the method isn't described so I can't be entirely sure, correct me if I'm wrong here). But since both the quadratic regression and high-order polynomial regression give unbiased estimates in this scenario, going from one to the other isn't trading off bias for variance at all, and so this would make it seem as if the bias-variance tradeoff is just about model selection. The only way I can think to make sense of bias in this context is that it basically refers to the mean squared error on the training set.

Can somebody clarify what bias and the bias-variance tradeoff mean in this context? I just want to make sure I understand what the terminology means in machine learning so I'm not confused when I get to heavier stuff. Thank you!",t2_gf0jz,False,,0,False,What do machine learners mean by bias and the bias-variance tradeoff?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_get7by,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Question,False,1,,False,self,False,,[],{},,,True,,1588829931.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hi all, so I come from a pretty heavy classical stats/probability background and I&amp;#39;ve been reading about machine learning, and I&amp;#39;m confused as to what machine learners are talking about when they talk about bias, especially with respect to the bias-variance tradeoff. To give you an example of what I&amp;#39;m confused about, suppose you have some data from a true model which is quadratic with an additive error:&lt;/p&gt;

&lt;p&gt;y = x&lt;sup&gt;2&lt;/sup&gt; + epsilon, epsilon ~ N(0, sigma&lt;sup&gt;2)&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The way I would describe bias and the bias-variance tradeoff from my probability background is: if you were to fit a quadratic regresion using ordinary least squares, your estimates would be unbiased. Alternatively, you could fit the same quadratic regression using a method like ridge regression to get estimates that add a little bit of bias but have a big reduction in variance, so you&amp;#39;ve traded some bias for some variance to reduce total MSE. &lt;/p&gt;

&lt;p&gt;But, I google &amp;quot;bias-variance tradeoff&amp;quot; and I get articles that basically explain it like this: if you fit a simple linear regression, it has high bias. If you fit a high-order polynomial regression, it has high variance. The goal is to find the best in-between model which would balance bias and variance to get the optimum MSE on a test set. This is usually indicated by something that looks like the OLS quadratic regression (though often the method isn&amp;#39;t described so I can&amp;#39;t be entirely sure, correct me if I&amp;#39;m wrong here). But since both the quadratic regression and high-order polynomial regression give unbiased estimates in this scenario, going from one to the other isn&amp;#39;t trading off bias for variance at all, and so this would make it seem as if the bias-variance tradeoff is just about model selection. The only way I can think to make sense of bias in this context is that it basically refers to the mean squared error on the training set.&lt;/p&gt;

&lt;p&gt;Can somebody clarify what bias and the bias-variance tradeoff mean in this context? I just want to make sure I understand what the terminology means in machine learning so I&amp;#39;m not confused when I get to heavier stuff. Thank you!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,get7by,True,,rcf1105,,3,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/get7by/what_do_machine_learners_mean_by_bias_and_the/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/get7by/what_do_machine_learners_mean_by_bias_and_the/,155203,1588801131.0,0,,False,,,,
,learnmachinelearning,,t2_zhiq2,False,,0,False,Predict Wins and Losses with Sci-kit Learn Decision Trees and SMS,[],r/learnmachinelearning,False,6,,0,70.0,False,t3_get3bg,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/8FUnpU0mtnLea7v2vxcctVYrm28Z_ZOpPErF5P9e5ig.jpg,False,,[],{},link,,False,,1588829572.0,text,6,,,text,twilio.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?auto=webp&amp;s=d443172d1e1fd3fff035f4ddcae2ae2f32718ff9', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=704d5ff8a5527dbe8a9c8040b9e2421b8832b282', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=55b6443b4372db0876ab18da3f86f80ff5d8954b', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e0cb43ee97763a49f422e8fdada935e79f590e0', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ae4c533749018a680e8d6dedd7d6d750f17d849', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3ed0a4921e93afa900f24a08d7d4486a9d3a8ac1', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/xluAeW0ZYQz8cmeOjX-gvGLhF-9bYj0qWxOem8Tzl9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=561ba27d816b234faec21065a354e8fb590c3977', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'GgaU-qidR2wgONCcrVwW5mnAb0Wk1z6XxjWFLlUqPvw'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,get3bg,True,,lizziepika,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/get3bg/predict_wins_and_losses_with_scikit_learn/,all_ads,False,https://www.twilio.com/blog/predict-wins-losses-scikitlearn-sms,155203,1588800772.0,0,,False,,,,
,learnmachinelearning,"I wrote a chatbot using a DQN as a dialog manager. As an example, let's say it is e-commerce. It accepts encoded information from users, so let's say the user says 'I want shoes'. All available shoes would be queried from a database and a list of available products would be returned. The DQN then will get a list of available actions as well as the state of the conversation as an input and then make the next best action. In our case, let's say the action is ""ask for size"".

I have 2 questions based on this system that I can explain more if needed

1: The available actions are input as a binary mask to the DQN where if an action is available, it is a 1 and if not, 0. Is this a proper approach and if not, what would be a better way to do it?

2: If we wanted to add additional actions without having to retrain every time, what would be the best way to do so? This is assuming the requested product is a sub-category of the first, so without adding 'high heels', how could we still query for shoes? My first thought was to do some kind of clustering like k-means where we could add high heels to a 'shoe' category, in the database have all high heels as a subset of shoes, so when we see 'I want high heels' we could classify high heels into shoes so the agent knows it is a shoe we are asking about.",t2_qjb55,False,,0,False,What is the best way to add actions to a chatbot?,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,,False,t3_geopio,False,dark,0.75,,public,2,0,{},,,False,[],,False,False,,{},Question,False,2,,False,self,False,,[],{},,,True,,1588815865.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I wrote a chatbot using a DQN as a dialog manager. As an example, let&amp;#39;s say it is e-commerce. It accepts encoded information from users, so let&amp;#39;s say the user says &amp;#39;I want shoes&amp;#39;. All available shoes would be queried from a database and a list of available products would be returned. The DQN then will get a list of available actions as well as the state of the conversation as an input and then make the next best action. In our case, let&amp;#39;s say the action is &amp;quot;ask for size&amp;quot;.&lt;/p&gt;

&lt;p&gt;I have 2 questions based on this system that I can explain more if needed&lt;/p&gt;

&lt;p&gt;1: The available actions are input as a binary mask to the DQN where if an action is available, it is a 1 and if not, 0. Is this a proper approach and if not, what would be a better way to do it?&lt;/p&gt;

&lt;p&gt;2: If we wanted to add additional actions without having to retrain every time, what would be the best way to do so? This is assuming the requested product is a sub-category of the first, so without adding &amp;#39;high heels&amp;#39;, how could we still query for shoes? My first thought was to do some kind of clustering like k-means where we could add high heels to a &amp;#39;shoe&amp;#39; category, in the database have all high heels as a subset of shoes, so when we see &amp;#39;I want high heels&amp;#39; we could classify high heels into shoes so the agent knows it is a shoe we are asking about.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,geopio,True,,Awill1aB,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geopio/what_is_the_best_way_to_add_actions_to_a_chatbot/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geopio/what_is_the_best_way_to_add_actions_to_a_chatbot/,155203,1588787065.0,0,,False,,,,
,learnmachinelearning,"So i have this project research and i have few questions. I looked up some research papers and some web articles but still having some basic questions.

1)Does this mean that it should be able to answer the question that what is happening in the video and audio (like user asks question and machine should be able to give answer) or is it sort of video/audio action describer?

2) No  idea how to implement this. I only know ML and basics of AI. Would like some guidance or any useful links for reference.",t2_6dqc4uv8,False,,0,False,Audio- video scene aware dialogue,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_geofbt,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},HELP,False,2,,False,self,False,,[],{},,,True,,1588814961.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So i have this project research and i have few questions. I looked up some research papers and some web articles but still having some basic questions.&lt;/p&gt;

&lt;p&gt;1)Does this mean that it should be able to answer the question that what is happening in the video and audio (like user asks question and machine should be able to give answer) or is it sort of video/audio action describer?&lt;/p&gt;

&lt;p&gt;2) No  idea how to implement this. I only know ML and basics of AI. Would like some guidance or any useful links for reference.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geofbt,True,,RayS0l0,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geofbt/audio_video_scene_aware_dialogue/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geofbt/audio_video_scene_aware_dialogue/,155203,1588786161.0,0,,False,,,,
,learnmachinelearning,,t2_uh03y,False,,0,False,My implementations of Deep Generative Models!,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_geo7zf,False,dark,1.0,,public,2,0,{},140.0,,False,[],,False,False,,{},,False,2,,False,https://b.thumbs.redditmedia.com/CPyu9UNNkEpH_BM9zftQ4il2ud3fX1RTtyynNcuYNes.jpg,False,,[],{},link,,False,,1588814318.0,text,6,,,text,github.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?auto=webp&amp;s=480ba39f8f4bdaa75c5b86fdcb150e7a2fbd1f3d', 'width': 300, 'height': 300}, 'resolutions': [{'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a992a8eb38f6335fe4627e8be40fb62927349f74', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/xNlqD2-QX1uruvDzuuvr80BF7hdcPcXBk0WLLoQxLzQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de262d0e2cef7ec7638907aad51e26f8e8401496', 'width': 216, 'height': 216}], 'variants': {}, 'id': 'GwTAT4o_dTpoQkFOoJhKxJv91Xty0MbAbemkdXNZbuA'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geo7zf,True,,wellfriedbeans,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geo7zf/my_implementations_of_deep_generative_models/,all_ads,False,https://github.com/ameya98/DeepGenerativeModels,155203,1588785518.0,0,,False,,,,
,learnmachinelearning,"I have an MS in computer science, where I took 2 courses in AI and ML.  Also, I took the big ML course offered on Coursera.



Is there a well known online resource where I can learn more advanced concepts in ML, deep learning, and NLP?",t2_2k1plfwa,False,,0,False,Best place to learn more intermediate and advanced machine learning?,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gek1fy,False,light,1.0,,public,4,0,{},,,False,[],,False,False,,{},Discussion,False,4,,False,self,False,,[],{},,,True,,1588800641.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have an MS in computer science, where I took 2 courses in AI and ML.  Also, I took the big ML course offered on Coursera.&lt;/p&gt;

&lt;p&gt;Is there a well known online resource where I can learn more advanced concepts in ML, deep learning, and NLP?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gek1fy,True,,memcpy94,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gek1fy/best_place_to_learn_more_intermediate_and/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gek1fy/best_place_to_learn_more_intermediate_and/,155203,1588771841.0,0,,False,,,,
,learnmachinelearning,"Sorry if this post is a little off-topic, I was not sure if this question is more suitable to r/learnpython or here. This question is very scikit-learn specific.

&amp;#x200B;

I am a student working on a project where we try to test how well various classifiers help with an asset allocation decision but we are having an issue with the label we will try to predict. Our label is a dummy variable, with various integers corresponding to different asset allocations. We have daily data but would like to incorporate monthly rebalancing such that this label must have the same integer value for a whole given month and can only (potentially) change at the turn of a month. 

&amp;#x200B;

How do reflect this constraint when using scikit-learn? Aggregating our daily data into montly time-series is not really an option due to the nature of our features. 

&amp;#x200B;

Thanks for any inputs!",t2_a08sq,False,,0,False,Tactical Asset Allocation with ML Classifiers,[],r/learnmachinelearning,False,6,,0,,False,t3_genm1a,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,False,self,False,,[],{},,,True,,1588812433.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if this post is a little off-topic, I was not sure if this question is more suitable to &lt;a href=""/r/learnpython""&gt;r/learnpython&lt;/a&gt; or here. This question is very scikit-learn specific.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;I am a student working on a project where we try to test how well various classifiers help with an asset allocation decision but we are having an issue with the label we will try to predict. Our label is a dummy variable, with various integers corresponding to different asset allocations. We have daily data but would like to incorporate monthly rebalancing such that this label must have the same integer value for a whole given month and can only (potentially) change at the turn of a month. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;How do reflect this constraint when using scikit-learn? Aggregating our daily data into montly time-series is not really an option due to the nature of our features. &lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for any inputs!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,genm1a,True,,blacksiddis,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/genm1a/tactical_asset_allocation_with_ml_classifiers/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/genm1a/tactical_asset_allocation_with_ml_classifiers/,155203,1588783633.0,0,,False,,,,
,learnmachinelearning,,t2_3f35usxe,False,,0,False,[DeepFake] How to make deep fake video,"[{'e': 'text', 't': 'Project'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_geriz9,False,light,0.67,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""[DeepFake] How to make deep fake video 'We will meet again' - Deep Lazy Guy"", 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Deep Lazy Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wnwf9j-6-eU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCroUoJv7loN07RSaGpmGfrw'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/geriz9', 'height': 338}",Project,False,1,,False,https://b.thumbs.redditmedia.com/sH5DkQl7GkAaidT97_VIn77ASeQsSkTz787sGQ0mizc.jpg,False,,[],{},rich:video,,False,,1588824677.0,richtext,6,,,text,youtube.com,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?auto=webp&amp;s=0329b426928729a12a7ca84e9e332eebd5d67ca2', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b864a0a39be62dbdb88691c96edb98ec635858e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e130f0b3183a7e8f195dcbd7fc3434a9b1cfbf58', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/jgFV2tWbWGLk-WzkCASs8uHqTxF9LMT3j5wXTQ0LJ78.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0bf7bb4d2157ecd4979802565853f6dc56e3882c', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'qMastc7r39UnAIROICUISMW_t6FGAWmqF1F4dfMcyrw'}], 'enabled': False}",[],[],False,e21fa83e-accf-11e9-ab9f-0ec7c4b24e8e,False,False,False,,[],False,,,,t5_3cqa1,,,#7193ff,geriz9,True,,catisfying,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geriz9/deepfake_how_to_make_deep_fake_video/,all_ads,False,https://www.youtube.com/watch?v=Wnwf9j-6-eU,155203,1588795877.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': ""[DeepFake] How to make deep fake video 'We will meet again' - Deep Lazy Guy"", 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/Wnwf9j-6-eU?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Deep Lazy Guy', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/Wnwf9j-6-eU/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCroUoJv7loN07RSaGpmGfrw'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,"Hey everybody ðŸ‘‹ðŸ½

Are there ML engineers who can tell me about their experience working in the field?? Currently, I am confused between ML and Blockchain. Can anybody tell me how to tackle this confused â€œpathâ€ problem??

Thanks ðŸ™",t2_2ufovsg1,False,,0,False,Machine learning engineers where you at??,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,,False,t3_gevapf,False,light,0.25,,public,0,0,{},,,False,[],,False,False,,{},HELP,False,0,,False,self,False,,[],{},,,True,,1588836955.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hey everybody ðŸ‘‹ðŸ½&lt;/p&gt;

&lt;p&gt;Are there ML engineers who can tell me about their experience working in the field?? Currently, I am confused between ML and Blockchain. Can anybody tell me how to tackle this confused â€œpathâ€ problem??&lt;/p&gt;

&lt;p&gt;Thanks ðŸ™&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,gevapf,True,,codefreak-123,,7,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gevapf/machine_learning_engineers_where_you_at/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gevapf/machine_learning_engineers_where_you_at/,155203,1588808155.0,0,,False,,,,
,learnmachinelearning," Sorry if my question is too generic, but I'm new to deep learning. I'm creating a GAN network to generate CT images.

The train dataset is of 848x848 image, and the generated ones should have to be the same size, but I get CUDA out of memory error.

I tried putting the batch size to 1 and still get the same error.

Is there any way for me to solve this issue since I can't decrease the batch size more?",t2_8iq8g3v,False,,0,False,Deep Convolutional GAN for CT images,[],r/learnmachinelearning,False,6,,0,,False,t3_geqy18,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588822873.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Sorry if my question is too generic, but I&amp;#39;m new to deep learning. I&amp;#39;m creating a GAN network to generate CT images.&lt;/p&gt;

&lt;p&gt;The train dataset is of 848x848 image, and the generated ones should have to be the same size, but I get CUDA out of memory error.&lt;/p&gt;

&lt;p&gt;I tried putting the batch size to 1 and still get the same error.&lt;/p&gt;

&lt;p&gt;Is there any way for me to solve this issue since I can&amp;#39;t decrease the batch size more?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geqy18,True,,brgreen25,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geqy18/deep_convolutional_gan_for_ct_images/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geqy18/deep_convolutional_gan_for_ct_images/,155203,1588794073.0,0,,False,,,,
,learnmachinelearning,"Currently I want a Data Science internship. I have also thought of doing the IBM AI and Engineering certification. My long run goal is to be a practitioner of  Machine Learning, I would just like help taking those first steps.",t2_2x9bm678,False,,0,False,"Hey, guys, I was a Data Analytics Intern at a company and would like to take the next step toward certification to be used toward another internship I will apply for. I want to listen to what you guys think.",[],r/learnmachinelearning,False,6,,0,,False,t3_geqy03,False,dark,0.99,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,True,self,False,,[],{},,,True,,1588822870.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Currently I want a Data Science internship. I have also thought of doing the IBM AI and Engineering certification. My long run goal is to be a practitioner of  Machine Learning, I would just like help taking those first steps.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geqy03,True,,T-ROY_T-REDDIT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geqy03/hey_guys_i_was_a_data_analytics_intern_at_a/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geqy03/hey_guys_i_was_a_data_analytics_intern_at_a/,155203,1588794070.0,0,,False,,,,
,learnmachinelearning,,t2_51mclnu7,False,,0,False,Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm,[],r/learnmachinelearning,False,6,,0,105.0,False,t3_gegp0y,False,dark,1.0,,public,5,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XoMwSeL8y3E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gegp0y', 'height': 338}",,False,5,,False,https://a.thumbs.redditmedia.com/iy2xWbMFepAV98HfNkqDZBQ2Ii6cwe3zt-_JzZ4XrI4.jpg,False,,[],{},rich:video,,False,,1588785798.0,text,6,,,text,youtube.com,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?auto=webp&amp;s=2195d1a4b0df679e92c7c7f3950487e562901d90', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cce0ccffa6e615dd514f0ec7e557e9177d16828', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=41215ac3276e6e72b35fa2ded761d88c9453d44d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/5VYQvvQ4vk1PxjgzLVM_d3bWTrx5R4bPod_bmwhYzI0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=349698b3d029d655f1ad3d227ea4e237aea6cd70', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'cVViJ9CbyRzhTg-gvLngavnp3QdbaBewhRcSu6vQou0'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gegp0y,True,,TheNerdyDevYT,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gegp0y/machine_learning_tutorials_from_novice_to_pro_15/,all_ads,False,https://www.youtube.com/watch?v=XoMwSeL8y3E,155203,1588756998.0,0,"{'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'Machine Learning Tutorials - From Novice To Pro - #15 - Intuition Behind KNN Algorithm', 'type': 'video', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/XoMwSeL8y3E?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'author_name': 'The Nerdy Dev', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/XoMwSeL8y3E/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCWWRLPeMNMeDhpfE7R6qCyw'}}",False,,,,
,learnmachinelearning,"I've got some background in ML working through some sample supervised classification tutorials using Python and TensorFlow.

I have data in a CSV file, with each row having input features and the last column being its classification label. I'm looking for options (IBM? Google?) that would let me upload the CSV, choose some type of model, train and test it, then be able to feed it new samples to classify. 

Any recommendations on what platforms could  can do this pretty straight forward?",t2_81ir9,False,,0,False,Best online platform for supervised classification?,"[{'e': 'text', 't': 'Request'}]",r/learnmachinelearning,False,6,,0,,False,t3_genkta,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},Request,False,1,,False,self,False,,[],{},,,True,,1588812325.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I&amp;#39;ve got some background in ML working through some sample supervised classification tutorials using Python and TensorFlow.&lt;/p&gt;

&lt;p&gt;I have data in a CSV file, with each row having input features and the last column being its classification label. I&amp;#39;m looking for options (IBM? Google?) that would let me upload the CSV, choose some type of model, train and test it, then be able to feed it new samples to classify. &lt;/p&gt;

&lt;p&gt;Any recommendations on what platforms could  can do this pretty straight forward?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,47a377e4-acd0-11e9-a2da-0e1a75f5dd52,False,False,False,,[],False,,,,t5_3cqa1,,,#0dd3bb,genkta,True,,timex40,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/genkta/best_online_platform_for_supervised_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/genkta/best_online_platform_for_supervised_classification/,155203,1588783525.0,0,,False,,,,
,learnmachinelearning,,t2_4q5pfd01,False,,0,False,"Amazon Alexa supports development of custom and smart home skills that get invoked after specific voice commands and general phrases. Check below the video weâ€™ve made to show how this skill works, including what youâ€™ll see on your phone at this moment.",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gefe1a,False,dark,0.78,,public,5,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_96', 'dash_url': 'https://v.redd.it/r2dv0x4pj3x41/DASHPlaylist.mpd', 'duration': 130, 'hls_url': 'https://v.redd.it/r2dv0x4pj3x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,5,,False,https://b.thumbs.redditmedia.com/kPhNAhx2raYBqcSAAWHKWE0jzcnqoP9-8oYi6rlzaMw.jpg,False,,[],{},hosted:video,,False,,1588778770.0,text,6,,,text,v.redd.it,False,,,,,,False,False,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?format=pjpg&amp;auto=webp&amp;s=10571f3ba1ab4970484f9acfa2c35ac15edcdc62', 'width': 1280, 'height': 720}, 'resolutions': [{'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9be88fa0219f44991bca0877fae4d05d42648b5b', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0b600f38cfa175af7e592020bfb9a9a317ef6b6b', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=dab893fde773ebc5c99689f37cfb3c8e7768aa0c', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b93e3e17439d843b3d6efd2968c8a2a917c51dc0', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=79889e9f539f4935f10192a427d1efff3c5f3772', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/kh6ydWZ5j6Uvx61M6nSnmPWb6pSOgn7TATvS-UuLPEw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=168ba31f13be6786fd524333a8f739aff43169ca', 'width': 1080, 'height': 607}], 'variants': {}, 'id': '_9XJytoPJ1pQ3G_cMOmkmvh2L9sWf4i6sfg3M14tI3A'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gefe1a,True,,alexandra_moroz,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gefe1a/amazon_alexa_supports_development_of_custom_and/,all_ads,False,https://v.redd.it/r2dv0x4pj3x41,155203,1588749970.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_720?source=fallback', 'height': 720, 'width': 1280, 'scrubber_media_url': 'https://v.redd.it/r2dv0x4pj3x41/DASH_96', 'dash_url': 'https://v.redd.it/r2dv0x4pj3x41/DASHPlaylist.mpd', 'duration': 130, 'hls_url': 'https://v.redd.it/r2dv0x4pj3x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
,learnmachinelearning,"I have a dataset which looks something like this 

[DataSet](https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;format=png&amp;auto=webp&amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10)

I want to build a recommendation system for research papers, based on content based recommendation system techniques.   
So if a user chooses his relevant keywords  my model should be able recommend papers similar to it.   


Can somebody point out how i can achieve this and create a model?   
Thanks",t2_y2rn6,False,,0,False,What model should I use in this scenario?,"[{'e': 'text', 't': 'HELP'}]",r/learnmachinelearning,False,6,,0,30.0,False,t3_geguyw,False,light,1.0,,public,3,0,{},140.0,,False,[],,False,False,,{},HELP,False,3,,False,https://b.thumbs.redditmedia.com/UYFLZASfDzu8Nqa94i33lFDwdOZG4tzM2FAunQ04RJo.jpg,False,,[],{},,,True,,1588786675.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I have a dataset which looks something like this &lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10""&gt;DataSet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I want to build a recommendation system for research papers, based on content based recommendation system techniques.&lt;br/&gt;
So if a user chooses his relevant keywords  my model should be able recommend papers similar to it.   &lt;/p&gt;

&lt;p&gt;Can somebody point out how i can achieve this and create a model?&lt;br/&gt;
Thanks&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,af5dfa18-accf-11e9-9669-0ec668ea0cbc,False,False,False,,[],False,,,,t5_3cqa1,,,#ea0027,geguyw,True,,megatronus8010,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geguyw/what_model_should_i_use_in_this_scenario/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geguyw/what_model_should_i_use_in_this_scenario/,155203,1588757875.0,0,,False,,,"{'b2dr72iv64x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 23, 'x': 108, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4dd23a36c017ac971afb2858c268de2957be390d'}, {'y': 47, 'x': 216, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b036bc634388caa6fc471d74f0536ef84f40b42e'}, {'y': 70, 'x': 320, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac8006ee963c26b8f42f7149c8231f753001a3bf'}, {'y': 141, 'x': 640, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2e198df5e3fb0676ee65d51ac3757cf4e3a7a4fa'}, {'y': 212, 'x': 960, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=11a74c00120275f6fcfb1c2e3eddd21b90f9277a'}, {'y': 239, 'x': 1080, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a6b6ddb71f090b8dd35e18f33b2b83f0b42488a'}], 's': {'y': 268, 'x': 1211, 'u': 'https://preview.redd.it/b2dr72iv64x41.png?width=1211&amp;format=png&amp;auto=webp&amp;s=f32bf99a2b0a1c5ad83d9a54ef43c583f1f5ee10'}, 'id': 'b2dr72iv64x41'}}",
,learnmachinelearning,,t2_rpxuc,False,,0,False,Telegram channel - Data Science Digest - Join us today!,[],r/learnmachinelearning,False,6,,0,140.0,False,t3_gem571,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://a.thumbs.redditmedia.com/p_OSeLJEt8j9bh0b4KstI_GJnPd7KbqMWvnBtZAFOQ8.jpg,False,,[],{},link,,False,,1588807682.0,text,6,,,text,t.me,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?auto=webp&amp;s=54c350f8b8e19f2b72605d5d263d846e82b9efdb', 'width': 320, 'height': 320}, 'resolutions': [{'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f912c632fb75a309352590e64aa4ee88ac06e6d', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2147e82a4176e76d99df38a96f0f91114055c813', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/ysAxd2d5xevVJGhZcK3J7hw23GZaBdNqm4druZ8J8_Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c3607d628e552bd0a4eb266b0ac2b4ed74594f7', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'M29R14E1g8-GPffRiVdjtQYI9XGFJhWIY9RBOIEZOg4'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gem571,True,,flyelephant,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gem571/telegram_channel_data_science_digest_join_us_today/,all_ads,False,https://t.me/DataScienceDigest,155203,1588778882.0,0,,False,,,,
,learnmachinelearning,"I work in neuroscience but I learned to program as a hobby when I was 12 and took a computational neuroscience course during my undergrad which was half machine learning. I've been interested in solidifying my practical skills in this domain so I recently tried out Andrew Ng's Deep Learning specialization on Coursera because it seemed like a decent review and you get a free certificate out of it. Unfortunately I didn't learn many new things, but it was a good refresher. Basically the first half of the undergrad course I mentioned but with more detail on sequence models and some tips for working in production vs academic research environments. Aside from working on projects and/or competitions, what resources would recommend going forward? I generally understand the mathematical formalisms and the intuition behind what I've seen so far. Are there any more advanced courses or textbooks I should read? The AI for Medicine specialization seemed relevant to me but not necessarily much more advanced.",t2_6e9cdhk4,False,,0,False,Intermediate Machine Learning Resources,[],r/learnmachinelearning,False,6,,0,,False,t3_geceem,False,dark,1.0,,public,8,0,{},,,False,[],,False,False,,{},,False,8,,False,self,False,,[],{},,,True,,1588764434.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I work in neuroscience but I learned to program as a hobby when I was 12 and took a computational neuroscience course during my undergrad which was half machine learning. I&amp;#39;ve been interested in solidifying my practical skills in this domain so I recently tried out Andrew Ng&amp;#39;s Deep Learning specialization on Coursera because it seemed like a decent review and you get a free certificate out of it. Unfortunately I didn&amp;#39;t learn many new things, but it was a good refresher. Basically the first half of the undergrad course I mentioned but with more detail on sequence models and some tips for working in production vs academic research environments. Aside from working on projects and/or competitions, what resources would recommend going forward? I generally understand the mathematical formalisms and the intuition behind what I&amp;#39;ve seen so far. Are there any more advanced courses or textbooks I should read? The AI for Medicine specialization seemed relevant to me but not necessarily much more advanced.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,geceem,True,,bekpey235,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/geceem/intermediate_machine_learning_resources/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/geceem/intermediate_machine_learning_resources/,155203,1588735634.0,1,,False,,,,
,learnmachinelearning,"I just publish an article on Face recognition in Javascript, with a focus on the main program for others to build on

[Post](https://heartbeat.fritz.ai/facial-recognition-system-with-javascript-f9659c381434)",t2_m7ce4vv,False,,0,False,Face recognition in Javascript,[],r/learnmachinelearning,False,6,,0,,False,t3_gelajo,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588804996.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I just publish an article on Face recognition in Javascript, with a focus on the main program for others to build on&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://heartbeat.fritz.ai/facial-recognition-system-with-javascript-f9659c381434""&gt;Post&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?auto=webp&amp;s=e81629334aba99b67a64b2c79d30a0c41311e5da', 'width': 1200, 'height': 800}, 'resolutions': [{'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=270b0a93a536e6b1b208a09cdf095ea6d7cd8490', 'width': 108, 'height': 72}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b19a9cd59188a6cf9b81bd7d85a128b2c83e4400', 'width': 216, 'height': 144}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=341d6ed80c204f8a65a08e6b616572833e71cd67', 'width': 320, 'height': 213}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a9f823de95cfa5e75e57589a34931d05829f022', 'width': 640, 'height': 426}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8611893a0dac4b3595ab62e51876ebe9ec3b8590', 'width': 960, 'height': 640}, {'url': 'https://external-preview.redd.it/2xEvxtvXQeW3gEDfdR4f5tnHAcbPAQLhRhljgz0kZlQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e15c2a7376a4b150ca1496321f1a65538f983dd1', 'width': 1080, 'height': 720}], 'variants': {}, 'id': 'CDYBcLs3ktXru7AvVyO17tpccldZUEgkozr2JQ6fC4o'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gelajo,True,,steveoni,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gelajo/face_recognition_in_javascript/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gelajo/face_recognition_in_javascript/,155203,1588776196.0,0,,False,,,,
,learnmachinelearning,,t2_2crnmmt9,False,,0,False,Learning Convolutional Neural Networks with Interactive Visualization,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,105.0,False,t3_gel01t,False,light,1.0,,public,1,0,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'height': 338}",140.0,,False,[],"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Learning Convolutional Neural Networks with Interactive Visualization', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IGOn-82OZ_8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,False,,"{'content': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'width': 600, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/gel01t', 'height': 338}",Discussion,False,1,,False,https://b.thumbs.redditmedia.com/xgS_VF2RuIeCGXnQYOxgfanhqosKIcA_lhO7KRuaHlM.jpg,False,,[],{},rich:video,,False,,1588804031.0,richtext,6,,,text,youtu.be,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?auto=webp&amp;s=a74e4edc2a8140a10d2c4523821fd91e6308b5bd', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1bf3f2e066393978750ad66d280663c28923ea21', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9ec9ccf4a4cff8a0aad4434979cad6ffce9fb7f', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/biCF69rrXx75fENX26GyBDbXSGH5_WBnBi0htB5hXIk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b105273682eff2a67dc8a45d1279b54f2819697a', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'ELZ_Z_iJXnNbzU3SFQaDBbYmeoZb-D3NKqAr3JSg7dc'}], 'enabled': False}",[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gel01t,True,,cmillionaire9,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gel01t/learning_convolutional_neural_networks_with/,all_ads,False,https://youtu.be/IGOn-82OZ_8,155203,1588775231.0,0,"{'oembed': {'provider_url': 'https://www.youtube.com/', 'title': 'Learning Convolutional Neural Networks with Interactive Visualization', 'html': '&lt;iframe width=""600"" height=""338"" src=""https://www.youtube.com/embed/IGOn-82OZ_8?feature=oembed&amp;enablejsapi=1"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen&gt;&lt;/iframe&gt;', 'thumbnail_width': 480, 'height': 338, 'width': 600, 'version': '1.0', 'author_name': 'Vecanoi', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/IGOn-82OZ_8/hqdefault.jpg', 'type': 'video', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/channel/UCj6vvRE6NAgxSc7eRCVHHiA'}, 'type': 'youtube.com'}",False,,,,
,learnmachinelearning,,t2_l8c0m,False,,0,False,Remembering long parameters lists for bash/python for reproducibility,[],r/learnmachinelearning,False,6,,0,90.0,False,t3_gekztl,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},,False,1,,False,https://b.thumbs.redditmedia.com/WE4V0sqyDOdZ67M0cR-HkHAoXI1B_Cfy8-YjxnjXHBg.jpg,False,,[],{},link,,False,,1588804009.0,text,6,,,text,angelov.ai,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?auto=webp&amp;s=2930dd6e5ffad1c594a311d13ab062ad2cc78e76', 'width': 1024, 'height': 661}, 'resolutions': [{'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d2f7de80ae4e4ae90486fb735595305d7bb6c25', 'width': 108, 'height': 69}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=64caf9917f3fdb13f1a1749f5363e976fad04eb8', 'width': 216, 'height': 139}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=56072aea5c75e0ee956eaf12816035501a336a47', 'width': 320, 'height': 206}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a0116003b0721f96bec410c9cbe21e2403c014b', 'width': 640, 'height': 413}, {'url': 'https://external-preview.redd.it/zSElK_EtYPxFTZE2qClwQOKumNQIcXuMMTvjHlbdu7E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b3307fe7dd8ac55ed9882869c9b346e4b3fa984', 'width': 960, 'height': 619}], 'variants': {}, 'id': 'hVsKygQuiJoRrEhYLXWvxjETmOnohY7EkBvSygg-6Dk'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gekztl,True,,23pointsNorth,,1,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekztl/remembering_long_parameters_lists_for_bashpython/,all_ads,False,https://angelov.ai/post/2020/remember-your-bash-spells/,155203,1588775209.0,0,,False,,,,
,learnmachinelearning,,t2_5lfdx43m,False,,0,False,Formula to predict values in logistic regression,"[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,47.0,False,t3_gekqo2,False,dark,1.0,,public,1,0,{},140.0,,False,[],,True,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/RPoJjdIO8ehF1dhs3lEbsIGRQkg4mTVtioL_kta59NI.jpg,False,,[],{},image,,False,,1588803121.0,richtext,6,,,text,i.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?auto=webp&amp;s=072ee71e908776aa2002c3f2741e9d0a300de2a4', 'width': 1536, 'height': 526}, 'resolutions': [{'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3777d24a39d49a77ee7c3579d4262f4316f2d1e2', 'width': 108, 'height': 36}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b5d584a6ba699daf7ca788e4054143e5fba5256', 'width': 216, 'height': 73}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4381db716b768b8318081cbb4f2e2f3dcaef6e63', 'width': 320, 'height': 109}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e318f8da562fb52147840c2f5f83584c38f25992', 'width': 640, 'height': 219}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6623153e1a72dd8045dba6ae5c583af68e22edec', 'width': 960, 'height': 328}, {'url': 'https://preview.redd.it/mxz1vgr8k5x41.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d51f837cd14b3b0c4978f62b754be2acccc11956', 'width': 1080, 'height': 369}], 'variants': {}, 'id': 'eBNTbd2c-VShmbf8jYgh2bM4riCZxvq7iB88z033HsA'}], 'enabled': True}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gekqo2,True,,invidae,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekqo2/formula_to_predict_values_in_logistic_regression/,all_ads,False,https://i.redd.it/mxz1vgr8k5x41.jpg,155203,1588774321.0,0,,False,,,,
,learnmachinelearning,"Hello, I'm writing a report about the mathematics behind feedforward neural networks. When I was presenting my proposal to the instructor, he said that I need to be careful using the superscript notation to refer to vectors/matrices, since it is not considered a ""standard"" math notation.

Here is an example of the superscript notation in question. The x^((i)) and y^((i)) refers to the i^(th) training example and the i^(th) ground-truth label respectively.

&amp;#x200B;

https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;format=png&amp;auto=webp&amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361

Is this superscript notation considered ""standard"" in maths, or is it just limited to the field of machine learning? I tried to search for sources but couldn't find the origin of the notation.

[This](https://stats.stackexchange.com/questions/193908/in-machine-learning-why-are-superscripts-used-instead-of-subscripts) stackoverflow thread also discusses this issue, but none of the answers there referenced any reputable sources. I probably need to find a reputable source in order to convince my instructor.

&amp;#x200B;

Thanks for the help!",t2_1u44tp6m,False,,0,False,"Is the superscript notation used in machine learning considered ""standard"" math notation?","[{'e': 'text', 't': 'Question'}]",r/learnmachinelearning,False,6,,0,32.0,False,t3_gekfgp,False,dark,1.0,,public,1,0,{},140.0,,False,[],,False,False,,{},Question,False,1,,False,https://b.thumbs.redditmedia.com/m_fflNj6_SA_UOLY__YCCoDNT0MI_fDaKklzAjdGzwY.jpg,False,,[],{},self,,True,,1588802060.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Hello, I&amp;#39;m writing a report about the mathematics behind feedforward neural networks. When I was presenting my proposal to the instructor, he said that I need to be careful using the superscript notation to refer to vectors/matrices, since it is not considered a &amp;quot;standard&amp;quot; math notation.&lt;/p&gt;

&lt;p&gt;Here is an example of the superscript notation in question. The x&lt;sup&gt;(i&lt;/sup&gt;) and y&lt;sup&gt;(i&lt;/sup&gt;) refers to the i&lt;sup&gt;th&lt;/sup&gt; training example and the i&lt;sup&gt;th&lt;/sup&gt; ground-truth label respectively.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361""&gt;https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is this superscript notation considered &amp;quot;standard&amp;quot; in maths, or is it just limited to the field of machine learning? I tried to search for sources but couldn&amp;#39;t find the origin of the notation.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://stats.stackexchange.com/questions/193908/in-machine-learning-why-are-superscripts-used-instead-of-subscripts""&gt;This&lt;/a&gt; stackoverflow thread also discusses this issue, but none of the answers there referenced any reputable sources. I probably need to find a reputable source in order to convince my instructor.&lt;/p&gt;

&lt;p&gt;&amp;#x200B;&lt;/p&gt;

&lt;p&gt;Thanks for the help!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?auto=webp&amp;s=e25be8cb5cf2448d39a7e5ffc877e4d466b776d3', 'width': 316, 'height': 316}, 'resolutions': [{'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9645cb9b1f6437724f04c3b63b841cf7766f27d6', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/GK4J2jdDCd68cmAzq4b5v8wOyVmJzcMn471wSxWrwMY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc7ee570d3611e1ef9314d658423c42908808820', 'width': 216, 'height': 216}], 'variants': {}, 'id': '63C1GbYQbI4tHZMkw99e-qlyoYaGnG58yfnmnFUJ34s'}], 'enabled': False}",[],[],False,ec81b8ee-accf-11e9-b8f8-0ebea2df7d78,False,False,False,,[],False,,,,t5_3cqa1,,,#ffb000,gekfgp,True,,Unturned3,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gekfgp/is_the_superscript_notation_used_in_machine/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gekfgp/is_the_superscript_notation_used_in_machine/,155203,1588773260.0,0,,False,,,"{'3gfr7tryf5x41': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 25, 'x': 108, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b59fc419d312e3e67b76afa296f7488df7bd0c53'}, {'y': 50, 'x': 216, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=535304b63d7d7820e4f9eb92ea27f9bc74abfcb3'}, {'y': 74, 'x': 320, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=566a47be4466462f39d564b1ab427cc3ee96e47d'}], 's': {'y': 77, 'x': 329, 'u': 'https://preview.redd.it/3gfr7tryf5x41.png?width=329&amp;format=png&amp;auto=webp&amp;s=0d18973679e967094b9182c7d042e6ae3d1aa361'}, 'id': '3gfr7tryf5x41'}}",
,learnmachinelearning," I had the opportunity to get lost in a world of algorithm evaluation on my latest blog article. It's a niche topic but anyone look at ML who finds this interesting such as me, might find my article a good read.

[https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/](https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/)",t2_5u3zdmy8,False,,0,False,Precision and Recall - Evaluating Classification Algorithms like secrets detection,[],r/learnmachinelearning,False,6,,0,,False,t3_gek4d3,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},self,,True,,1588800933.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;I had the opportunity to get lost in a world of algorithm evaluation on my latest blog article. It&amp;#39;s a niche topic but anyone look at ML who finds this interesting such as me, might find my article a good read.&lt;/p&gt;

&lt;p&gt;&lt;a href=""https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/""&gt;https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?auto=webp&amp;s=ae6357c9da049d40bed8d5d0353fc1fbf677478a', 'width': 1180, 'height': 690}, 'resolutions': [{'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01100f06dfcebd81841314d0bfec3faacf1d234e', 'width': 108, 'height': 63}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=59adb74871885c40894148326907fb341598ef9a', 'width': 216, 'height': 126}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e16a06d7f5ac9d057ccf232a9cd758148842c5c', 'width': 320, 'height': 187}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82cce0c7fce3ed30588c03483ee8f8d8cbbeface', 'width': 640, 'height': 374}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd98fff1c20505e4a81dae48712e03eff4e4a7d0', 'width': 960, 'height': 561}, {'url': 'https://external-preview.redd.it/5VICMPQbIIAZcq3Hr9YuM_l2ccfoAUZpueEEr2EvrBA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27d9314ef79c15fb956a356bf532689ccfd7ca36', 'width': 1080, 'height': 631}], 'variants': {}, 'id': 'DnxQVypYnehZ7JfD2358CJ7yTgUFxpd7o6UDsjXnnk8'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gek4d3,True,,Mackenzie-GG,,0,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gek4d3/precision_and_recall_evaluating_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gek4d3/precision_and_recall_evaluating_classification/,155203,1588772133.0,0,,False,,,,
,learnmachinelearning," Suppose I have timeseries (X) and I also have labels (binary) corresponding to each timestep. The label at timestep ""t"" is based on the value of X at timestep ""t"", i.e., for example if x\_t &gt; 0.6 then y\_t = 1 else 0. My objective here is to predict the label at (t+1) given historic data till time ""t"". Its upto us to decide how much of history we should use.

We can achieve the objective in 2 ways:

1. Train a LSTM using X and predict the value of X at time ""t+1"" and then classify X at time ""t+1"" to one of the classes (using another model or one additional layer of softmax). The training input here being X\[t-k: t\] (can use a window of size k to generate inputs from sequence) and the its corresponding output being X\[t+1\].
2. Train a LSTM using X and predict the label at time ""t+1"" directly. The training input here being X\[t-k: t\] (can use a window of size k to generate inputs from sequence) and the its corresponding output being the label of x at time ""t+1"". Since training data is historic data, we know the labels for each x.

I'm confused whether approach 2 make sense logically. I understand that LSTM will try to build a model and find a mapping between the inputs and outputs. But how different it will be from LSTM which is trained on the same inputs but its output was the label at time ""t"" instead of time ""t+1"".

Approach 1 make sense logically, but will approach 2 do the same, i.e., will the LSTM internally first try to predict the value of x at time t+1 and then classify x at time t+1.

Any comments or suggestions?",t2_83ul0,False,,0,False,Time series classification,[],r/learnmachinelearning,False,6,,0,,False,t3_gejgxl,False,dark,1.0,,public,1,0,{},,,False,[],,False,False,,{},,False,1,,False,self,False,,[],{},,,True,,1588798507.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;Suppose I have timeseries (X) and I also have labels (binary) corresponding to each timestep. The label at timestep &amp;quot;t&amp;quot; is based on the value of X at timestep &amp;quot;t&amp;quot;, i.e., for example if x_t &amp;gt; 0.6 then y_t = 1 else 0. My objective here is to predict the label at (t+1) given historic data till time &amp;quot;t&amp;quot;. Its upto us to decide how much of history we should use.&lt;/p&gt;

&lt;p&gt;We can achieve the objective in 2 ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Train a LSTM using X and predict the value of X at time &amp;quot;t+1&amp;quot; and then classify X at time &amp;quot;t+1&amp;quot; to one of the classes (using another model or one additional layer of softmax). The training input here being X[t-k: t] (can use a window of size k to generate inputs from sequence) and the its corresponding output being X[t+1].&lt;/li&gt;
&lt;li&gt;Train a LSTM using X and predict the label at time &amp;quot;t+1&amp;quot; directly. The training input here being X[t-k: t] (can use a window of size k to generate inputs from sequence) and the its corresponding output being the label of x at time &amp;quot;t+1&amp;quot;. Since training data is historic data, we know the labels for each x.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I&amp;#39;m confused whether approach 2 make sense logically. I understand that LSTM will try to build a model and find a mapping between the inputs and outputs. But how different it will be from LSTM which is trained on the same inputs but its output was the label at time &amp;quot;t&amp;quot; instead of time &amp;quot;t+1&amp;quot;.&lt;/p&gt;

&lt;p&gt;Approach 1 make sense logically, but will approach 2 do the same, i.e., will the LSTM internally first try to predict the value of x at time t+1 and then classify x at time t+1.&lt;/p&gt;

&lt;p&gt;Any comments or suggestions?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gejgxl,True,,Laboulaye,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gejgxl/time_series_classification/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gejgxl/time_series_classification/,155203,1588769707.0,0,,False,,,,
,learnmachinelearning,"So, I just finished testing and evaluating my CNN DL Model to classify images into one of ten classes.

I'd like to convert this work into a research paper and hence should do tests on other standardized datasets to show the results.

The problem is, that this a new model that works on a self made data-set and all the other datasets have only 2-3 classes that are a subset of my data-set.

Is there a way to adapt my model that predicts 10 classes so that it will work for these other datasets?

I'd like to hear your opinions on how you guys generally handle this problem.",t2_107f70iq,False,,0,False,Testing your shiny new model on standardized datasets,"[{'e': 'text', 't': 'Discussion'}]",r/learnmachinelearning,False,6,,0,,False,t3_gefgzu,False,light,1.0,,public,2,0,{},,,False,[],,False,False,,{},Discussion,False,2,,False,self,False,,[],{},,,True,,1588779217.0,richtext,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;So, I just finished testing and evaluating my CNN DL Model to classify images into one of ten classes.&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to convert this work into a research paper and hence should do tests on other standardized datasets to show the results.&lt;/p&gt;

&lt;p&gt;The problem is, that this a new model that works on a self made data-set and all the other datasets have only 2-3 classes that are a subset of my data-set.&lt;/p&gt;

&lt;p&gt;Is there a way to adapt my model that predicts 10 classes so that it will work for these other datasets?&lt;/p&gt;

&lt;p&gt;I&amp;#39;d like to hear your opinions on how you guys generally handle this problem.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,bed45ad2-accf-11e9-adf9-0e84e198baba,False,False,False,,[],False,,,,t5_3cqa1,,,#007373,gefgzu,True,,PsydeliX_,,3,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gefgzu/testing_your_shiny_new_model_on_standardized/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gefgzu/testing_your_shiny_new_model_on_standardized/,155203,1588750417.0,0,,False,,,,
,learnmachinelearning,"It doesn't matter if it's something trivial. As long as it's new information about machine learning you didn't know until this week, feel free to share!",t2_6l4z3,False,,0,False,TWIL (This Week I Learned) - Share something new that you have learned this week!,[],r/learnmachinelearning,False,6,,0,,False,t3_gef568,False,dark,1.0,,public,2,0,{},,,False,[],,False,False,,{},,False,2,,True,self,False,,[],{},,,True,,1588777475.0,text,6,,,text,self.learnmachinelearning,False,"&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;It doesn&amp;#39;t matter if it&amp;#39;s something trivial. As long as it&amp;#39;s new information about machine learning you didn&amp;#39;t know until this week, feel free to share!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;",,,,,False,True,False,False,False,,[],[],False,,False,False,False,,[],False,,,moderator,t5_3cqa1,,,,gef568,True,,AutoModerator,,1,False,all_ads,False,[],False,,/r/learnmachinelearning/comments/gef568/twil_this_week_i_learned_share_something_new_that/,all_ads,False,https://www.reddit.com/r/learnmachinelearning/comments/gef568/twil_this_week_i_learned_share_something_new_that/,155203,1588748675.0,0,,False,,,,
,learnmachinelearning,,t2_205ygpnb,False,,0,False,"Machine Learning with Python : Part 1: Model Persistent :: How to do and make : Independent and Dependent data , import Logistics Regression and Using Logistic Regression, fit and predict data, import pickle and dump data .",[],r/learnmachinelearning,False,6,,0,78.0,False,t3_gedazi,False,dark,1.0,,public,3,0,{},140.0,,False,[],"{'reddit_video': {'fallback_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_96', 'dash_url': 'https://v.redd.it/v9ilgjfgo2x41/DASHPlaylist.mpd', 'duration': 49, 'hls_url': 'https://v.redd.it/v9ilgjfgo2x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,False,,{},,False,3,,False,https://b.thumbs.redditmedia.com/I7Vmc0BkFoLDKcArg8KHnyfcFccUaAERQsw1uZdxAIU.jpg,False,,[],{},hosted:video,,False,,1588768277.0,text,6,,,text,v.redd.it,False,,,,,,False,True,False,False,False,"{'images': [{'source': {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?format=pjpg&amp;auto=webp&amp;s=4e8822c805190883939674eb03500991e4ab3254', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=85f06fad76fd1680af23e60059420f8d115ec137', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81b1199e8daa30be7c1f50bb9bb9ceb28e3f269d', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=51b434689d61d54b1d47e937a86d02dd4b72b1bc', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a7600f71f90ccaa87a812013d4cdccb2dd76f109', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e21102012a2cef3f3a8d9581ecbc5ec2766b2fa5', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/j6AtfHe959Gv7qNtH5m5yrOR7eHNtfA_yTkAtkP2Mwk.jpg?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=546cd91b1a1198af1f93feac7925aef7c69249b1', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'aX0JJlKw7sHMUW3xNiIJdBP1VuGoKlocQ_-L57lVc3c'}], 'enabled': False}",[],[],False,,False,False,False,,[],False,,,,t5_3cqa1,,,,gedazi,True,,iamrealadvait,,2,True,all_ads,False,[],False,,/r/learnmachinelearning/comments/gedazi/machine_learning_with_python_part_1_model/,all_ads,False,https://v.redd.it/v9ilgjfgo2x41,155203,1588739477.0,0,"{'reddit_video': {'fallback_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_1080?source=fallback', 'height': 1080, 'width': 1920, 'scrubber_media_url': 'https://v.redd.it/v9ilgjfgo2x41/DASH_96', 'dash_url': 'https://v.redd.it/v9ilgjfgo2x41/DASHPlaylist.mpd', 'duration': 49, 'hls_url': 'https://v.redd.it/v9ilgjfgo2x41/HLSPlaylist.m3u8', 'is_gif': False, 'transcoding_status': 'completed'}}",True,,,,
