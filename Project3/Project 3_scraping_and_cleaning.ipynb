{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Select a classification model which best identifies posts from the subreddit Learn Programming and Learn Machine Learning from the various models selected for testing. \n",
    "\n",
    "The project is split into 2 notebooks:\n",
    "\n",
    " - Notebook 1: Gathering and Cleaning of Data\n",
    " - Notebook 2: Exploratory Data Analysis, Modelling and Evaluation\n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "This project requires the application of Natural Language Processing Models ('NLP') to correctly classify the post contents to 2 similar subreddits based on the words used most and related to respective subreddit. This will allow for more accurate search results of the related posts based on the keywords entered by users and as part of the requirements, Naive Bayes classifier is used as well as Logistic Regression and Random Forest for evaluation. \n",
    "\n",
    "The steps taken include:-\n",
    " - scraping data from the respective subreddits selected \n",
    " - selecting the columns required for NLP classification\n",
    " - cleaning of data using BeautifulSoup, NTLK's Stopwords, Regex\n",
    "\n",
    "After the cleaning of the data, the total posts works out to be 1657 posts of which 58.72% are posts from Learn Programming and 41.28% are from Learn Machine Learning. This works out to be a baseline accuracy of 0.5869. The ratio of the posts are not completely equal but not substantial enough to affect the accuracy scores. Based on the exploratory data analysis performed on the selftext columns, there are common words which frequently appear on both subreddits therefore using stopwords of these common words in order to increase the accuracy of the models. Stemming is then selected when compared to lemmatization and original text based on the accuracy results from running Logistic Regression across all 3. While stemming may not always reduce the word form to actual forms but since the focus on the model is to arrive at accuracy, selections will be based primarily on the accuracy figures. \n",
    "\n",
    "6 models were built for the purpose of evaluation:-\n",
    " 1) Count Vectorizer with Naive Bayesian's MultinomialNB\n",
    " 2) Tfidf Vectorizer with Naive Bayesian's MultinomialNB\n",
    " 3) Tfidf Vectorizer with Logistic Regression\n",
    " 4) Count Vectorizer with Logistic Regression\n",
    " 5) Count Vectorizer with Random Forest\n",
    " 6) Tfidf Vectorizer with Random Forest\n",
    " \n",
    "All 6 models have higher cross-validated mean train scores when compared to their respective test scores which could indicate overfitting and the test scores were also in the approximately 0.9 range. This could have been a result of the 2 subreddits being vastly unrelated. However, at the same time, one could probably think that programming is part and parcel of machine learning. In terms of scorings, Model 1 and 4 are very close in terms of the test scores and results from classification reports. Both the models gave an accuracy of 0.93 and the f1 scores for both models are the same. However, Model 1 gave an equal precision score of 0.93 for both subreddits as compared to 0.91 and 0.94 for Model 4. In this case where there is no preference of class 0 over class 1 or vice versa, the equal precision score of Model 1 is preferred. Both the AUC scores for Model 1 and 4 were also very close at 0.973 and 0.978 respectively which also suggest both models are high in their probability of predictions and since the cross validation mean train score for Model 1 is slightly lower than that of Model 4 which means less overfitting, Model 1 on the overall is the best performing.\n",
    "\n",
    "For the next iteration, the classifier model should be applied on unseen data to validate the scoring and perhaps apply lemmatization and add other stop words to lower the overfitting results. The title can also be included together with the selftext since the question could be in the title itself. The accuracy model can also then be further validated using other subreddits.\n",
    " \n",
    "\n",
    "### Contents\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Gathering](#Data-Gathering)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# preprocessing imports\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# modeling imports\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to loop and scrape posts from reddit.com for 50 times to draw approximately 1200 unique posts to account for any null values and duplicate posts which may be dropped thereafter. \n",
    "\n",
    "A time.sleep of 1 second and user agent is assigned to prevent reddit from blocking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Data from Subreddit: Learn Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts = []\n",
    "#after = None\n",
    "#url = 'https://www.reddit.com/r/learnprogramming.json'\n",
    "\n",
    "#for a in range(50):\n",
    "#    if after == None:\n",
    "#        current_url = url\n",
    "#    else:\n",
    "#        current_url = url + '?after=' + after\n",
    "#    print(current_url)\n",
    "#    res = requests.get(current_url, headers={'User-agent': 'scruffy 1.0'})\n",
    "    \n",
    "#    if res.status_code != 200:\n",
    "#        print('Status error', res.status_code)\n",
    "#        break\n",
    "     \n",
    "#    current_dict = res.json()\n",
    "#    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "#    posts.extend(current_posts)\n",
    "#    after = current_dict['data']['after']\n",
    "    \n",
    "#    if a > 50:\n",
    "#        prev_posts = pd.read_csv('./data/learnprogramming.csv')\n",
    "#        current_df = pd.DataFrame()\n",
    "#    else:\n",
    "#        pd.DataFrame(posts).to_csv('./data/learnprogramming.csv', index = False)\n",
    "    \n",
    "    \n",
    "#    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check length of post\n",
    "#len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Data from Subreddit: Learn Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts = []\n",
    "#after = None\n",
    "#url = 'https://www.reddit.com/r/learnmachinelearning.json'\n",
    "\n",
    "#for a in range(50):\n",
    "#    if after == None:\n",
    "#        current_url = url\n",
    "#    else:\n",
    "#        current_url = url + '?after=' + after\n",
    "#    print(current_url)\n",
    "#    res = requests.get(current_url, headers={'User-agent': 'scruffy 1.0'})\n",
    "    \n",
    "#    if res.status_code != 200:\n",
    "#        print('Status error', res.status_code)\n",
    "#        break\n",
    "     \n",
    "#    current_dict = res.json()\n",
    "#    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "#    posts.extend(current_posts)\n",
    "#    after = current_dict['data']['after']\n",
    "    \n",
    "#    if a > 50:\n",
    "#        prev_posts = pd.read_csv('./data/learnmachinelearning.csv')\n",
    "#        current_df = pd.DataFrame()\n",
    "#    else:\n",
    "#        pd.DataFrame(posts).to_csv('./data/learnmachinelearning.csv', index = False)\n",
    "       \n",
    "#    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in csv files saved\n",
    "learnprogramming = pd.read_csv('./data/learnprogramming.csv')\n",
    "learnmachinelearning = pd.read_csv('./data/learnmachinelearning.csv')\n",
    "\n",
    "pd.set_option ('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of learnprogramming : (1252, 103)\n",
      "Shape of learnmachinelearning: (1239, 110)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of both datasets for sufficient line items\n",
    "print('Shape of learnprogramming :',learnprogramming.shape)\n",
    "print('Shape of learnmachinelearning:',learnmachinelearning.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine both dataframes for easier manipulation\n",
    "combined_df = pd.concat([learnprogramming, learnmachinelearning], ignore_index=True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1661, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check if the dataframes were combined \n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Reddit site, the key columns required would be the \n",
    "- Title which gives the title of the thread\n",
    "- Selftext which shows the contents of the thread\n",
    "- Subreddit which shows which subreddit the post is from\n",
    "\n",
    "The column for author has been retained in case it comes in handy as the 2 subreddits are quite closely related and the author could possibly be common to the threads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the dataframe with only the selected features from the features list\n",
    "features_list = ['author', 'title', 'selftext', 'subreddit']\n",
    "combined_df = combined_df[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michael0x2a</td>\n",
       "      <td>New? READ ME FIRST!</td>\n",
       "      <td># Welcome to /r/learnprogramming!\\n\\n## Quick ...</td>\n",
       "      <td>learnprogramming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>What have you been working on recently? [May 0...</td>\n",
       "      <td>What have you been working on recently? Feel f...</td>\n",
       "      <td>learnprogramming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dealoite</td>\n",
       "      <td>Does anyone else REGRET becoming a web developer?</td>\n",
       "      <td>I'm a web developer and I hate it.\\n\\nI enjoye...</td>\n",
       "      <td>learnprogramming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenSourcere42069</td>\n",
       "      <td>Programming portfolio?</td>\n",
       "      <td>Hello, I'm looking for some advice on making a...</td>\n",
       "      <td>learnprogramming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scorlibpl</td>\n",
       "      <td>Need advice about data science</td>\n",
       "      <td>Hey guys, so currently im a IT diploma student...</td>\n",
       "      <td>learnprogramming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                              title  \\\n",
       "0        michael0x2a                                New? READ ME FIRST!   \n",
       "1      AutoModerator  What have you been working on recently? [May 0...   \n",
       "2           Dealoite  Does anyone else REGRET becoming a web developer?   \n",
       "3  OpenSourcere42069                             Programming portfolio?   \n",
       "4          Scorlibpl                     Need advice about data science   \n",
       "\n",
       "                                            selftext         subreddit  \n",
       "0  # Welcome to /r/learnprogramming!\\n\\n## Quick ...  learnprogramming  \n",
       "1  What have you been working on recently? Feel f...  learnprogramming  \n",
       "2  I'm a web developer and I hate it.\\n\\nI enjoye...  learnprogramming  \n",
       "3  Hello, I'm looking for some advice on making a...  learnprogramming  \n",
       "4  Hey guys, so currently im a IT diploma student...  learnprogramming  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2491, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check combined dataframe\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates in title column\n",
    "combined_df.drop_duplicates(subset = ['title', 'selftext'], keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for number of line items\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author       0\n",
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null items\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NAN rows\n",
    "combined_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to encode the subreddit columns to identify which of the subreddits it belongs to. Learn Programming is mapped as 1 (positive) and Learn Machine Learning as 0 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['subreddit'].replace({'learnprogramming': 1, 'learnmachinelearning': 0}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function below to loop through each row in the dataset to perform the following for column specified:-\n",
    "   - remove html codes\n",
    "   - remove html links\n",
    "   - remove punctuations\n",
    "   - convert them to lower case and splitting into individual words\n",
    "   - remove stopwords\n",
    "   - join the text back to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contents(df, column_text):\n",
    "    # This removes html codes like <br>\n",
    "    removed_html = BeautifulSoup(column_text).get_text() \n",
    "    \n",
    "    # 2. This remove http followed by anything before a space, tab or newline links should not have any space or tab or newline between them\n",
    "    removed_links = re.sub(\"http.\\S+\", \" \", removed_html)\n",
    "                                                              \n",
    "    # 3. This removes non-letters\n",
    "    removed_punctuation = re.sub(\"[^a-zA-Z]\", \" \", removed_links)\n",
    "    \n",
    "                                                                  \n",
    "        \n",
    "    # 4. Convert to lower case, split into individual words.\n",
    "    lower_words = removed_punctuation.lower().split()\n",
    "    \n",
    "    # 4. This converts nltk's stopwords into a set\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Compare our remaining words with stop words and only keep words not in the stop words\n",
    "    meaningful_words = [words for words in lower_words if words not in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space and replace the joint text back to the dataframe\n",
    "    cleansed_text = \" \".join(meaningful_words)\n",
    "    df.replace(column_text, cleansed_text, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:398: UserWarning: \"https://heartbeat.fritz.ai/understanding-the-mathematics-behind-linear-regression-part-1-4390b7367787\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:398: UserWarning: \"https://heartbeat.fritz.ai/image-compression-using-different-machine-learning-techniques-5787c88515f8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  markup\n"
     ]
    }
   ],
   "source": [
    "for selftext in combined_df['selftext']:\n",
    "    clean_contents(combined_df, selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in combined_df['title']:\n",
    "    clean_contents(combined_df, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.586996\n",
       "0    0.413004\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Number of observations for each subreddit\n",
    "combined_df['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a copy of cleaned dataset\n",
    "combined_df.to_csv('data/cleandata.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
